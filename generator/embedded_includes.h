/****************************************************************************
 * Copyright (C) 2012-2015 Woboq GmbH
 * Olivier Goffart <contact at woboq.com>
 * https://woboq.com/codebrowser.html
 *
 * This file is part of the Woboq Code Browser.
 *
 * Commercial License Usage:
 * Licensees holding valid commercial licenses provided by Woboq may use
 * this file in accordance with the terms contained in a written agreement
 * between the licensee and Woboq.
 * For further information see https://woboq.com/codebrowser.html
 *
 * Alternatively, this work may be used under a Creative Commons
 * Attribution-NonCommercial-ShareAlike 3.0 (CC-BY-NC-SA 3.0) License.
 * http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US
 * This license does not allow you to use the code browser to assist the
 * development of your commercial software. If you intent to do so, consider
 * purchasing a commercial licence.
 ****************************************************************************/


#pragma once

#include <utility>
#include <vector>
#include <string>


struct EmbeddedFile {
    const char *filename;
    const char *content;
    size_t size;
    template <int N>
    constexpr EmbeddedFile(const char *filename, const char (&data)[N])
        : filename(filename) , content(data), size(N-1) {}
    constexpr EmbeddedFile () : filename(nullptr) , content(nullptr), size(0) {}
};

static constexpr EmbeddedFile EmbeddedFiles[] = {
     { "/builtins/__clang_cuda_builtin_vars.h" , "/*===---- cuda_builtin_vars.h - CUDA built-in variables ---------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __CUDA_BUILTIN_VARS_H\n"
"#define __CUDA_BUILTIN_VARS_H\n"
"\n"
"// Forward declares from vector_types.h.\n"
"struct uint3;\n"
"struct dim3;\n"
"\n"
"// The file implements built-in CUDA variables using __declspec(property).\n"
"// https://msdn.microsoft.com/en-us/library/yhfk0thd.aspx\n"
"// All read accesses of built-in variable fields get converted into calls to a\n"
"// getter function which in turn calls the appropriate builtin to fetch the\n"
"// value.\n"
"//\n"
"// Example:\n"
"//    int x = threadIdx.x;\n"
"// IR output:\n"
"//  %0 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x() #3\n"
"// PTX output:\n"
"//  mov.u32     %r2, %tid.x;\n"
"\n"
"#define __CUDA_DEVICE_BUILTIN(FIELD, INTRINSIC)                                \\\n"
"  __declspec(property(get = __fetch_builtin_##FIELD)) unsigned int FIELD;      \\\n"
"  static inline __attribute__((always_inline))                                 \\\n"
"      __attribute__((device)) unsigned int __fetch_builtin_##FIELD(void) {     \\\n"
"    return INTRINSIC;                                                          \\\n"
"  }\n"
"\n"
"#if __cplusplus >= 201103L\n"
"#define __DELETE =delete\n"
"#else\n"
"#define __DELETE\n"
"#endif\n"
"\n"
"// Make sure nobody can create instances of the special variable types.  nvcc\n"
"// also disallows taking address of special variables, so we disable address-of\n"
"// operator as well.\n"
"#define __CUDA_DISALLOW_BUILTINVAR_ACCESS(TypeName)                            \\\n"
"  __attribute__((device)) TypeName() __DELETE;                                 \\\n"
"  __attribute__((device)) TypeName(const TypeName &) __DELETE;                 \\\n"
"  __attribute__((device)) void operator=(const TypeName &) const __DELETE;     \\\n"
"  __attribute__((device)) TypeName *operator&() const __DELETE\n"
"\n"
"struct __cuda_builtin_threadIdx_t {\n"
"  __CUDA_DEVICE_BUILTIN(x,__nvvm_read_ptx_sreg_tid_x());\n"
"  __CUDA_DEVICE_BUILTIN(y,__nvvm_read_ptx_sreg_tid_y());\n"
"  __CUDA_DEVICE_BUILTIN(z,__nvvm_read_ptx_sreg_tid_z());\n"
"  // threadIdx should be convertible to uint3 (in fact in nvcc, it *is* a\n"
"  // uint3).  This function is defined after we pull in vector_types.h.\n"
"  __attribute__((device)) operator uint3() const;\n"
"private:\n"
"  __CUDA_DISALLOW_BUILTINVAR_ACCESS(__cuda_builtin_threadIdx_t);\n"
"};\n"
"\n"
"struct __cuda_builtin_blockIdx_t {\n"
"  __CUDA_DEVICE_BUILTIN(x,__nvvm_read_ptx_sreg_ctaid_x());\n"
"  __CUDA_DEVICE_BUILTIN(y,__nvvm_read_ptx_sreg_ctaid_y());\n"
"  __CUDA_DEVICE_BUILTIN(z,__nvvm_read_ptx_sreg_ctaid_z());\n"
"  // blockIdx should be convertible to uint3 (in fact in nvcc, it *is* a\n"
"  // uint3).  This function is defined after we pull in vector_types.h.\n"
"  __attribute__((device)) operator uint3() const;\n"
"private:\n"
"  __CUDA_DISALLOW_BUILTINVAR_ACCESS(__cuda_builtin_blockIdx_t);\n"
"};\n"
"\n"
"struct __cuda_builtin_blockDim_t {\n"
"  __CUDA_DEVICE_BUILTIN(x,__nvvm_read_ptx_sreg_ntid_x());\n"
"  __CUDA_DEVICE_BUILTIN(y,__nvvm_read_ptx_sreg_ntid_y());\n"
"  __CUDA_DEVICE_BUILTIN(z,__nvvm_read_ptx_sreg_ntid_z());\n"
"  // blockDim should be convertible to dim3 (in fact in nvcc, it *is* a\n"
"  // dim3).  This function is defined after we pull in vector_types.h.\n"
"  __attribute__((device)) operator dim3() const;\n"
"private:\n"
"  __CUDA_DISALLOW_BUILTINVAR_ACCESS(__cuda_builtin_blockDim_t);\n"
"};\n"
"\n"
"struct __cuda_builtin_gridDim_t {\n"
"  __CUDA_DEVICE_BUILTIN(x,__nvvm_read_ptx_sreg_nctaid_x());\n"
"  __CUDA_DEVICE_BUILTIN(y,__nvvm_read_ptx_sreg_nctaid_y());\n"
"  __CUDA_DEVICE_BUILTIN(z,__nvvm_read_ptx_sreg_nctaid_z());\n"
"  // gridDim should be convertible to dim3 (in fact in nvcc, it *is* a\n"
"  // dim3).  This function is defined after we pull in vector_types.h.\n"
"  __attribute__((device)) operator dim3() const;\n"
"private:\n"
"  __CUDA_DISALLOW_BUILTINVAR_ACCESS(__cuda_builtin_gridDim_t);\n"
"};\n"
"\n"
"#define __CUDA_BUILTIN_VAR                                                     \\\n"
"  extern const __attribute__((device)) __attribute__((weak))\n"
"__CUDA_BUILTIN_VAR __cuda_builtin_threadIdx_t threadIdx;\n"
"__CUDA_BUILTIN_VAR __cuda_builtin_blockIdx_t blockIdx;\n"
"__CUDA_BUILTIN_VAR __cuda_builtin_blockDim_t blockDim;\n"
"__CUDA_BUILTIN_VAR __cuda_builtin_gridDim_t gridDim;\n"
"\n"
"// warpSize should translate to read of %WARP_SZ but there's currently no\n"
"// builtin to do so. According to PTX v4.2 docs 'to date, all target\n"
"// architectures have a WARP_SZ value of 32'.\n"
"__attribute__((device)) const int warpSize = 32;\n"
"\n"
"#undef __CUDA_DEVICE_BUILTIN\n"
"#undef __CUDA_BUILTIN_VAR\n"
"#undef __CUDA_DISALLOW_BUILTINVAR_ACCESS\n"
"\n"
"#endif /* __CUDA_BUILTIN_VARS_H */\n"
"" } , 
 { "/builtins/__clang_cuda_cmath.h" , "/*===---- __clang_cuda_cmath.h - Device-side CUDA cmath support ------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#ifndef __CLANG_CUDA_CMATH_H__\n"
"#define __CLANG_CUDA_CMATH_H__\n"
"#ifndef __CUDA__\n"
"#error \"This file is for CUDA compilation only.\"\n"
"#endif\n"
"\n"
"#include <limits>\n"
"\n"
"// CUDA lets us use various std math functions on the device side.  This file\n"
"// works in concert with __clang_cuda_math_forward_declares.h to make this work.\n"
"//\n"
"// Specifically, the forward-declares header declares __device__ overloads for\n"
"// these functions in the global namespace, then pulls them into namespace std\n"
"// with 'using' statements.  Then this file implements those functions, after\n"
"// their implementations have been pulled in.\n"
"//\n"
"// It's important that we declare the functions in the global namespace and pull\n"
"// them into namespace std with using statements, as opposed to simply declaring\n"
"// these functions in namespace std, because our device functions need to\n"
"// overload the standard library functions, which may be declared in the global\n"
"// namespace or in std, depending on the degree of conformance of the stdlib\n"
"// implementation.  Declaring in the global namespace and pulling into namespace\n"
"// std covers all of the known knowns.\n"
"\n"
"#define __DEVICE__ static __device__ __inline__ __attribute__((always_inline))\n"
"\n"
"__DEVICE__ long long abs(long long __n) { return ::llabs(__n); }\n"
"__DEVICE__ long abs(long __n) { return ::labs(__n); }\n"
"__DEVICE__ float abs(float __x) { return ::fabsf(__x); }\n"
"__DEVICE__ double abs(double __x) { return ::fabs(__x); }\n"
"__DEVICE__ float acos(float __x) { return ::acosf(__x); }\n"
"__DEVICE__ float asin(float __x) { return ::asinf(__x); }\n"
"__DEVICE__ float atan(float __x) { return ::atanf(__x); }\n"
"__DEVICE__ float atan2(float __x, float __y) { return ::atan2f(__x, __y); }\n"
"__DEVICE__ float ceil(float __x) { return ::ceilf(__x); }\n"
"__DEVICE__ float cos(float __x) { return ::cosf(__x); }\n"
"__DEVICE__ float cosh(float __x) { return ::coshf(__x); }\n"
"__DEVICE__ float exp(float __x) { return ::expf(__x); }\n"
"__DEVICE__ float fabs(float __x) { return ::fabsf(__x); }\n"
"__DEVICE__ float floor(float __x) { return ::floorf(__x); }\n"
"__DEVICE__ float fmod(float __x, float __y) { return ::fmodf(__x, __y); }\n"
"__DEVICE__ int fpclassify(float __x) {\n"
"  return __builtin_fpclassify(FP_NAN, FP_INFINITE, FP_NORMAL, FP_SUBNORMAL,\n"
"                              FP_ZERO, __x);\n"
"}\n"
"__DEVICE__ int fpclassify(double __x) {\n"
"  return __builtin_fpclassify(FP_NAN, FP_INFINITE, FP_NORMAL, FP_SUBNORMAL,\n"
"                              FP_ZERO, __x);\n"
"}\n"
"__DEVICE__ float frexp(float __arg, int *__exp) {\n"
"  return ::frexpf(__arg, __exp);\n"
"}\n"
"\n"
"// For inscrutable reasons, the CUDA headers define these functions for us on\n"
"// Windows.\n"
"#ifndef _MSC_VER\n"
"__DEVICE__ bool isinf(float __x) { return ::__isinff(__x); }\n"
"__DEVICE__ bool isinf(double __x) { return ::__isinf(__x); }\n"
"__DEVICE__ bool isfinite(float __x) { return ::__finitef(__x); }\n"
"// For inscrutable reasons, __finite(), the double-precision version of\n"
"// __finitef, does not exist when compiling for MacOS.  __isfinited is available\n"
"// everywhere and is just as good.\n"
"__DEVICE__ bool isfinite(double __x) { return ::__isfinited(__x); }\n"
"__DEVICE__ bool isnan(float __x) { return ::__isnanf(__x); }\n"
"__DEVICE__ bool isnan(double __x) { return ::__isnan(__x); }\n"
"#endif\n"
"\n"
"__DEVICE__ bool isgreater(float __x, float __y) {\n"
"  return __builtin_isgreater(__x, __y);\n"
"}\n"
"__DEVICE__ bool isgreater(double __x, double __y) {\n"
"  return __builtin_isgreater(__x, __y);\n"
"}\n"
"__DEVICE__ bool isgreaterequal(float __x, float __y) {\n"
"  return __builtin_isgreaterequal(__x, __y);\n"
"}\n"
"__DEVICE__ bool isgreaterequal(double __x, double __y) {\n"
"  return __builtin_isgreaterequal(__x, __y);\n"
"}\n"
"__DEVICE__ bool isless(float __x, float __y) {\n"
"  return __builtin_isless(__x, __y);\n"
"}\n"
"__DEVICE__ bool isless(double __x, double __y) {\n"
"  return __builtin_isless(__x, __y);\n"
"}\n"
"__DEVICE__ bool islessequal(float __x, float __y) {\n"
"  return __builtin_islessequal(__x, __y);\n"
"}\n"
"__DEVICE__ bool islessequal(double __x, double __y) {\n"
"  return __builtin_islessequal(__x, __y);\n"
"}\n"
"__DEVICE__ bool islessgreater(float __x, float __y) {\n"
"  return __builtin_islessgreater(__x, __y);\n"
"}\n"
"__DEVICE__ bool islessgreater(double __x, double __y) {\n"
"  return __builtin_islessgreater(__x, __y);\n"
"}\n"
"__DEVICE__ bool isnormal(float __x) { return __builtin_isnormal(__x); }\n"
"__DEVICE__ bool isnormal(double __x) { return __builtin_isnormal(__x); }\n"
"__DEVICE__ bool isunordered(float __x, float __y) {\n"
"  return __builtin_isunordered(__x, __y);\n"
"}\n"
"__DEVICE__ bool isunordered(double __x, double __y) {\n"
"  return __builtin_isunordered(__x, __y);\n"
"}\n"
"__DEVICE__ float ldexp(float __arg, int __exp) {\n"
"  return ::ldexpf(__arg, __exp);\n"
"}\n"
"__DEVICE__ float log(float __x) { return ::logf(__x); }\n"
"__DEVICE__ float log10(float __x) { return ::log10f(__x); }\n"
"__DEVICE__ float modf(float __x, float *__iptr) { return ::modff(__x, __iptr); }\n"
"__DEVICE__ float pow(float __base, float __exp) {\n"
"  return ::powf(__base, __exp);\n"
"}\n"
"__DEVICE__ float pow(float __base, int __iexp) {\n"
"  return ::powif(__base, __iexp);\n"
"}\n"
"__DEVICE__ double pow(double __base, int __iexp) {\n"
"  return ::powi(__base, __iexp);\n"
"}\n"
"__DEVICE__ bool signbit(float __x) { return ::__signbitf(__x); }\n"
"__DEVICE__ bool signbit(double __x) { return ::__signbitd(__x); }\n"
"__DEVICE__ float sin(float __x) { return ::sinf(__x); }\n"
"__DEVICE__ float sinh(float __x) { return ::sinhf(__x); }\n"
"__DEVICE__ float sqrt(float __x) { return ::sqrtf(__x); }\n"
"__DEVICE__ float tan(float __x) { return ::tanf(__x); }\n"
"__DEVICE__ float tanh(float __x) { return ::tanhf(__x); }\n"
"\n"
"// Notably missing above is nexttoward.  We omit it because\n"
"// libdevice doesn't provide an implementation, and we don't want to be in the\n"
"// business of implementing tricky libm functions in this header.\n"
"\n"
"// Now we've defined everything we promised we'd define in\n"
"// __clang_cuda_math_forward_declares.h.  We need to do two additional things to\n"
"// fix up our math functions.\n"
"//\n"
"// 1) Define __device__ overloads for e.g. sin(int).  The CUDA headers define\n"
"//    only sin(float) and sin(double), which means that e.g. sin(0) is\n"
"//    ambiguous.\n"
"//\n"
"// 2) Pull the __device__ overloads of \"foobarf\" math functions into namespace\n"
"//    std.  These are defined in the CUDA headers in the global namespace,\n"
"//    independent of everything else we've done here.\n"
"\n"
"// We can't use std::enable_if, because we want to be pre-C++11 compatible.  But\n"
"// we go ahead and unconditionally define functions that are only available when\n"
"// compiling for C++11 to match the behavior of the CUDA headers.\n"
"template<bool __B, class __T = void>\n"
"struct __clang_cuda_enable_if {};\n"
"\n"
"template <class __T> struct __clang_cuda_enable_if<true, __T> {\n"
"  typedef __T type;\n"
"};\n"
"\n"
"// Defines an overload of __fn that accepts one integral argument, calls\n"
"// __fn((double)x), and returns __retty.\n"
"#define __CUDA_CLANG_FN_INTEGER_OVERLOAD_1(__retty, __fn)                      \\\n"
"  template <typename __T>                                                      \\\n"
"  __DEVICE__                                                                   \\\n"
"      typename __clang_cuda_enable_if<std::numeric_limits<__T>::is_integer,    \\\n"
"                                      __retty>::type                           \\\n"
"      __fn(__T __x) {                                                          \\\n"
"    return ::__fn((double)__x);                                                \\\n"
"  }\n"
"\n"
"// Defines an overload of __fn that accepts one two arithmetic arguments, calls\n"
"// __fn((double)x, (double)y), and returns a double.\n"
"//\n"
"// Note this is different from OVERLOAD_1, which generates an overload that\n"
"// accepts only *integral* arguments.\n"
"#define __CUDA_CLANG_FN_INTEGER_OVERLOAD_2(__retty, __fn)                      \\\n"
"  template <typename __T1, typename __T2>                                      \\\n"
"  __DEVICE__ typename __clang_cuda_enable_if<                                  \\\n"
"      std::numeric_limits<__T1>::is_specialized &&                             \\\n"
"          std::numeric_limits<__T2>::is_specialized,                           \\\n"
"      __retty>::type                                                           \\\n"
"  __fn(__T1 __x, __T2 __y) {                                                   \\\n"
"    return __fn((double)__x, (double)__y);                                     \\\n"
"  }\n"
"\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, acos)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, acosh)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, asin)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, asinh)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, atan)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, atan2);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, atanh)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, cbrt)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, ceil)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, copysign);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, cos)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, cosh)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, erf)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, erfc)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, exp)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, exp2)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, expm1)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, fabs)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, fdim);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, floor)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, fmax);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, fmin);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, fmod);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(int, fpclassify)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, hypot);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(int, ilogb)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(bool, isfinite)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(bool, isgreater);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(bool, isgreaterequal);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(bool, isinf);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(bool, isless);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(bool, islessequal);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(bool, islessgreater);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(bool, isnan);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(bool, isnormal)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(bool, isunordered);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, lgamma)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, log)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, log10)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, log1p)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, log2)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, logb)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(long long, llrint)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(long long, llround)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(long, lrint)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(long, lround)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, nearbyint);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, nextafter);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, pow);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_2(double, remainder);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, rint);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, round);\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(bool, signbit)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, sin)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, sinh)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, sqrt)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, tan)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, tanh)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, tgamma)\n"
"__CUDA_CLANG_FN_INTEGER_OVERLOAD_1(double, trunc);\n"
"\n"
"#undef __CUDA_CLANG_FN_INTEGER_OVERLOAD_1\n"
"#undef __CUDA_CLANG_FN_INTEGER_OVERLOAD_2\n"
"\n"
"// Overloads for functions that don't match the patterns expected by\n"
"// __CUDA_CLANG_FN_INTEGER_OVERLOAD_{1,2}.\n"
"template <typename __T1, typename __T2, typename __T3>\n"
"__DEVICE__ typename __clang_cuda_enable_if<\n"
"    std::numeric_limits<__T1>::is_specialized &&\n"
"        std::numeric_limits<__T2>::is_specialized &&\n"
"        std::numeric_limits<__T3>::is_specialized,\n"
"    double>::type\n"
"fma(__T1 __x, __T2 __y, __T3 __z) {\n"
"  return std::fma((double)__x, (double)__y, (double)__z);\n"
"}\n"
"\n"
"template <typename __T>\n"
"__DEVICE__ typename __clang_cuda_enable_if<std::numeric_limits<__T>::is_integer,\n"
"                                           double>::type\n"
"frexp(__T __x, int *__exp) {\n"
"  return std::frexp((double)__x, __exp);\n"
"}\n"
"\n"
"template <typename __T>\n"
"__DEVICE__ typename __clang_cuda_enable_if<std::numeric_limits<__T>::is_integer,\n"
"                                           double>::type\n"
"ldexp(__T __x, int __exp) {\n"
"  return std::ldexp((double)__x, __exp);\n"
"}\n"
"\n"
"template <typename __T1, typename __T2>\n"
"__DEVICE__ typename __clang_cuda_enable_if<\n"
"    std::numeric_limits<__T1>::is_specialized &&\n"
"        std::numeric_limits<__T2>::is_specialized,\n"
"    double>::type\n"
"remquo(__T1 __x, __T2 __y, int *__quo) {\n"
"  return std::remquo((double)__x, (double)__y, __quo);\n"
"}\n"
"\n"
"template <typename __T>\n"
"__DEVICE__ typename __clang_cuda_enable_if<std::numeric_limits<__T>::is_integer,\n"
"                                           double>::type\n"
"scalbln(__T __x, long __exp) {\n"
"  return std::scalbln((double)__x, __exp);\n"
"}\n"
"\n"
"template <typename __T>\n"
"__DEVICE__ typename __clang_cuda_enable_if<std::numeric_limits<__T>::is_integer,\n"
"                                           double>::type\n"
"scalbn(__T __x, int __exp) {\n"
"  return std::scalbn((double)__x, __exp);\n"
"}\n"
"\n"
"// We need to define these overloads in exactly the namespace our standard\n"
"// library uses (including the right inline namespace), otherwise they won't be\n"
"// picked up by other functions in the standard library (e.g. functions in\n"
"// <complex>).  Thus the ugliness below.\n"
"#ifdef _LIBCPP_BEGIN_NAMESPACE_STD\n"
"_LIBCPP_BEGIN_NAMESPACE_STD\n"
"#else\n"
"namespace std {\n"
"#ifdef _GLIBCXX_BEGIN_NAMESPACE_VERSION\n"
"_GLIBCXX_BEGIN_NAMESPACE_VERSION\n"
"#endif\n"
"#endif\n"
"\n"
"// Pull the new overloads we defined above into namespace std.\n"
"using ::acos;\n"
"using ::acosh;\n"
"using ::asin;\n"
"using ::asinh;\n"
"using ::atan;\n"
"using ::atan2;\n"
"using ::atanh;\n"
"using ::cbrt;\n"
"using ::ceil;\n"
"using ::copysign;\n"
"using ::cos;\n"
"using ::cosh;\n"
"using ::erf;\n"
"using ::erfc;\n"
"using ::exp;\n"
"using ::exp2;\n"
"using ::expm1;\n"
"using ::fabs;\n"
"using ::fdim;\n"
"using ::floor;\n"
"using ::fma;\n"
"using ::fmax;\n"
"using ::fmin;\n"
"using ::fmod;\n"
"using ::fpclassify;\n"
"using ::frexp;\n"
"using ::hypot;\n"
"using ::ilogb;\n"
"using ::isfinite;\n"
"using ::isgreater;\n"
"using ::isgreaterequal;\n"
"using ::isless;\n"
"using ::islessequal;\n"
"using ::islessgreater;\n"
"using ::isnormal;\n"
"using ::isunordered;\n"
"using ::ldexp;\n"
"using ::lgamma;\n"
"using ::llrint;\n"
"using ::llround;\n"
"using ::log;\n"
"using ::log10;\n"
"using ::log1p;\n"
"using ::log2;\n"
"using ::logb;\n"
"using ::lrint;\n"
"using ::lround;\n"
"using ::nearbyint;\n"
"using ::nextafter;\n"
"using ::pow;\n"
"using ::remainder;\n"
"using ::remquo;\n"
"using ::rint;\n"
"using ::round;\n"
"using ::scalbln;\n"
"using ::scalbn;\n"
"using ::signbit;\n"
"using ::sin;\n"
"using ::sinh;\n"
"using ::sqrt;\n"
"using ::tan;\n"
"using ::tanh;\n"
"using ::tgamma;\n"
"using ::trunc;\n"
"\n"
"// Well this is fun: We need to pull these symbols in for libc++, but we can't\n"
"// pull them in with libstdc++, because its ::isinf and ::isnan are different\n"
"// than its std::isinf and std::isnan.\n"
"#ifndef __GLIBCXX__\n"
"using ::isinf;\n"
"using ::isnan;\n"
"#endif\n"
"\n"
"// Finally, pull the \"foobarf\" functions that CUDA defines in its headers into\n"
"// namespace std.\n"
"using ::acosf;\n"
"using ::acoshf;\n"
"using ::asinf;\n"
"using ::asinhf;\n"
"using ::atan2f;\n"
"using ::atanf;\n"
"using ::atanhf;\n"
"using ::cbrtf;\n"
"using ::ceilf;\n"
"using ::copysignf;\n"
"using ::cosf;\n"
"using ::coshf;\n"
"using ::erfcf;\n"
"using ::erff;\n"
"using ::exp2f;\n"
"using ::expf;\n"
"using ::expm1f;\n"
"using ::fabsf;\n"
"using ::fdimf;\n"
"using ::floorf;\n"
"using ::fmaf;\n"
"using ::fmaxf;\n"
"using ::fminf;\n"
"using ::fmodf;\n"
"using ::frexpf;\n"
"using ::hypotf;\n"
"using ::ilogbf;\n"
"using ::ldexpf;\n"
"using ::lgammaf;\n"
"using ::llrintf;\n"
"using ::llroundf;\n"
"using ::log10f;\n"
"using ::log1pf;\n"
"using ::log2f;\n"
"using ::logbf;\n"
"using ::logf;\n"
"using ::lrintf;\n"
"using ::lroundf;\n"
"using ::modff;\n"
"using ::nearbyintf;\n"
"using ::nextafterf;\n"
"using ::powf;\n"
"using ::remainderf;\n"
"using ::remquof;\n"
"using ::rintf;\n"
"using ::roundf;\n"
"using ::scalblnf;\n"
"using ::scalbnf;\n"
"using ::sinf;\n"
"using ::sinhf;\n"
"using ::sqrtf;\n"
"using ::tanf;\n"
"using ::tanhf;\n"
"using ::tgammaf;\n"
"using ::truncf;\n"
"\n"
"#ifdef _LIBCPP_END_NAMESPACE_STD\n"
"_LIBCPP_END_NAMESPACE_STD\n"
"#else\n"
"#ifdef _GLIBCXX_BEGIN_NAMESPACE_VERSION\n"
"_GLIBCXX_END_NAMESPACE_VERSION\n"
"#endif\n"
"} // namespace std\n"
"#endif\n"
"\n"
"#undef __DEVICE__\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/__clang_cuda_complex_builtins.h" , "/*===-- __clang_cuda_complex_builtins - CUDA impls of runtime complex fns ---===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __CLANG_CUDA_COMPLEX_BUILTINS\n"
"#define __CLANG_CUDA_COMPLEX_BUILTINS\n"
"\n"
"// This header defines __muldc3, __mulsc3, __divdc3, and __divsc3.  These are\n"
"// libgcc functions that clang assumes are available when compiling c99 complex\n"
"// operations.  (These implementations come from libc++, and have been modified\n"
"// to work with CUDA.)\n"
"\n"
"extern \"C\" inline __device__ double _Complex __muldc3(double __a, double __b,\n"
"                                                      double __c, double __d) {\n"
"  double __ac = __a * __c;\n"
"  double __bd = __b * __d;\n"
"  double __ad = __a * __d;\n"
"  double __bc = __b * __c;\n"
"  double _Complex z;\n"
"  __real__(z) = __ac - __bd;\n"
"  __imag__(z) = __ad + __bc;\n"
"  if (std::isnan(__real__(z)) && std::isnan(__imag__(z))) {\n"
"    int __recalc = 0;\n"
"    if (std::isinf(__a) || std::isinf(__b)) {\n"
"      __a = std::copysign(std::isinf(__a) ? 1 : 0, __a);\n"
"      __b = std::copysign(std::isinf(__b) ? 1 : 0, __b);\n"
"      if (std::isnan(__c))\n"
"        __c = std::copysign(0, __c);\n"
"      if (std::isnan(__d))\n"
"        __d = std::copysign(0, __d);\n"
"      __recalc = 1;\n"
"    }\n"
"    if (std::isinf(__c) || std::isinf(__d)) {\n"
"      __c = std::copysign(std::isinf(__c) ? 1 : 0, __c);\n"
"      __d = std::copysign(std::isinf(__d) ? 1 : 0, __d);\n"
"      if (std::isnan(__a))\n"
"        __a = std::copysign(0, __a);\n"
"      if (std::isnan(__b))\n"
"        __b = std::copysign(0, __b);\n"
"      __recalc = 1;\n"
"    }\n"
"    if (!__recalc && (std::isinf(__ac) || std::isinf(__bd) ||\n"
"                      std::isinf(__ad) || std::isinf(__bc))) {\n"
"      if (std::isnan(__a))\n"
"        __a = std::copysign(0, __a);\n"
"      if (std::isnan(__b))\n"
"        __b = std::copysign(0, __b);\n"
"      if (std::isnan(__c))\n"
"        __c = std::copysign(0, __c);\n"
"      if (std::isnan(__d))\n"
"        __d = std::copysign(0, __d);\n"
"      __recalc = 1;\n"
"    }\n"
"    if (__recalc) {\n"
"      // Can't use std::numeric_limits<double>::infinity() -- that doesn't have\n"
"      // a device overload (and isn't constexpr before C++11, naturally).\n"
"      __real__(z) = __builtin_huge_valf() * (__a * __c - __b * __d);\n"
"      __imag__(z) = __builtin_huge_valf() * (__a * __d + __b * __c);\n"
"    }\n"
"  }\n"
"  return z;\n"
"}\n"
"\n"
"extern \"C\" inline __device__ float _Complex __mulsc3(float __a, float __b,\n"
"                                                     float __c, float __d) {\n"
"  float __ac = __a * __c;\n"
"  float __bd = __b * __d;\n"
"  float __ad = __a * __d;\n"
"  float __bc = __b * __c;\n"
"  float _Complex z;\n"
"  __real__(z) = __ac - __bd;\n"
"  __imag__(z) = __ad + __bc;\n"
"  if (std::isnan(__real__(z)) && std::isnan(__imag__(z))) {\n"
"    int __recalc = 0;\n"
"    if (std::isinf(__a) || std::isinf(__b)) {\n"
"      __a = std::copysign(std::isinf(__a) ? 1 : 0, __a);\n"
"      __b = std::copysign(std::isinf(__b) ? 1 : 0, __b);\n"
"      if (std::isnan(__c))\n"
"        __c = std::copysign(0, __c);\n"
"      if (std::isnan(__d))\n"
"        __d = std::copysign(0, __d);\n"
"      __recalc = 1;\n"
"    }\n"
"    if (std::isinf(__c) || std::isinf(__d)) {\n"
"      __c = std::copysign(std::isinf(__c) ? 1 : 0, __c);\n"
"      __d = std::copysign(std::isinf(__d) ? 1 : 0, __d);\n"
"      if (std::isnan(__a))\n"
"        __a = std::copysign(0, __a);\n"
"      if (std::isnan(__b))\n"
"        __b = std::copysign(0, __b);\n"
"      __recalc = 1;\n"
"    }\n"
"    if (!__recalc && (std::isinf(__ac) || std::isinf(__bd) ||\n"
"                      std::isinf(__ad) || std::isinf(__bc))) {\n"
"      if (std::isnan(__a))\n"
"        __a = std::copysign(0, __a);\n"
"      if (std::isnan(__b))\n"
"        __b = std::copysign(0, __b);\n"
"      if (std::isnan(__c))\n"
"        __c = std::copysign(0, __c);\n"
"      if (std::isnan(__d))\n"
"        __d = std::copysign(0, __d);\n"
"      __recalc = 1;\n"
"    }\n"
"    if (__recalc) {\n"
"      __real__(z) = __builtin_huge_valf() * (__a * __c - __b * __d);\n"
"      __imag__(z) = __builtin_huge_valf() * (__a * __d + __b * __c);\n"
"    }\n"
"  }\n"
"  return z;\n"
"}\n"
"\n"
"extern \"C\" inline __device__ double _Complex __divdc3(double __a, double __b,\n"
"                                                      double __c, double __d) {\n"
"  int __ilogbw = 0;\n"
"  // Can't use std::max, because that's defined in <algorithm>, and we don't\n"
"  // want to pull that in for every compile.  The CUDA headers define\n"
"  // ::max(float, float) and ::max(double, double), which is sufficient for us.\n"
"  double __logbw = std::logb(max(std::abs(__c), std::abs(__d)));\n"
"  if (std::isfinite(__logbw)) {\n"
"    __ilogbw = (int)__logbw;\n"
"    __c = std::scalbn(__c, -__ilogbw);\n"
"    __d = std::scalbn(__d, -__ilogbw);\n"
"  }\n"
"  double __denom = __c * __c + __d * __d;\n"
"  double _Complex z;\n"
"  __real__(z) = std::scalbn((__a * __c + __b * __d) / __denom, -__ilogbw);\n"
"  __imag__(z) = std::scalbn((__b * __c - __a * __d) / __denom, -__ilogbw);\n"
"  if (std::isnan(__real__(z)) && std::isnan(__imag__(z))) {\n"
"    if ((__denom == 0.0) && (!std::isnan(__a) || !std::isnan(__b))) {\n"
"      __real__(z) = std::copysign(__builtin_huge_valf(), __c) * __a;\n"
"      __imag__(z) = std::copysign(__builtin_huge_valf(), __c) * __b;\n"
"    } else if ((std::isinf(__a) || std::isinf(__b)) && std::isfinite(__c) &&\n"
"               std::isfinite(__d)) {\n"
"      __a = std::copysign(std::isinf(__a) ? 1.0 : 0.0, __a);\n"
"      __b = std::copysign(std::isinf(__b) ? 1.0 : 0.0, __b);\n"
"      __real__(z) = __builtin_huge_valf() * (__a * __c + __b * __d);\n"
"      __imag__(z) = __builtin_huge_valf() * (__b * __c - __a * __d);\n"
"    } else if (std::isinf(__logbw) && __logbw > 0.0 && std::isfinite(__a) &&\n"
"               std::isfinite(__b)) {\n"
"      __c = std::copysign(std::isinf(__c) ? 1.0 : 0.0, __c);\n"
"      __d = std::copysign(std::isinf(__d) ? 1.0 : 0.0, __d);\n"
"      __real__(z) = 0.0 * (__a * __c + __b * __d);\n"
"      __imag__(z) = 0.0 * (__b * __c - __a * __d);\n"
"    }\n"
"  }\n"
"  return z;\n"
"}\n"
"\n"
"extern \"C\" inline __device__ float _Complex __divsc3(float __a, float __b,\n"
"                                                     float __c, float __d) {\n"
"  int __ilogbw = 0;\n"
"  float __logbw = std::logb(max(std::abs(__c), std::abs(__d)));\n"
"  if (std::isfinite(__logbw)) {\n"
"    __ilogbw = (int)__logbw;\n"
"    __c = std::scalbn(__c, -__ilogbw);\n"
"    __d = std::scalbn(__d, -__ilogbw);\n"
"  }\n"
"  float __denom = __c * __c + __d * __d;\n"
"  float _Complex z;\n"
"  __real__(z) = std::scalbn((__a * __c + __b * __d) / __denom, -__ilogbw);\n"
"  __imag__(z) = std::scalbn((__b * __c - __a * __d) / __denom, -__ilogbw);\n"
"  if (std::isnan(__real__(z)) && std::isnan(__imag__(z))) {\n"
"    if ((__denom == 0) && (!std::isnan(__a) || !std::isnan(__b))) {\n"
"      __real__(z) = std::copysign(__builtin_huge_valf(), __c) * __a;\n"
"      __imag__(z) = std::copysign(__builtin_huge_valf(), __c) * __b;\n"
"    } else if ((std::isinf(__a) || std::isinf(__b)) && std::isfinite(__c) &&\n"
"               std::isfinite(__d)) {\n"
"      __a = std::copysign(std::isinf(__a) ? 1 : 0, __a);\n"
"      __b = std::copysign(std::isinf(__b) ? 1 : 0, __b);\n"
"      __real__(z) = __builtin_huge_valf() * (__a * __c + __b * __d);\n"
"      __imag__(z) = __builtin_huge_valf() * (__b * __c - __a * __d);\n"
"    } else if (std::isinf(__logbw) && __logbw > 0 && std::isfinite(__a) &&\n"
"               std::isfinite(__b)) {\n"
"      __c = std::copysign(std::isinf(__c) ? 1 : 0, __c);\n"
"      __d = std::copysign(std::isinf(__d) ? 1 : 0, __d);\n"
"      __real__(z) = 0 * (__a * __c + __b * __d);\n"
"      __imag__(z) = 0 * (__b * __c - __a * __d);\n"
"    }\n"
"  }\n"
"  return z;\n"
"}\n"
"\n"
"#endif // __CLANG_CUDA_COMPLEX_BUILTINS\n"
"" } , 
 { "/builtins/__clang_cuda_device_functions.h" , "/*===---- __clang_cuda_device_functions.h - CUDA runtime support -----------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __CLANG_CUDA_DEVICE_FUNCTIONS_H__\n"
"#define __CLANG_CUDA_DEVICE_FUNCTIONS_H__\n"
"\n"
"#if CUDA_VERSION < 9000\n"
"#error This file is intended to be used with CUDA-9+ only.\n"
"#endif\n"
"\n"
"// __DEVICE__ is a helper macro with common set of attributes for the wrappers\n"
"// we implement in this file. We need static in order to avoid emitting unused\n"
"// functions and __forceinline__ helps inlining these wrappers at -O1.\n"
"#pragma push_macro(\"__DEVICE__\")\n"
"#define __DEVICE__ static __device__ __forceinline__\n"
"\n"
"// libdevice provides fast low precision and slow full-recision implementations\n"
"// for some functions. Which one gets selected depends on\n"
"// __CLANG_CUDA_APPROX_TRANSCENDENTALS__ which gets defined by clang if\n"
"// -ffast-math or -fcuda-approx-transcendentals are in effect.\n"
"#pragma push_macro(\"__FAST_OR_SLOW\")\n"
"#if defined(__CLANG_CUDA_APPROX_TRANSCENDENTALS__)\n"
"#define __FAST_OR_SLOW(fast, slow) fast\n"
"#else\n"
"#define __FAST_OR_SLOW(fast, slow) slow\n"
"#endif\n"
"\n"
"__DEVICE__ int __all(int __a) { return __nvvm_vote_all(__a); }\n"
"__DEVICE__ int __any(int __a) { return __nvvm_vote_any(__a); }\n"
"__DEVICE__ unsigned int __ballot(int __a) { return __nvvm_vote_ballot(__a); }\n"
"__DEVICE__ unsigned int __brev(unsigned int __a) { return __nv_brev(__a); }\n"
"__DEVICE__ unsigned long long __brevll(unsigned long long __a) {\n"
"  return __nv_brevll(__a);\n"
"}\n"
"__DEVICE__ void __brkpt() { asm volatile(\"brkpt;\"); }\n"
"__DEVICE__ void __brkpt(int __a) { __brkpt(); }\n"
"__DEVICE__ unsigned int __byte_perm(unsigned int __a, unsigned int __b,\n"
"                                    unsigned int __c) {\n"
"  return __nv_byte_perm(__a, __b, __c);\n"
"}\n"
"__DEVICE__ int __clz(int __a) { return __nv_clz(__a); }\n"
"__DEVICE__ int __clzll(long long __a) { return __nv_clzll(__a); }\n"
"__DEVICE__ float __cosf(float __a) { return __nv_fast_cosf(__a); }\n"
"__DEVICE__ double __dAtomicAdd(double *__p, double __v) {\n"
"  return __nvvm_atom_add_gen_d(__p, __v);\n"
"}\n"
"__DEVICE__ double __dAtomicAdd_block(double *__p, double __v) {\n"
"  return __nvvm_atom_cta_add_gen_d(__p, __v);\n"
"}\n"
"__DEVICE__ double __dAtomicAdd_system(double *__p, double __v) {\n"
"  return __nvvm_atom_sys_add_gen_d(__p, __v);\n"
"}\n"
"__DEVICE__ double __dadd_rd(double __a, double __b) {\n"
"  return __nv_dadd_rd(__a, __b);\n"
"}\n"
"__DEVICE__ double __dadd_rn(double __a, double __b) {\n"
"  return __nv_dadd_rn(__a, __b);\n"
"}\n"
"__DEVICE__ double __dadd_ru(double __a, double __b) {\n"
"  return __nv_dadd_ru(__a, __b);\n"
"}\n"
"__DEVICE__ double __dadd_rz(double __a, double __b) {\n"
"  return __nv_dadd_rz(__a, __b);\n"
"}\n"
"__DEVICE__ double __ddiv_rd(double __a, double __b) {\n"
"  return __nv_ddiv_rd(__a, __b);\n"
"}\n"
"__DEVICE__ double __ddiv_rn(double __a, double __b) {\n"
"  return __nv_ddiv_rn(__a, __b);\n"
"}\n"
"__DEVICE__ double __ddiv_ru(double __a, double __b) {\n"
"  return __nv_ddiv_ru(__a, __b);\n"
"}\n"
"__DEVICE__ double __ddiv_rz(double __a, double __b) {\n"
"  return __nv_ddiv_rz(__a, __b);\n"
"}\n"
"__DEVICE__ double __dmul_rd(double __a, double __b) {\n"
"  return __nv_dmul_rd(__a, __b);\n"
"}\n"
"__DEVICE__ double __dmul_rn(double __a, double __b) {\n"
"  return __nv_dmul_rn(__a, __b);\n"
"}\n"
"__DEVICE__ double __dmul_ru(double __a, double __b) {\n"
"  return __nv_dmul_ru(__a, __b);\n"
"}\n"
"__DEVICE__ double __dmul_rz(double __a, double __b) {\n"
"  return __nv_dmul_rz(__a, __b);\n"
"}\n"
"__DEVICE__ float __double2float_rd(double __a) {\n"
"  return __nv_double2float_rd(__a);\n"
"}\n"
"__DEVICE__ float __double2float_rn(double __a) {\n"
"  return __nv_double2float_rn(__a);\n"
"}\n"
"__DEVICE__ float __double2float_ru(double __a) {\n"
"  return __nv_double2float_ru(__a);\n"
"}\n"
"__DEVICE__ float __double2float_rz(double __a) {\n"
"  return __nv_double2float_rz(__a);\n"
"}\n"
"__DEVICE__ int __double2hiint(double __a) { return __nv_double2hiint(__a); }\n"
"__DEVICE__ int __double2int_rd(double __a) { return __nv_double2int_rd(__a); }\n"
"__DEVICE__ int __double2int_rn(double __a) { return __nv_double2int_rn(__a); }\n"
"__DEVICE__ int __double2int_ru(double __a) { return __nv_double2int_ru(__a); }\n"
"__DEVICE__ int __double2int_rz(double __a) { return __nv_double2int_rz(__a); }\n"
"__DEVICE__ long long __double2ll_rd(double __a) {\n"
"  return __nv_double2ll_rd(__a);\n"
"}\n"
"__DEVICE__ long long __double2ll_rn(double __a) {\n"
"  return __nv_double2ll_rn(__a);\n"
"}\n"
"__DEVICE__ long long __double2ll_ru(double __a) {\n"
"  return __nv_double2ll_ru(__a);\n"
"}\n"
"__DEVICE__ long long __double2ll_rz(double __a) {\n"
"  return __nv_double2ll_rz(__a);\n"
"}\n"
"__DEVICE__ int __double2loint(double __a) { return __nv_double2loint(__a); }\n"
"__DEVICE__ unsigned int __double2uint_rd(double __a) {\n"
"  return __nv_double2uint_rd(__a);\n"
"}\n"
"__DEVICE__ unsigned int __double2uint_rn(double __a) {\n"
"  return __nv_double2uint_rn(__a);\n"
"}\n"
"__DEVICE__ unsigned int __double2uint_ru(double __a) {\n"
"  return __nv_double2uint_ru(__a);\n"
"}\n"
"__DEVICE__ unsigned int __double2uint_rz(double __a) {\n"
"  return __nv_double2uint_rz(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __double2ull_rd(double __a) {\n"
"  return __nv_double2ull_rd(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __double2ull_rn(double __a) {\n"
"  return __nv_double2ull_rn(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __double2ull_ru(double __a) {\n"
"  return __nv_double2ull_ru(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __double2ull_rz(double __a) {\n"
"  return __nv_double2ull_rz(__a);\n"
"}\n"
"__DEVICE__ long long __double_as_longlong(double __a) {\n"
"  return __nv_double_as_longlong(__a);\n"
"}\n"
"__DEVICE__ double __drcp_rd(double __a) { return __nv_drcp_rd(__a); }\n"
"__DEVICE__ double __drcp_rn(double __a) { return __nv_drcp_rn(__a); }\n"
"__DEVICE__ double __drcp_ru(double __a) { return __nv_drcp_ru(__a); }\n"
"__DEVICE__ double __drcp_rz(double __a) { return __nv_drcp_rz(__a); }\n"
"__DEVICE__ double __dsqrt_rd(double __a) { return __nv_dsqrt_rd(__a); }\n"
"__DEVICE__ double __dsqrt_rn(double __a) { return __nv_dsqrt_rn(__a); }\n"
"__DEVICE__ double __dsqrt_ru(double __a) { return __nv_dsqrt_ru(__a); }\n"
"__DEVICE__ double __dsqrt_rz(double __a) { return __nv_dsqrt_rz(__a); }\n"
"__DEVICE__ double __dsub_rd(double __a, double __b) {\n"
"  return __nv_dsub_rd(__a, __b);\n"
"}\n"
"__DEVICE__ double __dsub_rn(double __a, double __b) {\n"
"  return __nv_dsub_rn(__a, __b);\n"
"}\n"
"__DEVICE__ double __dsub_ru(double __a, double __b) {\n"
"  return __nv_dsub_ru(__a, __b);\n"
"}\n"
"__DEVICE__ double __dsub_rz(double __a, double __b) {\n"
"  return __nv_dsub_rz(__a, __b);\n"
"}\n"
"__DEVICE__ float __exp10f(float __a) { return __nv_fast_exp10f(__a); }\n"
"__DEVICE__ float __expf(float __a) { return __nv_fast_expf(__a); }\n"
"__DEVICE__ float __fAtomicAdd(float *__p, float __v) {\n"
"  return __nvvm_atom_add_gen_f(__p, __v);\n"
"}\n"
"__DEVICE__ float __fAtomicAdd_block(float *__p, float __v) {\n"
"  return __nvvm_atom_cta_add_gen_f(__p, __v);\n"
"}\n"
"__DEVICE__ float __fAtomicAdd_system(float *__p, float __v) {\n"
"  return __nvvm_atom_sys_add_gen_f(__p, __v);\n"
"}\n"
"__DEVICE__ float __fAtomicExch(float *__p, float __v) {\n"
"  return __nv_int_as_float(\n"
"      __nvvm_atom_xchg_gen_i((int *)__p, __nv_float_as_int(__v)));\n"
"}\n"
"__DEVICE__ float __fAtomicExch_block(float *__p, float __v) {\n"
"  return __nv_int_as_float(\n"
"      __nvvm_atom_cta_xchg_gen_i((int *)__p, __nv_float_as_int(__v)));\n"
"}\n"
"__DEVICE__ float __fAtomicExch_system(float *__p, float __v) {\n"
"  return __nv_int_as_float(\n"
"      __nvvm_atom_sys_xchg_gen_i((int *)__p, __nv_float_as_int(__v)));\n"
"}\n"
"__DEVICE__ float __fadd_rd(float __a, float __b) {\n"
"  return __nv_fadd_rd(__a, __b);\n"
"}\n"
"__DEVICE__ float __fadd_rn(float __a, float __b) {\n"
"  return __nv_fadd_rn(__a, __b);\n"
"}\n"
"__DEVICE__ float __fadd_ru(float __a, float __b) {\n"
"  return __nv_fadd_ru(__a, __b);\n"
"}\n"
"__DEVICE__ float __fadd_rz(float __a, float __b) {\n"
"  return __nv_fadd_rz(__a, __b);\n"
"}\n"
"__DEVICE__ float __fdiv_rd(float __a, float __b) {\n"
"  return __nv_fdiv_rd(__a, __b);\n"
"}\n"
"__DEVICE__ float __fdiv_rn(float __a, float __b) {\n"
"  return __nv_fdiv_rn(__a, __b);\n"
"}\n"
"__DEVICE__ float __fdiv_ru(float __a, float __b) {\n"
"  return __nv_fdiv_ru(__a, __b);\n"
"}\n"
"__DEVICE__ float __fdiv_rz(float __a, float __b) {\n"
"  return __nv_fdiv_rz(__a, __b);\n"
"}\n"
"__DEVICE__ float __fdividef(float __a, float __b) {\n"
"  return __nv_fast_fdividef(__a, __b);\n"
"}\n"
"__DEVICE__ int __ffs(int __a) { return __nv_ffs(__a); }\n"
"__DEVICE__ int __ffsll(long long __a) { return __nv_ffsll(__a); }\n"
"__DEVICE__ int __finite(double __a) { return __nv_isfinited(__a); }\n"
"__DEVICE__ int __finitef(float __a) { return __nv_finitef(__a); }\n"
"__DEVICE__ int __float2int_rd(float __a) { return __nv_float2int_rd(__a); }\n"
"__DEVICE__ int __float2int_rn(float __a) { return __nv_float2int_rn(__a); }\n"
"__DEVICE__ int __float2int_ru(float __a) { return __nv_float2int_ru(__a); }\n"
"__DEVICE__ int __float2int_rz(float __a) { return __nv_float2int_rz(__a); }\n"
"__DEVICE__ long long __float2ll_rd(float __a) { return __nv_float2ll_rd(__a); }\n"
"__DEVICE__ long long __float2ll_rn(float __a) { return __nv_float2ll_rn(__a); }\n"
"__DEVICE__ long long __float2ll_ru(float __a) { return __nv_float2ll_ru(__a); }\n"
"__DEVICE__ long long __float2ll_rz(float __a) { return __nv_float2ll_rz(__a); }\n"
"__DEVICE__ unsigned int __float2uint_rd(float __a) {\n"
"  return __nv_float2uint_rd(__a);\n"
"}\n"
"__DEVICE__ unsigned int __float2uint_rn(float __a) {\n"
"  return __nv_float2uint_rn(__a);\n"
"}\n"
"__DEVICE__ unsigned int __float2uint_ru(float __a) {\n"
"  return __nv_float2uint_ru(__a);\n"
"}\n"
"__DEVICE__ unsigned int __float2uint_rz(float __a) {\n"
"  return __nv_float2uint_rz(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __float2ull_rd(float __a) {\n"
"  return __nv_float2ull_rd(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __float2ull_rn(float __a) {\n"
"  return __nv_float2ull_rn(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __float2ull_ru(float __a) {\n"
"  return __nv_float2ull_ru(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __float2ull_rz(float __a) {\n"
"  return __nv_float2ull_rz(__a);\n"
"}\n"
"__DEVICE__ int __float_as_int(float __a) { return __nv_float_as_int(__a); }\n"
"__DEVICE__ unsigned int __float_as_uint(float __a) {\n"
"  return __nv_float_as_uint(__a);\n"
"}\n"
"__DEVICE__ double __fma_rd(double __a, double __b, double __c) {\n"
"  return __nv_fma_rd(__a, __b, __c);\n"
"}\n"
"__DEVICE__ double __fma_rn(double __a, double __b, double __c) {\n"
"  return __nv_fma_rn(__a, __b, __c);\n"
"}\n"
"__DEVICE__ double __fma_ru(double __a, double __b, double __c) {\n"
"  return __nv_fma_ru(__a, __b, __c);\n"
"}\n"
"__DEVICE__ double __fma_rz(double __a, double __b, double __c) {\n"
"  return __nv_fma_rz(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_ieee_rd(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_ieee_rd(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_ieee_rn(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_ieee_rn(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_ieee_ru(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_ieee_ru(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_ieee_rz(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_ieee_rz(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_rd(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_rd(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_rn(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_rn(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_ru(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_ru(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmaf_rz(float __a, float __b, float __c) {\n"
"  return __nv_fmaf_rz(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __fmul_rd(float __a, float __b) {\n"
"  return __nv_fmul_rd(__a, __b);\n"
"}\n"
"__DEVICE__ float __fmul_rn(float __a, float __b) {\n"
"  return __nv_fmul_rn(__a, __b);\n"
"}\n"
"__DEVICE__ float __fmul_ru(float __a, float __b) {\n"
"  return __nv_fmul_ru(__a, __b);\n"
"}\n"
"__DEVICE__ float __fmul_rz(float __a, float __b) {\n"
"  return __nv_fmul_rz(__a, __b);\n"
"}\n"
"__DEVICE__ float __frcp_rd(float __a) { return __nv_frcp_rd(__a); }\n"
"__DEVICE__ float __frcp_rn(float __a) { return __nv_frcp_rn(__a); }\n"
"__DEVICE__ float __frcp_ru(float __a) { return __nv_frcp_ru(__a); }\n"
"__DEVICE__ float __frcp_rz(float __a) { return __nv_frcp_rz(__a); }\n"
"__DEVICE__ float __frsqrt_rn(float __a) { return __nv_frsqrt_rn(__a); }\n"
"__DEVICE__ float __fsqrt_rd(float __a) { return __nv_fsqrt_rd(__a); }\n"
"__DEVICE__ float __fsqrt_rn(float __a) { return __nv_fsqrt_rn(__a); }\n"
"__DEVICE__ float __fsqrt_ru(float __a) { return __nv_fsqrt_ru(__a); }\n"
"__DEVICE__ float __fsqrt_rz(float __a) { return __nv_fsqrt_rz(__a); }\n"
"__DEVICE__ float __fsub_rd(float __a, float __b) {\n"
"  return __nv_fsub_rd(__a, __b);\n"
"}\n"
"__DEVICE__ float __fsub_rn(float __a, float __b) {\n"
"  return __nv_fsub_rn(__a, __b);\n"
"}\n"
"__DEVICE__ float __fsub_ru(float __a, float __b) {\n"
"  return __nv_fsub_ru(__a, __b);\n"
"}\n"
"__DEVICE__ float __fsub_rz(float __a, float __b) {\n"
"  return __nv_fsub_rz(__a, __b);\n"
"}\n"
"__DEVICE__ int __hadd(int __a, int __b) { return __nv_hadd(__a, __b); }\n"
"__DEVICE__ double __hiloint2double(int __a, int __b) {\n"
"  return __nv_hiloint2double(__a, __b);\n"
"}\n"
"__DEVICE__ int __iAtomicAdd(int *__p, int __v) {\n"
"  return __nvvm_atom_add_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicAdd_block(int *__p, int __v) {\n"
"  __nvvm_atom_cta_add_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicAdd_system(int *__p, int __v) {\n"
"  __nvvm_atom_sys_add_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicAnd(int *__p, int __v) {\n"
"  return __nvvm_atom_and_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicAnd_block(int *__p, int __v) {\n"
"  return __nvvm_atom_cta_and_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicAnd_system(int *__p, int __v) {\n"
"  return __nvvm_atom_sys_and_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicCAS(int *__p, int __cmp, int __v) {\n"
"  return __nvvm_atom_cas_gen_i(__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicCAS_block(int *__p, int __cmp, int __v) {\n"
"  return __nvvm_atom_cta_cas_gen_i(__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicCAS_system(int *__p, int __cmp, int __v) {\n"
"  return __nvvm_atom_sys_cas_gen_i(__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicExch(int *__p, int __v) {\n"
"  return __nvvm_atom_xchg_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicExch_block(int *__p, int __v) {\n"
"  return __nvvm_atom_cta_xchg_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicExch_system(int *__p, int __v) {\n"
"  return __nvvm_atom_sys_xchg_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicMax(int *__p, int __v) {\n"
"  return __nvvm_atom_max_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicMax_block(int *__p, int __v) {\n"
"  return __nvvm_atom_cta_max_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicMax_system(int *__p, int __v) {\n"
"  return __nvvm_atom_sys_max_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicMin(int *__p, int __v) {\n"
"  return __nvvm_atom_min_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicMin_block(int *__p, int __v) {\n"
"  return __nvvm_atom_cta_min_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicMin_system(int *__p, int __v) {\n"
"  return __nvvm_atom_sys_min_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicOr(int *__p, int __v) {\n"
"  return __nvvm_atom_or_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicOr_block(int *__p, int __v) {\n"
"  return __nvvm_atom_cta_or_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicOr_system(int *__p, int __v) {\n"
"  return __nvvm_atom_sys_or_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicXor(int *__p, int __v) {\n"
"  return __nvvm_atom_xor_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicXor_block(int *__p, int __v) {\n"
"  return __nvvm_atom_cta_xor_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ int __iAtomicXor_system(int *__p, int __v) {\n"
"  return __nvvm_atom_sys_xor_gen_i(__p, __v);\n"
"}\n"
"__DEVICE__ long long __illAtomicMax(long long *__p, long long __v) {\n"
"  return __nvvm_atom_max_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __illAtomicMax_block(long long *__p, long long __v) {\n"
"  return __nvvm_atom_cta_max_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __illAtomicMax_system(long long *__p, long long __v) {\n"
"  return __nvvm_atom_sys_max_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __illAtomicMin(long long *__p, long long __v) {\n"
"  return __nvvm_atom_min_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __illAtomicMin_block(long long *__p, long long __v) {\n"
"  return __nvvm_atom_cta_min_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __illAtomicMin_system(long long *__p, long long __v) {\n"
"  return __nvvm_atom_sys_min_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ double __int2double_rn(int __a) { return __nv_int2double_rn(__a); }\n"
"__DEVICE__ float __int2float_rd(int __a) { return __nv_int2float_rd(__a); }\n"
"__DEVICE__ float __int2float_rn(int __a) { return __nv_int2float_rn(__a); }\n"
"__DEVICE__ float __int2float_ru(int __a) { return __nv_int2float_ru(__a); }\n"
"__DEVICE__ float __int2float_rz(int __a) { return __nv_int2float_rz(__a); }\n"
"__DEVICE__ float __int_as_float(int __a) { return __nv_int_as_float(__a); }\n"
"__DEVICE__ int __isfinited(double __a) { return __nv_isfinited(__a); }\n"
"__DEVICE__ int __isinf(double __a) { return __nv_isinfd(__a); }\n"
"__DEVICE__ int __isinff(float __a) { return __nv_isinff(__a); }\n"
"__DEVICE__ int __isnan(double __a) { return __nv_isnand(__a); }\n"
"__DEVICE__ int __isnanf(float __a) { return __nv_isnanf(__a); }\n"
"__DEVICE__ double __ll2double_rd(long long __a) {\n"
"  return __nv_ll2double_rd(__a);\n"
"}\n"
"__DEVICE__ double __ll2double_rn(long long __a) {\n"
"  return __nv_ll2double_rn(__a);\n"
"}\n"
"__DEVICE__ double __ll2double_ru(long long __a) {\n"
"  return __nv_ll2double_ru(__a);\n"
"}\n"
"__DEVICE__ double __ll2double_rz(long long __a) {\n"
"  return __nv_ll2double_rz(__a);\n"
"}\n"
"__DEVICE__ float __ll2float_rd(long long __a) { return __nv_ll2float_rd(__a); }\n"
"__DEVICE__ float __ll2float_rn(long long __a) { return __nv_ll2float_rn(__a); }\n"
"__DEVICE__ float __ll2float_ru(long long __a) { return __nv_ll2float_ru(__a); }\n"
"__DEVICE__ float __ll2float_rz(long long __a) { return __nv_ll2float_rz(__a); }\n"
"__DEVICE__ long long __llAtomicAnd(long long *__p, long long __v) {\n"
"  return __nvvm_atom_and_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicAnd_block(long long *__p, long long __v) {\n"
"  return __nvvm_atom_cta_and_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicAnd_system(long long *__p, long long __v) {\n"
"  return __nvvm_atom_sys_and_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicOr(long long *__p, long long __v) {\n"
"  return __nvvm_atom_or_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicOr_block(long long *__p, long long __v) {\n"
"  return __nvvm_atom_cta_or_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicOr_system(long long *__p, long long __v) {\n"
"  return __nvvm_atom_sys_or_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicXor(long long *__p, long long __v) {\n"
"  return __nvvm_atom_xor_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicXor_block(long long *__p, long long __v) {\n"
"  return __nvvm_atom_cta_xor_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ long long __llAtomicXor_system(long long *__p, long long __v) {\n"
"  return __nvvm_atom_sys_xor_gen_ll(__p, __v);\n"
"}\n"
"__DEVICE__ float __log10f(float __a) { return __nv_fast_log10f(__a); }\n"
"__DEVICE__ float __log2f(float __a) { return __nv_fast_log2f(__a); }\n"
"__DEVICE__ float __logf(float __a) { return __nv_fast_logf(__a); }\n"
"__DEVICE__ double __longlong_as_double(long long __a) {\n"
"  return __nv_longlong_as_double(__a);\n"
"}\n"
"__DEVICE__ int __mul24(int __a, int __b) { return __nv_mul24(__a, __b); }\n"
"__DEVICE__ long long __mul64hi(long long __a, long long __b) {\n"
"  return __nv_mul64hi(__a, __b);\n"
"}\n"
"__DEVICE__ int __mulhi(int __a, int __b) { return __nv_mulhi(__a, __b); }\n"
"__DEVICE__ unsigned int __pm0(void) { return __nvvm_read_ptx_sreg_pm0(); }\n"
"__DEVICE__ unsigned int __pm1(void) { return __nvvm_read_ptx_sreg_pm1(); }\n"
"__DEVICE__ unsigned int __pm2(void) { return __nvvm_read_ptx_sreg_pm2(); }\n"
"__DEVICE__ unsigned int __pm3(void) { return __nvvm_read_ptx_sreg_pm3(); }\n"
"__DEVICE__ int __popc(int __a) { return __nv_popc(__a); }\n"
"__DEVICE__ int __popcll(long long __a) { return __nv_popcll(__a); }\n"
"__DEVICE__ float __powf(float __a, float __b) {\n"
"  return __nv_fast_powf(__a, __b);\n"
"}\n"
"\n"
"// Parameter must have a known integer value.\n"
"#define __prof_trigger(__a) asm __volatile__(\"pmevent \\t%0;\" ::\"i\"(__a))\n"
"__DEVICE__ int __rhadd(int __a, int __b) { return __nv_rhadd(__a, __b); }\n"
"__DEVICE__ unsigned int __sad(int __a, int __b, unsigned int __c) {\n"
"  return __nv_sad(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float __saturatef(float __a) { return __nv_saturatef(__a); }\n"
"__DEVICE__ int __signbitd(double __a) { return __nv_signbitd(__a); }\n"
"__DEVICE__ int __signbitf(float __a) { return __nv_signbitf(__a); }\n"
"__DEVICE__ void __sincosf(float __a, float *__sptr, float *__cptr) {\n"
"  return __nv_fast_sincosf(__a, __sptr, __cptr);\n"
"}\n"
"__DEVICE__ float __sinf(float __a) { return __nv_fast_sinf(__a); }\n"
"__DEVICE__ int __syncthreads_and(int __a) { return __nvvm_bar0_and(__a); }\n"
"__DEVICE__ int __syncthreads_count(int __a) { return __nvvm_bar0_popc(__a); }\n"
"__DEVICE__ int __syncthreads_or(int __a) { return __nvvm_bar0_or(__a); }\n"
"__DEVICE__ float __tanf(float __a) { return __nv_fast_tanf(__a); }\n"
"__DEVICE__ void __threadfence(void) { __nvvm_membar_gl(); }\n"
"__DEVICE__ void __threadfence_block(void) { __nvvm_membar_cta(); };\n"
"__DEVICE__ void __threadfence_system(void) { __nvvm_membar_sys(); };\n"
"__DEVICE__ void __trap(void) { asm volatile(\"trap;\"); }\n"
"__DEVICE__ unsigned int __uAtomicAdd(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_add_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicAdd_block(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_cta_add_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicAdd_system(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_sys_add_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicAnd(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_and_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicAnd_block(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_cta_and_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicAnd_system(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_sys_and_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicCAS(unsigned int *__p, unsigned int __cmp,\n"
"                                     unsigned int __v) {\n"
"  return __nvvm_atom_cas_gen_i((int *)__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ unsigned int\n"
"__uAtomicCAS_block(unsigned int *__p, unsigned int __cmp, unsigned int __v) {\n"
"  return __nvvm_atom_cta_cas_gen_i((int *)__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ unsigned int\n"
"__uAtomicCAS_system(unsigned int *__p, unsigned int __cmp, unsigned int __v) {\n"
"  return __nvvm_atom_sys_cas_gen_i((int *)__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicDec(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_dec_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicDec_block(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_cta_dec_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicDec_system(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_sys_dec_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicExch(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_xchg_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicExch_block(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_cta_xchg_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicExch_system(unsigned int *__p,\n"
"                                             unsigned int __v) {\n"
"  return __nvvm_atom_sys_xchg_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicInc(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_inc_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicInc_block(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_cta_inc_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicInc_system(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_sys_inc_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicMax(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_max_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicMax_block(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_cta_max_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicMax_system(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_sys_max_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicMin(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_min_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicMin_block(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_cta_min_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicMin_system(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_sys_min_gen_ui(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicOr(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_or_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicOr_block(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_cta_or_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicOr_system(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_sys_or_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicXor(unsigned int *__p, unsigned int __v) {\n"
"  return __nvvm_atom_xor_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicXor_block(unsigned int *__p,\n"
"                                           unsigned int __v) {\n"
"  return __nvvm_atom_cta_xor_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uAtomicXor_system(unsigned int *__p,\n"
"                                            unsigned int __v) {\n"
"  return __nvvm_atom_sys_xor_gen_i((int *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __uhadd(unsigned int __a, unsigned int __b) {\n"
"  return __nv_uhadd(__a, __b);\n"
"}\n"
"__DEVICE__ double __uint2double_rn(unsigned int __a) {\n"
"  return __nv_uint2double_rn(__a);\n"
"}\n"
"__DEVICE__ float __uint2float_rd(unsigned int __a) {\n"
"  return __nv_uint2float_rd(__a);\n"
"}\n"
"__DEVICE__ float __uint2float_rn(unsigned int __a) {\n"
"  return __nv_uint2float_rn(__a);\n"
"}\n"
"__DEVICE__ float __uint2float_ru(unsigned int __a) {\n"
"  return __nv_uint2float_ru(__a);\n"
"}\n"
"__DEVICE__ float __uint2float_rz(unsigned int __a) {\n"
"  return __nv_uint2float_rz(__a);\n"
"}\n"
"__DEVICE__ float __uint_as_float(unsigned int __a) {\n"
"  return __nv_uint_as_float(__a);\n"
"} //\n"
"__DEVICE__ double __ull2double_rd(unsigned long long __a) {\n"
"  return __nv_ull2double_rd(__a);\n"
"}\n"
"__DEVICE__ double __ull2double_rn(unsigned long long __a) {\n"
"  return __nv_ull2double_rn(__a);\n"
"}\n"
"__DEVICE__ double __ull2double_ru(unsigned long long __a) {\n"
"  return __nv_ull2double_ru(__a);\n"
"}\n"
"__DEVICE__ double __ull2double_rz(unsigned long long __a) {\n"
"  return __nv_ull2double_rz(__a);\n"
"}\n"
"__DEVICE__ float __ull2float_rd(unsigned long long __a) {\n"
"  return __nv_ull2float_rd(__a);\n"
"}\n"
"__DEVICE__ float __ull2float_rn(unsigned long long __a) {\n"
"  return __nv_ull2float_rn(__a);\n"
"}\n"
"__DEVICE__ float __ull2float_ru(unsigned long long __a) {\n"
"  return __nv_ull2float_ru(__a);\n"
"}\n"
"__DEVICE__ float __ull2float_rz(unsigned long long __a) {\n"
"  return __nv_ull2float_rz(__a);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicAdd(unsigned long long *__p,\n"
"                                             unsigned long long __v) {\n"
"  return __nvvm_atom_add_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicAdd_block(unsigned long long *__p,\n"
"                                                   unsigned long long __v) {\n"
"  return __nvvm_atom_cta_add_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicAdd_system(unsigned long long *__p,\n"
"                                                    unsigned long long __v) {\n"
"  return __nvvm_atom_sys_add_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicAnd(unsigned long long *__p,\n"
"                                             unsigned long long __v) {\n"
"  return __nvvm_atom_and_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicAnd_block(unsigned long long *__p,\n"
"                                                   unsigned long long __v) {\n"
"  return __nvvm_atom_cta_and_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicAnd_system(unsigned long long *__p,\n"
"                                                    unsigned long long __v) {\n"
"  return __nvvm_atom_sys_and_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicCAS(unsigned long long *__p,\n"
"                                             unsigned long long __cmp,\n"
"                                             unsigned long long __v) {\n"
"  return __nvvm_atom_cas_gen_ll((long long *)__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicCAS_block(unsigned long long *__p,\n"
"                                                   unsigned long long __cmp,\n"
"                                                   unsigned long long __v) {\n"
"  return __nvvm_atom_cta_cas_gen_ll((long long *)__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicCAS_system(unsigned long long *__p,\n"
"                                                    unsigned long long __cmp,\n"
"                                                    unsigned long long __v) {\n"
"  return __nvvm_atom_sys_cas_gen_ll((long long *)__p, __cmp, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicExch(unsigned long long *__p,\n"
"                                              unsigned long long __v) {\n"
"  return __nvvm_atom_xchg_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicExch_block(unsigned long long *__p,\n"
"                                                    unsigned long long __v) {\n"
"  return __nvvm_atom_cta_xchg_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicExch_system(unsigned long long *__p,\n"
"                                                     unsigned long long __v) {\n"
"  return __nvvm_atom_sys_xchg_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicMax(unsigned long long *__p,\n"
"                                             unsigned long long __v) {\n"
"  return __nvvm_atom_max_gen_ull(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicMax_block(unsigned long long *__p,\n"
"                                                   unsigned long long __v) {\n"
"  return __nvvm_atom_cta_max_gen_ull(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicMax_system(unsigned long long *__p,\n"
"                                                    unsigned long long __v) {\n"
"  return __nvvm_atom_sys_max_gen_ull(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicMin(unsigned long long *__p,\n"
"                                             unsigned long long __v) {\n"
"  return __nvvm_atom_min_gen_ull(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicMin_block(unsigned long long *__p,\n"
"                                                   unsigned long long __v) {\n"
"  return __nvvm_atom_cta_min_gen_ull(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicMin_system(unsigned long long *__p,\n"
"                                                    unsigned long long __v) {\n"
"  return __nvvm_atom_sys_min_gen_ull(__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicOr(unsigned long long *__p,\n"
"                                            unsigned long long __v) {\n"
"  return __nvvm_atom_or_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicOr_block(unsigned long long *__p,\n"
"                                                  unsigned long long __v) {\n"
"  return __nvvm_atom_cta_or_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicOr_system(unsigned long long *__p,\n"
"                                                   unsigned long long __v) {\n"
"  return __nvvm_atom_sys_or_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicXor(unsigned long long *__p,\n"
"                                             unsigned long long __v) {\n"
"  return __nvvm_atom_xor_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicXor_block(unsigned long long *__p,\n"
"                                                   unsigned long long __v) {\n"
"  return __nvvm_atom_cta_xor_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned long long __ullAtomicXor_system(unsigned long long *__p,\n"
"                                                    unsigned long long __v) {\n"
"  return __nvvm_atom_sys_xor_gen_ll((long long *)__p, __v);\n"
"}\n"
"__DEVICE__ unsigned int __umul24(unsigned int __a, unsigned int __b) {\n"
"  return __nv_umul24(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned long long __umul64hi(unsigned long long __a,\n"
"                                         unsigned long long __b) {\n"
"  return __nv_umul64hi(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __umulhi(unsigned int __a, unsigned int __b) {\n"
"  return __nv_umulhi(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __urhadd(unsigned int __a, unsigned int __b) {\n"
"  return __nv_urhadd(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __usad(unsigned int __a, unsigned int __b,\n"
"                               unsigned int __c) {\n"
"  return __nv_usad(__a, __b, __c);\n"
"}\n"
"\n"
"#if CUDA_VERSION >= 9000 && CUDA_VERSION < 9020\n"
"__DEVICE__ unsigned int __vabs2(unsigned int __a) { return __nv_vabs2(__a); }\n"
"__DEVICE__ unsigned int __vabs4(unsigned int __a) { return __nv_vabs4(__a); }\n"
"__DEVICE__ unsigned int __vabsdiffs2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vabsdiffs2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vabsdiffs4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vabsdiffs4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vabsdiffu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vabsdiffu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vabsdiffu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vabsdiffu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vabsss2(unsigned int __a) {\n"
"  return __nv_vabsss2(__a);\n"
"}\n"
"__DEVICE__ unsigned int __vabsss4(unsigned int __a) {\n"
"  return __nv_vabsss4(__a);\n"
"}\n"
"__DEVICE__ unsigned int __vadd2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vadd2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vadd4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vadd4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vaddss2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vaddss2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vaddss4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vaddss4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vaddus2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vaddus2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vaddus4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vaddus4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vavgs2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vavgs2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vavgs4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vavgs4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vavgu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vavgu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vavgu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vavgu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpeq2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpeq2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpeq4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpeq4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpges2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpges2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpges4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpges4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgeu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpgeu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgeu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpgeu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgts2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpgts2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgts4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpgts4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgtu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpgtu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgtu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpgtu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmples2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmples2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmples4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmples4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpleu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpleu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpleu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpleu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmplts2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmplts2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmplts4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmplts4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpltu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpltu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpltu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpltu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpne2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpne2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vcmpne4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vcmpne4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vhaddu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vhaddu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vhaddu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vhaddu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vmaxs2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vmaxs2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vmaxs4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vmaxs4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vmaxu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vmaxu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vmaxu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vmaxu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vmins2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vmins2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vmins4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vmins4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vminu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vminu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vminu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vminu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vneg2(unsigned int __a) { return __nv_vneg2(__a); }\n"
"__DEVICE__ unsigned int __vneg4(unsigned int __a) { return __nv_vneg4(__a); }\n"
"__DEVICE__ unsigned int __vnegss2(unsigned int __a) {\n"
"  return __nv_vnegss2(__a);\n"
"}\n"
"__DEVICE__ unsigned int __vnegss4(unsigned int __a) {\n"
"  return __nv_vnegss4(__a);\n"
"}\n"
"__DEVICE__ unsigned int __vsads2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsads2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsads4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsads4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsadu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsadu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsadu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsadu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vseteq2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vseteq2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vseteq4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vseteq4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetges2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetges2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetges4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetges4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgeu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetgeu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgeu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetgeu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgts2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetgts2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgts4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetgts4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgtu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetgtu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgtu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetgtu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetles2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetles2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetles4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetles4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetleu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetleu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetleu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetleu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetlts2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetlts2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetlts4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetlts4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetltu2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetltu2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetltu4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetltu4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetne2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetne2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsetne4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsetne4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsub2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsub2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsub4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsub4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsubss2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsubss2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsubss4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsubss4(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsubus2(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsubus2(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int __vsubus4(unsigned int __a, unsigned int __b) {\n"
"  return __nv_vsubus4(__a, __b);\n"
"}\n"
"#else // CUDA_VERSION >= 9020\n"
"// CUDA no longer provides inline assembly (or bitcode) implementation of these\n"
"// functions, so we have to reimplment them. The implementation is naive and is\n"
"// not optimized for performance.\n"
"\n"
"// Helper function to convert N-bit boolean subfields into all-0 or all-1.\n"
"// E.g. __bool2mask(0x01000100,8) -> 0xff00ff00\n"
"//      __bool2mask(0x00010000,16) -> 0xffff0000\n"
"__DEVICE__ unsigned int __bool2mask(unsigned int __a, int shift) {\n"
"  return (__a << shift) - __a;\n"
"}\n"
"__DEVICE__ unsigned int __vabs2(unsigned int __a) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff2.s32.s32.s32 %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(0), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vabs4(unsigned int __a) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff4.s32.s32.s32 %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(0), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vabsdiffs2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff2.s32.s32.s32 %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"\n"
"__DEVICE__ unsigned int __vabsdiffs4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff4.s32.s32.s32 %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vabsdiffu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff2.u32.u32.u32 %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vabsdiffu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff4.u32.u32.u32 %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vabsss2(unsigned int __a) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff2.s32.s32.s32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(0), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vabsss4(unsigned int __a) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff4.s32.s32.s32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(0), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vadd2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vadd2.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vadd4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vadd4.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vaddss2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vadd2.s32.s32.s32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vaddss4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vadd4.s32.s32.s32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vaddus2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vadd2.u32.u32.u32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vaddus4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vadd4.u32.u32.u32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vavgs2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vavrg2.s32.s32.s32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vavgs4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vavrg4.s32.s32.s32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vavgu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vavrg2.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vavgu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vavrg4.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vseteq2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.u32.u32.eq %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpeq2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vseteq2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vseteq4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.u32.u32.eq %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpeq4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vseteq4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetges2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.s32.s32.ge %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpges2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetges2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetges4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.s32.s32.ge %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpges4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetges4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgeu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.u32.u32.ge %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgeu2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetgeu2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgeu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.u32.u32.ge %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgeu4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetgeu4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgts2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.s32.s32.gt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgts2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetgts2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgts4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.s32.s32.gt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgts4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetgts4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgtu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.u32.u32.gt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgtu2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetgtu2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetgtu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.u32.u32.gt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpgtu4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetgtu4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetles2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.s32.s32.le %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmples2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetles2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetles4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.s32.s32.le %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmples4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetles4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetleu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.u32.u32.le %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpleu2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetleu2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetleu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.u32.u32.le %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpleu4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetleu4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetlts2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.s32.s32.lt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmplts2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetlts2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetlts4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.s32.s32.lt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmplts4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetlts4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetltu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.u32.u32.lt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpltu2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetltu2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetltu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.u32.u32.lt %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpltu4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetltu4(__a, __b), 8);\n"
"}\n"
"__DEVICE__ unsigned int __vsetne2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset2.u32.u32.ne %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpne2(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetne2(__a, __b), 16);\n"
"}\n"
"__DEVICE__ unsigned int __vsetne4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vset4.u32.u32.ne %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vcmpne4(unsigned int __a, unsigned int __b) {\n"
"  return __bool2mask(__vsetne4(__a, __b), 8);\n"
"}\n"
"\n"
"// Based on ITEM 23 in AIM-239: http://dspace.mit.edu/handle/1721.1/6086\n"
"// (a & b) + (a | b) = a + b = (a ^ b) + 2 * (a & b) =>\n"
"// (a + b) / 2 = ((a ^ b) >> 1) + (a & b)\n"
"// To operate on multiple sub-elements we need to make sure to mask out bits\n"
"// that crossed over into adjacent elements during the shift.\n"
"__DEVICE__ unsigned int __vhaddu2(unsigned int __a, unsigned int __b) {\n"
"  return (((__a ^ __b) >> 1) & ~0x80008000u) + (__a & __b);\n"
"}\n"
"__DEVICE__ unsigned int __vhaddu4(unsigned int __a, unsigned int __b) {\n"
"  return (((__a ^ __b) >> 1) & ~0x80808080u) + (__a & __b);\n"
"}\n"
"\n"
"__DEVICE__ unsigned int __vmaxs2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  if ((__a & 0x8000) && (__b & 0x8000)) {\n"
"    // Work around a bug in ptxas which produces invalid result if low element\n"
"    // is negative.\n"
"    unsigned mask = __vcmpgts2(__a, __b);\n"
"    r = (__a & mask) | (__b & ~mask);\n"
"  } else {\n"
"    asm(\"vmax2.s32.s32.s32 %0,%1,%2,%3;\"\n"
"        : \"=r\"(r)\n"
"        : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  }\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vmaxs4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vmax4.s32.s32.s32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vmaxu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vmax2.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vmaxu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vmax4.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vmins2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vmin2.s32.s32.s32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vmins4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vmin4.s32.s32.s32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vminu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vmin2.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vminu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vmin4.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vsads2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff2.s32.s32.s32.add %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vsads4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff4.s32.s32.s32.add %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vsadu2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff2.u32.u32.u32.add %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vsadu4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vabsdiff4.u32.u32.u32.add %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"\n"
"__DEVICE__ unsigned int __vsub2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vsub2.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vneg2(unsigned int __a) { return __vsub2(0, __a); }\n"
"\n"
"__DEVICE__ unsigned int __vsub4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vsub4.u32.u32.u32 %0,%1,%2,%3;\" : \"=r\"(r) : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vneg4(unsigned int __a) { return __vsub4(0, __a); }\n"
"__DEVICE__ unsigned int __vsubss2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vsub2.s32.s32.s32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vnegss2(unsigned int __a) {\n"
"  return __vsubss2(0, __a);\n"
"}\n"
"__DEVICE__ unsigned int __vsubss4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vsub4.s32.s32.s32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vnegss4(unsigned int __a) {\n"
"  return __vsubss4(0, __a);\n"
"}\n"
"__DEVICE__ unsigned int __vsubus2(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vsub2.u32.u32.u32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"__DEVICE__ unsigned int __vsubus4(unsigned int __a, unsigned int __b) {\n"
"  unsigned int r;\n"
"  asm(\"vsub4.u32.u32.u32.sat %0,%1,%2,%3;\"\n"
"      : \"=r\"(r)\n"
"      : \"r\"(__a), \"r\"(__b), \"r\"(0));\n"
"  return r;\n"
"}\n"
"#endif // CUDA_VERSION >= 9020\n"
"__DEVICE__ int abs(int __a) { return __nv_abs(__a); }\n"
"__DEVICE__ double acos(double __a) { return __nv_acos(__a); }\n"
"__DEVICE__ float acosf(float __a) { return __nv_acosf(__a); }\n"
"__DEVICE__ double acosh(double __a) { return __nv_acosh(__a); }\n"
"__DEVICE__ float acoshf(float __a) { return __nv_acoshf(__a); }\n"
"__DEVICE__ double asin(double __a) { return __nv_asin(__a); }\n"
"__DEVICE__ float asinf(float __a) { return __nv_asinf(__a); }\n"
"__DEVICE__ double asinh(double __a) { return __nv_asinh(__a); }\n"
"__DEVICE__ float asinhf(float __a) { return __nv_asinhf(__a); }\n"
"__DEVICE__ double atan(double __a) { return __nv_atan(__a); }\n"
"__DEVICE__ double atan2(double __a, double __b) { return __nv_atan2(__a, __b); }\n"
"__DEVICE__ float atan2f(float __a, float __b) { return __nv_atan2f(__a, __b); }\n"
"__DEVICE__ float atanf(float __a) { return __nv_atanf(__a); }\n"
"__DEVICE__ double atanh(double __a) { return __nv_atanh(__a); }\n"
"__DEVICE__ float atanhf(float __a) { return __nv_atanhf(__a); }\n"
"__DEVICE__ double cbrt(double __a) { return __nv_cbrt(__a); }\n"
"__DEVICE__ float cbrtf(float __a) { return __nv_cbrtf(__a); }\n"
"__DEVICE__ double ceil(double __a) { return __nv_ceil(__a); }\n"
"__DEVICE__ float ceilf(float __a) { return __nv_ceilf(__a); }\n"
"__DEVICE__ int clock() { return __nvvm_read_ptx_sreg_clock(); }\n"
"__DEVICE__ long long clock64() { return __nvvm_read_ptx_sreg_clock64(); }\n"
"__DEVICE__ double copysign(double __a, double __b) {\n"
"  return __nv_copysign(__a, __b);\n"
"}\n"
"__DEVICE__ float copysignf(float __a, float __b) {\n"
"  return __nv_copysignf(__a, __b);\n"
"}\n"
"__DEVICE__ double cos(double __a) { return __nv_cos(__a); }\n"
"__DEVICE__ float cosf(float __a) {\n"
"  return __FAST_OR_SLOW(__nv_fast_cosf, __nv_cosf)(__a);\n"
"}\n"
"__DEVICE__ double cosh(double __a) { return __nv_cosh(__a); }\n"
"__DEVICE__ float coshf(float __a) { return __nv_coshf(__a); }\n"
"__DEVICE__ double cospi(double __a) { return __nv_cospi(__a); }\n"
"__DEVICE__ float cospif(float __a) { return __nv_cospif(__a); }\n"
"__DEVICE__ double cyl_bessel_i0(double __a) { return __nv_cyl_bessel_i0(__a); }\n"
"__DEVICE__ float cyl_bessel_i0f(float __a) { return __nv_cyl_bessel_i0f(__a); }\n"
"__DEVICE__ double cyl_bessel_i1(double __a) { return __nv_cyl_bessel_i1(__a); }\n"
"__DEVICE__ float cyl_bessel_i1f(float __a) { return __nv_cyl_bessel_i1f(__a); }\n"
"__DEVICE__ double erf(double __a) { return __nv_erf(__a); }\n"
"__DEVICE__ double erfc(double __a) { return __nv_erfc(__a); }\n"
"__DEVICE__ float erfcf(float __a) { return __nv_erfcf(__a); }\n"
"__DEVICE__ double erfcinv(double __a) { return __nv_erfcinv(__a); }\n"
"__DEVICE__ float erfcinvf(float __a) { return __nv_erfcinvf(__a); }\n"
"__DEVICE__ double erfcx(double __a) { return __nv_erfcx(__a); }\n"
"__DEVICE__ float erfcxf(float __a) { return __nv_erfcxf(__a); }\n"
"__DEVICE__ float erff(float __a) { return __nv_erff(__a); }\n"
"__DEVICE__ double erfinv(double __a) { return __nv_erfinv(__a); }\n"
"__DEVICE__ float erfinvf(float __a) { return __nv_erfinvf(__a); }\n"
"__DEVICE__ double exp(double __a) { return __nv_exp(__a); }\n"
"__DEVICE__ double exp10(double __a) { return __nv_exp10(__a); }\n"
"__DEVICE__ float exp10f(float __a) { return __nv_exp10f(__a); }\n"
"__DEVICE__ double exp2(double __a) { return __nv_exp2(__a); }\n"
"__DEVICE__ float exp2f(float __a) { return __nv_exp2f(__a); }\n"
"__DEVICE__ float expf(float __a) { return __nv_expf(__a); }\n"
"__DEVICE__ double expm1(double __a) { return __nv_expm1(__a); }\n"
"__DEVICE__ float expm1f(float __a) { return __nv_expm1f(__a); }\n"
"__DEVICE__ double fabs(double __a) { return __nv_fabs(__a); }\n"
"__DEVICE__ float fabsf(float __a) { return __nv_fabsf(__a); }\n"
"__DEVICE__ double fdim(double __a, double __b) { return __nv_fdim(__a, __b); }\n"
"__DEVICE__ float fdimf(float __a, float __b) { return __nv_fdimf(__a, __b); }\n"
"__DEVICE__ double fdivide(double __a, double __b) { return __a / __b; }\n"
"__DEVICE__ float fdividef(float __a, float __b) {\n"
"#if __FAST_MATH__ && !__CUDA_PREC_DIV\n"
"  return __nv_fast_fdividef(__a, __b);\n"
"#else\n"
"  return __a / __b;\n"
"#endif\n"
"}\n"
"__DEVICE__ double floor(double __f) { return __nv_floor(__f); }\n"
"__DEVICE__ float floorf(float __f) { return __nv_floorf(__f); }\n"
"__DEVICE__ double fma(double __a, double __b, double __c) {\n"
"  return __nv_fma(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float fmaf(float __a, float __b, float __c) {\n"
"  return __nv_fmaf(__a, __b, __c);\n"
"}\n"
"__DEVICE__ double fmax(double __a, double __b) { return __nv_fmax(__a, __b); }\n"
"__DEVICE__ float fmaxf(float __a, float __b) { return __nv_fmaxf(__a, __b); }\n"
"__DEVICE__ double fmin(double __a, double __b) { return __nv_fmin(__a, __b); }\n"
"__DEVICE__ float fminf(float __a, float __b) { return __nv_fminf(__a, __b); }\n"
"__DEVICE__ double fmod(double __a, double __b) { return __nv_fmod(__a, __b); }\n"
"__DEVICE__ float fmodf(float __a, float __b) { return __nv_fmodf(__a, __b); }\n"
"__DEVICE__ double frexp(double __a, int *__b) { return __nv_frexp(__a, __b); }\n"
"__DEVICE__ float frexpf(float __a, int *__b) { return __nv_frexpf(__a, __b); }\n"
"__DEVICE__ double hypot(double __a, double __b) { return __nv_hypot(__a, __b); }\n"
"__DEVICE__ float hypotf(float __a, float __b) { return __nv_hypotf(__a, __b); }\n"
"__DEVICE__ int ilogb(double __a) { return __nv_ilogb(__a); }\n"
"__DEVICE__ int ilogbf(float __a) { return __nv_ilogbf(__a); }\n"
"__DEVICE__ double j0(double __a) { return __nv_j0(__a); }\n"
"__DEVICE__ float j0f(float __a) { return __nv_j0f(__a); }\n"
"__DEVICE__ double j1(double __a) { return __nv_j1(__a); }\n"
"__DEVICE__ float j1f(float __a) { return __nv_j1f(__a); }\n"
"__DEVICE__ double jn(int __n, double __a) { return __nv_jn(__n, __a); }\n"
"__DEVICE__ float jnf(int __n, float __a) { return __nv_jnf(__n, __a); }\n"
"#if defined(__LP64__)\n"
"__DEVICE__ long labs(long __a) { return llabs(__a); };\n"
"#else\n"
"__DEVICE__ long labs(long __a) { return __nv_abs(__a); };\n"
"#endif\n"
"__DEVICE__ double ldexp(double __a, int __b) { return __nv_ldexp(__a, __b); }\n"
"__DEVICE__ float ldexpf(float __a, int __b) { return __nv_ldexpf(__a, __b); }\n"
"__DEVICE__ double lgamma(double __a) { return __nv_lgamma(__a); }\n"
"__DEVICE__ float lgammaf(float __a) { return __nv_lgammaf(__a); }\n"
"__DEVICE__ long long llabs(long long __a) { return __nv_llabs(__a); }\n"
"__DEVICE__ long long llmax(long long __a, long long __b) {\n"
"  return __nv_llmax(__a, __b);\n"
"}\n"
"__DEVICE__ long long llmin(long long __a, long long __b) {\n"
"  return __nv_llmin(__a, __b);\n"
"}\n"
"__DEVICE__ long long llrint(double __a) { return __nv_llrint(__a); }\n"
"__DEVICE__ long long llrintf(float __a) { return __nv_llrintf(__a); }\n"
"__DEVICE__ long long llround(double __a) { return __nv_llround(__a); }\n"
"__DEVICE__ long long llroundf(float __a) { return __nv_llroundf(__a); }\n"
"__DEVICE__ double log(double __a) { return __nv_log(__a); }\n"
"__DEVICE__ double log10(double __a) { return __nv_log10(__a); }\n"
"__DEVICE__ float log10f(float __a) { return __nv_log10f(__a); }\n"
"__DEVICE__ double log1p(double __a) { return __nv_log1p(__a); }\n"
"__DEVICE__ float log1pf(float __a) { return __nv_log1pf(__a); }\n"
"__DEVICE__ double log2(double __a) { return __nv_log2(__a); }\n"
"__DEVICE__ float log2f(float __a) {\n"
"  return __FAST_OR_SLOW(__nv_fast_log2f, __nv_log2f)(__a);\n"
"}\n"
"__DEVICE__ double logb(double __a) { return __nv_logb(__a); }\n"
"__DEVICE__ float logbf(float __a) { return __nv_logbf(__a); }\n"
"__DEVICE__ float logf(float __a) {\n"
"  return __FAST_OR_SLOW(__nv_fast_logf, __nv_logf)(__a);\n"
"}\n"
"#if defined(__LP64__)\n"
"__DEVICE__ long lrint(double __a) { return llrint(__a); }\n"
"__DEVICE__ long lrintf(float __a) { return __float2ll_rn(__a); }\n"
"__DEVICE__ long lround(double __a) { return llround(__a); }\n"
"__DEVICE__ long lroundf(float __a) { return llroundf(__a); }\n"
"#else\n"
"__DEVICE__ long lrint(double __a) { return (long)rint(__a); }\n"
"__DEVICE__ long lrintf(float __a) { return __float2int_rn(__a); }\n"
"__DEVICE__ long lround(double __a) { return round(__a); }\n"
"__DEVICE__ long lroundf(float __a) { return roundf(__a); }\n"
"#endif\n"
"__DEVICE__ int max(int __a, int __b) { return __nv_max(__a, __b); }\n"
"__DEVICE__ void *memcpy(void *__a, const void *__b, size_t __c) {\n"
"  return __builtin_memcpy(__a, __b, __c);\n"
"}\n"
"__DEVICE__ void *memset(void *__a, int __b, size_t __c) {\n"
"  return __builtin_memset(__a, __b, __c);\n"
"}\n"
"__DEVICE__ int min(int __a, int __b) { return __nv_min(__a, __b); }\n"
"__DEVICE__ double modf(double __a, double *__b) { return __nv_modf(__a, __b); }\n"
"__DEVICE__ float modff(float __a, float *__b) { return __nv_modff(__a, __b); }\n"
"__DEVICE__ double nearbyint(double __a) { return __nv_nearbyint(__a); }\n"
"__DEVICE__ float nearbyintf(float __a) { return __nv_nearbyintf(__a); }\n"
"__DEVICE__ double nextafter(double __a, double __b) {\n"
"  return __nv_nextafter(__a, __b);\n"
"}\n"
"__DEVICE__ float nextafterf(float __a, float __b) {\n"
"  return __nv_nextafterf(__a, __b);\n"
"}\n"
"__DEVICE__ double norm(int __dim, const double *__t) {\n"
"  return __nv_norm(__dim, __t);\n"
"}\n"
"__DEVICE__ double norm3d(double __a, double __b, double __c) {\n"
"  return __nv_norm3d(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float norm3df(float __a, float __b, float __c) {\n"
"  return __nv_norm3df(__a, __b, __c);\n"
"}\n"
"__DEVICE__ double norm4d(double __a, double __b, double __c, double __d) {\n"
"  return __nv_norm4d(__a, __b, __c, __d);\n"
"}\n"
"__DEVICE__ float norm4df(float __a, float __b, float __c, float __d) {\n"
"  return __nv_norm4df(__a, __b, __c, __d);\n"
"}\n"
"__DEVICE__ double normcdf(double __a) { return __nv_normcdf(__a); }\n"
"__DEVICE__ float normcdff(float __a) { return __nv_normcdff(__a); }\n"
"__DEVICE__ double normcdfinv(double __a) { return __nv_normcdfinv(__a); }\n"
"__DEVICE__ float normcdfinvf(float __a) { return __nv_normcdfinvf(__a); }\n"
"__DEVICE__ float normf(int __dim, const float *__t) {\n"
"  return __nv_normf(__dim, __t);\n"
"}\n"
"__DEVICE__ double pow(double __a, double __b) { return __nv_pow(__a, __b); }\n"
"__DEVICE__ float powf(float __a, float __b) { return __nv_powf(__a, __b); }\n"
"__DEVICE__ double powi(double __a, int __b) { return __nv_powi(__a, __b); }\n"
"__DEVICE__ float powif(float __a, int __b) { return __nv_powif(__a, __b); }\n"
"__DEVICE__ double rcbrt(double __a) { return __nv_rcbrt(__a); }\n"
"__DEVICE__ float rcbrtf(float __a) { return __nv_rcbrtf(__a); }\n"
"__DEVICE__ double remainder(double __a, double __b) {\n"
"  return __nv_remainder(__a, __b);\n"
"}\n"
"__DEVICE__ float remainderf(float __a, float __b) {\n"
"  return __nv_remainderf(__a, __b);\n"
"}\n"
"__DEVICE__ double remquo(double __a, double __b, int *__c) {\n"
"  return __nv_remquo(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float remquof(float __a, float __b, int *__c) {\n"
"  return __nv_remquof(__a, __b, __c);\n"
"}\n"
"__DEVICE__ double rhypot(double __a, double __b) {\n"
"  return __nv_rhypot(__a, __b);\n"
"}\n"
"__DEVICE__ float rhypotf(float __a, float __b) {\n"
"  return __nv_rhypotf(__a, __b);\n"
"}\n"
"__DEVICE__ double rint(double __a) { return __nv_rint(__a); }\n"
"__DEVICE__ float rintf(float __a) { return __nv_rintf(__a); }\n"
"__DEVICE__ double rnorm(int __a, const double *__b) {\n"
"  return __nv_rnorm(__a, __b);\n"
"}\n"
"__DEVICE__ double rnorm3d(double __a, double __b, double __c) {\n"
"  return __nv_rnorm3d(__a, __b, __c);\n"
"}\n"
"__DEVICE__ float rnorm3df(float __a, float __b, float __c) {\n"
"  return __nv_rnorm3df(__a, __b, __c);\n"
"}\n"
"__DEVICE__ double rnorm4d(double __a, double __b, double __c, double __d) {\n"
"  return __nv_rnorm4d(__a, __b, __c, __d);\n"
"}\n"
"__DEVICE__ float rnorm4df(float __a, float __b, float __c, float __d) {\n"
"  return __nv_rnorm4df(__a, __b, __c, __d);\n"
"}\n"
"__DEVICE__ float rnormf(int __dim, const float *__t) {\n"
"  return __nv_rnormf(__dim, __t);\n"
"}\n"
"__DEVICE__ double round(double __a) { return __nv_round(__a); }\n"
"__DEVICE__ float roundf(float __a) { return __nv_roundf(__a); }\n"
"__DEVICE__ double rsqrt(double __a) { return __nv_rsqrt(__a); }\n"
"__DEVICE__ float rsqrtf(float __a) { return __nv_rsqrtf(__a); }\n"
"__DEVICE__ double scalbn(double __a, int __b) { return __nv_scalbn(__a, __b); }\n"
"__DEVICE__ float scalbnf(float __a, int __b) { return __nv_scalbnf(__a, __b); }\n"
"__DEVICE__ double scalbln(double __a, long __b) {\n"
"  if (__b > INT_MAX)\n"
"    return __a > 0 ? HUGE_VAL : -HUGE_VAL;\n"
"  if (__b < INT_MIN)\n"
"    return __a > 0 ? 0.0 : -0.0;\n"
"  return scalbn(__a, (int)__b);\n"
"}\n"
"__DEVICE__ float scalblnf(float __a, long __b) {\n"
"  if (__b > INT_MAX)\n"
"    return __a > 0 ? HUGE_VALF : -HUGE_VALF;\n"
"  if (__b < INT_MIN)\n"
"    return __a > 0 ? 0.f : -0.f;\n"
"  return scalbnf(__a, (int)__b);\n"
"}\n"
"__DEVICE__ double sin(double __a) { return __nv_sin(__a); }\n"
"__DEVICE__ void sincos(double __a, double *__sptr, double *__cptr) {\n"
"  return __nv_sincos(__a, __sptr, __cptr);\n"
"}\n"
"__DEVICE__ void sincosf(float __a, float *__sptr, float *__cptr) {\n"
"  return __FAST_OR_SLOW(__nv_fast_sincosf, __nv_sincosf)(__a, __sptr, __cptr);\n"
"}\n"
"__DEVICE__ void sincospi(double __a, double *__sptr, double *__cptr) {\n"
"  return __nv_sincospi(__a, __sptr, __cptr);\n"
"}\n"
"__DEVICE__ void sincospif(float __a, float *__sptr, float *__cptr) {\n"
"  return __nv_sincospif(__a, __sptr, __cptr);\n"
"}\n"
"__DEVICE__ float sinf(float __a) {\n"
"  return __FAST_OR_SLOW(__nv_fast_sinf, __nv_sinf)(__a);\n"
"}\n"
"__DEVICE__ double sinh(double __a) { return __nv_sinh(__a); }\n"
"__DEVICE__ float sinhf(float __a) { return __nv_sinhf(__a); }\n"
"__DEVICE__ double sinpi(double __a) { return __nv_sinpi(__a); }\n"
"__DEVICE__ float sinpif(float __a) { return __nv_sinpif(__a); }\n"
"__DEVICE__ double sqrt(double __a) { return __nv_sqrt(__a); }\n"
"__DEVICE__ float sqrtf(float __a) { return __nv_sqrtf(__a); }\n"
"__DEVICE__ double tan(double __a) { return __nv_tan(__a); }\n"
"__DEVICE__ float tanf(float __a) { return __nv_tanf(__a); }\n"
"__DEVICE__ double tanh(double __a) { return __nv_tanh(__a); }\n"
"__DEVICE__ float tanhf(float __a) { return __nv_tanhf(__a); }\n"
"__DEVICE__ double tgamma(double __a) { return __nv_tgamma(__a); }\n"
"__DEVICE__ float tgammaf(float __a) { return __nv_tgammaf(__a); }\n"
"__DEVICE__ double trunc(double __a) { return __nv_trunc(__a); }\n"
"__DEVICE__ float truncf(float __a) { return __nv_truncf(__a); }\n"
"__DEVICE__ unsigned long long ullmax(unsigned long long __a,\n"
"                                     unsigned long long __b) {\n"
"  return __nv_ullmax(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned long long ullmin(unsigned long long __a,\n"
"                                     unsigned long long __b) {\n"
"  return __nv_ullmin(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int umax(unsigned int __a, unsigned int __b) {\n"
"  return __nv_umax(__a, __b);\n"
"}\n"
"__DEVICE__ unsigned int umin(unsigned int __a, unsigned int __b) {\n"
"  return __nv_umin(__a, __b);\n"
"}\n"
"__DEVICE__ double y0(double __a) { return __nv_y0(__a); }\n"
"__DEVICE__ float y0f(float __a) { return __nv_y0f(__a); }\n"
"__DEVICE__ double y1(double __a) { return __nv_y1(__a); }\n"
"__DEVICE__ float y1f(float __a) { return __nv_y1f(__a); }\n"
"__DEVICE__ double yn(int __a, double __b) { return __nv_yn(__a, __b); }\n"
"__DEVICE__ float ynf(int __a, float __b) { return __nv_ynf(__a, __b); }\n"
"\n"
"#pragma pop_macro(\"__DEVICE__\")\n"
"#pragma pop_macro(\"__FAST_OR_SLOW\")\n"
"#endif // __CLANG_CUDA_DEVICE_FUNCTIONS_H__\n"
"" } , 
 { "/builtins/__clang_cuda_intrinsics.h" , "/*===--- __clang_cuda_intrinsics.h - Device-side CUDA intrinsic wrappers ---===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#ifndef __CLANG_CUDA_INTRINSICS_H__\n"
"#define __CLANG_CUDA_INTRINSICS_H__\n"
"#ifndef __CUDA__\n"
"#error \"This file is for CUDA compilation only.\"\n"
"#endif\n"
"\n"
"// sm_30 intrinsics: __shfl_{up,down,xor}.\n"
"\n"
"#define __SM_30_INTRINSICS_H__\n"
"#define __SM_30_INTRINSICS_HPP__\n"
"\n"
"#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 300\n"
"\n"
"#pragma push_macro(\"__MAKE_SHUFFLES\")\n"
"#define __MAKE_SHUFFLES(__FnName, __IntIntrinsic, __FloatIntrinsic, __Mask,    \\\n"
"                        __Type)                                                \\\n"
"  inline __device__ int __FnName(int __val, __Type __offset,                   \\\n"
"                                 int __width = warpSize) {                     \\\n"
"    return __IntIntrinsic(__val, __offset,                                     \\\n"
"                          ((warpSize - __width) << 8) | (__Mask));             \\\n"
"  }                                                                            \\\n"
"  inline __device__ float __FnName(float __val, __Type __offset,               \\\n"
"                                   int __width = warpSize) {                   \\\n"
"    return __FloatIntrinsic(__val, __offset,                                   \\\n"
"                            ((warpSize - __width) << 8) | (__Mask));           \\\n"
"  }                                                                            \\\n"
"  inline __device__ unsigned int __FnName(unsigned int __val, __Type __offset, \\\n"
"                                          int __width = warpSize) {            \\\n"
"    return static_cast<unsigned int>(                                          \\\n"
"        ::__FnName(static_cast<int>(__val), __offset, __width));               \\\n"
"  }                                                                            \\\n"
"  inline __device__ long long __FnName(long long __val, __Type __offset,       \\\n"
"                                       int __width = warpSize) {               \\\n"
"    struct __Bits {                                                            \\\n"
"      int __a, __b;                                                            \\\n"
"    };                                                                         \\\n"
"    _Static_assert(sizeof(__val) == sizeof(__Bits));                           \\\n"
"    _Static_assert(sizeof(__Bits) == 2 * sizeof(int));                         \\\n"
"    __Bits __tmp;                                                              \\\n"
"    memcpy(&__val, &__tmp, sizeof(__val));                                     \\\n"
"    __tmp.__a = ::__FnName(__tmp.__a, __offset, __width);                      \\\n"
"    __tmp.__b = ::__FnName(__tmp.__b, __offset, __width);                      \\\n"
"    long long __ret;                                                           \\\n"
"    memcpy(&__ret, &__tmp, sizeof(__tmp));                                     \\\n"
"    return __ret;                                                              \\\n"
"  }                                                                            \\\n"
"  inline __device__ long __FnName(long __val, __Type __offset,                 \\\n"
"                                  int __width = warpSize) {                    \\\n"
"    _Static_assert(sizeof(long) == sizeof(long long) ||                        \\\n"
"                   sizeof(long) == sizeof(int));                               \\\n"
"    if (sizeof(long) == sizeof(long long)) {                                   \\\n"
"      return static_cast<long>(                                                \\\n"
"          ::__FnName(static_cast<long long>(__val), __offset, __width));       \\\n"
"    } else if (sizeof(long) == sizeof(int)) {                                  \\\n"
"      return static_cast<long>(                                                \\\n"
"          ::__FnName(static_cast<int>(__val), __offset, __width));             \\\n"
"    }                                                                          \\\n"
"  }                                                                            \\\n"
"  inline __device__ unsigned long __FnName(                                    \\\n"
"      unsigned long __val, __Type __offset, int __width = warpSize) {          \\\n"
"    return static_cast<unsigned long>(                                         \\\n"
"        ::__FnName(static_cast<long>(__val), __offset, __width));              \\\n"
"  }                                                                            \\\n"
"  inline __device__ unsigned long long __FnName(                               \\\n"
"      unsigned long long __val, __Type __offset, int __width = warpSize) {     \\\n"
"    return static_cast<unsigned long long>(::__FnName(                         \\\n"
"        static_cast<unsigned long long>(__val), __offset, __width));           \\\n"
"  }                                                                            \\\n"
"  inline __device__ double __FnName(double __val, __Type __offset,             \\\n"
"                                    int __width = warpSize) {                  \\\n"
"    long long __tmp;                                                           \\\n"
"    _Static_assert(sizeof(__tmp) == sizeof(__val));                            \\\n"
"    memcpy(&__tmp, &__val, sizeof(__val));                                     \\\n"
"    __tmp = ::__FnName(__tmp, __offset, __width);                              \\\n"
"    double __ret;                                                              \\\n"
"    memcpy(&__ret, &__tmp, sizeof(__ret));                                     \\\n"
"    return __ret;                                                              \\\n"
"  }\n"
"\n"
"__MAKE_SHUFFLES(__shfl, __nvvm_shfl_idx_i32, __nvvm_shfl_idx_f32, 0x1f, int);\n"
"// We use 0 rather than 31 as our mask, because shfl.up applies to lanes >=\n"
"// maxLane.\n"
"__MAKE_SHUFFLES(__shfl_up, __nvvm_shfl_up_i32, __nvvm_shfl_up_f32, 0,\n"
"                unsigned int);\n"
"__MAKE_SHUFFLES(__shfl_down, __nvvm_shfl_down_i32, __nvvm_shfl_down_f32, 0x1f,\n"
"                unsigned int);\n"
"__MAKE_SHUFFLES(__shfl_xor, __nvvm_shfl_bfly_i32, __nvvm_shfl_bfly_f32, 0x1f,\n"
"                int);\n"
"#pragma pop_macro(\"__MAKE_SHUFFLES\")\n"
"\n"
"#endif // !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 300\n"
"\n"
"#if CUDA_VERSION >= 9000\n"
"#if (!defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 300)\n"
"// __shfl_sync_* variants available in CUDA-9\n"
"#pragma push_macro(\"__MAKE_SYNC_SHUFFLES\")\n"
"#define __MAKE_SYNC_SHUFFLES(__FnName, __IntIntrinsic, __FloatIntrinsic,       \\\n"
"                             __Mask, __Type)                                   \\\n"
"  inline __device__ int __FnName(unsigned int __mask, int __val,               \\\n"
"                                 __Type __offset, int __width = warpSize) {    \\\n"
"    return __IntIntrinsic(__mask, __val, __offset,                             \\\n"
"                          ((warpSize - __width) << 8) | (__Mask));             \\\n"
"  }                                                                            \\\n"
"  inline __device__ float __FnName(unsigned int __mask, float __val,           \\\n"
"                                   __Type __offset, int __width = warpSize) {  \\\n"
"    return __FloatIntrinsic(__mask, __val, __offset,                           \\\n"
"                            ((warpSize - __width) << 8) | (__Mask));           \\\n"
"  }                                                                            \\\n"
"  inline __device__ unsigned int __FnName(unsigned int __mask,                 \\\n"
"                                          unsigned int __val, __Type __offset, \\\n"
"                                          int __width = warpSize) {            \\\n"
"    return static_cast<unsigned int>(                                          \\\n"
"        ::__FnName(__mask, static_cast<int>(__val), __offset, __width));       \\\n"
"  }                                                                            \\\n"
"  inline __device__ long long __FnName(unsigned int __mask, long long __val,   \\\n"
"                                       __Type __offset,                        \\\n"
"                                       int __width = warpSize) {               \\\n"
"    struct __Bits {                                                            \\\n"
"      int __a, __b;                                                            \\\n"
"    };                                                                         \\\n"
"    _Static_assert(sizeof(__val) == sizeof(__Bits));                           \\\n"
"    _Static_assert(sizeof(__Bits) == 2 * sizeof(int));                         \\\n"
"    __Bits __tmp;                                                              \\\n"
"    memcpy(&__val, &__tmp, sizeof(__val));                                     \\\n"
"    __tmp.__a = ::__FnName(__mask, __tmp.__a, __offset, __width);              \\\n"
"    __tmp.__b = ::__FnName(__mask, __tmp.__b, __offset, __width);              \\\n"
"    long long __ret;                                                           \\\n"
"    memcpy(&__ret, &__tmp, sizeof(__tmp));                                     \\\n"
"    return __ret;                                                              \\\n"
"  }                                                                            \\\n"
"  inline __device__ unsigned long long __FnName(                               \\\n"
"      unsigned int __mask, unsigned long long __val, __Type __offset,          \\\n"
"      int __width = warpSize) {                                                \\\n"
"    return static_cast<unsigned long long>(::__FnName(                         \\\n"
"        __mask, static_cast<unsigned long long>(__val), __offset, __width));   \\\n"
"  }                                                                            \\\n"
"  inline __device__ long __FnName(unsigned int __mask, long __val,             \\\n"
"                                  __Type __offset, int __width = warpSize) {   \\\n"
"    _Static_assert(sizeof(long) == sizeof(long long) ||                        \\\n"
"                   sizeof(long) == sizeof(int));                               \\\n"
"    if (sizeof(long) == sizeof(long long)) {                                   \\\n"
"      return static_cast<long>(::__FnName(                                     \\\n"
"          __mask, static_cast<long long>(__val), __offset, __width));          \\\n"
"    } else if (sizeof(long) == sizeof(int)) {                                  \\\n"
"      return static_cast<long>(                                                \\\n"
"          ::__FnName(__mask, static_cast<int>(__val), __offset, __width));     \\\n"
"    }                                                                          \\\n"
"  }                                                                            \\\n"
"  inline __device__ unsigned long __FnName(                                    \\\n"
"      unsigned int __mask, unsigned long __val, __Type __offset,               \\\n"
"      int __width = warpSize) {                                                \\\n"
"    return static_cast<unsigned long>(                                         \\\n"
"        ::__FnName(__mask, static_cast<long>(__val), __offset, __width));      \\\n"
"  }                                                                            \\\n"
"  inline __device__ double __FnName(unsigned int __mask, double __val,         \\\n"
"                                    __Type __offset, int __width = warpSize) { \\\n"
"    long long __tmp;                                                           \\\n"
"    _Static_assert(sizeof(__tmp) == sizeof(__val));                            \\\n"
"    memcpy(&__tmp, &__val, sizeof(__val));                                     \\\n"
"    __tmp = ::__FnName(__mask, __tmp, __offset, __width);                      \\\n"
"    double __ret;                                                              \\\n"
"    memcpy(&__ret, &__tmp, sizeof(__ret));                                     \\\n"
"    return __ret;                                                              \\\n"
"  }\n"
"__MAKE_SYNC_SHUFFLES(__shfl_sync, __nvvm_shfl_sync_idx_i32,\n"
"                     __nvvm_shfl_sync_idx_f32, 0x1f, int);\n"
"// We use 0 rather than 31 as our mask, because shfl.up applies to lanes >=\n"
"// maxLane.\n"
"__MAKE_SYNC_SHUFFLES(__shfl_up_sync, __nvvm_shfl_sync_up_i32,\n"
"                     __nvvm_shfl_sync_up_f32, 0, unsigned int);\n"
"__MAKE_SYNC_SHUFFLES(__shfl_down_sync, __nvvm_shfl_sync_down_i32,\n"
"                     __nvvm_shfl_sync_down_f32, 0x1f, unsigned int);\n"
"__MAKE_SYNC_SHUFFLES(__shfl_xor_sync, __nvvm_shfl_sync_bfly_i32,\n"
"                     __nvvm_shfl_sync_bfly_f32, 0x1f, int);\n"
"#pragma pop_macro(\"__MAKE_SYNC_SHUFFLES\")\n"
"\n"
"inline __device__ void __syncwarp(unsigned int mask = 0xffffffff) {\n"
"  return __nvvm_bar_warp_sync(mask);\n"
"}\n"
"\n"
"inline __device__ void __barrier_sync(unsigned int id) {\n"
"  __nvvm_barrier_sync(id);\n"
"}\n"
"\n"
"inline __device__ void __barrier_sync_count(unsigned int id,\n"
"                                            unsigned int count) {\n"
"  __nvvm_barrier_sync_cnt(id, count);\n"
"}\n"
"\n"
"inline __device__ int __all_sync(unsigned int mask, int pred) {\n"
"  return __nvvm_vote_all_sync(mask, pred);\n"
"}\n"
"\n"
"inline __device__ int __any_sync(unsigned int mask, int pred) {\n"
"  return __nvvm_vote_any_sync(mask, pred);\n"
"}\n"
"\n"
"inline __device__ int __uni_sync(unsigned int mask, int pred) {\n"
"  return __nvvm_vote_uni_sync(mask, pred);\n"
"}\n"
"\n"
"inline __device__ unsigned int __ballot_sync(unsigned int mask, int pred) {\n"
"  return __nvvm_vote_ballot_sync(mask, pred);\n"
"}\n"
"\n"
"inline __device__ unsigned int __activemask() { return __nvvm_vote_ballot(1); }\n"
"\n"
"inline __device__ unsigned int __fns(unsigned mask, unsigned base, int offset) {\n"
"  return __nvvm_fns(mask, base, offset);\n"
"}\n"
"\n"
"#endif // !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 300\n"
"\n"
"// Define __match* builtins CUDA-9 headers expect to see.\n"
"#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 700\n"
"inline __device__ unsigned int __match32_any_sync(unsigned int mask,\n"
"                                                  unsigned int value) {\n"
"  return __nvvm_match_any_sync_i32(mask, value);\n"
"}\n"
"\n"
"inline __device__ unsigned long long\n"
"__match64_any_sync(unsigned int mask, unsigned long long value) {\n"
"  return __nvvm_match_any_sync_i64(mask, value);\n"
"}\n"
"\n"
"inline __device__ unsigned int\n"
"__match32_all_sync(unsigned int mask, unsigned int value, int *pred) {\n"
"  return __nvvm_match_all_sync_i32p(mask, value, pred);\n"
"}\n"
"\n"
"inline __device__ unsigned long long\n"
"__match64_all_sync(unsigned int mask, unsigned long long value, int *pred) {\n"
"  return __nvvm_match_all_sync_i64p(mask, value, pred);\n"
"}\n"
"#include \"crt/sm_70_rt.hpp\"\n"
"\n"
"#endif // !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 700\n"
"#endif // __CUDA_VERSION >= 9000\n"
"\n"
"// sm_32 intrinsics: __ldg and __funnelshift_{l,lc,r,rc}.\n"
"\n"
"// Prevent the vanilla sm_32 intrinsics header from being included.\n"
"#define __SM_32_INTRINSICS_H__\n"
"#define __SM_32_INTRINSICS_HPP__\n"
"\n"
"#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 320\n"
"\n"
"inline __device__ char __ldg(const char *ptr) { return __nvvm_ldg_c(ptr); }\n"
"inline __device__ short __ldg(const short *ptr) { return __nvvm_ldg_s(ptr); }\n"
"inline __device__ int __ldg(const int *ptr) { return __nvvm_ldg_i(ptr); }\n"
"inline __device__ long __ldg(const long *ptr) { return __nvvm_ldg_l(ptr); }\n"
"inline __device__ long long __ldg(const long long *ptr) {\n"
"  return __nvvm_ldg_ll(ptr);\n"
"}\n"
"inline __device__ unsigned char __ldg(const unsigned char *ptr) {\n"
"  return __nvvm_ldg_uc(ptr);\n"
"}\n"
"inline __device__ signed char __ldg(const signed char *ptr) {\n"
"  return __nvvm_ldg_uc((const unsigned char *)ptr);\n"
"}\n"
"inline __device__ unsigned short __ldg(const unsigned short *ptr) {\n"
"  return __nvvm_ldg_us(ptr);\n"
"}\n"
"inline __device__ unsigned int __ldg(const unsigned int *ptr) {\n"
"  return __nvvm_ldg_ui(ptr);\n"
"}\n"
"inline __device__ unsigned long __ldg(const unsigned long *ptr) {\n"
"  return __nvvm_ldg_ul(ptr);\n"
"}\n"
"inline __device__ unsigned long long __ldg(const unsigned long long *ptr) {\n"
"  return __nvvm_ldg_ull(ptr);\n"
"}\n"
"inline __device__ float __ldg(const float *ptr) { return __nvvm_ldg_f(ptr); }\n"
"inline __device__ double __ldg(const double *ptr) { return __nvvm_ldg_d(ptr); }\n"
"\n"
"inline __device__ char2 __ldg(const char2 *ptr) {\n"
"  typedef char c2 __attribute__((ext_vector_type(2)));\n"
"  // We can assume that ptr is aligned at least to char2's alignment, but the\n"
"  // load will assume that ptr is aligned to char2's alignment.  This is only\n"
"  // safe if alignof(c2) <= alignof(char2).\n"
"  c2 rv = __nvvm_ldg_c2(reinterpret_cast<const c2 *>(ptr));\n"
"  char2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"inline __device__ char4 __ldg(const char4 *ptr) {\n"
"  typedef char c4 __attribute__((ext_vector_type(4)));\n"
"  c4 rv = __nvvm_ldg_c4(reinterpret_cast<const c4 *>(ptr));\n"
"  char4 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  ret.z = rv[2];\n"
"  ret.w = rv[3];\n"
"  return ret;\n"
"}\n"
"inline __device__ short2 __ldg(const short2 *ptr) {\n"
"  typedef short s2 __attribute__((ext_vector_type(2)));\n"
"  s2 rv = __nvvm_ldg_s2(reinterpret_cast<const s2 *>(ptr));\n"
"  short2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"inline __device__ short4 __ldg(const short4 *ptr) {\n"
"  typedef short s4 __attribute__((ext_vector_type(4)));\n"
"  s4 rv = __nvvm_ldg_s4(reinterpret_cast<const s4 *>(ptr));\n"
"  short4 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  ret.z = rv[2];\n"
"  ret.w = rv[3];\n"
"  return ret;\n"
"}\n"
"inline __device__ int2 __ldg(const int2 *ptr) {\n"
"  typedef int i2 __attribute__((ext_vector_type(2)));\n"
"  i2 rv = __nvvm_ldg_i2(reinterpret_cast<const i2 *>(ptr));\n"
"  int2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"inline __device__ int4 __ldg(const int4 *ptr) {\n"
"  typedef int i4 __attribute__((ext_vector_type(4)));\n"
"  i4 rv = __nvvm_ldg_i4(reinterpret_cast<const i4 *>(ptr));\n"
"  int4 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  ret.z = rv[2];\n"
"  ret.w = rv[3];\n"
"  return ret;\n"
"}\n"
"inline __device__ longlong2 __ldg(const longlong2 *ptr) {\n"
"  typedef long long ll2 __attribute__((ext_vector_type(2)));\n"
"  ll2 rv = __nvvm_ldg_ll2(reinterpret_cast<const ll2 *>(ptr));\n"
"  longlong2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"\n"
"inline __device__ uchar2 __ldg(const uchar2 *ptr) {\n"
"  typedef unsigned char uc2 __attribute__((ext_vector_type(2)));\n"
"  uc2 rv = __nvvm_ldg_uc2(reinterpret_cast<const uc2 *>(ptr));\n"
"  uchar2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"inline __device__ uchar4 __ldg(const uchar4 *ptr) {\n"
"  typedef unsigned char uc4 __attribute__((ext_vector_type(4)));\n"
"  uc4 rv = __nvvm_ldg_uc4(reinterpret_cast<const uc4 *>(ptr));\n"
"  uchar4 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  ret.z = rv[2];\n"
"  ret.w = rv[3];\n"
"  return ret;\n"
"}\n"
"inline __device__ ushort2 __ldg(const ushort2 *ptr) {\n"
"  typedef unsigned short us2 __attribute__((ext_vector_type(2)));\n"
"  us2 rv = __nvvm_ldg_us2(reinterpret_cast<const us2 *>(ptr));\n"
"  ushort2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"inline __device__ ushort4 __ldg(const ushort4 *ptr) {\n"
"  typedef unsigned short us4 __attribute__((ext_vector_type(4)));\n"
"  us4 rv = __nvvm_ldg_us4(reinterpret_cast<const us4 *>(ptr));\n"
"  ushort4 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  ret.z = rv[2];\n"
"  ret.w = rv[3];\n"
"  return ret;\n"
"}\n"
"inline __device__ uint2 __ldg(const uint2 *ptr) {\n"
"  typedef unsigned int ui2 __attribute__((ext_vector_type(2)));\n"
"  ui2 rv = __nvvm_ldg_ui2(reinterpret_cast<const ui2 *>(ptr));\n"
"  uint2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"inline __device__ uint4 __ldg(const uint4 *ptr) {\n"
"  typedef unsigned int ui4 __attribute__((ext_vector_type(4)));\n"
"  ui4 rv = __nvvm_ldg_ui4(reinterpret_cast<const ui4 *>(ptr));\n"
"  uint4 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  ret.z = rv[2];\n"
"  ret.w = rv[3];\n"
"  return ret;\n"
"}\n"
"inline __device__ ulonglong2 __ldg(const ulonglong2 *ptr) {\n"
"  typedef unsigned long long ull2 __attribute__((ext_vector_type(2)));\n"
"  ull2 rv = __nvvm_ldg_ull2(reinterpret_cast<const ull2 *>(ptr));\n"
"  ulonglong2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"\n"
"inline __device__ float2 __ldg(const float2 *ptr) {\n"
"  typedef float f2 __attribute__((ext_vector_type(2)));\n"
"  f2 rv = __nvvm_ldg_f2(reinterpret_cast<const f2 *>(ptr));\n"
"  float2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"inline __device__ float4 __ldg(const float4 *ptr) {\n"
"  typedef float f4 __attribute__((ext_vector_type(4)));\n"
"  f4 rv = __nvvm_ldg_f4(reinterpret_cast<const f4 *>(ptr));\n"
"  float4 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  ret.z = rv[2];\n"
"  ret.w = rv[3];\n"
"  return ret;\n"
"}\n"
"inline __device__ double2 __ldg(const double2 *ptr) {\n"
"  typedef double d2 __attribute__((ext_vector_type(2)));\n"
"  d2 rv = __nvvm_ldg_d2(reinterpret_cast<const d2 *>(ptr));\n"
"  double2 ret;\n"
"  ret.x = rv[0];\n"
"  ret.y = rv[1];\n"
"  return ret;\n"
"}\n"
"\n"
"// TODO: Implement these as intrinsics, so the backend can work its magic on\n"
"// these.  Alternatively, we could implement these as plain C and try to get\n"
"// llvm to recognize the relevant patterns.\n"
"inline __device__ unsigned __funnelshift_l(unsigned low32, unsigned high32,\n"
"                                           unsigned shiftWidth) {\n"
"  unsigned result;\n"
"  asm(\"shf.l.wrap.b32 %0, %1, %2, %3;\"\n"
"      : \"=r\"(result)\n"
"      : \"r\"(low32), \"r\"(high32), \"r\"(shiftWidth));\n"
"  return result;\n"
"}\n"
"inline __device__ unsigned __funnelshift_lc(unsigned low32, unsigned high32,\n"
"                                            unsigned shiftWidth) {\n"
"  unsigned result;\n"
"  asm(\"shf.l.clamp.b32 %0, %1, %2, %3;\"\n"
"      : \"=r\"(result)\n"
"      : \"r\"(low32), \"r\"(high32), \"r\"(shiftWidth));\n"
"  return result;\n"
"}\n"
"inline __device__ unsigned __funnelshift_r(unsigned low32, unsigned high32,\n"
"                                           unsigned shiftWidth) {\n"
"  unsigned result;\n"
"  asm(\"shf.r.wrap.b32 %0, %1, %2, %3;\"\n"
"      : \"=r\"(result)\n"
"      : \"r\"(low32), \"r\"(high32), \"r\"(shiftWidth));\n"
"  return result;\n"
"}\n"
"inline __device__ unsigned __funnelshift_rc(unsigned low32, unsigned high32,\n"
"                                            unsigned shiftWidth) {\n"
"  unsigned ret;\n"
"  asm(\"shf.r.clamp.b32 %0, %1, %2, %3;\"\n"
"      : \"=r\"(ret)\n"
"      : \"r\"(low32), \"r\"(high32), \"r\"(shiftWidth));\n"
"  return ret;\n"
"}\n"
"\n"
"#endif // !defined(__CUDA_ARCH__) || __CUDA_ARCH__ >= 320\n"
"\n"
"#endif // defined(__CLANG_CUDA_INTRINSICS_H__)\n"
"" } , 
 { "/builtins/__clang_cuda_libdevice_declares.h" , "/*===-- __clang_cuda_libdevice_declares.h - decls for libdevice functions --===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __CLANG_CUDA_LIBDEVICE_DECLARES_H__\n"
"#define __CLANG_CUDA_LIBDEVICE_DECLARES_H__\n"
"\n"
"extern \"C\" {\n"
"\n"
"__device__ int __nv_abs(int __a);\n"
"__device__ double __nv_acos(double __a);\n"
"__device__ float __nv_acosf(float __a);\n"
"__device__ double __nv_acosh(double __a);\n"
"__device__ float __nv_acoshf(float __a);\n"
"__device__ double __nv_asin(double __a);\n"
"__device__ float __nv_asinf(float __a);\n"
"__device__ double __nv_asinh(double __a);\n"
"__device__ float __nv_asinhf(float __a);\n"
"__device__ double __nv_atan2(double __a, double __b);\n"
"__device__ float __nv_atan2f(float __a, float __b);\n"
"__device__ double __nv_atan(double __a);\n"
"__device__ float __nv_atanf(float __a);\n"
"__device__ double __nv_atanh(double __a);\n"
"__device__ float __nv_atanhf(float __a);\n"
"__device__ int __nv_brev(int __a);\n"
"__device__ long long __nv_brevll(long long __a);\n"
"__device__ int __nv_byte_perm(int __a, int __b, int __c);\n"
"__device__ double __nv_cbrt(double __a);\n"
"__device__ float __nv_cbrtf(float __a);\n"
"__device__ double __nv_ceil(double __a);\n"
"__device__ float __nv_ceilf(float __a);\n"
"__device__ int __nv_clz(int __a);\n"
"__device__ int __nv_clzll(long long __a);\n"
"__device__ double __nv_copysign(double __a, double __b);\n"
"__device__ float __nv_copysignf(float __a, float __b);\n"
"__device__ double __nv_cos(double __a);\n"
"__device__ float __nv_cosf(float __a);\n"
"__device__ double __nv_cosh(double __a);\n"
"__device__ float __nv_coshf(float __a);\n"
"__device__ double __nv_cospi(double __a);\n"
"__device__ float __nv_cospif(float __a);\n"
"__device__ double __nv_cyl_bessel_i0(double __a);\n"
"__device__ float __nv_cyl_bessel_i0f(float __a);\n"
"__device__ double __nv_cyl_bessel_i1(double __a);\n"
"__device__ float __nv_cyl_bessel_i1f(float __a);\n"
"__device__ double __nv_dadd_rd(double __a, double __b);\n"
"__device__ double __nv_dadd_rn(double __a, double __b);\n"
"__device__ double __nv_dadd_ru(double __a, double __b);\n"
"__device__ double __nv_dadd_rz(double __a, double __b);\n"
"__device__ double __nv_ddiv_rd(double __a, double __b);\n"
"__device__ double __nv_ddiv_rn(double __a, double __b);\n"
"__device__ double __nv_ddiv_ru(double __a, double __b);\n"
"__device__ double __nv_ddiv_rz(double __a, double __b);\n"
"__device__ double __nv_dmul_rd(double __a, double __b);\n"
"__device__ double __nv_dmul_rn(double __a, double __b);\n"
"__device__ double __nv_dmul_ru(double __a, double __b);\n"
"__device__ double __nv_dmul_rz(double __a, double __b);\n"
"__device__ float __nv_double2float_rd(double __a);\n"
"__device__ float __nv_double2float_rn(double __a);\n"
"__device__ float __nv_double2float_ru(double __a);\n"
"__device__ float __nv_double2float_rz(double __a);\n"
"__device__ int __nv_double2hiint(double __a);\n"
"__device__ int __nv_double2int_rd(double __a);\n"
"__device__ int __nv_double2int_rn(double __a);\n"
"__device__ int __nv_double2int_ru(double __a);\n"
"__device__ int __nv_double2int_rz(double __a);\n"
"__device__ long long __nv_double2ll_rd(double __a);\n"
"__device__ long long __nv_double2ll_rn(double __a);\n"
"__device__ long long __nv_double2ll_ru(double __a);\n"
"__device__ long long __nv_double2ll_rz(double __a);\n"
"__device__ int __nv_double2loint(double __a);\n"
"__device__ unsigned int __nv_double2uint_rd(double __a);\n"
"__device__ unsigned int __nv_double2uint_rn(double __a);\n"
"__device__ unsigned int __nv_double2uint_ru(double __a);\n"
"__device__ unsigned int __nv_double2uint_rz(double __a);\n"
"__device__ unsigned long long __nv_double2ull_rd(double __a);\n"
"__device__ unsigned long long __nv_double2ull_rn(double __a);\n"
"__device__ unsigned long long __nv_double2ull_ru(double __a);\n"
"__device__ unsigned long long __nv_double2ull_rz(double __a);\n"
"__device__ unsigned long long __nv_double_as_longlong(double __a);\n"
"__device__ double __nv_drcp_rd(double __a);\n"
"__device__ double __nv_drcp_rn(double __a);\n"
"__device__ double __nv_drcp_ru(double __a);\n"
"__device__ double __nv_drcp_rz(double __a);\n"
"__device__ double __nv_dsqrt_rd(double __a);\n"
"__device__ double __nv_dsqrt_rn(double __a);\n"
"__device__ double __nv_dsqrt_ru(double __a);\n"
"__device__ double __nv_dsqrt_rz(double __a);\n"
"__device__ double __nv_dsub_rd(double __a, double __b);\n"
"__device__ double __nv_dsub_rn(double __a, double __b);\n"
"__device__ double __nv_dsub_ru(double __a, double __b);\n"
"__device__ double __nv_dsub_rz(double __a, double __b);\n"
"__device__ double __nv_erfc(double __a);\n"
"__device__ float __nv_erfcf(float __a);\n"
"__device__ double __nv_erfcinv(double __a);\n"
"__device__ float __nv_erfcinvf(float __a);\n"
"__device__ double __nv_erfcx(double __a);\n"
"__device__ float __nv_erfcxf(float __a);\n"
"__device__ double __nv_erf(double __a);\n"
"__device__ float __nv_erff(float __a);\n"
"__device__ double __nv_erfinv(double __a);\n"
"__device__ float __nv_erfinvf(float __a);\n"
"__device__ double __nv_exp10(double __a);\n"
"__device__ float __nv_exp10f(float __a);\n"
"__device__ double __nv_exp2(double __a);\n"
"__device__ float __nv_exp2f(float __a);\n"
"__device__ double __nv_exp(double __a);\n"
"__device__ float __nv_expf(float __a);\n"
"__device__ double __nv_expm1(double __a);\n"
"__device__ float __nv_expm1f(float __a);\n"
"__device__ double __nv_fabs(double __a);\n"
"__device__ float __nv_fabsf(float __a);\n"
"__device__ float __nv_fadd_rd(float __a, float __b);\n"
"__device__ float __nv_fadd_rn(float __a, float __b);\n"
"__device__ float __nv_fadd_ru(float __a, float __b);\n"
"__device__ float __nv_fadd_rz(float __a, float __b);\n"
"__device__ float __nv_fast_cosf(float __a);\n"
"__device__ float __nv_fast_exp10f(float __a);\n"
"__device__ float __nv_fast_expf(float __a);\n"
"__device__ float __nv_fast_fdividef(float __a, float __b);\n"
"__device__ float __nv_fast_log10f(float __a);\n"
"__device__ float __nv_fast_log2f(float __a);\n"
"__device__ float __nv_fast_logf(float __a);\n"
"__device__ float __nv_fast_powf(float __a, float __b);\n"
"__device__ void __nv_fast_sincosf(float __a, float *__sptr, float *__cptr);\n"
"__device__ float __nv_fast_sinf(float __a);\n"
"__device__ float __nv_fast_tanf(float __a);\n"
"__device__ double __nv_fdim(double __a, double __b);\n"
"__device__ float __nv_fdimf(float __a, float __b);\n"
"__device__ float __nv_fdiv_rd(float __a, float __b);\n"
"__device__ float __nv_fdiv_rn(float __a, float __b);\n"
"__device__ float __nv_fdiv_ru(float __a, float __b);\n"
"__device__ float __nv_fdiv_rz(float __a, float __b);\n"
"__device__ int __nv_ffs(int __a);\n"
"__device__ int __nv_ffsll(long long __a);\n"
"__device__ int __nv_finitef(float __a);\n"
"__device__ unsigned short __nv_float2half_rn(float __a);\n"
"__device__ int __nv_float2int_rd(float __a);\n"
"__device__ int __nv_float2int_rn(float __a);\n"
"__device__ int __nv_float2int_ru(float __a);\n"
"__device__ int __nv_float2int_rz(float __a);\n"
"__device__ long long __nv_float2ll_rd(float __a);\n"
"__device__ long long __nv_float2ll_rn(float __a);\n"
"__device__ long long __nv_float2ll_ru(float __a);\n"
"__device__ long long __nv_float2ll_rz(float __a);\n"
"__device__ unsigned int __nv_float2uint_rd(float __a);\n"
"__device__ unsigned int __nv_float2uint_rn(float __a);\n"
"__device__ unsigned int __nv_float2uint_ru(float __a);\n"
"__device__ unsigned int __nv_float2uint_rz(float __a);\n"
"__device__ unsigned long long __nv_float2ull_rd(float __a);\n"
"__device__ unsigned long long __nv_float2ull_rn(float __a);\n"
"__device__ unsigned long long __nv_float2ull_ru(float __a);\n"
"__device__ unsigned long long __nv_float2ull_rz(float __a);\n"
"__device__ int __nv_float_as_int(float __a);\n"
"__device__ unsigned int __nv_float_as_uint(float __a);\n"
"__device__ double __nv_floor(double __a);\n"
"__device__ float __nv_floorf(float __a);\n"
"__device__ double __nv_fma(double __a, double __b, double __c);\n"
"__device__ float __nv_fmaf(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_ieee_rd(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_ieee_rn(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_ieee_ru(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_ieee_rz(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_rd(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_rn(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_ru(float __a, float __b, float __c);\n"
"__device__ float __nv_fmaf_rz(float __a, float __b, float __c);\n"
"__device__ double __nv_fma_rd(double __a, double __b, double __c);\n"
"__device__ double __nv_fma_rn(double __a, double __b, double __c);\n"
"__device__ double __nv_fma_ru(double __a, double __b, double __c);\n"
"__device__ double __nv_fma_rz(double __a, double __b, double __c);\n"
"__device__ double __nv_fmax(double __a, double __b);\n"
"__device__ float __nv_fmaxf(float __a, float __b);\n"
"__device__ double __nv_fmin(double __a, double __b);\n"
"__device__ float __nv_fminf(float __a, float __b);\n"
"__device__ double __nv_fmod(double __a, double __b);\n"
"__device__ float __nv_fmodf(float __a, float __b);\n"
"__device__ float __nv_fmul_rd(float __a, float __b);\n"
"__device__ float __nv_fmul_rn(float __a, float __b);\n"
"__device__ float __nv_fmul_ru(float __a, float __b);\n"
"__device__ float __nv_fmul_rz(float __a, float __b);\n"
"__device__ float __nv_frcp_rd(float __a);\n"
"__device__ float __nv_frcp_rn(float __a);\n"
"__device__ float __nv_frcp_ru(float __a);\n"
"__device__ float __nv_frcp_rz(float __a);\n"
"__device__ double __nv_frexp(double __a, int *__b);\n"
"__device__ float __nv_frexpf(float __a, int *__b);\n"
"__device__ float __nv_frsqrt_rn(float __a);\n"
"__device__ float __nv_fsqrt_rd(float __a);\n"
"__device__ float __nv_fsqrt_rn(float __a);\n"
"__device__ float __nv_fsqrt_ru(float __a);\n"
"__device__ float __nv_fsqrt_rz(float __a);\n"
"__device__ float __nv_fsub_rd(float __a, float __b);\n"
"__device__ float __nv_fsub_rn(float __a, float __b);\n"
"__device__ float __nv_fsub_ru(float __a, float __b);\n"
"__device__ float __nv_fsub_rz(float __a, float __b);\n"
"__device__ int __nv_hadd(int __a, int __b);\n"
"__device__ float __nv_half2float(unsigned short __h);\n"
"__device__ double __nv_hiloint2double(int __a, int __b);\n"
"__device__ double __nv_hypot(double __a, double __b);\n"
"__device__ float __nv_hypotf(float __a, float __b);\n"
"__device__ int __nv_ilogb(double __a);\n"
"__device__ int __nv_ilogbf(float __a);\n"
"__device__ double __nv_int2double_rn(int __a);\n"
"__device__ float __nv_int2float_rd(int __a);\n"
"__device__ float __nv_int2float_rn(int __a);\n"
"__device__ float __nv_int2float_ru(int __a);\n"
"__device__ float __nv_int2float_rz(int __a);\n"
"__device__ float __nv_int_as_float(int __a);\n"
"__device__ int __nv_isfinited(double __a);\n"
"__device__ int __nv_isinfd(double __a);\n"
"__device__ int __nv_isinff(float __a);\n"
"__device__ int __nv_isnand(double __a);\n"
"__device__ int __nv_isnanf(float __a);\n"
"__device__ double __nv_j0(double __a);\n"
"__device__ float __nv_j0f(float __a);\n"
"__device__ double __nv_j1(double __a);\n"
"__device__ float __nv_j1f(float __a);\n"
"__device__ float __nv_jnf(int __a, float __b);\n"
"__device__ double __nv_jn(int __a, double __b);\n"
"__device__ double __nv_ldexp(double __a, int __b);\n"
"__device__ float __nv_ldexpf(float __a, int __b);\n"
"__device__ double __nv_lgamma(double __a);\n"
"__device__ float __nv_lgammaf(float __a);\n"
"__device__ double __nv_ll2double_rd(long long __a);\n"
"__device__ double __nv_ll2double_rn(long long __a);\n"
"__device__ double __nv_ll2double_ru(long long __a);\n"
"__device__ double __nv_ll2double_rz(long long __a);\n"
"__device__ float __nv_ll2float_rd(long long __a);\n"
"__device__ float __nv_ll2float_rn(long long __a);\n"
"__device__ float __nv_ll2float_ru(long long __a);\n"
"__device__ float __nv_ll2float_rz(long long __a);\n"
"__device__ long long __nv_llabs(long long __a);\n"
"__device__ long long __nv_llmax(long long __a, long long __b);\n"
"__device__ long long __nv_llmin(long long __a, long long __b);\n"
"__device__ long long __nv_llrint(double __a);\n"
"__device__ long long __nv_llrintf(float __a);\n"
"__device__ long long __nv_llround(double __a);\n"
"__device__ long long __nv_llroundf(float __a);\n"
"__device__ double __nv_log10(double __a);\n"
"__device__ float __nv_log10f(float __a);\n"
"__device__ double __nv_log1p(double __a);\n"
"__device__ float __nv_log1pf(float __a);\n"
"__device__ double __nv_log2(double __a);\n"
"__device__ float __nv_log2f(float __a);\n"
"__device__ double __nv_logb(double __a);\n"
"__device__ float __nv_logbf(float __a);\n"
"__device__ double __nv_log(double __a);\n"
"__device__ float __nv_logf(float __a);\n"
"__device__ double __nv_longlong_as_double(long long __a);\n"
"__device__ int __nv_max(int __a, int __b);\n"
"__device__ int __nv_min(int __a, int __b);\n"
"__device__ double __nv_modf(double __a, double *__b);\n"
"__device__ float __nv_modff(float __a, float *__b);\n"
"__device__ int __nv_mul24(int __a, int __b);\n"
"__device__ long long __nv_mul64hi(long long __a, long long __b);\n"
"__device__ int __nv_mulhi(int __a, int __b);\n"
"__device__ double __nv_nan(const signed char *__a);\n"
"__device__ float __nv_nanf(const signed char *__a);\n"
"__device__ double __nv_nearbyint(double __a);\n"
"__device__ float __nv_nearbyintf(float __a);\n"
"__device__ double __nv_nextafter(double __a, double __b);\n"
"__device__ float __nv_nextafterf(float __a, float __b);\n"
"__device__ double __nv_norm3d(double __a, double __b, double __c);\n"
"__device__ float __nv_norm3df(float __a, float __b, float __c);\n"
"__device__ double __nv_norm4d(double __a, double __b, double __c, double __d);\n"
"__device__ float __nv_norm4df(float __a, float __b, float __c, float __d);\n"
"__device__ double __nv_normcdf(double __a);\n"
"__device__ float __nv_normcdff(float __a);\n"
"__device__ double __nv_normcdfinv(double __a);\n"
"__device__ float __nv_normcdfinvf(float __a);\n"
"__device__ float __nv_normf(int __a, const float *__b);\n"
"__device__ double __nv_norm(int __a, const double *__b);\n"
"__device__ int __nv_popc(int __a);\n"
"__device__ int __nv_popcll(long long __a);\n"
"__device__ double __nv_pow(double __a, double __b);\n"
"__device__ float __nv_powf(float __a, float __b);\n"
"__device__ double __nv_powi(double __a, int __b);\n"
"__device__ float __nv_powif(float __a, int __b);\n"
"__device__ double __nv_rcbrt(double __a);\n"
"__device__ float __nv_rcbrtf(float __a);\n"
"__device__ double __nv_rcp64h(double __a);\n"
"__device__ double __nv_remainder(double __a, double __b);\n"
"__device__ float __nv_remainderf(float __a, float __b);\n"
"__device__ double __nv_remquo(double __a, double __b, int *__c);\n"
"__device__ float __nv_remquof(float __a, float __b, int *__c);\n"
"__device__ int __nv_rhadd(int __a, int __b);\n"
"__device__ double __nv_rhypot(double __a, double __b);\n"
"__device__ float __nv_rhypotf(float __a, float __b);\n"
"__device__ double __nv_rint(double __a);\n"
"__device__ float __nv_rintf(float __a);\n"
"__device__ double __nv_rnorm3d(double __a, double __b, double __c);\n"
"__device__ float __nv_rnorm3df(float __a, float __b, float __c);\n"
"__device__ double __nv_rnorm4d(double __a, double __b, double __c, double __d);\n"
"__device__ float __nv_rnorm4df(float __a, float __b, float __c, float __d);\n"
"__device__ float __nv_rnormf(int __a, const float *__b);\n"
"__device__ double __nv_rnorm(int __a, const double *__b);\n"
"__device__ double __nv_round(double __a);\n"
"__device__ float __nv_roundf(float __a);\n"
"__device__ double __nv_rsqrt(double __a);\n"
"__device__ float __nv_rsqrtf(float __a);\n"
"__device__ int __nv_sad(int __a, int __b, int __c);\n"
"__device__ float __nv_saturatef(float __a);\n"
"__device__ double __nv_scalbn(double __a, int __b);\n"
"__device__ float __nv_scalbnf(float __a, int __b);\n"
"__device__ int __nv_signbitd(double __a);\n"
"__device__ int __nv_signbitf(float __a);\n"
"__device__ void __nv_sincos(double __a, double *__b, double *__c);\n"
"__device__ void __nv_sincosf(float __a, float *__b, float *__c);\n"
"__device__ void __nv_sincospi(double __a, double *__b, double *__c);\n"
"__device__ void __nv_sincospif(float __a, float *__b, float *__c);\n"
"__device__ double __nv_sin(double __a);\n"
"__device__ float __nv_sinf(float __a);\n"
"__device__ double __nv_sinh(double __a);\n"
"__device__ float __nv_sinhf(float __a);\n"
"__device__ double __nv_sinpi(double __a);\n"
"__device__ float __nv_sinpif(float __a);\n"
"__device__ double __nv_sqrt(double __a);\n"
"__device__ float __nv_sqrtf(float __a);\n"
"__device__ double __nv_tan(double __a);\n"
"__device__ float __nv_tanf(float __a);\n"
"__device__ double __nv_tanh(double __a);\n"
"__device__ float __nv_tanhf(float __a);\n"
"__device__ double __nv_tgamma(double __a);\n"
"__device__ float __nv_tgammaf(float __a);\n"
"__device__ double __nv_trunc(double __a);\n"
"__device__ float __nv_truncf(float __a);\n"
"__device__ int __nv_uhadd(unsigned int __a, unsigned int __b);\n"
"__device__ double __nv_uint2double_rn(unsigned int __i);\n"
"__device__ float __nv_uint2float_rd(unsigned int __a);\n"
"__device__ float __nv_uint2float_rn(unsigned int __a);\n"
"__device__ float __nv_uint2float_ru(unsigned int __a);\n"
"__device__ float __nv_uint2float_rz(unsigned int __a);\n"
"__device__ float __nv_uint_as_float(unsigned int __a);\n"
"__device__ double __nv_ull2double_rd(unsigned long long __a);\n"
"__device__ double __nv_ull2double_rn(unsigned long long __a);\n"
"__device__ double __nv_ull2double_ru(unsigned long long __a);\n"
"__device__ double __nv_ull2double_rz(unsigned long long __a);\n"
"__device__ float __nv_ull2float_rd(unsigned long long __a);\n"
"__device__ float __nv_ull2float_rn(unsigned long long __a);\n"
"__device__ float __nv_ull2float_ru(unsigned long long __a);\n"
"__device__ float __nv_ull2float_rz(unsigned long long __a);\n"
"__device__ unsigned long long __nv_ullmax(unsigned long long __a,\n"
"                                          unsigned long long __b);\n"
"__device__ unsigned long long __nv_ullmin(unsigned long long __a,\n"
"                                          unsigned long long __b);\n"
"__device__ unsigned int __nv_umax(unsigned int __a, unsigned int __b);\n"
"__device__ unsigned int __nv_umin(unsigned int __a, unsigned int __b);\n"
"__device__ unsigned int __nv_umul24(unsigned int __a, unsigned int __b);\n"
"__device__ unsigned long long __nv_umul64hi(unsigned long long __a,\n"
"                                            unsigned long long __b);\n"
"__device__ unsigned int __nv_umulhi(unsigned int __a, unsigned int __b);\n"
"__device__ unsigned int __nv_urhadd(unsigned int __a, unsigned int __b);\n"
"__device__ unsigned int __nv_usad(unsigned int __a, unsigned int __b,\n"
"                                  unsigned int __c);\n"
"#if CUDA_VERSION >= 9000 && CUDA_VERSION < 9020\n"
"__device__ int __nv_vabs2(int __a);\n"
"__device__ int __nv_vabs4(int __a);\n"
"__device__ int __nv_vabsdiffs2(int __a, int __b);\n"
"__device__ int __nv_vabsdiffs4(int __a, int __b);\n"
"__device__ int __nv_vabsdiffu2(int __a, int __b);\n"
"__device__ int __nv_vabsdiffu4(int __a, int __b);\n"
"__device__ int __nv_vabsss2(int __a);\n"
"__device__ int __nv_vabsss4(int __a);\n"
"__device__ int __nv_vadd2(int __a, int __b);\n"
"__device__ int __nv_vadd4(int __a, int __b);\n"
"__device__ int __nv_vaddss2(int __a, int __b);\n"
"__device__ int __nv_vaddss4(int __a, int __b);\n"
"__device__ int __nv_vaddus2(int __a, int __b);\n"
"__device__ int __nv_vaddus4(int __a, int __b);\n"
"__device__ int __nv_vavgs2(int __a, int __b);\n"
"__device__ int __nv_vavgs4(int __a, int __b);\n"
"__device__ int __nv_vavgu2(int __a, int __b);\n"
"__device__ int __nv_vavgu4(int __a, int __b);\n"
"__device__ int __nv_vcmpeq2(int __a, int __b);\n"
"__device__ int __nv_vcmpeq4(int __a, int __b);\n"
"__device__ int __nv_vcmpges2(int __a, int __b);\n"
"__device__ int __nv_vcmpges4(int __a, int __b);\n"
"__device__ int __nv_vcmpgeu2(int __a, int __b);\n"
"__device__ int __nv_vcmpgeu4(int __a, int __b);\n"
"__device__ int __nv_vcmpgts2(int __a, int __b);\n"
"__device__ int __nv_vcmpgts4(int __a, int __b);\n"
"__device__ int __nv_vcmpgtu2(int __a, int __b);\n"
"__device__ int __nv_vcmpgtu4(int __a, int __b);\n"
"__device__ int __nv_vcmples2(int __a, int __b);\n"
"__device__ int __nv_vcmples4(int __a, int __b);\n"
"__device__ int __nv_vcmpleu2(int __a, int __b);\n"
"__device__ int __nv_vcmpleu4(int __a, int __b);\n"
"__device__ int __nv_vcmplts2(int __a, int __b);\n"
"__device__ int __nv_vcmplts4(int __a, int __b);\n"
"__device__ int __nv_vcmpltu2(int __a, int __b);\n"
"__device__ int __nv_vcmpltu4(int __a, int __b);\n"
"__device__ int __nv_vcmpne2(int __a, int __b);\n"
"__device__ int __nv_vcmpne4(int __a, int __b);\n"
"__device__ int __nv_vhaddu2(int __a, int __b);\n"
"__device__ int __nv_vhaddu4(int __a, int __b);\n"
"__device__ int __nv_vmaxs2(int __a, int __b);\n"
"__device__ int __nv_vmaxs4(int __a, int __b);\n"
"__device__ int __nv_vmaxu2(int __a, int __b);\n"
"__device__ int __nv_vmaxu4(int __a, int __b);\n"
"__device__ int __nv_vmins2(int __a, int __b);\n"
"__device__ int __nv_vmins4(int __a, int __b);\n"
"__device__ int __nv_vminu2(int __a, int __b);\n"
"__device__ int __nv_vminu4(int __a, int __b);\n"
"__device__ int __nv_vneg2(int __a);\n"
"__device__ int __nv_vneg4(int __a);\n"
"__device__ int __nv_vnegss2(int __a);\n"
"__device__ int __nv_vnegss4(int __a);\n"
"__device__ int __nv_vsads2(int __a, int __b);\n"
"__device__ int __nv_vsads4(int __a, int __b);\n"
"__device__ int __nv_vsadu2(int __a, int __b);\n"
"__device__ int __nv_vsadu4(int __a, int __b);\n"
"__device__ int __nv_vseteq2(int __a, int __b);\n"
"__device__ int __nv_vseteq4(int __a, int __b);\n"
"__device__ int __nv_vsetges2(int __a, int __b);\n"
"__device__ int __nv_vsetges4(int __a, int __b);\n"
"__device__ int __nv_vsetgeu2(int __a, int __b);\n"
"__device__ int __nv_vsetgeu4(int __a, int __b);\n"
"__device__ int __nv_vsetgts2(int __a, int __b);\n"
"__device__ int __nv_vsetgts4(int __a, int __b);\n"
"__device__ int __nv_vsetgtu2(int __a, int __b);\n"
"__device__ int __nv_vsetgtu4(int __a, int __b);\n"
"__device__ int __nv_vsetles2(int __a, int __b);\n"
"__device__ int __nv_vsetles4(int __a, int __b);\n"
"__device__ int __nv_vsetleu2(int __a, int __b);\n"
"__device__ int __nv_vsetleu4(int __a, int __b);\n"
"__device__ int __nv_vsetlts2(int __a, int __b);\n"
"__device__ int __nv_vsetlts4(int __a, int __b);\n"
"__device__ int __nv_vsetltu2(int __a, int __b);\n"
"__device__ int __nv_vsetltu4(int __a, int __b);\n"
"__device__ int __nv_vsetne2(int __a, int __b);\n"
"__device__ int __nv_vsetne4(int __a, int __b);\n"
"__device__ int __nv_vsub2(int __a, int __b);\n"
"__device__ int __nv_vsub4(int __a, int __b);\n"
"__device__ int __nv_vsubss2(int __a, int __b);\n"
"__device__ int __nv_vsubss4(int __a, int __b);\n"
"__device__ int __nv_vsubus2(int __a, int __b);\n"
"__device__ int __nv_vsubus4(int __a, int __b);\n"
"#endif  // CUDA_VERSION\n"
"__device__ double __nv_y0(double __a);\n"
"__device__ float __nv_y0f(float __a);\n"
"__device__ double __nv_y1(double __a);\n"
"__device__ float __nv_y1f(float __a);\n"
"__device__ float __nv_ynf(int __a, float __b);\n"
"__device__ double __nv_yn(int __a, double __b);\n"
"} // extern \"C\"\n"
"#endif // __CLANG_CUDA_LIBDEVICE_DECLARES_H__\n"
"" } , 
 { "/builtins/__clang_cuda_math_forward_declares.h" , "/*===- __clang_math_forward_declares.h - Prototypes of __device__ math fns --===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#ifndef __CLANG__CUDA_MATH_FORWARD_DECLARES_H__\n"
"#define __CLANG__CUDA_MATH_FORWARD_DECLARES_H__\n"
"#ifndef __CUDA__\n"
"#error \"This file is for CUDA compilation only.\"\n"
"#endif\n"
"\n"
"// This file forward-declares of some math functions we (or the CUDA headers)\n"
"// will define later.  We need to do this, and do it before cmath is included,\n"
"// because the standard library may have constexpr math functions.  In the\n"
"// absence of a prior __device__ decl, those constexpr functions may become\n"
"// implicitly host+device.  host+device functions can't be overloaded, so that\n"
"// would preclude the use of our own __device__ overloads for these functions.\n"
"\n"
"#pragma push_macro(\"__DEVICE__\")\n"
"#define __DEVICE__                                                             \\\n"
"  static __inline__ __attribute__((always_inline)) __attribute__((device))\n"
"\n"
"__DEVICE__ double abs(double);\n"
"__DEVICE__ float abs(float);\n"
"__DEVICE__ int abs(int);\n"
"__DEVICE__ long abs(long);\n"
"__DEVICE__ long long abs(long long);\n"
"__DEVICE__ double acos(double);\n"
"__DEVICE__ float acos(float);\n"
"__DEVICE__ double acosh(double);\n"
"__DEVICE__ float acosh(float);\n"
"__DEVICE__ double asin(double);\n"
"__DEVICE__ float asin(float);\n"
"__DEVICE__ double asinh(double);\n"
"__DEVICE__ float asinh(float);\n"
"__DEVICE__ double atan2(double, double);\n"
"__DEVICE__ float atan2(float, float);\n"
"__DEVICE__ double atan(double);\n"
"__DEVICE__ float atan(float);\n"
"__DEVICE__ double atanh(double);\n"
"__DEVICE__ float atanh(float);\n"
"__DEVICE__ double cbrt(double);\n"
"__DEVICE__ float cbrt(float);\n"
"__DEVICE__ double ceil(double);\n"
"__DEVICE__ float ceil(float);\n"
"__DEVICE__ double copysign(double, double);\n"
"__DEVICE__ float copysign(float, float);\n"
"__DEVICE__ double cos(double);\n"
"__DEVICE__ float cos(float);\n"
"__DEVICE__ double cosh(double);\n"
"__DEVICE__ float cosh(float);\n"
"__DEVICE__ double erfc(double);\n"
"__DEVICE__ float erfc(float);\n"
"__DEVICE__ double erf(double);\n"
"__DEVICE__ float erf(float);\n"
"__DEVICE__ double exp2(double);\n"
"__DEVICE__ float exp2(float);\n"
"__DEVICE__ double exp(double);\n"
"__DEVICE__ float exp(float);\n"
"__DEVICE__ double expm1(double);\n"
"__DEVICE__ float expm1(float);\n"
"__DEVICE__ double fabs(double);\n"
"__DEVICE__ float fabs(float);\n"
"__DEVICE__ double fdim(double, double);\n"
"__DEVICE__ float fdim(float, float);\n"
"__DEVICE__ double floor(double);\n"
"__DEVICE__ float floor(float);\n"
"__DEVICE__ double fma(double, double, double);\n"
"__DEVICE__ float fma(float, float, float);\n"
"__DEVICE__ double fmax(double, double);\n"
"__DEVICE__ float fmax(float, float);\n"
"__DEVICE__ double fmin(double, double);\n"
"__DEVICE__ float fmin(float, float);\n"
"__DEVICE__ double fmod(double, double);\n"
"__DEVICE__ float fmod(float, float);\n"
"__DEVICE__ int fpclassify(double);\n"
"__DEVICE__ int fpclassify(float);\n"
"__DEVICE__ double frexp(double, int *);\n"
"__DEVICE__ float frexp(float, int *);\n"
"__DEVICE__ double hypot(double, double);\n"
"__DEVICE__ float hypot(float, float);\n"
"__DEVICE__ int ilogb(double);\n"
"__DEVICE__ int ilogb(float);\n"
"__DEVICE__ bool isfinite(double);\n"
"__DEVICE__ bool isfinite(float);\n"
"__DEVICE__ bool isgreater(double, double);\n"
"__DEVICE__ bool isgreaterequal(double, double);\n"
"__DEVICE__ bool isgreaterequal(float, float);\n"
"__DEVICE__ bool isgreater(float, float);\n"
"__DEVICE__ bool isinf(double);\n"
"__DEVICE__ bool isinf(float);\n"
"__DEVICE__ bool isless(double, double);\n"
"__DEVICE__ bool islessequal(double, double);\n"
"__DEVICE__ bool islessequal(float, float);\n"
"__DEVICE__ bool isless(float, float);\n"
"__DEVICE__ bool islessgreater(double, double);\n"
"__DEVICE__ bool islessgreater(float, float);\n"
"__DEVICE__ bool isnan(double);\n"
"__DEVICE__ bool isnan(float);\n"
"__DEVICE__ bool isnormal(double);\n"
"__DEVICE__ bool isnormal(float);\n"
"__DEVICE__ bool isunordered(double, double);\n"
"__DEVICE__ bool isunordered(float, float);\n"
"__DEVICE__ long labs(long);\n"
"__DEVICE__ double ldexp(double, int);\n"
"__DEVICE__ float ldexp(float, int);\n"
"__DEVICE__ double lgamma(double);\n"
"__DEVICE__ float lgamma(float);\n"
"__DEVICE__ long long llabs(long long);\n"
"__DEVICE__ long long llrint(double);\n"
"__DEVICE__ long long llrint(float);\n"
"__DEVICE__ double log10(double);\n"
"__DEVICE__ float log10(float);\n"
"__DEVICE__ double log1p(double);\n"
"__DEVICE__ float log1p(float);\n"
"__DEVICE__ double log2(double);\n"
"__DEVICE__ float log2(float);\n"
"__DEVICE__ double logb(double);\n"
"__DEVICE__ float logb(float);\n"
"__DEVICE__ double log(double);\n"
"__DEVICE__ float log(float);\n"
"__DEVICE__ long lrint(double);\n"
"__DEVICE__ long lrint(float);\n"
"__DEVICE__ long lround(double);\n"
"__DEVICE__ long lround(float);\n"
"__DEVICE__ long long llround(float); // No llround(double).\n"
"__DEVICE__ double modf(double, double *);\n"
"__DEVICE__ float modf(float, float *);\n"
"__DEVICE__ double nan(const char *);\n"
"__DEVICE__ float nanf(const char *);\n"
"__DEVICE__ double nearbyint(double);\n"
"__DEVICE__ float nearbyint(float);\n"
"__DEVICE__ double nextafter(double, double);\n"
"__DEVICE__ float nextafter(float, float);\n"
"__DEVICE__ double pow(double, double);\n"
"__DEVICE__ double pow(double, int);\n"
"__DEVICE__ float pow(float, float);\n"
"__DEVICE__ float pow(float, int);\n"
"__DEVICE__ double remainder(double, double);\n"
"__DEVICE__ float remainder(float, float);\n"
"__DEVICE__ double remquo(double, double, int *);\n"
"__DEVICE__ float remquo(float, float, int *);\n"
"__DEVICE__ double rint(double);\n"
"__DEVICE__ float rint(float);\n"
"__DEVICE__ double round(double);\n"
"__DEVICE__ float round(float);\n"
"__DEVICE__ double scalbln(double, long);\n"
"__DEVICE__ float scalbln(float, long);\n"
"__DEVICE__ double scalbn(double, int);\n"
"__DEVICE__ float scalbn(float, int);\n"
"__DEVICE__ bool signbit(double);\n"
"__DEVICE__ bool signbit(float);\n"
"__DEVICE__ double sin(double);\n"
"__DEVICE__ float sin(float);\n"
"__DEVICE__ double sinh(double);\n"
"__DEVICE__ float sinh(float);\n"
"__DEVICE__ double sqrt(double);\n"
"__DEVICE__ float sqrt(float);\n"
"__DEVICE__ double tan(double);\n"
"__DEVICE__ float tan(float);\n"
"__DEVICE__ double tanh(double);\n"
"__DEVICE__ float tanh(float);\n"
"__DEVICE__ double tgamma(double);\n"
"__DEVICE__ float tgamma(float);\n"
"__DEVICE__ double trunc(double);\n"
"__DEVICE__ float trunc(float);\n"
"\n"
"// Notably missing above is nexttoward, which we don't define on\n"
"// the device side because libdevice doesn't give us an implementation, and we\n"
"// don't want to be in the business of writing one ourselves.\n"
"\n"
"// We need to define these overloads in exactly the namespace our standard\n"
"// library uses (including the right inline namespace), otherwise they won't be\n"
"// picked up by other functions in the standard library (e.g. functions in\n"
"// <complex>).  Thus the ugliness below.\n"
"#ifdef _LIBCPP_BEGIN_NAMESPACE_STD\n"
"_LIBCPP_BEGIN_NAMESPACE_STD\n"
"#else\n"
"namespace std {\n"
"#ifdef _GLIBCXX_BEGIN_NAMESPACE_VERSION\n"
"_GLIBCXX_BEGIN_NAMESPACE_VERSION\n"
"#endif\n"
"#endif\n"
"\n"
"using ::abs;\n"
"using ::acos;\n"
"using ::acosh;\n"
"using ::asin;\n"
"using ::asinh;\n"
"using ::atan;\n"
"using ::atan2;\n"
"using ::atanh;\n"
"using ::cbrt;\n"
"using ::ceil;\n"
"using ::copysign;\n"
"using ::cos;\n"
"using ::cosh;\n"
"using ::erf;\n"
"using ::erfc;\n"
"using ::exp;\n"
"using ::exp2;\n"
"using ::expm1;\n"
"using ::fabs;\n"
"using ::fdim;\n"
"using ::floor;\n"
"using ::fma;\n"
"using ::fmax;\n"
"using ::fmin;\n"
"using ::fmod;\n"
"using ::fpclassify;\n"
"using ::frexp;\n"
"using ::hypot;\n"
"using ::ilogb;\n"
"using ::isfinite;\n"
"using ::isgreater;\n"
"using ::isgreaterequal;\n"
"using ::isinf;\n"
"using ::isless;\n"
"using ::islessequal;\n"
"using ::islessgreater;\n"
"using ::isnan;\n"
"using ::isnormal;\n"
"using ::isunordered;\n"
"using ::labs;\n"
"using ::ldexp;\n"
"using ::lgamma;\n"
"using ::llabs;\n"
"using ::llrint;\n"
"using ::log;\n"
"using ::log10;\n"
"using ::log1p;\n"
"using ::log2;\n"
"using ::logb;\n"
"using ::lrint;\n"
"using ::lround;\n"
"using ::llround;\n"
"using ::modf;\n"
"using ::nan;\n"
"using ::nanf;\n"
"using ::nearbyint;\n"
"using ::nextafter;\n"
"using ::pow;\n"
"using ::remainder;\n"
"using ::remquo;\n"
"using ::rint;\n"
"using ::round;\n"
"using ::scalbln;\n"
"using ::scalbn;\n"
"using ::signbit;\n"
"using ::sin;\n"
"using ::sinh;\n"
"using ::sqrt;\n"
"using ::tan;\n"
"using ::tanh;\n"
"using ::tgamma;\n"
"using ::trunc;\n"
"\n"
"#ifdef _LIBCPP_END_NAMESPACE_STD\n"
"_LIBCPP_END_NAMESPACE_STD\n"
"#else\n"
"#ifdef _GLIBCXX_BEGIN_NAMESPACE_VERSION\n"
"_GLIBCXX_END_NAMESPACE_VERSION\n"
"#endif\n"
"} // namespace std\n"
"#endif\n"
"\n"
"#pragma pop_macro(\"__DEVICE__\")\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/__clang_cuda_runtime_wrapper.h" , "/*===---- __clang_cuda_runtime_wrapper.h - CUDA runtime support -------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"/*\n"
" * WARNING: This header is intended to be directly -include'd by\n"
" * the compiler and is not supposed to be included by users.\n"
" *\n"
" * CUDA headers are implemented in a way that currently makes it\n"
" * impossible for user code to #include directly when compiling with\n"
" * Clang. They present different view of CUDA-supplied functions\n"
" * depending on where in NVCC's compilation pipeline the headers are\n"
" * included. Neither of these modes provides function definitions with\n"
" * correct attributes, so we use preprocessor to force the headers\n"
" * into a form that Clang can use.\n"
" *\n"
" * Similarly to NVCC which -include's cuda_runtime.h, Clang -include's\n"
" * this file during every CUDA compilation.\n"
" */\n"
"\n"
"#ifndef __CLANG_CUDA_RUNTIME_WRAPPER_H__\n"
"#define __CLANG_CUDA_RUNTIME_WRAPPER_H__\n"
"\n"
"#if defined(__CUDA__) && defined(__clang__)\n"
"\n"
"// Include some forward declares that must come before cmath.\n"
"#include <__clang_cuda_math_forward_declares.h>\n"
"\n"
"// Include some standard headers to avoid CUDA headers including them\n"
"// while some required macros (like __THROW) are in a weird state.\n"
"#include <cmath>\n"
"#include <cstdlib>\n"
"#include <stdlib.h>\n"
"\n"
"// Preserve common macros that will be changed below by us or by CUDA\n"
"// headers.\n"
"#pragma push_macro(\"__THROW\")\n"
"#pragma push_macro(\"__CUDA_ARCH__\")\n"
"\n"
"// WARNING: Preprocessor hacks below are based on specific details of\n"
"// CUDA-7.x headers and are not expected to work with any other\n"
"// version of CUDA headers.\n"
"#include \"cuda.h\"\n"
"#if !defined(CUDA_VERSION)\n"
"#error \"cuda.h did not define CUDA_VERSION\"\n"
"#elif CUDA_VERSION < 7000 || CUDA_VERSION > 10000\n"
"#error \"Unsupported CUDA version!\"\n"
"#endif\n"
"\n"
"#pragma push_macro(\"__CUDA_INCLUDE_COMPILER_INTERNAL_HEADERS__\")\n"
"#if CUDA_VERSION >= 10000\n"
"#define __CUDA_INCLUDE_COMPILER_INTERNAL_HEADERS__\n"
"#endif\n"
"\n"
"// Make largest subset of device functions available during host\n"
"// compilation -- SM_35 for the time being.\n"
"#ifndef __CUDA_ARCH__\n"
"#define __CUDA_ARCH__ 350\n"
"#endif\n"
"\n"
"#include \"__clang_cuda_builtin_vars.h\"\n"
"\n"
"// No need for device_launch_parameters.h as __clang_cuda_builtin_vars.h above\n"
"// has taken care of builtin variables declared in the file.\n"
"#define __DEVICE_LAUNCH_PARAMETERS_H__\n"
"\n"
"// {math,device}_functions.h only have declarations of the\n"
"// functions. We don't need them as we're going to pull in their\n"
"// definitions from .hpp files.\n"
"#define __DEVICE_FUNCTIONS_H__\n"
"#define __MATH_FUNCTIONS_H__\n"
"#define __COMMON_FUNCTIONS_H__\n"
"// device_functions_decls is replaced by __clang_cuda_device_functions.h\n"
"// included below.\n"
"#define __DEVICE_FUNCTIONS_DECLS_H__\n"
"\n"
"#undef __CUDACC__\n"
"#if CUDA_VERSION < 9000\n"
"#define __CUDABE__\n"
"#else\n"
"#define __CUDA_LIBDEVICE__\n"
"#endif\n"
"// Disables definitions of device-side runtime support stubs in\n"
"// cuda_device_runtime_api.h\n"
"#include \"driver_types.h\"\n"
"#include \"host_config.h\"\n"
"#include \"host_defines.h\"\n"
"\n"
"// Temporarily replace \"nv_weak\" with weak, so __attribute__((nv_weak)) in\n"
"// cuda_device_runtime_api.h ends up being __attribute__((weak)) which is the\n"
"// functional equivalent of what we need.\n"
"#pragma push_macro(\"nv_weak\")\n"
"#define nv_weak weak\n"
"#undef __CUDABE__\n"
"#undef __CUDA_LIBDEVICE__\n"
"#define __CUDACC__\n"
"#include \"cuda_runtime.h\"\n"
"\n"
"#pragma pop_macro(\"nv_weak\")\n"
"#undef __CUDACC__\n"
"#define __CUDABE__\n"
"\n"
"// CUDA headers use __nvvm_memcpy and __nvvm_memset which Clang does\n"
"// not have at the moment. Emulate them with a builtin memcpy/memset.\n"
"#define __nvvm_memcpy(s, d, n, a) __builtin_memcpy(s, d, n)\n"
"#define __nvvm_memset(d, c, n, a) __builtin_memset(d, c, n)\n"
"\n"
"#if CUDA_VERSION < 9000\n"
"#include \"crt/device_runtime.h\"\n"
"#endif\n"
"#include \"crt/host_runtime.h\"\n"
"// device_runtime.h defines __cxa_* macros that will conflict with\n"
"// cxxabi.h.\n"
"// FIXME: redefine these as __device__ functions.\n"
"#undef __cxa_vec_ctor\n"
"#undef __cxa_vec_cctor\n"
"#undef __cxa_vec_dtor\n"
"#undef __cxa_vec_new\n"
"#undef __cxa_vec_new2\n"
"#undef __cxa_vec_new3\n"
"#undef __cxa_vec_delete2\n"
"#undef __cxa_vec_delete\n"
"#undef __cxa_vec_delete3\n"
"#undef __cxa_pure_virtual\n"
"\n"
"// math_functions.hpp expects this host function be defined on MacOS, but it\n"
"// ends up not being there because of the games we play here.  Just define it\n"
"// ourselves; it's simple enough.\n"
"#ifdef __APPLE__\n"
"inline __host__ double __signbitd(double x) {\n"
"  return std::signbit(x);\n"
"}\n"
"#endif\n"
"\n"
"// CUDA 9.1 no longer provides declarations for libdevice functions, so we need\n"
"// to provide our own.\n"
"#include <__clang_cuda_libdevice_declares.h>\n"
"\n"
"// Wrappers for many device-side standard library functions became compiler\n"
"// builtins in CUDA-9 and have been removed from the CUDA headers. Clang now\n"
"// provides its own implementation of the wrappers.\n"
"#if CUDA_VERSION >= 9000\n"
"#include <__clang_cuda_device_functions.h>\n"
"#endif\n"
"\n"
"// __THROW is redefined to be empty by device_functions_decls.h in CUDA. Clang's\n"
"// counterpart does not do it, so we need to make it empty here to keep\n"
"// following CUDA includes happy.\n"
"#undef __THROW\n"
"#define __THROW\n"
"\n"
"// CUDA 8.0.41 relies on __USE_FAST_MATH__ and __CUDA_PREC_DIV's values.\n"
"// Previous versions used to check whether they are defined or not.\n"
"// CU_DEVICE_INVALID macro is only defined in 8.0.41, so we use it\n"
"// here to detect the switch.\n"
"\n"
"#if defined(CU_DEVICE_INVALID)\n"
"#if !defined(__USE_FAST_MATH__)\n"
"#define __USE_FAST_MATH__ 0\n"
"#endif\n"
"\n"
"#if !defined(__CUDA_PREC_DIV)\n"
"#define __CUDA_PREC_DIV 0\n"
"#endif\n"
"#endif\n"
"\n"
"// Temporarily poison __host__ macro to ensure it's not used by any of\n"
"// the headers we're about to include.\n"
"#pragma push_macro(\"__host__\")\n"
"#define __host__ UNEXPECTED_HOST_ATTRIBUTE\n"
"\n"
"// device_functions.hpp and math_functions*.hpp use 'static\n"
"// __forceinline__' (with no __device__) for definitions of device\n"
"// functions. Temporarily redefine __forceinline__ to include\n"
"// __device__.\n"
"#pragma push_macro(\"__forceinline__\")\n"
"#define __forceinline__ __device__ __inline__ __attribute__((always_inline))\n"
"#if CUDA_VERSION < 9000\n"
"#include \"device_functions.hpp\"\n"
"#endif\n"
"\n"
"// math_function.hpp uses the __USE_FAST_MATH__ macro to determine whether we\n"
"// get the slow-but-accurate or fast-but-inaccurate versions of functions like\n"
"// sin and exp.  This is controlled in clang by -fcuda-approx-transcendentals.\n"
"//\n"
"// device_functions.hpp uses __USE_FAST_MATH__ for a different purpose (fast vs.\n"
"// slow divides), so we need to scope our define carefully here.\n"
"#pragma push_macro(\"__USE_FAST_MATH__\")\n"
"#if defined(__CLANG_CUDA_APPROX_TRANSCENDENTALS__)\n"
"#define __USE_FAST_MATH__ 1\n"
"#endif\n"
"\n"
"#if CUDA_VERSION >= 9000\n"
"// CUDA-9.2 needs host-side memcpy for some host functions in\n"
"// device_functions.hpp\n"
"#if CUDA_VERSION >= 9020\n"
"#include <string.h>\n"
"#endif\n"
"#include \"crt/math_functions.hpp\"\n"
"#else\n"
"#include \"math_functions.hpp\"\n"
"#endif\n"
"\n"
"#pragma pop_macro(\"__USE_FAST_MATH__\")\n"
"\n"
"#if CUDA_VERSION < 9000\n"
"#include \"math_functions_dbl_ptx3.hpp\"\n"
"#endif\n"
"#pragma pop_macro(\"__forceinline__\")\n"
"\n"
"// Pull in host-only functions that are only available when neither\n"
"// __CUDACC__ nor __CUDABE__ are defined.\n"
"#undef __MATH_FUNCTIONS_HPP__\n"
"#undef __CUDABE__\n"
"#if CUDA_VERSION < 9000\n"
"#include \"math_functions.hpp\"\n"
"#endif\n"
"// Alas, additional overloads for these functions are hard to get to.\n"
"// Considering that we only need these overloads for a few functions,\n"
"// we can provide them here.\n"
"static inline float rsqrt(float __a) { return rsqrtf(__a); }\n"
"static inline float rcbrt(float __a) { return rcbrtf(__a); }\n"
"static inline float sinpi(float __a) { return sinpif(__a); }\n"
"static inline float cospi(float __a) { return cospif(__a); }\n"
"static inline void sincospi(float __a, float *__b, float *__c) {\n"
"  return sincospif(__a, __b, __c);\n"
"}\n"
"static inline float erfcinv(float __a) { return erfcinvf(__a); }\n"
"static inline float normcdfinv(float __a) { return normcdfinvf(__a); }\n"
"static inline float normcdf(float __a) { return normcdff(__a); }\n"
"static inline float erfcx(float __a) { return erfcxf(__a); }\n"
"\n"
"#if CUDA_VERSION < 9000\n"
"// For some reason single-argument variant is not always declared by\n"
"// CUDA headers. Alas, device_functions.hpp included below needs it.\n"
"static inline __device__ void __brkpt(int __c) { __brkpt(); }\n"
"#endif\n"
"\n"
"// Now include *.hpp with definitions of various GPU functions.  Alas,\n"
"// a lot of thins get declared/defined with __host__ attribute which\n"
"// we don't want and we have to define it out. We also have to include\n"
"// {device,math}_functions.hpp again in order to extract the other\n"
"// branch of #if/else inside.\n"
"#define __host__\n"
"#undef __CUDABE__\n"
"#define __CUDACC__\n"
"#if CUDA_VERSION >= 9000\n"
"// Some atomic functions became compiler builtins in CUDA-9 , so we need their\n"
"// declarations.\n"
"#include \"device_atomic_functions.h\"\n"
"#endif\n"
"#undef __DEVICE_FUNCTIONS_HPP__\n"
"#include \"device_atomic_functions.hpp\"\n"
"#if CUDA_VERSION >= 9000\n"
"#include \"crt/device_functions.hpp\"\n"
"#include \"crt/device_double_functions.hpp\"\n"
"#else\n"
"#include \"device_functions.hpp\"\n"
"#define __CUDABE__\n"
"#include \"device_double_functions.h\"\n"
"#undef __CUDABE__\n"
"#endif\n"
"#include \"sm_20_atomic_functions.hpp\"\n"
"#include \"sm_20_intrinsics.hpp\"\n"
"#include \"sm_32_atomic_functions.hpp\"\n"
"\n"
"// Don't include sm_30_intrinsics.h and sm_32_intrinsics.h.  These define the\n"
"// __shfl and __ldg intrinsics using inline (volatile) asm, but we want to\n"
"// define them using builtins so that the optimizer can reason about and across\n"
"// these instructions.  In particular, using intrinsics for ldg gets us the\n"
"// [addr+imm] addressing mode, which, although it doesn't actually exist in the\n"
"// hardware, seems to generate faster machine code because ptxas can more easily\n"
"// reason about our code.\n"
"\n"
"#if CUDA_VERSION >= 8000\n"
"#pragma push_macro(\"__CUDA_ARCH__\")\n"
"#undef __CUDA_ARCH__\n"
"#include \"sm_60_atomic_functions.hpp\"\n"
"#include \"sm_61_intrinsics.hpp\"\n"
"#pragma pop_macro(\"__CUDA_ARCH__\")\n"
"#endif\n"
"\n"
"#undef __MATH_FUNCTIONS_HPP__\n"
"\n"
"// math_functions.hpp defines ::signbit as a __host__ __device__ function.  This\n"
"// conflicts with libstdc++'s constexpr ::signbit, so we have to rename\n"
"// math_function.hpp's ::signbit.  It's guarded by #undef signbit, but that's\n"
"// conditional on __GNUC__.  :)\n"
"#pragma push_macro(\"signbit\")\n"
"#pragma push_macro(\"__GNUC__\")\n"
"#undef __GNUC__\n"
"#define signbit __ignored_cuda_signbit\n"
"\n"
"// CUDA-9 omits device-side definitions of some math functions if it sees\n"
"// include guard from math.h wrapper from libstdc++. We have to undo the header\n"
"// guard temporarily to get the definitions we need.\n"
"#pragma push_macro(\"_GLIBCXX_MATH_H\")\n"
"#pragma push_macro(\"_LIBCPP_VERSION\")\n"
"#if CUDA_VERSION >= 9000\n"
"#undef _GLIBCXX_MATH_H\n"
"// We also need to undo another guard that checks for libc++ 3.8+\n"
"#ifdef _LIBCPP_VERSION\n"
"#define _LIBCPP_VERSION 3700\n"
"#endif\n"
"#endif\n"
"\n"
"#if CUDA_VERSION >= 9000\n"
"#include \"crt/math_functions.hpp\"\n"
"#else\n"
"#include \"math_functions.hpp\"\n"
"#endif\n"
"#pragma pop_macro(\"_GLIBCXX_MATH_H\")\n"
"#pragma pop_macro(\"_LIBCPP_VERSION\")\n"
"#pragma pop_macro(\"__GNUC__\")\n"
"#pragma pop_macro(\"signbit\")\n"
"\n"
"#pragma pop_macro(\"__host__\")\n"
"\n"
"#include \"texture_indirect_functions.h\"\n"
"\n"
"// Restore state of __CUDA_ARCH__ and __THROW we had on entry.\n"
"#pragma pop_macro(\"__CUDA_ARCH__\")\n"
"#pragma pop_macro(\"__THROW\")\n"
"\n"
"// Set up compiler macros expected to be seen during compilation.\n"
"#undef __CUDABE__\n"
"#define __CUDACC__\n"
"\n"
"extern \"C\" {\n"
"// Device-side CUDA system calls.\n"
"// http://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html#system-calls\n"
"// We need these declarations and wrappers for device-side\n"
"// malloc/free/printf calls to work without relying on\n"
"// -fcuda-disable-target-call-checks option.\n"
"__device__ int vprintf(const char *, const char *);\n"
"__device__ void free(void *) __attribute((nothrow));\n"
"__device__ void *malloc(size_t) __attribute((nothrow)) __attribute__((malloc));\n"
"__device__ void __assertfail(const char *__message, const char *__file,\n"
"                             unsigned __line, const char *__function,\n"
"                             size_t __charSize) __attribute__((noreturn));\n"
"\n"
"// In order for standard assert() macro on linux to work we need to\n"
"// provide device-side __assert_fail()\n"
"__device__ static inline void __assert_fail(const char *__message,\n"
"                                            const char *__file, unsigned __line,\n"
"                                            const char *__function) {\n"
"  __assertfail(__message, __file, __line, __function, sizeof(char));\n"
"}\n"
"\n"
"// Clang will convert printf into vprintf, but we still need\n"
"// device-side declaration for it.\n"
"__device__ int printf(const char *, ...);\n"
"} // extern \"C\"\n"
"\n"
"// We also need device-side std::malloc and std::free.\n"
"namespace std {\n"
"__device__ static inline void free(void *__ptr) { ::free(__ptr); }\n"
"__device__ static inline void *malloc(size_t __size) {\n"
"  return ::malloc(__size);\n"
"}\n"
"} // namespace std\n"
"\n"
"// Out-of-line implementations from __clang_cuda_builtin_vars.h.  These need to\n"
"// come after we've pulled in the definition of uint3 and dim3.\n"
"\n"
"__device__ inline __cuda_builtin_threadIdx_t::operator uint3() const {\n"
"  uint3 ret;\n"
"  ret.x = x;\n"
"  ret.y = y;\n"
"  ret.z = z;\n"
"  return ret;\n"
"}\n"
"\n"
"__device__ inline __cuda_builtin_blockIdx_t::operator uint3() const {\n"
"  uint3 ret;\n"
"  ret.x = x;\n"
"  ret.y = y;\n"
"  ret.z = z;\n"
"  return ret;\n"
"}\n"
"\n"
"__device__ inline __cuda_builtin_blockDim_t::operator dim3() const {\n"
"  return dim3(x, y, z);\n"
"}\n"
"\n"
"__device__ inline __cuda_builtin_gridDim_t::operator dim3() const {\n"
"  return dim3(x, y, z);\n"
"}\n"
"\n"
"#include <__clang_cuda_cmath.h>\n"
"#include <__clang_cuda_intrinsics.h>\n"
"#include <__clang_cuda_complex_builtins.h>\n"
"\n"
"// curand_mtgp32_kernel helpfully redeclares blockDim and threadIdx in host\n"
"// mode, giving them their \"proper\" types of dim3 and uint3.  This is\n"
"// incompatible with the types we give in __clang_cuda_builtin_vars.h.  As as\n"
"// hack, force-include the header (nvcc doesn't include it by default) but\n"
"// redefine dim3 and uint3 to our builtin types.  (Thankfully dim3 and uint3 are\n"
"// only used here for the redeclarations of blockDim and threadIdx.)\n"
"#pragma push_macro(\"dim3\")\n"
"#pragma push_macro(\"uint3\")\n"
"#define dim3 __cuda_builtin_blockDim_t\n"
"#define uint3 __cuda_builtin_threadIdx_t\n"
"#include \"curand_mtgp32_kernel.h\"\n"
"#pragma pop_macro(\"dim3\")\n"
"#pragma pop_macro(\"uint3\")\n"
"#pragma pop_macro(\"__USE_FAST_MATH__\")\n"
"#pragma pop_macro(\"__CUDA_INCLUDE_COMPILER_INTERNAL_HEADERS__\")\n"
"\n"
"#endif // __CUDA__\n"
"#endif // __CLANG_CUDA_RUNTIME_WRAPPER_H__\n"
"" } , 
 { "/builtins/__stddef_max_align_t.h" , "/*===---- __stddef_max_align_t.h - Definition of max_align_t for modules ---===\n"
" *\n"
" * Copyright (c) 2014 Chandler Carruth\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __CLANG_MAX_ALIGN_T_DEFINED\n"
"#define __CLANG_MAX_ALIGN_T_DEFINED\n"
"\n"
"#if defined(_MSC_VER)\n"
"typedef double max_align_t;\n"
"#elif defined(__APPLE__)\n"
"typedef long double max_align_t;\n"
"#else\n"
"// Define 'max_align_t' to match the GCC definition.\n"
"typedef struct {\n"
"  long long __clang_max_align_nonce1\n"
"      __attribute__((__aligned__(__alignof__(long long))));\n"
"  long double __clang_max_align_nonce2\n"
"      __attribute__((__aligned__(__alignof__(long double))));\n"
"} max_align_t;\n"
"#endif\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/__wmmintrin_aes.h" , "/*===---- __wmmintrin_aes.h - AES intrinsics -------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __WMMINTRIN_H\n"
"#error \"Never use <__wmmintrin_aes.h> directly; include <wmmintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __WMMINTRIN_AES_H\n"
"#define __WMMINTRIN_AES_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"aes\"), __min_vector_width__(128)))\n"
"\n"
"/// Performs a single round of AES encryption using the Equivalent\n"
"///    Inverse Cipher, transforming the state value from the first source\n"
"///    operand using a 128-bit round key value contained in the second source\n"
"///    operand, and writes the result to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VAESENC </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit integer vector containing the state value.\n"
"/// \\param __R\n"
"///    A 128-bit integer vector containing the round key value.\n"
"/// \\returns A 128-bit integer vector containing the encrypted value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_aesenc_si128(__m128i __V, __m128i __R)\n"
"{\n"
"  return (__m128i)__builtin_ia32_aesenc128((__v2di)__V, (__v2di)__R);\n"
"}\n"
"\n"
"/// Performs the final round of AES encryption using the Equivalent\n"
"///    Inverse Cipher, transforming the state value from the first source\n"
"///    operand using a 128-bit round key value contained in the second source\n"
"///    operand, and writes the result to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VAESENCLAST </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit integer vector containing the state value.\n"
"/// \\param __R\n"
"///    A 128-bit integer vector containing the round key value.\n"
"/// \\returns A 128-bit integer vector containing the encrypted value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_aesenclast_si128(__m128i __V, __m128i __R)\n"
"{\n"
"  return (__m128i)__builtin_ia32_aesenclast128((__v2di)__V, (__v2di)__R);\n"
"}\n"
"\n"
"/// Performs a single round of AES decryption using the Equivalent\n"
"///    Inverse Cipher, transforming the state value from the first source\n"
"///    operand using a 128-bit round key value contained in the second source\n"
"///    operand, and writes the result to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VAESDEC </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit integer vector containing the state value.\n"
"/// \\param __R\n"
"///    A 128-bit integer vector containing the round key value.\n"
"/// \\returns A 128-bit integer vector containing the decrypted value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_aesdec_si128(__m128i __V, __m128i __R)\n"
"{\n"
"  return (__m128i)__builtin_ia32_aesdec128((__v2di)__V, (__v2di)__R);\n"
"}\n"
"\n"
"/// Performs the final round of AES decryption using the Equivalent\n"
"///    Inverse Cipher, transforming the state value from the first source\n"
"///    operand using a 128-bit round key value contained in the second source\n"
"///    operand, and writes the result to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VAESDECLAST </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit integer vector containing the state value.\n"
"/// \\param __R\n"
"///    A 128-bit integer vector containing the round key value.\n"
"/// \\returns A 128-bit integer vector containing the decrypted value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_aesdeclast_si128(__m128i __V, __m128i __R)\n"
"{\n"
"  return (__m128i)__builtin_ia32_aesdeclast128((__v2di)__V, (__v2di)__R);\n"
"}\n"
"\n"
"/// Applies the AES InvMixColumns() transformation to an expanded key\n"
"///    contained in the source operand, and writes the result to the\n"
"///    destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VAESIMC </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit integer vector containing the expanded key.\n"
"/// \\returns A 128-bit integer vector containing the transformed value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_aesimc_si128(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_ia32_aesimc128((__v2di)__V);\n"
"}\n"
"\n"
"/// Generates a round key for AES encryption, operating on 128-bit data\n"
"///    specified in the first source operand and using an 8-bit round constant\n"
"///    specified by the second source operand, and writes the result to the\n"
"///    destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_aeskeygenassist_si128(__m128i C, const int R);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> AESKEYGENASSIST </c> instruction.\n"
"///\n"
"/// \\param C\n"
"///    A 128-bit integer vector that is used to generate the AES encryption key.\n"
"/// \\param R\n"
"///    An 8-bit round constant used to generate the AES encryption key.\n"
"/// \\returns A 128-bit round key for AES encryption.\n"
"#define _mm_aeskeygenassist_si128(C, R) \\\n"
"  (__m128i)__builtin_ia32_aeskeygenassist128((__v2di)(__m128i)(C), (int)(R))\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif  /* __WMMINTRIN_AES_H */\n"
"" } , 
 { "/builtins/__wmmintrin_pclmul.h" , "/*===---- __wmmintrin_pclmul.h - PCMUL intrinsics ---------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __WMMINTRIN_H\n"
"#error \"Never use <__wmmintrin_pclmul.h> directly; include <wmmintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __WMMINTRIN_PCLMUL_H\n"
"#define __WMMINTRIN_PCLMUL_H\n"
"\n"
"/// Multiplies two 64-bit integer values, which are selected from source\n"
"///    operands using the immediate-value operand. The multiplication is a\n"
"///    carry-less multiplication, and the 128-bit integer product is stored in\n"
"///    the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_clmulepi64_si128(__m128i __X, __m128i __Y, const int __I);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCLMULQDQ </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    A 128-bit vector of [2 x i64] containing one of the source operands.\n"
"/// \\param __Y\n"
"///    A 128-bit vector of [2 x i64] containing one of the source operands.\n"
"/// \\param __I\n"
"///    An immediate value specifying which 64-bit values to select from the\n"
"///    operands. Bit 0 is used to select a value from operand \\a __X, and bit\n"
"///    4 is used to select a value from operand \\a __Y: \\n\n"
"///    Bit[0]=0 indicates that bits[63:0] of operand \\a __X are used. \\n\n"
"///    Bit[0]=1 indicates that bits[127:64] of operand \\a __X are used. \\n\n"
"///    Bit[4]=0 indicates that bits[63:0] of operand \\a __Y are used. \\n\n"
"///    Bit[4]=1 indicates that bits[127:64] of operand \\a __Y are used.\n"
"/// \\returns The 128-bit integer vector containing the result of the carry-less\n"
"///    multiplication of the selected 64-bit values.\n"
"#define _mm_clmulepi64_si128(X, Y, I) \\\n"
"  ((__m128i)__builtin_ia32_pclmulqdq128((__v2di)(__m128i)(X), \\\n"
"                                        (__v2di)(__m128i)(Y), (char)(I)))\n"
"\n"
"#endif /* __WMMINTRIN_PCLMUL_H */\n"
"" } , 
 { "/builtins/adxintrin.h" , "/*===---- adxintrin.h - ADX intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <adxintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __ADXINTRIN_H\n"
"#define __ADXINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__))\n"
"\n"
"/* Intrinsics that are available only if __ADX__ defined */\n"
"static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__(\"adx\")))\n"
"_addcarryx_u32(unsigned char __cf, unsigned int __x, unsigned int __y,\n"
"               unsigned int *__p)\n"
"{\n"
"  return __builtin_ia32_addcarryx_u32(__cf, __x, __y, __p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__(\"adx\")))\n"
"_addcarryx_u64(unsigned char __cf, unsigned long long __x,\n"
"               unsigned long long __y, unsigned long long  *__p)\n"
"{\n"
"  return __builtin_ia32_addcarryx_u64(__cf, __x, __y, __p);\n"
"}\n"
"#endif\n"
"\n"
"/* Intrinsics that are also available if __ADX__ undefined */\n"
"static __inline unsigned char __DEFAULT_FN_ATTRS\n"
"_addcarry_u32(unsigned char __cf, unsigned int __x, unsigned int __y,\n"
"              unsigned int *__p)\n"
"{\n"
"  return __builtin_ia32_addcarryx_u32(__cf, __x, __y, __p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline unsigned char __DEFAULT_FN_ATTRS\n"
"_addcarry_u64(unsigned char __cf, unsigned long long __x,\n"
"              unsigned long long __y, unsigned long long  *__p)\n"
"{\n"
"  return __builtin_ia32_addcarryx_u64(__cf, __x, __y, __p);\n"
"}\n"
"#endif\n"
"\n"
"static __inline unsigned char __DEFAULT_FN_ATTRS\n"
"_subborrow_u32(unsigned char __cf, unsigned int __x, unsigned int __y,\n"
"              unsigned int *__p)\n"
"{\n"
"  return __builtin_ia32_subborrow_u32(__cf, __x, __y, __p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline unsigned char __DEFAULT_FN_ATTRS\n"
"_subborrow_u64(unsigned char __cf, unsigned long long __x,\n"
"               unsigned long long __y, unsigned long long  *__p)\n"
"{\n"
"  return __builtin_ia32_subborrow_u64(__cf, __x, __y, __p);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __ADXINTRIN_H */\n"
"" } , 
 { "/builtins/ammintrin.h" , "/*===---- ammintrin.h - SSE4a intrinsics -----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __AMMINTRIN_H\n"
"#define __AMMINTRIN_H\n"
"\n"
"#include <pmmintrin.h>\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"sse4a\"), __min_vector_width__(128)))\n"
"\n"
"/// Extracts the specified bits from the lower 64 bits of the 128-bit\n"
"///    integer vector operand at the index \\a idx and of the length \\a len.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_extracti_si64(__m128i x, const int len, const int idx);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> EXTRQ </c> instruction.\n"
"///\n"
"/// \\param x\n"
"///    The value from which bits are extracted.\n"
"/// \\param len\n"
"///    Bits [5:0] specify the length; the other bits are ignored. If bits [5:0]\n"
"///    are zero, the length is interpreted as 64.\n"
"/// \\param idx\n"
"///    Bits [5:0] specify the index of the least significant bit; the other\n"
"///    bits are ignored. If the sum of the index and length is greater than 64,\n"
"///    the result is undefined. If the length and index are both zero, bits\n"
"///    [63:0] of parameter \\a x are extracted. If the length is zero but the\n"
"///    index is non-zero, the result is undefined.\n"
"/// \\returns A 128-bit integer vector whose lower 64 bits contain the bits\n"
"///    extracted from the source operand.\n"
"#define _mm_extracti_si64(x, len, idx) \\\n"
"  ((__m128i)__builtin_ia32_extrqi((__v2di)(__m128i)(x), \\\n"
"                                  (char)(len), (char)(idx)))\n"
"\n"
"/// Extracts the specified bits from the lower 64 bits of the 128-bit\n"
"///    integer vector operand at the index and of the length specified by\n"
"///    \\a __y.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> EXTRQ </c> instruction.\n"
"///\n"
"/// \\param __x\n"
"///    The value from which bits are extracted.\n"
"/// \\param __y\n"
"///    Specifies the index of the least significant bit at [13:8] and the\n"
"///    length at [5:0]; all other bits are ignored. If bits [5:0] are zero, the\n"
"///    length is interpreted as 64. If the sum of the index and length is\n"
"///    greater than 64, the result is undefined. If the length and index are\n"
"///    both zero, bits [63:0] of parameter \\a __x are extracted. If the length\n"
"///    is zero but the index is non-zero, the result is undefined.\n"
"/// \\returns A 128-bit vector whose lower 64 bits contain the bits extracted\n"
"///    from the source operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_extract_si64(__m128i __x, __m128i __y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_extrq((__v2di)__x, (__v16qi)__y);\n"
"}\n"
"\n"
"/// Inserts bits of a specified length from the source integer vector\n"
"///    \\a y into the lower 64 bits of the destination integer vector \\a x at\n"
"///    the index \\a idx and of the length \\a len.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_inserti_si64(__m128i x, __m128i y, const int len,\n"
"/// const int idx);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> INSERTQ </c> instruction.\n"
"///\n"
"/// \\param x\n"
"///    The destination operand where bits will be inserted. The inserted bits\n"
"///    are defined by the length \\a len and by the index \\a idx specifying the\n"
"///    least significant bit.\n"
"/// \\param y\n"
"///    The source operand containing the bits to be extracted. The extracted\n"
"///    bits are the least significant bits of operand \\a y of length \\a len.\n"
"/// \\param len\n"
"///    Bits [5:0] specify the length; the other bits are ignored. If bits [5:0]\n"
"///    are zero, the length is interpreted as 64.\n"
"/// \\param idx\n"
"///    Bits [5:0] specify the index of the least significant bit; the other\n"
"///    bits are ignored. If the sum of the index and length is greater than 64,\n"
"///    the result is undefined. If the length and index are both zero, bits\n"
"///    [63:0] of parameter \\a y are inserted into parameter \\a x. If the length\n"
"///    is zero but the index is non-zero, the result is undefined.\n"
"/// \\returns A 128-bit integer vector containing the original lower 64-bits of\n"
"///    destination operand \\a x with the specified bitfields replaced by the\n"
"///    lower bits of source operand \\a y. The upper 64 bits of the return value\n"
"///    are undefined.\n"
"#define _mm_inserti_si64(x, y, len, idx) \\\n"
"  ((__m128i)__builtin_ia32_insertqi((__v2di)(__m128i)(x), \\\n"
"                                    (__v2di)(__m128i)(y), \\\n"
"                                    (char)(len), (char)(idx)))\n"
"\n"
"/// Inserts bits of a specified length from the source integer vector\n"
"///    \\a __y into the lower 64 bits of the destination integer vector \\a __x\n"
"///    at the index and of the length specified by \\a __y.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> INSERTQ </c> instruction.\n"
"///\n"
"/// \\param __x\n"
"///    The destination operand where bits will be inserted. The inserted bits\n"
"///    are defined by the length and by the index of the least significant bit\n"
"///    specified by operand \\a __y.\n"
"/// \\param __y\n"
"///    The source operand containing the bits to be extracted. The extracted\n"
"///    bits are the least significant bits of operand \\a __y with length\n"
"///    specified by bits [69:64]. These are inserted into the destination at the\n"
"///    index specified by bits [77:72]; all other bits are ignored. If bits\n"
"///    [69:64] are zero, the length is interpreted as 64. If the sum of the\n"
"///    index and length is greater than 64, the result is undefined. If the\n"
"///    length and index are both zero, bits [63:0] of parameter \\a __y are\n"
"///    inserted into parameter \\a __x. If the length is zero but the index is\n"
"///    non-zero, the result is undefined.\n"
"/// \\returns A 128-bit integer vector containing the original lower 64-bits of\n"
"///    destination operand \\a __x with the specified bitfields replaced by the\n"
"///    lower bits of source operand \\a __y. The upper 64 bits of the return\n"
"///    value are undefined.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_insert_si64(__m128i __x, __m128i __y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_insertq((__v2di)__x, (__v2di)__y);\n"
"}\n"
"\n"
"/// Stores a 64-bit double-precision value in a 64-bit memory location.\n"
"///    To minimize caching, the data is flagged as non-temporal (unlikely to be\n"
"///    used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVNTSD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    The 64-bit memory location used to store the register value.\n"
"/// \\param __a\n"
"///    The 64-bit double-precision floating-point register value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_stream_sd(double *__p, __m128d __a)\n"
"{\n"
"  __builtin_ia32_movntsd(__p, (__v2df)__a);\n"
"}\n"
"\n"
"/// Stores a 32-bit single-precision floating-point value in a 32-bit\n"
"///    memory location. To minimize caching, the data is flagged as\n"
"///    non-temporal (unlikely to be used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVNTSS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    The 32-bit memory location used to store the register value.\n"
"/// \\param __a\n"
"///    The 32-bit single-precision floating-point register value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_stream_ss(float *__p, __m128 __a)\n"
"{\n"
"  __builtin_ia32_movntss(__p, (__v4sf)__a);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __AMMINTRIN_H */\n"
"" } , 
 { "/builtins/arm64intr.h" , "/*===---- arm64intr.h - ARM64 Windows intrinsics -------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"/* Only include this if we're compiling for the windows platform. */\n"
"#ifndef _MSC_VER\n"
"#include_next <arm64intr.h>\n"
"#else\n"
"\n"
"#ifndef __ARM64INTR_H\n"
"#define __ARM64INTR_H\n"
"\n"
"typedef enum\n"
"{\n"
"  _ARM64_BARRIER_SY    = 0xF,\n"
"  _ARM64_BARRIER_ST    = 0xE,\n"
"  _ARM64_BARRIER_LD    = 0xD,\n"
"  _ARM64_BARRIER_ISH   = 0xB,\n"
"  _ARM64_BARRIER_ISHST = 0xA,\n"
"  _ARM64_BARRIER_ISHLD = 0x9,\n"
"  _ARM64_BARRIER_NSH   = 0x7,\n"
"  _ARM64_BARRIER_NSHST = 0x6,\n"
"  _ARM64_BARRIER_NSHLD = 0x5,\n"
"  _ARM64_BARRIER_OSH   = 0x3,\n"
"  _ARM64_BARRIER_OSHST = 0x2,\n"
"  _ARM64_BARRIER_OSHLD = 0x1\n"
"} _ARM64INTR_BARRIER_TYPE;\n"
"\n"
"#endif /* __ARM64INTR_H */\n"
"#endif /* _MSC_VER */\n"
"" } , 
 { "/builtins/arm_acle.h" , "/*===---- arm_acle.h - ARM Non-Neon intrinsics -----------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __ARM_ACLE_H\n"
"#define __ARM_ACLE_H\n"
"\n"
"#ifndef __ARM_ACLE\n"
"#error \"ACLE intrinsics support not enabled.\"\n"
"#endif\n"
"\n"
"#include <stdint.h>\n"
"\n"
"#if defined(__cplusplus)\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"/* 8 SYNCHRONIZATION, BARRIER AND HINT INTRINSICS */\n"
"/* 8.3 Memory barriers */\n"
"#if !defined(_MSC_VER)\n"
"#define __dmb(i) __builtin_arm_dmb(i)\n"
"#define __dsb(i) __builtin_arm_dsb(i)\n"
"#define __isb(i) __builtin_arm_isb(i)\n"
"#endif\n"
"\n"
"/* 8.4 Hints */\n"
"\n"
"#if !defined(_MSC_VER)\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__)) __wfi(void) {\n"
"  __builtin_arm_wfi();\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__)) __wfe(void) {\n"
"  __builtin_arm_wfe();\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__)) __sev(void) {\n"
"  __builtin_arm_sev();\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__)) __sevl(void) {\n"
"  __builtin_arm_sevl();\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__)) __yield(void) {\n"
"  __builtin_arm_yield();\n"
"}\n"
"#endif\n"
"\n"
"#if __ARM_32BIT_STATE\n"
"#define __dbg(t) __builtin_arm_dbg(t)\n"
"#endif\n"
"\n"
"/* 8.5 Swap */\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__swp(uint32_t __x, volatile uint32_t *__p) {\n"
"  uint32_t v;\n"
"  do\n"
"    v = __builtin_arm_ldrex(__p);\n"
"  while (__builtin_arm_strex(__x, __p));\n"
"  return v;\n"
"}\n"
"\n"
"/* 8.6 Memory prefetch intrinsics */\n"
"/* 8.6.1 Data prefetch */\n"
"#define __pld(addr) __pldx(0, 0, 0, addr)\n"
"\n"
"#if __ARM_32BIT_STATE\n"
"#define __pldx(access_kind, cache_level, retention_policy, addr) \\\n"
"  __builtin_arm_prefetch(addr, access_kind, 1)\n"
"#else\n"
"#define __pldx(access_kind, cache_level, retention_policy, addr) \\\n"
"  __builtin_arm_prefetch(addr, access_kind, cache_level, retention_policy, 1)\n"
"#endif\n"
"\n"
"/* 8.6.2 Instruction prefetch */\n"
"#define __pli(addr) __plix(0, 0, addr)\n"
"\n"
"#if __ARM_32BIT_STATE\n"
"#define __plix(cache_level, retention_policy, addr) \\\n"
"  __builtin_arm_prefetch(addr, 0, 0)\n"
"#else\n"
"#define __plix(cache_level, retention_policy, addr) \\\n"
"  __builtin_arm_prefetch(addr, 0, cache_level, retention_policy, 0)\n"
"#endif\n"
"\n"
"/* 8.7 NOP */\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__)) __nop(void) {\n"
"  __builtin_arm_nop();\n"
"}\n"
"\n"
"/* 9 DATA-PROCESSING INTRINSICS */\n"
"/* 9.2 Miscellaneous data-processing intrinsics */\n"
"/* ROR */\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__ror(uint32_t __x, uint32_t __y) {\n"
"  __y %= 32;\n"
"  if (__y == 0)\n"
"    return __x;\n"
"  return (__x >> __y) | (__x << (32 - __y));\n"
"}\n"
"\n"
"static __inline__ uint64_t __attribute__((__always_inline__, __nodebug__))\n"
"__rorll(uint64_t __x, uint32_t __y) {\n"
"  __y %= 64;\n"
"  if (__y == 0)\n"
"    return __x;\n"
"  return (__x >> __y) | (__x << (64 - __y));\n"
"}\n"
"\n"
"static __inline__ unsigned long __attribute__((__always_inline__, __nodebug__))\n"
"__rorl(unsigned long __x, uint32_t __y) {\n"
"#if __SIZEOF_LONG__ == 4\n"
"  return __ror(__x, __y);\n"
"#else\n"
"  return __rorll(__x, __y);\n"
"#endif\n"
"}\n"
"\n"
"\n"
"/* CLZ */\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__clz(uint32_t __t) {\n"
"  return __builtin_clz(__t);\n"
"}\n"
"\n"
"static __inline__ unsigned long __attribute__((__always_inline__, __nodebug__))\n"
"__clzl(unsigned long __t) {\n"
"  return __builtin_clzl(__t);\n"
"}\n"
"\n"
"static __inline__ uint64_t __attribute__((__always_inline__, __nodebug__))\n"
"__clzll(uint64_t __t) {\n"
"  return __builtin_clzll(__t);\n"
"}\n"
"\n"
"/* REV */\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__rev(uint32_t __t) {\n"
"  return __builtin_bswap32(__t);\n"
"}\n"
"\n"
"static __inline__ unsigned long __attribute__((__always_inline__, __nodebug__))\n"
"__revl(unsigned long __t) {\n"
"#if __SIZEOF_LONG__ == 4\n"
"  return __builtin_bswap32(__t);\n"
"#else\n"
"  return __builtin_bswap64(__t);\n"
"#endif\n"
"}\n"
"\n"
"static __inline__ uint64_t __attribute__((__always_inline__, __nodebug__))\n"
"__revll(uint64_t __t) {\n"
"  return __builtin_bswap64(__t);\n"
"}\n"
"\n"
"/* REV16 */\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__rev16(uint32_t __t) {\n"
"  return __ror(__rev(__t), 16);\n"
"}\n"
"\n"
"static __inline__ uint64_t __attribute__((__always_inline__, __nodebug__))\n"
"__rev16ll(uint64_t __t) {\n"
"  return (((uint64_t)__rev16(__t >> 32)) << 32) | __rev16(__t);\n"
"}\n"
"\n"
"static __inline__ unsigned long __attribute__((__always_inline__, __nodebug__))\n"
"__rev16l(unsigned long __t) {\n"
"#if __SIZEOF_LONG__ == 4\n"
"    return __rev16(__t);\n"
"#else\n"
"    return __rev16ll(__t);\n"
"#endif\n"
"}\n"
"\n"
"/* REVSH */\n"
"static __inline__ int16_t __attribute__((__always_inline__, __nodebug__))\n"
"__revsh(int16_t __t) {\n"
"  return __builtin_bswap16(__t);\n"
"}\n"
"\n"
"/* RBIT */\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__rbit(uint32_t __t) {\n"
"  return __builtin_arm_rbit(__t);\n"
"}\n"
"\n"
"static __inline__ uint64_t __attribute__((__always_inline__, __nodebug__))\n"
"__rbitll(uint64_t __t) {\n"
"#if __ARM_32BIT_STATE\n"
"  return (((uint64_t)__builtin_arm_rbit(__t)) << 32) |\n"
"         __builtin_arm_rbit(__t >> 32);\n"
"#else\n"
"  return __builtin_arm_rbit64(__t);\n"
"#endif\n"
"}\n"
"\n"
"static __inline__ unsigned long __attribute__((__always_inline__, __nodebug__))\n"
"__rbitl(unsigned long __t) {\n"
"#if __SIZEOF_LONG__ == 4\n"
"  return __rbit(__t);\n"
"#else\n"
"  return __rbitll(__t);\n"
"#endif\n"
"}\n"
"\n"
"/*\n"
" * 9.3 16-bit multiplications\n"
" */\n"
"#if __ARM_FEATURE_DSP\n"
"static __inline__ int32_t __attribute__((__always_inline__,__nodebug__))\n"
"__smulbb(int32_t __a, int32_t __b) {\n"
"  return __builtin_arm_smulbb(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__,__nodebug__))\n"
"__smulbt(int32_t __a, int32_t __b) {\n"
"  return __builtin_arm_smulbt(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__,__nodebug__))\n"
"__smultb(int32_t __a, int32_t __b) {\n"
"  return __builtin_arm_smultb(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__,__nodebug__))\n"
"__smultt(int32_t __a, int32_t __b) {\n"
"  return __builtin_arm_smultt(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__,__nodebug__))\n"
"__smulwb(int32_t __a, int32_t __b) {\n"
"  return __builtin_arm_smulwb(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__,__nodebug__))\n"
"__smulwt(int32_t __a, int32_t __b) {\n"
"  return __builtin_arm_smulwt(__a, __b);\n"
"}\n"
"#endif\n"
"\n"
"/*\n"
" * 9.4 Saturating intrinsics\n"
" *\n"
" * FIXME: Change guard to their corrosponding __ARM_FEATURE flag when Q flag\n"
" * intrinsics are implemented and the flag is enabled.\n"
" */\n"
"/* 9.4.1 Width-specified saturation intrinsics */\n"
"#if __ARM_FEATURE_SAT\n"
"#define __ssat(x, y) __builtin_arm_ssat(x, y)\n"
"#define __usat(x, y) __builtin_arm_usat(x, y)\n"
"#endif\n"
"\n"
"/* 9.4.2 Saturating addition and subtraction intrinsics */\n"
"#if __ARM_FEATURE_DSP\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__qadd(int32_t __t, int32_t __v) {\n"
"  return __builtin_arm_qadd(__t, __v);\n"
"}\n"
"\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__qsub(int32_t __t, int32_t __v) {\n"
"  return __builtin_arm_qsub(__t, __v);\n"
"}\n"
"\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__qdbl(int32_t __t) {\n"
"  return __builtin_arm_qadd(__t, __t);\n"
"}\n"
"#endif\n"
"\n"
"/* 9.4.3 Accumultating multiplications */\n"
"#if __ARM_FEATURE_DSP\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlabb(int32_t __a, int32_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlabb(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlabt(int32_t __a, int32_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlabt(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlatb(int32_t __a, int32_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlatb(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlatt(int32_t __a, int32_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlatt(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlawb(int32_t __a, int32_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlawb(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlawt(int32_t __a, int32_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlawt(__a, __b, __c);\n"
"}\n"
"#endif\n"
"\n"
"\n"
"/* 9.5.4 Parallel 16-bit saturation */\n"
"#if __ARM_FEATURE_SIMD32\n"
"#define __ssat16(x, y) __builtin_arm_ssat16(x, y)\n"
"#define __usat16(x, y) __builtin_arm_usat16(x, y)\n"
"#endif\n"
"\n"
"/* 9.5.5 Packing and unpacking */\n"
"#if __ARM_FEATURE_SIMD32\n"
"typedef int32_t int8x4_t;\n"
"typedef int32_t int16x2_t;\n"
"typedef uint32_t uint8x4_t;\n"
"typedef uint32_t uint16x2_t;\n"
"\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__sxtab16(int16x2_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_sxtab16(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__sxtb16(int8x4_t __a) {\n"
"  return __builtin_arm_sxtb16(__a);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uxtab16(int16x2_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_uxtab16(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uxtb16(int8x4_t __a) {\n"
"  return __builtin_arm_uxtb16(__a);\n"
"}\n"
"#endif\n"
"\n"
"/* 9.5.6 Parallel selection */\n"
"#if __ARM_FEATURE_SIMD32\n"
"static __inline__ uint8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__sel(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_sel(__a, __b);\n"
"}\n"
"#endif\n"
"\n"
"/* 9.5.7 Parallel 8-bit addition and subtraction */\n"
"#if __ARM_FEATURE_SIMD32\n"
"static __inline__ int8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__qadd8(int8x4_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_qadd8(__a, __b);\n"
"}\n"
"static __inline__ int8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__qsub8(int8x4_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_qsub8(__a, __b);\n"
"}\n"
"static __inline__ int8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__sadd8(int8x4_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_sadd8(__a, __b);\n"
"}\n"
"static __inline__ int8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__shadd8(int8x4_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_shadd8(__a, __b);\n"
"}\n"
"static __inline__ int8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__shsub8(int8x4_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_shsub8(__a, __b);\n"
"}\n"
"static __inline__ int8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__ssub8(int8x4_t __a, int8x4_t __b) {\n"
"  return __builtin_arm_ssub8(__a, __b);\n"
"}\n"
"static __inline__ uint8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__uadd8(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_uadd8(__a, __b);\n"
"}\n"
"static __inline__ uint8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__uhadd8(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_uhadd8(__a, __b);\n"
"}\n"
"static __inline__ uint8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__uhsub8(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_uhsub8(__a, __b);\n"
"}\n"
"static __inline__ uint8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__uqadd8(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_uqadd8(__a, __b);\n"
"}\n"
"static __inline__ uint8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__uqsub8(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_uqsub8(__a, __b);\n"
"}\n"
"static __inline__ uint8x4_t __attribute__((__always_inline__, __nodebug__))\n"
"__usub8(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_usub8(__a, __b);\n"
"}\n"
"#endif\n"
"\n"
"/* 9.5.8 Sum of 8-bit absolute differences */\n"
"#if __ARM_FEATURE_SIMD32\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__usad8(uint8x4_t __a, uint8x4_t __b) {\n"
"  return __builtin_arm_usad8(__a, __b);\n"
"}\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__usada8(uint8x4_t __a, uint8x4_t __b, uint32_t __c) {\n"
"  return __builtin_arm_usada8(__a, __b, __c);\n"
"}\n"
"#endif\n"
"\n"
"/* 9.5.9 Parallel 16-bit addition and subtraction */\n"
"#if __ARM_FEATURE_SIMD32\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__qadd16(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_qadd16(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__qasx(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_qasx(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__qsax(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_qsax(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__qsub16(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_qsub16(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__sadd16(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_sadd16(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__sasx(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_sasx(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__shadd16(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_shadd16(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__shasx(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_shasx(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__shsax(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_shsax(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__shsub16(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_shsub16(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__ssax(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_ssax(__a, __b);\n"
"}\n"
"static __inline__ int16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__ssub16(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_ssub16(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uadd16(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uadd16(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uasx(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uasx(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uhadd16(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uhadd16(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uhasx(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uhasx(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uhsax(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uhsax(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uhsub16(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uhsub16(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uqadd16(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uqadd16(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uqasx(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uqasx(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uqsax(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uqsax(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__uqsub16(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_uqsub16(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__usax(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_usax(__a, __b);\n"
"}\n"
"static __inline__ uint16x2_t __attribute__((__always_inline__, __nodebug__))\n"
"__usub16(uint16x2_t __a, uint16x2_t __b) {\n"
"  return __builtin_arm_usub16(__a, __b);\n"
"}\n"
"#endif\n"
"\n"
"/* 9.5.10 Parallel 16-bit multiplications */\n"
"#if __ARM_FEATURE_SIMD32\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlad(int16x2_t __a, int16x2_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlad(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smladx(int16x2_t __a, int16x2_t __b, int32_t __c) {\n"
"  return __builtin_arm_smladx(__a, __b, __c);\n"
"}\n"
"static __inline__ int64_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlald(int16x2_t __a, int16x2_t __b, int64_t __c) {\n"
"  return __builtin_arm_smlald(__a, __b, __c);\n"
"}\n"
"static __inline__ int64_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlaldx(int16x2_t __a, int16x2_t __b, int64_t __c) {\n"
"  return __builtin_arm_smlaldx(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlsd(int16x2_t __a, int16x2_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlsd(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlsdx(int16x2_t __a, int16x2_t __b, int32_t __c) {\n"
"  return __builtin_arm_smlsdx(__a, __b, __c);\n"
"}\n"
"static __inline__ int64_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlsld(int16x2_t __a, int16x2_t __b, int64_t __c) {\n"
"  return __builtin_arm_smlsld(__a, __b, __c);\n"
"}\n"
"static __inline__ int64_t __attribute__((__always_inline__, __nodebug__))\n"
"__smlsldx(int16x2_t __a, int16x2_t __b, int64_t __c) {\n"
"  return __builtin_arm_smlsldx(__a, __b, __c);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smuad(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_smuad(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smuadx(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_smuadx(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smusd(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_smusd(__a, __b);\n"
"}\n"
"static __inline__ int32_t __attribute__((__always_inline__, __nodebug__))\n"
"__smusdx(int16x2_t __a, int16x2_t __b) {\n"
"  return __builtin_arm_smusdx(__a, __b);\n"
"}\n"
"#endif\n"
"\n"
"/* 9.7 CRC32 intrinsics */\n"
"#if __ARM_FEATURE_CRC32\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32b(uint32_t __a, uint8_t __b) {\n"
"  return __builtin_arm_crc32b(__a, __b);\n"
"}\n"
"\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32h(uint32_t __a, uint16_t __b) {\n"
"  return __builtin_arm_crc32h(__a, __b);\n"
"}\n"
"\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32w(uint32_t __a, uint32_t __b) {\n"
"  return __builtin_arm_crc32w(__a, __b);\n"
"}\n"
"\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32d(uint32_t __a, uint64_t __b) {\n"
"  return __builtin_arm_crc32d(__a, __b);\n"
"}\n"
"\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32cb(uint32_t __a, uint8_t __b) {\n"
"  return __builtin_arm_crc32cb(__a, __b);\n"
"}\n"
"\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32ch(uint32_t __a, uint16_t __b) {\n"
"  return __builtin_arm_crc32ch(__a, __b);\n"
"}\n"
"\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32cw(uint32_t __a, uint32_t __b) {\n"
"  return __builtin_arm_crc32cw(__a, __b);\n"
"}\n"
"\n"
"static __inline__ uint32_t __attribute__((__always_inline__, __nodebug__))\n"
"__crc32cd(uint32_t __a, uint64_t __b) {\n"
"  return __builtin_arm_crc32cd(__a, __b);\n"
"}\n"
"#endif\n"
"\n"
"/* 10.1 Special register intrinsics */\n"
"#define __arm_rsr(sysreg) __builtin_arm_rsr(sysreg)\n"
"#define __arm_rsr64(sysreg) __builtin_arm_rsr64(sysreg)\n"
"#define __arm_rsrp(sysreg) __builtin_arm_rsrp(sysreg)\n"
"#define __arm_wsr(sysreg, v) __builtin_arm_wsr(sysreg, v)\n"
"#define __arm_wsr64(sysreg, v) __builtin_arm_wsr64(sysreg, v)\n"
"#define __arm_wsrp(sysreg, v) __builtin_arm_wsrp(sysreg, v)\n"
"\n"
"#if defined(__cplusplus)\n"
"}\n"
"#endif\n"
"\n"
"#endif /* __ARM_ACLE_H */\n"
"" } , 
 { "/builtins/arm_fp16.h" , "/*===---- arm_fp16.h - ARM FP16 intrinsics ---------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __ARM_FP16_H\n"
"#define __ARM_FP16_H\n"
"\n"
"#include <stdint.h>\n"
"\n"
"typedef __fp16 float16_t;\n"
"#define __ai static __inline__ __attribute__((__always_inline__, __nodebug__))\n"
"\n"
"#if defined(__ARM_FEATURE_FP16_SCALAR_ARITHMETIC) && defined(__aarch64__)\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vabdh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vabdh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vabdh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vabdh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vabsh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vabsh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vabsh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vabsh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vaddh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vaddh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vaddh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vaddh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcageh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcageh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcageh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcageh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcagth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcagth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcagth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcagth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcaleh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcaleh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcaleh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcaleh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcalth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcalth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcalth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcalth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vceqh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vceqh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vceqh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vceqh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vceqzh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vceqzh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vceqzh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vceqzh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcgeh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgeh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcgeh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgeh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcgezh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgezh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcgezh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgezh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcgth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcgth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcgtzh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgtzh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcgtzh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcgtzh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcleh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcleh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcleh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcleh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vclezh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vclezh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vclezh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vclezh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vclth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vclth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vclth_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vclth_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcltzh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcltzh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcltzh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcltzh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_s16_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvth_n_s16_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_s16_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvth_n_s16_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_s32_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvth_n_s32_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_s32_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvth_n_s32_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_s64_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvth_n_s64_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_s64_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvth_n_s64_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_u16_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvth_n_u16_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_u16_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvth_n_u16_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_u32_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvth_n_u32_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_u32_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvth_n_u32_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_u64_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvth_n_u64_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_u64_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvth_n_u64_f16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvth_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvth_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvth_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvth_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvth_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvth_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvth_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvth_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvth_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvth_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvth_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvth_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtah_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtah_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtah_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtah_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtah_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtah_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtah_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtah_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtah_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtah_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtah_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtah_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtah_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtah_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtah_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtah_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtah_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtah_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtah_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtah_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtah_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtah_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtah_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtah_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"__ai float16_t vcvth_f16_u32(uint32_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_u32(__p0);\n"
"  return __ret;\n"
"}\n"
"#else\n"
"__ai float16_t vcvth_f16_u32(uint32_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_u32(__p0);\n"
"  return __ret;\n"
"}\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"__ai float16_t vcvth_f16_u64(uint64_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_u64(__p0);\n"
"  return __ret;\n"
"}\n"
"#else\n"
"__ai float16_t vcvth_f16_u64(uint64_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_u64(__p0);\n"
"  return __ret;\n"
"}\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"__ai float16_t vcvth_f16_u16(uint16_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_u16(__p0);\n"
"  return __ret;\n"
"}\n"
"#else\n"
"__ai float16_t vcvth_f16_u16(uint16_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_u16(__p0);\n"
"  return __ret;\n"
"}\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"__ai float16_t vcvth_f16_s32(int32_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_s32(__p0);\n"
"  return __ret;\n"
"}\n"
"#else\n"
"__ai float16_t vcvth_f16_s32(int32_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_s32(__p0);\n"
"  return __ret;\n"
"}\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"__ai float16_t vcvth_f16_s64(int64_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_s64(__p0);\n"
"  return __ret;\n"
"}\n"
"#else\n"
"__ai float16_t vcvth_f16_s64(int64_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_s64(__p0);\n"
"  return __ret;\n"
"}\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"__ai float16_t vcvth_f16_s16(int16_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_s16(__p0);\n"
"  return __ret;\n"
"}\n"
"#else\n"
"__ai float16_t vcvth_f16_s16(int16_t __p0) {\n"
"  float16_t __ret;\n"
"  __ret = (float16_t) __builtin_neon_vcvth_f16_s16(__p0);\n"
"  return __ret;\n"
"}\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_f16_u32(__p0, __p1) __extension__ ({ \\\n"
"  uint32_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_u32(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_f16_u32(__p0, __p1) __extension__ ({ \\\n"
"  uint32_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_u32(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_f16_u64(__p0, __p1) __extension__ ({ \\\n"
"  uint64_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_u64(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_f16_u64(__p0, __p1) __extension__ ({ \\\n"
"  uint64_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_u64(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_f16_u16(__p0, __p1) __extension__ ({ \\\n"
"  uint16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_u16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_f16_u16(__p0, __p1) __extension__ ({ \\\n"
"  uint16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_u16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_f16_s32(__p0, __p1) __extension__ ({ \\\n"
"  int32_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_s32(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_f16_s32(__p0, __p1) __extension__ ({ \\\n"
"  int32_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_s32(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_f16_s64(__p0, __p1) __extension__ ({ \\\n"
"  int64_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_s64(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_f16_s64(__p0, __p1) __extension__ ({ \\\n"
"  int64_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_s64(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvth_n_f16_s16(__p0, __p1) __extension__ ({ \\\n"
"  int16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_s16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvth_n_f16_s16(__p0, __p1) __extension__ ({ \\\n"
"  int16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vcvth_n_f16_s16(__s0, __p1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtmh_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtmh_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtmh_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtmh_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtmh_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtmh_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtmh_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtmh_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtmh_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtmh_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtmh_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtmh_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtmh_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtmh_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtmh_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtmh_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtmh_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtmh_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtmh_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtmh_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtmh_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtmh_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtmh_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtmh_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtnh_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtnh_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtnh_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtnh_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtnh_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtnh_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtnh_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtnh_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtnh_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtnh_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtnh_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtnh_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtnh_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtnh_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtnh_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtnh_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtnh_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtnh_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtnh_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtnh_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtnh_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtnh_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtnh_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtnh_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtph_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtph_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtph_s16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int16_t __ret; \\\n"
"  __ret = (int16_t) __builtin_neon_vcvtph_s16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtph_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtph_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtph_s32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int32_t __ret; \\\n"
"  __ret = (int32_t) __builtin_neon_vcvtph_s32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtph_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtph_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtph_s64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  int64_t __ret; \\\n"
"  __ret = (int64_t) __builtin_neon_vcvtph_s64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtph_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtph_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtph_u16_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint16_t __ret; \\\n"
"  __ret = (uint16_t) __builtin_neon_vcvtph_u16_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtph_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtph_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtph_u32_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint32_t __ret; \\\n"
"  __ret = (uint32_t) __builtin_neon_vcvtph_u32_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vcvtph_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtph_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vcvtph_u64_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  uint64_t __ret; \\\n"
"  __ret = (uint64_t) __builtin_neon_vcvtph_u64_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vdivh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vdivh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vdivh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vdivh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vfmah_f16(__p0, __p1, __p2) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __s2 = __p2; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vfmah_f16(__s0, __s1, __s2); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vfmah_f16(__p0, __p1, __p2) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __s2 = __p2; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vfmah_f16(__s0, __s1, __s2); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vfmsh_f16(__p0, __p1, __p2) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __s2 = __p2; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vfmsh_f16(__s0, __s1, __s2); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vfmsh_f16(__p0, __p1, __p2) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __s2 = __p2; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vfmsh_f16(__s0, __s1, __s2); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vmaxh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmaxh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vmaxh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmaxh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vmaxnmh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmaxnmh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vmaxnmh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmaxnmh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vminh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vminh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vminh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vminh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vminnmh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vminnmh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vminnmh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vminnmh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vmulh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmulh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vmulh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmulh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vmulxh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmulxh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vmulxh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vmulxh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vnegh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vnegh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vnegh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vnegh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrecpeh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrecpeh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrecpeh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrecpeh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrecpsh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrecpsh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrecpsh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrecpsh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrecpxh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrecpxh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrecpxh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrecpxh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrndh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrndh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrndah_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndah_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrndah_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndah_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrndih_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndih_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrndih_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndih_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrndmh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndmh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrndmh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndmh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrndnh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndnh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrndnh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndnh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrndph_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndph_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrndph_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndph_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrndxh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndxh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrndxh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrndxh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrsqrteh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrsqrteh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrsqrteh_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrsqrteh_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vrsqrtsh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrsqrtsh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vrsqrtsh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vrsqrtsh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vsqrth_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vsqrth_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vsqrth_f16(__p0) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vsqrth_f16(__s0); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#ifdef __LITTLE_ENDIAN__\n"
"#define vsubh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vsubh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#else\n"
"#define vsubh_f16(__p0, __p1) __extension__ ({ \\\n"
"  float16_t __s0 = __p0; \\\n"
"  float16_t __s1 = __p1; \\\n"
"  float16_t __ret; \\\n"
"  __ret = (float16_t) __builtin_neon_vsubh_f16(__s0, __s1); \\\n"
"  __ret; \\\n"
"})\n"
"#endif\n"
"\n"
"#endif\n"
"\n"
"#undef __ai\n"
"\n"
"#endif /* __ARM_FP16_H */\n"
"" } , 
 { "/builtins/armintr.h" , "/*===---- armintr.h - ARM Windows intrinsics -------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"/* Only include this if we're compiling for the windows platform. */\n"
"#ifndef _MSC_VER\n"
"#include_next <armintr.h>\n"
"#else\n"
"\n"
"#ifndef __ARMINTR_H\n"
"#define __ARMINTR_H\n"
"\n"
"typedef enum\n"
"{\n"
"  _ARM_BARRIER_SY    = 0xF,\n"
"  _ARM_BARRIER_ST    = 0xE,\n"
"  _ARM_BARRIER_ISH   = 0xB,\n"
"  _ARM_BARRIER_ISHST = 0xA,\n"
"  _ARM_BARRIER_NSH   = 0x7,\n"
"  _ARM_BARRIER_NSHST = 0x6,\n"
"  _ARM_BARRIER_OSH   = 0x3,\n"
"  _ARM_BARRIER_OSHST = 0x2\n"
"} _ARMINTR_BARRIER_TYPE;\n"
"\n"
"#endif /* __ARMINTR_H */\n"
"#endif /* _MSC_VER */\n"
"" } , 
 { "/builtins/avx2intrin.h" , "/*===---- avx2intrin.h - AVX2 intrinsics -----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <avx2intrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __AVX2INTRIN_H\n"
"#define __AVX2INTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS256 __attribute__((__always_inline__, __nodebug__, __target__(\"avx2\"), __min_vector_width__(256)))\n"
"#define __DEFAULT_FN_ATTRS128 __attribute__((__always_inline__, __nodebug__, __target__(\"avx2\"), __min_vector_width__(128)))\n"
"\n"
"/* SSE4 Multiple Packed Sums of Absolute Difference.  */\n"
"#define _mm256_mpsadbw_epu8(X, Y, M) \\\n"
"  (__m256i)__builtin_ia32_mpsadbw256((__v32qi)(__m256i)(X), \\\n"
"                                     (__v32qi)(__m256i)(Y), (int)(M))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_abs_epi8(__m256i __a)\n"
"{\n"
"    return (__m256i)__builtin_ia32_pabsb256((__v32qi)__a);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_abs_epi16(__m256i __a)\n"
"{\n"
"    return (__m256i)__builtin_ia32_pabsw256((__v16hi)__a);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_abs_epi32(__m256i __a)\n"
"{\n"
"    return (__m256i)__builtin_ia32_pabsd256((__v8si)__a);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_packs_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_packsswb256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_packs_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_packssdw256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_packus_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_packuswb256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_packus_epi32(__m256i __V1, __m256i __V2)\n"
"{\n"
"  return (__m256i) __builtin_ia32_packusdw256((__v8si)__V1, (__v8si)__V2);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_add_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v32qu)__a + (__v32qu)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_add_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v16hu)__a + (__v16hu)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_add_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v8su)__a + (__v8su)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_add_epi64(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v4du)__a + (__v4du)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_adds_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_paddsb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_adds_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_paddsw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_adds_epu8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_paddusb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_adds_epu16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_paddusw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"#define _mm256_alignr_epi8(a, b, n) \\\n"
"  (__m256i)__builtin_ia32_palignr256((__v32qi)(__m256i)(a), \\\n"
"                                     (__v32qi)(__m256i)(b), (n))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_and_si256(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v4du)__a & (__v4du)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_andnot_si256(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)(~(__v4du)__a & (__v4du)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_avg_epu8(__m256i __a, __m256i __b)\n"
"{\n"
"  typedef unsigned short __v32hu __attribute__((__vector_size__(64)));\n"
"  return (__m256i)__builtin_convertvector(\n"
"               ((__builtin_convertvector((__v32qu)__a, __v32hu) +\n"
"                 __builtin_convertvector((__v32qu)__b, __v32hu)) + 1)\n"
"                 >> 1, __v32qu);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_avg_epu16(__m256i __a, __m256i __b)\n"
"{\n"
"  typedef unsigned int __v16su __attribute__((__vector_size__(64)));\n"
"  return (__m256i)__builtin_convertvector(\n"
"               ((__builtin_convertvector((__v16hu)__a, __v16su) +\n"
"                 __builtin_convertvector((__v16hu)__b, __v16su)) + 1)\n"
"                 >> 1, __v16hu);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_blendv_epi8(__m256i __V1, __m256i __V2, __m256i __M)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pblendvb256((__v32qi)__V1, (__v32qi)__V2,\n"
"                                              (__v32qi)__M);\n"
"}\n"
"\n"
"#define _mm256_blend_epi16(V1, V2, M) \\\n"
"  (__m256i)__builtin_ia32_pblendw256((__v16hi)(__m256i)(V1), \\\n"
"                                     (__v16hi)(__m256i)(V2), (int)(M))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpeq_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v32qi)__a == (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpeq_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v16hi)__a == (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpeq_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v8si)__a == (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpeq_epi64(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v4di)__a == (__v4di)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpgt_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  /* This function always performs a signed comparison, but __v32qi is a char\n"
"     which may be signed or unsigned, so use __v32qs. */\n"
"  return (__m256i)((__v32qs)__a > (__v32qs)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpgt_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v16hi)__a > (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpgt_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v8si)__a > (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmpgt_epi64(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v4di)__a > (__v4di)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_hadd_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_phaddw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_hadd_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_phaddd256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_hadds_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_phaddsw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_hsub_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_phsubw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_hsub_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_phsubd256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_hsubs_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_phsubsw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_maddubs_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_pmaddubsw256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_madd_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmaddwd256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_max_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmaxsb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_max_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmaxsw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_max_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmaxsd256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_max_epu8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmaxub256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_max_epu16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmaxuw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_max_epu32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmaxud256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_min_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pminsb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_min_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pminsw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_min_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pminsd256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_min_epu8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pminub256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_min_epu16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pminuw256 ((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_min_epu32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pminud256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ int __DEFAULT_FN_ATTRS256\n"
"_mm256_movemask_epi8(__m256i __a)\n"
"{\n"
"  return __builtin_ia32_pmovmskb256((__v32qi)__a);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepi8_epi16(__m128i __V)\n"
"{\n"
"  /* This function always performs a signed extension, but __v16qi is a char\n"
"     which may be signed or unsigned, so use __v16qs. */\n"
"  return (__m256i)__builtin_convertvector((__v16qs)__V, __v16hi);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepi8_epi32(__m128i __V)\n"
"{\n"
"  /* This function always performs a signed extension, but __v16qi is a char\n"
"     which may be signed or unsigned, so use __v16qs. */\n"
"  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepi8_epi64(__m128i __V)\n"
"{\n"
"  /* This function always performs a signed extension, but __v16qi is a char\n"
"     which may be signed or unsigned, so use __v16qs. */\n"
"  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4di);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepi16_epi32(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector((__v8hi)__V, __v8si);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepi16_epi64(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4di);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepi32_epi64(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector((__v4si)__V, __v4di);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepu8_epi16(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector((__v16qu)__V, __v16hi);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepu8_epi32(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepu8_epi64(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4di);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepu16_epi32(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector((__v8hu)__V, __v8si);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepu16_epi64(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4di);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtepu32_epi64(__m128i __V)\n"
"{\n"
"  return (__m256i)__builtin_convertvector((__v4su)__V, __v4di);\n"
"}\n"
"\n"
"static __inline__  __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_mul_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmuldq256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_mulhrs_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmulhrsw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_mulhi_epu16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmulhuw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_mulhi_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pmulhw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_mullo_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v16hu)__a * (__v16hu)__b);\n"
"}\n"
"\n"
"static __inline__  __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_mullo_epi32 (__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v8su)__a * (__v8su)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_mul_epu32(__m256i __a, __m256i __b)\n"
"{\n"
"  return __builtin_ia32_pmuludq256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_or_si256(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v4du)__a | (__v4du)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sad_epu8(__m256i __a, __m256i __b)\n"
"{\n"
"  return __builtin_ia32_psadbw256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_shuffle_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pshufb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"#define _mm256_shuffle_epi32(a, imm) \\\n"
"  (__m256i)__builtin_ia32_pshufd256((__v8si)(__m256i)(a), (int)(imm))\n"
"\n"
"#define _mm256_shufflehi_epi16(a, imm) \\\n"
"  (__m256i)__builtin_ia32_pshufhw256((__v16hi)(__m256i)(a), (int)(imm))\n"
"\n"
"#define _mm256_shufflelo_epi16(a, imm) \\\n"
"  (__m256i)__builtin_ia32_pshuflw256((__v16hi)(__m256i)(a), (int)(imm))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sign_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_psignb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sign_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_psignw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sign_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"    return (__m256i)__builtin_ia32_psignd256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"#define _mm256_slli_si256(a, imm) \\\n"
"  (__m256i)__builtin_ia32_pslldqi256_byteshift((__v4di)(__m256i)(a), (int)(imm))\n"
"\n"
"#define _mm256_bslli_epi128(a, imm) \\\n"
"  (__m256i)__builtin_ia32_pslldqi256_byteshift((__v4di)(__m256i)(a), (int)(imm))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_slli_epi16(__m256i __a, int __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psllwi256((__v16hi)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sll_epi16(__m256i __a, __m128i __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psllw256((__v16hi)__a, (__v8hi)__count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_slli_epi32(__m256i __a, int __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pslldi256((__v8si)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sll_epi32(__m256i __a, __m128i __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_pslld256((__v8si)__a, (__v4si)__count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_slli_epi64(__m256i __a, int __count)\n"
"{\n"
"  return __builtin_ia32_psllqi256((__v4di)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sll_epi64(__m256i __a, __m128i __count)\n"
"{\n"
"  return __builtin_ia32_psllq256((__v4di)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srai_epi16(__m256i __a, int __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrawi256((__v16hi)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sra_epi16(__m256i __a, __m128i __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psraw256((__v16hi)__a, (__v8hi)__count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srai_epi32(__m256i __a, int __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psradi256((__v8si)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sra_epi32(__m256i __a, __m128i __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrad256((__v8si)__a, (__v4si)__count);\n"
"}\n"
"\n"
"#define _mm256_srli_si256(a, imm) \\\n"
"  (__m256i)__builtin_ia32_psrldqi256_byteshift((__m256i)(a), (int)(imm))\n"
"\n"
"#define _mm256_bsrli_epi128(a, imm) \\\n"
"  (__m256i)__builtin_ia32_psrldqi256_byteshift((__m256i)(a), (int)(imm))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srli_epi16(__m256i __a, int __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrlwi256((__v16hi)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srl_epi16(__m256i __a, __m128i __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrlw256((__v16hi)__a, (__v8hi)__count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srli_epi32(__m256i __a, int __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrldi256((__v8si)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srl_epi32(__m256i __a, __m128i __count)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrld256((__v8si)__a, (__v4si)__count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srli_epi64(__m256i __a, int __count)\n"
"{\n"
"  return __builtin_ia32_psrlqi256((__v4di)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srl_epi64(__m256i __a, __m128i __count)\n"
"{\n"
"  return __builtin_ia32_psrlq256((__v4di)__a, __count);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sub_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v32qu)__a - (__v32qu)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sub_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v16hu)__a - (__v16hu)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sub_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v8su)__a - (__v8su)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sub_epi64(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v4du)__a - (__v4du)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_subs_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psubsb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_subs_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psubsw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_subs_epu8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psubusb256((__v32qi)__a, (__v32qi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_subs_epu16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psubusw256((__v16hi)__a, (__v16hi)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpackhi_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 8, 32+8, 9, 32+9, 10, 32+10, 11, 32+11, 12, 32+12, 13, 32+13, 14, 32+14, 15, 32+15, 24, 32+24, 25, 32+25, 26, 32+26, 27, 32+27, 28, 32+28, 29, 32+29, 30, 32+30, 31, 32+31);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpackhi_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpackhi_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 2, 8+2, 3, 8+3, 6, 8+6, 7, 8+7);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpackhi_epi64(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 1, 4+1, 3, 4+3);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpacklo_epi8(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 0, 32+0, 1, 32+1, 2, 32+2, 3, 32+3, 4, 32+4, 5, 32+5, 6, 32+6, 7, 32+7, 16, 32+16, 17, 32+17, 18, 32+18, 19, 32+19, 20, 32+20, 21, 32+21, 22, 32+22, 23, 32+23);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpacklo_epi16(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpacklo_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 0, 8+0, 1, 8+1, 4, 8+4, 5, 8+5);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_unpacklo_epi64(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 0, 4+0, 2, 4+2);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_xor_si256(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)((__v4du)__a ^ (__v4du)__b);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_stream_load_si256(__m256i const *__V)\n"
"{\n"
"  typedef __v4di __v4di_aligned __attribute__((aligned(32)));\n"
"  return (__m256i)__builtin_nontemporal_load((const __v4di_aligned *)__V);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_broadcastss_ps(__m128 __X)\n"
"{\n"
"  return (__m128)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_broadcastsd_pd(__m128d __a)\n"
"{\n"
"  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_broadcastss_ps(__m128 __X)\n"
"{\n"
"  return (__m256)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_broadcastsd_pd(__m128d __X)\n"
"{\n"
"  return (__m256d)__builtin_shufflevector((__v2df)__X, (__v2df)__X, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_broadcastsi128_si256(__m128i __X)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 1, 0, 1);\n"
"}\n"
"\n"
"#define _mm_blend_epi32(V1, V2, M) \\\n"
"  (__m128i)__builtin_ia32_pblendd128((__v4si)(__m128i)(V1), \\\n"
"                                     (__v4si)(__m128i)(V2), (int)(M))\n"
"\n"
"#define _mm256_blend_epi32(V1, V2, M) \\\n"
"  (__m256i)__builtin_ia32_pblendd256((__v8si)(__m256i)(V1), \\\n"
"                                     (__v8si)(__m256i)(V2), (int)(M))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_broadcastb_epi8(__m128i __X)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_broadcastw_epi16(__m128i __X)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_broadcastd_epi32(__m128i __X)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_broadcastq_epi64(__m128i __X)\n"
"{\n"
"  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_broadcastb_epi8(__m128i __X)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_broadcastw_epi16(__m128i __X)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0);\n"
"}\n"
"\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_broadcastd_epi32(__m128i __X)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_broadcastq_epi64(__m128i __X)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_permutevar8x32_epi32(__m256i __a, __m256i __b)\n"
"{\n"
"  return (__m256i)__builtin_ia32_permvarsi256((__v8si)__a, (__v8si)__b);\n"
"}\n"
"\n"
"#define _mm256_permute4x64_pd(V, M) \\\n"
"  (__m256d)__builtin_ia32_permdf256((__v4df)(__m256d)(V), (int)(M))\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_permutevar8x32_ps(__m256 __a, __m256i __b)\n"
"{\n"
"  return (__m256)__builtin_ia32_permvarsf256((__v8sf)__a, (__v8si)__b);\n"
"}\n"
"\n"
"#define _mm256_permute4x64_epi64(V, M) \\\n"
"  (__m256i)__builtin_ia32_permdi256((__v4di)(__m256i)(V), (int)(M))\n"
"\n"
"#define _mm256_permute2x128_si256(V1, V2, M) \\\n"
"  (__m256i)__builtin_ia32_permti256((__m256i)(V1), (__m256i)(V2), (int)(M))\n"
"\n"
"#define _mm256_extracti128_si256(V, M) \\\n"
"  (__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(V), (int)(M))\n"
"\n"
"#define _mm256_inserti128_si256(V1, V2, M) \\\n"
"  (__m256i)__builtin_ia32_insert128i256((__v4di)(__m256i)(V1), \\\n"
"                                        (__v2di)(__m128i)(V2), (int)(M))\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_maskload_epi32(int const *__X, __m256i __M)\n"
"{\n"
"  return (__m256i)__builtin_ia32_maskloadd256((const __v8si *)__X, (__v8si)__M);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_maskload_epi64(long long const *__X, __m256i __M)\n"
"{\n"
"  return (__m256i)__builtin_ia32_maskloadq256((const __v4di *)__X, (__v4di)__M);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_maskload_epi32(int const *__X, __m128i __M)\n"
"{\n"
"  return (__m128i)__builtin_ia32_maskloadd((const __v4si *)__X, (__v4si)__M);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_maskload_epi64(long long const *__X, __m128i __M)\n"
"{\n"
"  return (__m128i)__builtin_ia32_maskloadq((const __v2di *)__X, (__v2di)__M);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS256\n"
"_mm256_maskstore_epi32(int *__X, __m256i __M, __m256i __Y)\n"
"{\n"
"  __builtin_ia32_maskstored256((__v8si *)__X, (__v8si)__M, (__v8si)__Y);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS256\n"
"_mm256_maskstore_epi64(long long *__X, __m256i __M, __m256i __Y)\n"
"{\n"
"  __builtin_ia32_maskstoreq256((__v4di *)__X, (__v4di)__M, (__v4di)__Y);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS128\n"
"_mm_maskstore_epi32(int *__X, __m128i __M, __m128i __Y)\n"
"{\n"
"  __builtin_ia32_maskstored((__v4si *)__X, (__v4si)__M, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS128\n"
"_mm_maskstore_epi64(long long *__X, __m128i __M, __m128i __Y)\n"
"{\n"
"  __builtin_ia32_maskstoreq(( __v2di *)__X, (__v2di)__M, (__v2di)__Y);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sllv_epi32(__m256i __X, __m256i __Y)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psllv8si((__v8si)__X, (__v8si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_sllv_epi32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psllv4si((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_sllv_epi64(__m256i __X, __m256i __Y)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psllv4di((__v4di)__X, (__v4di)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_sllv_epi64(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psllv2di((__v2di)__X, (__v2di)__Y);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srav_epi32(__m256i __X, __m256i __Y)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrav8si((__v8si)__X, (__v8si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_srav_epi32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrav4si((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srlv_epi32(__m256i __X, __m256i __Y)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrlv8si((__v8si)__X, (__v8si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_srlv_epi32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrlv4si((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_srlv_epi64(__m256i __X, __m256i __Y)\n"
"{\n"
"  return (__m256i)__builtin_ia32_psrlv4di((__v4di)__X, (__v4di)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS128\n"
"_mm_srlv_epi64(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrlv2di((__v2di)__X, (__v2di)__Y);\n"
"}\n"
"\n"
"#define _mm_mask_i32gather_pd(a, m, i, mask, s) \\\n"
"  (__m128d)__builtin_ia32_gatherd_pd((__v2df)(__m128i)(a), \\\n"
"                                     (double const *)(m), \\\n"
"                                     (__v4si)(__m128i)(i), \\\n"
"                                     (__v2df)(__m128d)(mask), (s))\n"
"\n"
"#define _mm256_mask_i32gather_pd(a, m, i, mask, s) \\\n"
"  (__m256d)__builtin_ia32_gatherd_pd256((__v4df)(__m256d)(a), \\\n"
"                                        (double const *)(m), \\\n"
"                                        (__v4si)(__m128i)(i), \\\n"
"                                        (__v4df)(__m256d)(mask), (s))\n"
"\n"
"#define _mm_mask_i64gather_pd(a, m, i, mask, s) \\\n"
"  (__m128d)__builtin_ia32_gatherq_pd((__v2df)(__m128d)(a), \\\n"
"                                     (double const *)(m), \\\n"
"                                     (__v2di)(__m128i)(i), \\\n"
"                                     (__v2df)(__m128d)(mask), (s))\n"
"\n"
"#define _mm256_mask_i64gather_pd(a, m, i, mask, s) \\\n"
"  (__m256d)__builtin_ia32_gatherq_pd256((__v4df)(__m256d)(a), \\\n"
"                                        (double const *)(m), \\\n"
"                                        (__v4di)(__m256i)(i), \\\n"
"                                        (__v4df)(__m256d)(mask), (s))\n"
"\n"
"#define _mm_mask_i32gather_ps(a, m, i, mask, s) \\\n"
"  (__m128)__builtin_ia32_gatherd_ps((__v4sf)(__m128)(a), \\\n"
"                                    (float const *)(m), \\\n"
"                                    (__v4si)(__m128i)(i), \\\n"
"                                    (__v4sf)(__m128)(mask), (s))\n"
"\n"
"#define _mm256_mask_i32gather_ps(a, m, i, mask, s) \\\n"
"  (__m256)__builtin_ia32_gatherd_ps256((__v8sf)(__m256)(a), \\\n"
"                                       (float const *)(m), \\\n"
"                                       (__v8si)(__m256i)(i), \\\n"
"                                       (__v8sf)(__m256)(mask), (s))\n"
"\n"
"#define _mm_mask_i64gather_ps(a, m, i, mask, s) \\\n"
"  (__m128)__builtin_ia32_gatherq_ps((__v4sf)(__m128)(a), \\\n"
"                                    (float const *)(m), \\\n"
"                                    (__v2di)(__m128i)(i), \\\n"
"                                    (__v4sf)(__m128)(mask), (s))\n"
"\n"
"#define _mm256_mask_i64gather_ps(a, m, i, mask, s) \\\n"
"  (__m128)__builtin_ia32_gatherq_ps256((__v4sf)(__m128)(a), \\\n"
"                                       (float const *)(m), \\\n"
"                                       (__v4di)(__m256i)(i), \\\n"
"                                       (__v4sf)(__m128)(mask), (s))\n"
"\n"
"#define _mm_mask_i32gather_epi32(a, m, i, mask, s) \\\n"
"  (__m128i)__builtin_ia32_gatherd_d((__v4si)(__m128i)(a), \\\n"
"                                    (int const *)(m), \\\n"
"                                    (__v4si)(__m128i)(i), \\\n"
"                                    (__v4si)(__m128i)(mask), (s))\n"
"\n"
"#define _mm256_mask_i32gather_epi32(a, m, i, mask, s) \\\n"
"  (__m256i)__builtin_ia32_gatherd_d256((__v8si)(__m256i)(a), \\\n"
"                                       (int const *)(m), \\\n"
"                                       (__v8si)(__m256i)(i), \\\n"
"                                       (__v8si)(__m256i)(mask), (s))\n"
"\n"
"#define _mm_mask_i64gather_epi32(a, m, i, mask, s) \\\n"
"  (__m128i)__builtin_ia32_gatherq_d((__v4si)(__m128i)(a), \\\n"
"                                    (int const *)(m), \\\n"
"                                    (__v2di)(__m128i)(i), \\\n"
"                                    (__v4si)(__m128i)(mask), (s))\n"
"\n"
"#define _mm256_mask_i64gather_epi32(a, m, i, mask, s) \\\n"
"  (__m128i)__builtin_ia32_gatherq_d256((__v4si)(__m128i)(a), \\\n"
"                                       (int const *)(m), \\\n"
"                                       (__v4di)(__m256i)(i), \\\n"
"                                       (__v4si)(__m128i)(mask), (s))\n"
"\n"
"#define _mm_mask_i32gather_epi64(a, m, i, mask, s) \\\n"
"  (__m128i)__builtin_ia32_gatherd_q((__v2di)(__m128i)(a), \\\n"
"                                    (long long const *)(m), \\\n"
"                                    (__v4si)(__m128i)(i), \\\n"
"                                    (__v2di)(__m128i)(mask), (s))\n"
"\n"
"#define _mm256_mask_i32gather_epi64(a, m, i, mask, s) \\\n"
"  (__m256i)__builtin_ia32_gatherd_q256((__v4di)(__m256i)(a), \\\n"
"                                       (long long const *)(m), \\\n"
"                                       (__v4si)(__m128i)(i), \\\n"
"                                       (__v4di)(__m256i)(mask), (s))\n"
"\n"
"#define _mm_mask_i64gather_epi64(a, m, i, mask, s) \\\n"
"  (__m128i)__builtin_ia32_gatherq_q((__v2di)(__m128i)(a), \\\n"
"                                    (long long const *)(m), \\\n"
"                                    (__v2di)(__m128i)(i), \\\n"
"                                    (__v2di)(__m128i)(mask), (s))\n"
"\n"
"#define _mm256_mask_i64gather_epi64(a, m, i, mask, s) \\\n"
"  (__m256i)__builtin_ia32_gatherq_q256((__v4di)(__m256i)(a), \\\n"
"                                       (long long const *)(m), \\\n"
"                                       (__v4di)(__m256i)(i), \\\n"
"                                       (__v4di)(__m256i)(mask), (s))\n"
"\n"
"#define _mm_i32gather_pd(m, i, s) \\\n"
"  (__m128d)__builtin_ia32_gatherd_pd((__v2df)_mm_undefined_pd(), \\\n"
"                                     (double const *)(m), \\\n"
"                                     (__v4si)(__m128i)(i), \\\n"
"                                     (__v2df)_mm_cmpeq_pd(_mm_setzero_pd(), \\\n"
"                                                          _mm_setzero_pd()), \\\n"
"                                     (s))\n"
"\n"
"#define _mm256_i32gather_pd(m, i, s) \\\n"
"  (__m256d)__builtin_ia32_gatherd_pd256((__v4df)_mm256_undefined_pd(), \\\n"
"                                        (double const *)(m), \\\n"
"                                        (__v4si)(__m128i)(i), \\\n"
"                                        (__v4df)_mm256_cmp_pd(_mm256_setzero_pd(), \\\n"
"                                                              _mm256_setzero_pd(), \\\n"
"                                                              _CMP_EQ_OQ), \\\n"
"                                        (s))\n"
"\n"
"#define _mm_i64gather_pd(m, i, s) \\\n"
"  (__m128d)__builtin_ia32_gatherq_pd((__v2df)_mm_undefined_pd(), \\\n"
"                                     (double const *)(m), \\\n"
"                                     (__v2di)(__m128i)(i), \\\n"
"                                     (__v2df)_mm_cmpeq_pd(_mm_setzero_pd(), \\\n"
"                                                          _mm_setzero_pd()), \\\n"
"                                     (s))\n"
"\n"
"#define _mm256_i64gather_pd(m, i, s) \\\n"
"  (__m256d)__builtin_ia32_gatherq_pd256((__v4df)_mm256_undefined_pd(), \\\n"
"                                        (double const *)(m), \\\n"
"                                        (__v4di)(__m256i)(i), \\\n"
"                                        (__v4df)_mm256_cmp_pd(_mm256_setzero_pd(), \\\n"
"                                                              _mm256_setzero_pd(), \\\n"
"                                                              _CMP_EQ_OQ), \\\n"
"                                        (s))\n"
"\n"
"#define _mm_i32gather_ps(m, i, s) \\\n"
"  (__m128)__builtin_ia32_gatherd_ps((__v4sf)_mm_undefined_ps(), \\\n"
"                                    (float const *)(m), \\\n"
"                                    (__v4si)(__m128i)(i), \\\n"
"                                    (__v4sf)_mm_cmpeq_ps(_mm_setzero_ps(), \\\n"
"                                                         _mm_setzero_ps()), \\\n"
"                                    (s))\n"
"\n"
"#define _mm256_i32gather_ps(m, i, s) \\\n"
"  (__m256)__builtin_ia32_gatherd_ps256((__v8sf)_mm256_undefined_ps(), \\\n"
"                                       (float const *)(m), \\\n"
"                                       (__v8si)(__m256i)(i), \\\n"
"                                       (__v8sf)_mm256_cmp_ps(_mm256_setzero_ps(), \\\n"
"                                                             _mm256_setzero_ps(), \\\n"
"                                                             _CMP_EQ_OQ), \\\n"
"                                       (s))\n"
"\n"
"#define _mm_i64gather_ps(m, i, s) \\\n"
"  (__m128)__builtin_ia32_gatherq_ps((__v4sf)_mm_undefined_ps(), \\\n"
"                                    (float const *)(m), \\\n"
"                                    (__v2di)(__m128i)(i), \\\n"
"                                    (__v4sf)_mm_cmpeq_ps(_mm_setzero_ps(), \\\n"
"                                                         _mm_setzero_ps()), \\\n"
"                                    (s))\n"
"\n"
"#define _mm256_i64gather_ps(m, i, s) \\\n"
"  (__m128)__builtin_ia32_gatherq_ps256((__v4sf)_mm_undefined_ps(), \\\n"
"                                       (float const *)(m), \\\n"
"                                       (__v4di)(__m256i)(i), \\\n"
"                                       (__v4sf)_mm_cmpeq_ps(_mm_setzero_ps(), \\\n"
"                                                            _mm_setzero_ps()), \\\n"
"                                       (s))\n"
"\n"
"#define _mm_i32gather_epi32(m, i, s) \\\n"
"  (__m128i)__builtin_ia32_gatherd_d((__v4si)_mm_undefined_si128(), \\\n"
"                                    (int const *)(m), (__v4si)(__m128i)(i), \\\n"
"                                    (__v4si)_mm_set1_epi32(-1), (s))\n"
"\n"
"#define _mm256_i32gather_epi32(m, i, s) \\\n"
"  (__m256i)__builtin_ia32_gatherd_d256((__v8si)_mm256_undefined_si256(), \\\n"
"                                       (int const *)(m), (__v8si)(__m256i)(i), \\\n"
"                                       (__v8si)_mm256_set1_epi32(-1), (s))\n"
"\n"
"#define _mm_i64gather_epi32(m, i, s) \\\n"
"  (__m128i)__builtin_ia32_gatherq_d((__v4si)_mm_undefined_si128(), \\\n"
"                                    (int const *)(m), (__v2di)(__m128i)(i), \\\n"
"                                    (__v4si)_mm_set1_epi32(-1), (s))\n"
"\n"
"#define _mm256_i64gather_epi32(m, i, s) \\\n"
"  (__m128i)__builtin_ia32_gatherq_d256((__v4si)_mm_undefined_si128(), \\\n"
"                                       (int const *)(m), (__v4di)(__m256i)(i), \\\n"
"                                       (__v4si)_mm_set1_epi32(-1), (s))\n"
"\n"
"#define _mm_i32gather_epi64(m, i, s) \\\n"
"  (__m128i)__builtin_ia32_gatherd_q((__v2di)_mm_undefined_si128(), \\\n"
"                                    (long long const *)(m), \\\n"
"                                    (__v4si)(__m128i)(i), \\\n"
"                                    (__v2di)_mm_set1_epi64x(-1), (s))\n"
"\n"
"#define _mm256_i32gather_epi64(m, i, s) \\\n"
"  (__m256i)__builtin_ia32_gatherd_q256((__v4di)_mm256_undefined_si256(), \\\n"
"                                       (long long const *)(m), \\\n"
"                                       (__v4si)(__m128i)(i), \\\n"
"                                       (__v4di)_mm256_set1_epi64x(-1), (s))\n"
"\n"
"#define _mm_i64gather_epi64(m, i, s) \\\n"
"  (__m128i)__builtin_ia32_gatherq_q((__v2di)_mm_undefined_si128(), \\\n"
"                                    (long long const *)(m), \\\n"
"                                    (__v2di)(__m128i)(i), \\\n"
"                                    (__v2di)_mm_set1_epi64x(-1), (s))\n"
"\n"
"#define _mm256_i64gather_epi64(m, i, s) \\\n"
"  (__m256i)__builtin_ia32_gatherq_q256((__v4di)_mm256_undefined_si256(), \\\n"
"                                       (long long const *)(m), \\\n"
"                                       (__v4di)(__m256i)(i), \\\n"
"                                       (__v4di)_mm256_set1_epi64x(-1), (s))\n"
"\n"
"#undef __DEFAULT_FN_ATTRS256\n"
"#undef __DEFAULT_FN_ATTRS128\n"
"\n"
"#endif /* __AVX2INTRIN_H */\n"
"" } , 
 { "/builtins/avxintrin.h" , "/*===---- avxintrin.h - AVX intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <avxintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __AVXINTRIN_H\n"
"#define __AVXINTRIN_H\n"
"\n"
"typedef double __v4df __attribute__ ((__vector_size__ (32)));\n"
"typedef float __v8sf __attribute__ ((__vector_size__ (32)));\n"
"typedef long long __v4di __attribute__ ((__vector_size__ (32)));\n"
"typedef int __v8si __attribute__ ((__vector_size__ (32)));\n"
"typedef short __v16hi __attribute__ ((__vector_size__ (32)));\n"
"typedef char __v32qi __attribute__ ((__vector_size__ (32)));\n"
"\n"
"/* Unsigned types */\n"
"typedef unsigned long long __v4du __attribute__ ((__vector_size__ (32)));\n"
"typedef unsigned int __v8su __attribute__ ((__vector_size__ (32)));\n"
"typedef unsigned short __v16hu __attribute__ ((__vector_size__ (32)));\n"
"typedef unsigned char __v32qu __attribute__ ((__vector_size__ (32)));\n"
"\n"
"/* We need an explicitly signed variant for char. Note that this shouldn't\n"
" * appear in the interface though. */\n"
"typedef signed char __v32qs __attribute__((__vector_size__(32)));\n"
"\n"
"typedef float __m256 __attribute__ ((__vector_size__ (32)));\n"
"typedef double __m256d __attribute__((__vector_size__(32)));\n"
"typedef long long __m256i __attribute__((__vector_size__(32)));\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"avx\"), __min_vector_width__(256)))\n"
"#define __DEFAULT_FN_ATTRS128 __attribute__((__always_inline__, __nodebug__, __target__(\"avx\"), __min_vector_width__(128)))\n"
"\n"
"/* Arithmetic */\n"
"/// Adds two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the sums of both\n"
"///    operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_add_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)((__v4df)__a+(__v4df)__b);\n"
"}\n"
"\n"
"/// Adds two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the sums of both\n"
"///    operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_add_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)((__v8sf)__a+(__v8sf)__b);\n"
"}\n"
"\n"
"/// Subtracts two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSUBPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the minuend.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing the subtrahend.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the differences between\n"
"///    both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_sub_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)((__v4df)__a-(__v4df)__b);\n"
"}\n"
"\n"
"/// Subtracts two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSUBPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the minuend.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing the subtrahend.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the differences between\n"
"///    both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_sub_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)((__v8sf)__a-(__v8sf)__b);\n"
"}\n"
"\n"
"/// Adds the even-indexed values and subtracts the odd-indexed values of\n"
"///    two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDSUBPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the left source operand.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing the right source operand.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the alternating sums\n"
"///    and differences between both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_addsub_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)__builtin_ia32_addsubpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Adds the even-indexed values and subtracts the odd-indexed values of\n"
"///    two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDSUBPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the left source operand.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing the right source operand.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the alternating sums and\n"
"///    differences between both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_addsub_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)__builtin_ia32_addsubps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/// Divides two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDIVPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the dividend.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing the divisor.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the quotients of both\n"
"///    operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_div_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)((__v4df)__a/(__v4df)__b);\n"
"}\n"
"\n"
"/// Divides two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDIVPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the dividend.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing the divisor.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the quotients of both\n"
"///    operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_div_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)((__v8sf)__a/(__v8sf)__b);\n"
"}\n"
"\n"
"/// Compares two 256-bit vectors of [4 x double] and returns the greater\n"
"///    of each pair of values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMAXPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the operands.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the maximum values\n"
"///    between both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_max_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)__builtin_ia32_maxpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Compares two 256-bit vectors of [8 x float] and returns the greater\n"
"///    of each pair of values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMAXPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the operands.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the maximum values\n"
"///    between both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_max_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)__builtin_ia32_maxps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/// Compares two 256-bit vectors of [4 x double] and returns the lesser\n"
"///    of each pair of values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMINPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the operands.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the minimum values\n"
"///    between both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_min_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)__builtin_ia32_minpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Compares two 256-bit vectors of [8 x float] and returns the lesser\n"
"///    of each pair of values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMINPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the operands.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the minimum values\n"
"///    between both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_min_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)__builtin_ia32_minps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/// Multiplies two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMULPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the operands.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the products of both\n"
"///    operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_mul_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)((__v4df)__a * (__v4df)__b);\n"
"}\n"
"\n"
"/// Multiplies two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMULPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the operands.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the products of both\n"
"///    operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_mul_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)((__v8sf)__a * (__v8sf)__b);\n"
"}\n"
"\n"
"/// Calculates the square roots of the values in a 256-bit vector of\n"
"///    [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSQRTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns A 256-bit vector of [4 x double] containing the square roots of the\n"
"///    values in the operand.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_sqrt_pd(__m256d __a)\n"
"{\n"
"  return (__m256d)__builtin_ia32_sqrtpd256((__v4df)__a);\n"
"}\n"
"\n"
"/// Calculates the square roots of the values in a 256-bit vector of\n"
"///    [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSQRTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 256-bit vector of [8 x float] containing the square roots of the\n"
"///    values in the operand.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_sqrt_ps(__m256 __a)\n"
"{\n"
"  return (__m256)__builtin_ia32_sqrtps256((__v8sf)__a);\n"
"}\n"
"\n"
"/// Calculates the reciprocal square roots of the values in a 256-bit\n"
"///    vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VRSQRTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 256-bit vector of [8 x float] containing the reciprocal square\n"
"///    roots of the values in the operand.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_rsqrt_ps(__m256 __a)\n"
"{\n"
"  return (__m256)__builtin_ia32_rsqrtps256((__v8sf)__a);\n"
"}\n"
"\n"
"/// Calculates the reciprocals of the values in a 256-bit vector of\n"
"///    [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VRCPPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 256-bit vector of [8 x float] containing the reciprocals of the\n"
"///    values in the operand.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_rcp_ps(__m256 __a)\n"
"{\n"
"  return (__m256)__builtin_ia32_rcpps256((__v8sf)__a);\n"
"}\n"
"\n"
"/// Rounds the values in a 256-bit vector of [4 x double] as specified\n"
"///    by the byte operand. The source values are rounded to integer values and\n"
"///    returned as 64-bit double-precision floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_round_pd(__m256d V, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPD </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param M\n"
"///    An integer value that specifies the rounding operation. \\n\n"
"///    Bits [7:4] are reserved. \\n\n"
"///    Bit [3] is a precision exception value: \\n\n"
"///      0: A normal PE exception is used. \\n\n"
"///      1: The PE field is not updated. \\n\n"
"///    Bit [2] is the rounding control source: \\n\n"
"///      0: Use bits [1:0] of \\a M. \\n\n"
"///      1: Use the current MXCSR setting. \\n\n"
"///    Bits [1:0] contain the rounding control definition: \\n\n"
"///      00: Nearest. \\n\n"
"///      01: Downward (toward negative infinity). \\n\n"
"///      10: Upward (toward positive infinity). \\n\n"
"///      11: Truncated.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the rounded values.\n"
"#define _mm256_round_pd(V, M) \\\n"
"    (__m256d)__builtin_ia32_roundpd256((__v4df)(__m256d)(V), (M))\n"
"\n"
"/// Rounds the values stored in a 256-bit vector of [8 x float] as\n"
"///    specified by the byte operand. The source values are rounded to integer\n"
"///    values and returned as floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_round_ps(__m256 V, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPS </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param M\n"
"///    An integer value that specifies the rounding operation. \\n\n"
"///    Bits [7:4] are reserved. \\n\n"
"///    Bit [3] is a precision exception value: \\n\n"
"///      0: A normal PE exception is used. \\n\n"
"///      1: The PE field is not updated. \\n\n"
"///    Bit [2] is the rounding control source: \\n\n"
"///      0: Use bits [1:0] of \\a M. \\n\n"
"///      1: Use the current MXCSR setting. \\n\n"
"///    Bits [1:0] contain the rounding control definition: \\n\n"
"///      00: Nearest. \\n\n"
"///      01: Downward (toward negative infinity). \\n\n"
"///      10: Upward (toward positive infinity). \\n\n"
"///      11: Truncated.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the rounded values.\n"
"#define _mm256_round_ps(V, M) \\\n"
"  (__m256)__builtin_ia32_roundps256((__v8sf)(__m256)(V), (M))\n"
"\n"
"/// Rounds up the values stored in a 256-bit vector of [4 x double]. The\n"
"///    source values are rounded up to integer values and returned as 64-bit\n"
"///    double-precision floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_ceil_pd(__m256d V);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPD </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns A 256-bit vector of [4 x double] containing the rounded up values.\n"
"#define _mm256_ceil_pd(V)  _mm256_round_pd((V), _MM_FROUND_CEIL)\n"
"\n"
"/// Rounds down the values stored in a 256-bit vector of [4 x double].\n"
"///    The source values are rounded down to integer values and returned as\n"
"///    64-bit double-precision floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_floor_pd(__m256d V);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPD </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns A 256-bit vector of [4 x double] containing the rounded down\n"
"///    values.\n"
"#define _mm256_floor_pd(V) _mm256_round_pd((V), _MM_FROUND_FLOOR)\n"
"\n"
"/// Rounds up the values stored in a 256-bit vector of [8 x float]. The\n"
"///    source values are rounded up to integer values and returned as\n"
"///    floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_ceil_ps(__m256 V);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPS </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 256-bit vector of [8 x float] containing the rounded up values.\n"
"#define _mm256_ceil_ps(V)  _mm256_round_ps((V), _MM_FROUND_CEIL)\n"
"\n"
"/// Rounds down the values stored in a 256-bit vector of [8 x float]. The\n"
"///    source values are rounded down to integer values and returned as\n"
"///    floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_floor_ps(__m256 V);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPS </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 256-bit vector of [8 x float] containing the rounded down values.\n"
"#define _mm256_floor_ps(V) _mm256_round_ps((V), _MM_FROUND_FLOOR)\n"
"\n"
"/* Logical */\n"
"/// Performs a bitwise AND of two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VANDPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the bitwise AND of the\n"
"///    values between both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_and_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)((__v4du)__a & (__v4du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VANDPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the bitwise AND of the\n"
"///    values between both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_and_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)((__v8su)__a & (__v8su)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 256-bit vectors of [4 x double], using\n"
"///    the one's complement of the values contained in the first source operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VANDNPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the left source operand. The\n"
"///    one's complement of this value is used in the bitwise AND.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing the right source operand.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the bitwise AND of the\n"
"///    values of the second operand and the one's complement of the first\n"
"///    operand.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_andnot_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)(~(__v4du)__a & (__v4du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 256-bit vectors of [8 x float], using\n"
"///    the one's complement of the values contained in the first source operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VANDNPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the left source operand. The\n"
"///    one's complement of this value is used in the bitwise AND.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing the right source operand.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the bitwise AND of the\n"
"///    values of the second operand and the one's complement of the first\n"
"///    operand.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_andnot_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)(~(__v8su)__a & (__v8su)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise OR of two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VORPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the bitwise OR of the\n"
"///    values between both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_or_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)((__v4du)__a | (__v4du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise OR of two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VORPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the bitwise OR of the\n"
"///    values between both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_or_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)((__v8su)__a | (__v8su)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise XOR of two 256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the bitwise XOR of the\n"
"///    values between both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_xor_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)((__v4du)__a ^ (__v4du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise XOR of two 256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the bitwise XOR of the\n"
"///    values between both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_xor_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)((__v8su)__a ^ (__v8su)__b);\n"
"}\n"
"\n"
"/* Horizontal arithmetic */\n"
"/// Horizontally adds the adjacent pairs of values contained in two\n"
"///    256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHADDPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"///    The horizontal sums of the values are returned in the even-indexed\n"
"///    elements of a vector of [4 x double].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"///    The horizontal sums of the values are returned in the odd-indexed\n"
"///    elements of a vector of [4 x double].\n"
"/// \\returns A 256-bit vector of [4 x double] containing the horizontal sums of\n"
"///    both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_hadd_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)__builtin_ia32_haddpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in two\n"
"///    256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHADDPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"///    The horizontal sums of the values are returned in the elements with\n"
"///    index 0, 1, 4, 5 of a vector of [8 x float].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"///    The horizontal sums of the values are returned in the elements with\n"
"///    index 2, 3, 6, 7 of a vector of [8 x float].\n"
"/// \\returns A 256-bit vector of [8 x float] containing the horizontal sums of\n"
"///    both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_hadd_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)__builtin_ia32_haddps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in two\n"
"///    256-bit vectors of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHSUBPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"///    The horizontal differences between the values are returned in the\n"
"///    even-indexed elements of a vector of [4 x double].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing one of the source operands.\n"
"///    The horizontal differences between the values are returned in the\n"
"///    odd-indexed elements of a vector of [4 x double].\n"
"/// \\returns A 256-bit vector of [4 x double] containing the horizontal\n"
"///    differences of both operands.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_hsub_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return (__m256d)__builtin_ia32_hsubpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in two\n"
"///    256-bit vectors of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHSUBPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"///    The horizontal differences between the values are returned in the\n"
"///    elements with index 0, 1, 4, 5 of a vector of [8 x float].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float] containing one of the source operands.\n"
"///    The horizontal differences between the values are returned in the\n"
"///    elements with index 2, 3, 6, 7 of a vector of [8 x float].\n"
"/// \\returns A 256-bit vector of [8 x float] containing the horizontal\n"
"///    differences of both operands.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_hsub_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return (__m256)__builtin_ia32_hsubps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/* Vector permutations */\n"
"/// Copies the values in a 128-bit vector of [2 x double] as specified\n"
"///    by the 128-bit integer vector operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __c\n"
"///    A 128-bit integer vector operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bit [1]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned\n"
"///         vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [63:0] of the\n"
"///         returned vector. \\n\n"
"///    Bit [65]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [127:64] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [127:64] of the\n"
"///         returned vector.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the copied values.\n"
"static __inline __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_permutevar_pd(__m128d __a, __m128i __c)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vpermilvarpd((__v2df)__a, (__v2di)__c);\n"
"}\n"
"\n"
"/// Copies the values in a 256-bit vector of [4 x double] as specified\n"
"///    by the 256-bit integer vector operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param __c\n"
"///    A 256-bit integer vector operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bit [1]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned\n"
"///         vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [63:0] of the\n"
"///         returned vector. \\n\n"
"///    Bit [65]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [127:64] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [127:64] of the\n"
"///         returned vector. \\n\n"
"///    Bit [129]: \\n\n"
"///      0: Bits [191:128] of the source are copied to bits [191:128] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [255:192] of the source are copied to bits [191:128] of the\n"
"///         returned vector. \\n\n"
"///    Bit [193]: \\n\n"
"///      0: Bits [191:128] of the source are copied to bits [255:192] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [255:192] of the source are copied to bits [255:192] of the\n"
"///    returned vector.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the copied values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_permutevar_pd(__m256d __a, __m256i __c)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vpermilvarpd256((__v4df)__a, (__v4di)__c);\n"
"}\n"
"\n"
"/// Copies the values stored in a 128-bit vector of [4 x float] as\n"
"///    specified by the 128-bit integer vector operand.\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __c\n"
"///    A 128-bit integer vector operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bits [1:0]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///    Bits [33:32]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///    Bits [65:64]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///    Bits [97:96]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [127:96] of the\n"
"///          returned vector.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied values.\n"
"static __inline __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_permutevar_ps(__m128 __a, __m128i __c)\n"
"{\n"
"  return (__m128)__builtin_ia32_vpermilvarps((__v4sf)__a, (__v4si)__c);\n"
"}\n"
"\n"
"/// Copies the values stored in a 256-bit vector of [8 x float] as\n"
"///    specified by the 256-bit integer vector operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param __c\n"
"///    A 256-bit integer vector operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bits [1:0]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///    Bits [33:32]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///    Bits [65:64]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///    Bits [97:96]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///    Bits [129:128]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///    Bits [161:160]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///    Bits [193:192]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///    Bits [225:224]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [255:224] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [255:224] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [255:224] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [255:224] of the\n"
"///          returned vector.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the copied values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_permutevar_ps(__m256 __a, __m256i __c)\n"
"{\n"
"  return (__m256)__builtin_ia32_vpermilvarps256((__v8sf)__a, (__v8si)__c);\n"
"}\n"
"\n"
"/// Copies the values in a 128-bit vector of [2 x double] as specified\n"
"///    by the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_permute_pd(__m128d A, const int C);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param C\n"
"///    An immediate integer operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bit [0]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned\n"
"///         vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [63:0] of the\n"
"///         returned vector. \\n\n"
"///    Bit [1]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [127:64] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [127:64] of the\n"
"///         returned vector.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the copied values.\n"
"#define _mm_permute_pd(A, C) \\\n"
"  (__m128d)__builtin_ia32_vpermilpd((__v2df)(__m128d)(A), (int)(C))\n"
"\n"
"/// Copies the values in a 256-bit vector of [4 x double] as specified by\n"
"///    the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_permute_pd(__m256d A, const int C);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param C\n"
"///    An immediate integer operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bit [0]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned\n"
"///         vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [63:0] of the\n"
"///         returned vector. \\n\n"
"///    Bit [1]: \\n\n"
"///      0: Bits [63:0] of the source are copied to bits [127:64] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [127:64] of the source are copied to bits [127:64] of the\n"
"///         returned vector. \\n\n"
"///    Bit [2]: \\n\n"
"///      0: Bits [191:128] of the source are copied to bits [191:128] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [255:192] of the source are copied to bits [191:128] of the\n"
"///         returned vector. \\n\n"
"///    Bit [3]: \\n\n"
"///      0: Bits [191:128] of the source are copied to bits [255:192] of the\n"
"///         returned vector. \\n\n"
"///      1: Bits [255:192] of the source are copied to bits [255:192] of the\n"
"///         returned vector.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the copied values.\n"
"#define _mm256_permute_pd(A, C) \\\n"
"  (__m256d)__builtin_ia32_vpermilpd256((__v4df)(__m256d)(A), (int)(C))\n"
"\n"
"/// Copies the values in a 128-bit vector of [4 x float] as specified by\n"
"///    the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_permute_ps(__m128 A, const int C);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param C\n"
"///    An immediate integer operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bits [1:0]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///    Bits [3:2]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///    Bits [5:4]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///    Bits [7:6]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [127:96] of the\n"
"///          returned vector.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied values.\n"
"#define _mm_permute_ps(A, C) \\\n"
"  (__m128)__builtin_ia32_vpermilps((__v4sf)(__m128)(A), (int)(C))\n"
"\n"
"/// Copies the values in a 256-bit vector of [8 x float] as specified by\n"
"///    the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_permute_ps(__m256 A, const int C);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param C\n"
"///    An immediate integer operand specifying how the values are to be\n"
"///    copied. \\n\n"
"///    Bits [1:0]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [31:0] of the\n"
"///          returned vector. \\n\n"
"///    Bits [3:2]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [63:32] of the\n"
"///          returned vector. \\n\n"
"///    Bits [5:4]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [95:64] of the\n"
"///          returned vector. \\n\n"
"///    Bits [7:6]: \\n\n"
"///      00: Bits [31:0] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [63:32] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [95:64] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [127:96] of the source are copied to bits [127:96] of the\n"
"///          returned vector. \\n\n"
"///    Bits [1:0]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [159:128] of the\n"
"///          returned vector. \\n\n"
"///    Bits [3:2]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [191:160] of the\n"
"///          returned vector. \\n\n"
"///    Bits [5:4]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [223:192] of the\n"
"///          returned vector. \\n\n"
"///    Bits [7:6]: \\n\n"
"///      00: Bits [159:128] of the source are copied to bits [255:224] of the\n"
"///          returned vector. \\n\n"
"///      01: Bits [191:160] of the source are copied to bits [255:224] of the\n"
"///          returned vector. \\n\n"
"///      10: Bits [223:192] of the source are copied to bits [255:224] of the\n"
"///          returned vector. \\n\n"
"///      11: Bits [255:224] of the source are copied to bits [255:224] of the\n"
"///          returned vector.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the copied values.\n"
"#define _mm256_permute_ps(A, C) \\\n"
"  (__m256)__builtin_ia32_vpermilps256((__v8sf)(__m256)(A), (int)(C))\n"
"\n"
"/// Permutes 128-bit data values stored in two 256-bit vectors of\n"
"///    [4 x double], as specified by the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_permute2f128_pd(__m256d V1, __m256d V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERM2F128 </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param V2\n"
"///    A 256-bit vector of [4 x double.\n"
"/// \\param M\n"
"///    An immediate integer operand specifying how the values are to be\n"
"///    permuted. \\n\n"
"///    Bits [1:0]: \\n\n"
"///      00: Bits [127:0] of operand \\a V1 are copied to bits [127:0] of the\n"
"///          destination. \\n\n"
"///      01: Bits [255:128] of operand \\a V1 are copied to bits [127:0] of the\n"
"///          destination. \\n\n"
"///      10: Bits [127:0] of operand \\a V2 are copied to bits [127:0] of the\n"
"///          destination. \\n\n"
"///      11: Bits [255:128] of operand \\a V2 are copied to bits [127:0] of the\n"
"///          destination. \\n\n"
"///    Bits [5:4]: \\n\n"
"///      00: Bits [127:0] of operand \\a V1 are copied to bits [255:128] of the\n"
"///          destination. \\n\n"
"///      01: Bits [255:128] of operand \\a V1 are copied to bits [255:128] of the\n"
"///          destination. \\n\n"
"///      10: Bits [127:0] of operand \\a V2 are copied to bits [255:128] of the\n"
"///          destination. \\n\n"
"///      11: Bits [255:128] of operand \\a V2 are copied to bits [255:128] of the\n"
"///          destination.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the copied values.\n"
"#define _mm256_permute2f128_pd(V1, V2, M) \\\n"
"  (__m256d)__builtin_ia32_vperm2f128_pd256((__v4df)(__m256d)(V1), \\\n"
"                                           (__v4df)(__m256d)(V2), (int)(M))\n"
"\n"
"/// Permutes 128-bit data values stored in two 256-bit vectors of\n"
"///    [8 x float], as specified by the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_permute2f128_ps(__m256 V1, __m256 V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERM2F128 </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param V2\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param M\n"
"///    An immediate integer operand specifying how the values are to be\n"
"///    permuted. \\n\n"
"///    Bits [1:0]: \\n\n"
"///    00: Bits [127:0] of operand \\a V1 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    01: Bits [255:128] of operand \\a V1 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    10: Bits [127:0] of operand \\a V2 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    11: Bits [255:128] of operand \\a V2 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    Bits [5:4]: \\n\n"
"///    00: Bits [127:0] of operand \\a V1 are copied to bits [255:128] of the\n"
"///    destination. \\n\n"
"///    01: Bits [255:128] of operand \\a V1 are copied to bits [255:128] of the\n"
"///    destination. \\n\n"
"///    10: Bits [127:0] of operand \\a V2 are copied to bits [255:128] of the\n"
"///    destination. \\n\n"
"///    11: Bits [255:128] of operand \\a V2 are copied to bits [255:128] of the\n"
"///    destination.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the copied values.\n"
"#define _mm256_permute2f128_ps(V1, V2, M) \\\n"
"  (__m256)__builtin_ia32_vperm2f128_ps256((__v8sf)(__m256)(V1), \\\n"
"                                          (__v8sf)(__m256)(V2), (int)(M))\n"
"\n"
"/// Permutes 128-bit data values stored in two 256-bit integer vectors,\n"
"///    as specified by the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256i _mm256_permute2f128_si256(__m256i V1, __m256i V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERM2F128 </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit integer vector.\n"
"/// \\param V2\n"
"///    A 256-bit integer vector.\n"
"/// \\param M\n"
"///    An immediate integer operand specifying how the values are to be copied.\n"
"///    Bits [1:0]: \\n\n"
"///    00: Bits [127:0] of operand \\a V1 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    01: Bits [255:128] of operand \\a V1 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    10: Bits [127:0] of operand \\a V2 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    11: Bits [255:128] of operand \\a V2 are copied to bits [127:0] of the\n"
"///    destination. \\n\n"
"///    Bits [5:4]: \\n\n"
"///    00: Bits [127:0] of operand \\a V1 are copied to bits [255:128] of the\n"
"///    destination. \\n\n"
"///    01: Bits [255:128] of operand \\a V1 are copied to bits [255:128] of the\n"
"///    destination. \\n\n"
"///    10: Bits [127:0] of operand \\a V2 are copied to bits [255:128] of the\n"
"///    destination. \\n\n"
"///    11: Bits [255:128] of operand \\a V2 are copied to bits [255:128] of the\n"
"///    destination.\n"
"/// \\returns A 256-bit integer vector containing the copied values.\n"
"#define _mm256_permute2f128_si256(V1, V2, M) \\\n"
"  (__m256i)__builtin_ia32_vperm2f128_si256((__v8si)(__m256i)(V1), \\\n"
"                                           (__v8si)(__m256i)(V2), (int)(M))\n"
"\n"
"/* Vector Blend */\n"
"/// Merges 64-bit double-precision data values stored in either of the\n"
"///    two 256-bit vectors of [4 x double], as specified by the immediate\n"
"///    integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_blend_pd(__m256d V1, __m256d V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDPD </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param V2\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param M\n"
"///    An immediate integer operand, with mask bits [3:0] specifying how the\n"
"///    values are to be copied. The position of the mask bit corresponds to the\n"
"///    index of a copied value. When a mask bit is 0, the corresponding 64-bit\n"
"///    element in operand \\a V1 is copied to the same position in the\n"
"///    destination. When a mask bit is 1, the corresponding 64-bit element in\n"
"///    operand \\a V2 is copied to the same position in the destination.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the copied values.\n"
"#define _mm256_blend_pd(V1, V2, M) \\\n"
"  (__m256d)__builtin_ia32_blendpd256((__v4df)(__m256d)(V1), \\\n"
"                                     (__v4df)(__m256d)(V2), (int)(M))\n"
"\n"
"/// Merges 32-bit single-precision data values stored in either of the\n"
"///    two 256-bit vectors of [8 x float], as specified by the immediate\n"
"///    integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_blend_ps(__m256 V1, __m256 V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDPS </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param V2\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param M\n"
"///    An immediate integer operand, with mask bits [7:0] specifying how the\n"
"///    values are to be copied. The position of the mask bit corresponds to the\n"
"///    index of a copied value. When a mask bit is 0, the corresponding 32-bit\n"
"///    element in operand \\a V1 is copied to the same position in the\n"
"///    destination. When a mask bit is 1, the corresponding 32-bit element in\n"
"///    operand \\a V2 is copied to the same position in the destination.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the copied values.\n"
"#define _mm256_blend_ps(V1, V2, M) \\\n"
"  (__m256)__builtin_ia32_blendps256((__v8sf)(__m256)(V1), \\\n"
"                                    (__v8sf)(__m256)(V2), (int)(M))\n"
"\n"
"/// Merges 64-bit double-precision data values stored in either of the\n"
"///    two 256-bit vectors of [4 x double], as specified by the 256-bit vector\n"
"///    operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDVPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param __c\n"
"///    A 256-bit vector operand, with mask bits 255, 191, 127, and 63 specifying\n"
"///    how the values are to be copied. The position of the mask bit corresponds\n"
"///    to the most significant bit of a copied value. When a mask bit is 0, the\n"
"///    corresponding 64-bit element in operand \\a __a is copied to the same\n"
"///    position in the destination. When a mask bit is 1, the corresponding\n"
"///    64-bit element in operand \\a __b is copied to the same position in the\n"
"///    destination.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the copied values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_blendv_pd(__m256d __a, __m256d __b, __m256d __c)\n"
"{\n"
"  return (__m256d)__builtin_ia32_blendvpd256(\n"
"    (__v4df)__a, (__v4df)__b, (__v4df)__c);\n"
"}\n"
"\n"
"/// Merges 32-bit single-precision data values stored in either of the\n"
"///    two 256-bit vectors of [8 x float], as specified by the 256-bit vector\n"
"///    operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDVPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param __c\n"
"///    A 256-bit vector operand, with mask bits 255, 223, 191, 159, 127, 95, 63,\n"
"///    and 31 specifying how the values are to be copied. The position of the\n"
"///    mask bit corresponds to the most significant bit of a copied value. When\n"
"///    a mask bit is 0, the corresponding 32-bit element in operand \\a __a is\n"
"///    copied to the same position in the destination. When a mask bit is 1, the\n"
"///    corresponding 32-bit element in operand \\a __b is copied to the same\n"
"///    position in the destination.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the copied values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_blendv_ps(__m256 __a, __m256 __b, __m256 __c)\n"
"{\n"
"  return (__m256)__builtin_ia32_blendvps256(\n"
"    (__v8sf)__a, (__v8sf)__b, (__v8sf)__c);\n"
"}\n"
"\n"
"/* Vector Dot Product */\n"
"/// Computes two dot products in parallel, using the lower and upper\n"
"///    halves of two [8 x float] vectors as input to the two computations, and\n"
"///    returning the two dot products in the lower and upper halves of the\n"
"///    [8 x float] result.\n"
"///\n"
"///    The immediate integer operand controls which input elements will\n"
"///    contribute to the dot product, and where the final results are returned.\n"
"///    In general, for each dot product, the four corresponding elements of the\n"
"///    input vectors are multiplied; the first two and second two products are\n"
"///    summed, then the two sums are added to form the final result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_dp_ps(__m256 V1, __m256 V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDPPS </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A vector of [8 x float] values, treated as two [4 x float] vectors.\n"
"/// \\param V2\n"
"///    A vector of [8 x float] values, treated as two [4 x float] vectors.\n"
"/// \\param M\n"
"///    An immediate integer argument. Bits [7:4] determine which elements of\n"
"///    the input vectors are used, with bit [4] corresponding to the lowest\n"
"///    element and bit [7] corresponding to the highest element of each [4 x\n"
"///    float] subvector. If a bit is set, the corresponding elements from the\n"
"///    two input vectors are used as an input for dot product; otherwise that\n"
"///    input is treated as zero. Bits [3:0] determine which elements of the\n"
"///    result will receive a copy of the final dot product, with bit [0]\n"
"///    corresponding to the lowest element and bit [3] corresponding to the\n"
"///    highest element of each [4 x float] subvector. If a bit is set, the dot\n"
"///    product is returned in the corresponding element; otherwise that element\n"
"///    is set to zero. The bitmask is applied in the same way to each of the\n"
"///    two parallel dot product computations.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the two dot products.\n"
"#define _mm256_dp_ps(V1, V2, M) \\\n"
"  (__m256)__builtin_ia32_dpps256((__v8sf)(__m256)(V1), \\\n"
"                                 (__v8sf)(__m256)(V2), (M))\n"
"\n"
"/* Vector shuffle */\n"
"/// Selects 8 float values from the 256-bit operands of [8 x float], as\n"
"///    specified by the immediate value operand.\n"
"///\n"
"///    The four selected elements in each operand are copied to the destination\n"
"///    according to the bits specified in the immediate operand. The selected\n"
"///    elements from the first 256-bit operand are copied to bits [63:0] and\n"
"///    bits [191:128] of the destination, and the selected elements from the\n"
"///    second 256-bit operand are copied to bits [127:64] and bits [255:192] of\n"
"///    the destination. For example, if bits [7:0] of the immediate operand\n"
"///    contain a value of 0xFF, the 256-bit destination vector would contain the\n"
"///    following values: b[7], b[7], a[7], a[7], b[3], b[3], a[3], a[3].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_shuffle_ps(__m256 a, __m256 b, const int mask);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSHUFPS </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 256-bit vector of [8 x float]. The four selected elements in this\n"
"///    operand are copied to bits [63:0] and bits [191:128] in the destination,\n"
"///    according to the bits specified in the immediate operand.\n"
"/// \\param b\n"
"///    A 256-bit vector of [8 x float]. The four selected elements in this\n"
"///    operand are copied to bits [127:64] and bits [255:192] in the\n"
"///    destination, according to the bits specified in the immediate operand.\n"
"/// \\param mask\n"
"///    An immediate value containing an 8-bit value specifying which elements to\n"
"///    copy from \\a a and \\a b \\n.\n"
"///    Bits [3:0] specify the values copied from operand \\a a. \\n\n"
"///    Bits [7:4] specify the values copied from operand \\a b. \\n\n"
"///    The destinations within the 256-bit destination are assigned values as\n"
"///    follows, according to the bit value assignments described below: \\n\n"
"///    Bits [1:0] are used to assign values to bits [31:0] and [159:128] in the\n"
"///    destination. \\n\n"
"///    Bits [3:2] are used to assign values to bits [63:32] and [191:160] in the\n"
"///    destination. \\n\n"
"///    Bits [5:4] are used to assign values to bits [95:64] and [223:192] in the\n"
"///    destination. \\n\n"
"///    Bits [7:6] are used to assign values to bits [127:96] and [255:224] in\n"
"///    the destination. \\n\n"
"///    Bit value assignments: \\n\n"
"///    00: Bits [31:0] and [159:128] are copied from the selected operand. \\n\n"
"///    01: Bits [63:32] and [191:160] are copied from the selected operand. \\n\n"
"///    10: Bits [95:64] and [223:192] are copied from the selected operand. \\n\n"
"///    11: Bits [127:96] and [255:224] are copied from the selected operand.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the shuffled values.\n"
"#define _mm256_shuffle_ps(a, b, mask) \\\n"
"  (__m256)__builtin_ia32_shufps256((__v8sf)(__m256)(a), \\\n"
"                                   (__v8sf)(__m256)(b), (int)(mask))\n"
"\n"
"/// Selects four double-precision values from the 256-bit operands of\n"
"///    [4 x double], as specified by the immediate value operand.\n"
"///\n"
"///    The selected elements from the first 256-bit operand are copied to bits\n"
"///    [63:0] and bits [191:128] in the destination, and the selected elements\n"
"///    from the second 256-bit operand are copied to bits [127:64] and bits\n"
"///    [255:192] in the destination. For example, if bits [3:0] of the immediate\n"
"///    operand contain a value of 0xF, the 256-bit destination vector would\n"
"///    contain the following values: b[3], a[3], b[1], a[1].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_shuffle_pd(__m256d a, __m256d b, const int mask);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSHUFPD </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param b\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param mask\n"
"///    An immediate value containing 8-bit values specifying which elements to\n"
"///    copy from \\a a and \\a b: \\n\n"
"///    Bit [0]=0: Bits [63:0] are copied from \\a a to bits [63:0] of the\n"
"///    destination. \\n\n"
"///    Bit [0]=1: Bits [127:64] are copied from \\a a to bits [63:0] of the\n"
"///    destination. \\n\n"
"///    Bit [1]=0: Bits [63:0] are copied from \\a b to bits [127:64] of the\n"
"///    destination. \\n\n"
"///    Bit [1]=1: Bits [127:64] are copied from \\a b to bits [127:64] of the\n"
"///    destination. \\n\n"
"///    Bit [2]=0: Bits [191:128] are copied from \\a a to bits [191:128] of the\n"
"///    destination. \\n\n"
"///    Bit [2]=1: Bits [255:192] are copied from \\a a to bits [191:128] of the\n"
"///    destination. \\n\n"
"///    Bit [3]=0: Bits [191:128] are copied from \\a b to bits [255:192] of the\n"
"///    destination. \\n\n"
"///    Bit [3]=1: Bits [255:192] are copied from \\a b to bits [255:192] of the\n"
"///    destination.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the shuffled values.\n"
"#define _mm256_shuffle_pd(a, b, mask) \\\n"
"  (__m256d)__builtin_ia32_shufpd256((__v4df)(__m256d)(a), \\\n"
"                                    (__v4df)(__m256d)(b), (int)(mask))\n"
"\n"
"/* Compare */\n"
"#define _CMP_EQ_OQ    0x00 /* Equal (ordered, non-signaling)  */\n"
"#define _CMP_LT_OS    0x01 /* Less-than (ordered, signaling)  */\n"
"#define _CMP_LE_OS    0x02 /* Less-than-or-equal (ordered, signaling)  */\n"
"#define _CMP_UNORD_Q  0x03 /* Unordered (non-signaling)  */\n"
"#define _CMP_NEQ_UQ   0x04 /* Not-equal (unordered, non-signaling)  */\n"
"#define _CMP_NLT_US   0x05 /* Not-less-than (unordered, signaling)  */\n"
"#define _CMP_NLE_US   0x06 /* Not-less-than-or-equal (unordered, signaling)  */\n"
"#define _CMP_ORD_Q    0x07 /* Ordered (non-signaling)   */\n"
"#define _CMP_EQ_UQ    0x08 /* Equal (unordered, non-signaling)  */\n"
"#define _CMP_NGE_US   0x09 /* Not-greater-than-or-equal (unordered, signaling)  */\n"
"#define _CMP_NGT_US   0x0a /* Not-greater-than (unordered, signaling)  */\n"
"#define _CMP_FALSE_OQ 0x0b /* False (ordered, non-signaling)  */\n"
"#define _CMP_NEQ_OQ   0x0c /* Not-equal (ordered, non-signaling)  */\n"
"#define _CMP_GE_OS    0x0d /* Greater-than-or-equal (ordered, signaling)  */\n"
"#define _CMP_GT_OS    0x0e /* Greater-than (ordered, signaling)  */\n"
"#define _CMP_TRUE_UQ  0x0f /* True (unordered, non-signaling)  */\n"
"#define _CMP_EQ_OS    0x10 /* Equal (ordered, signaling)  */\n"
"#define _CMP_LT_OQ    0x11 /* Less-than (ordered, non-signaling)  */\n"
"#define _CMP_LE_OQ    0x12 /* Less-than-or-equal (ordered, non-signaling)  */\n"
"#define _CMP_UNORD_S  0x13 /* Unordered (signaling)  */\n"
"#define _CMP_NEQ_US   0x14 /* Not-equal (unordered, signaling)  */\n"
"#define _CMP_NLT_UQ   0x15 /* Not-less-than (unordered, non-signaling)  */\n"
"#define _CMP_NLE_UQ   0x16 /* Not-less-than-or-equal (unordered, non-signaling)  */\n"
"#define _CMP_ORD_S    0x17 /* Ordered (signaling)  */\n"
"#define _CMP_EQ_US    0x18 /* Equal (unordered, signaling)  */\n"
"#define _CMP_NGE_UQ   0x19 /* Not-greater-than-or-equal (unordered, non-signaling)  */\n"
"#define _CMP_NGT_UQ   0x1a /* Not-greater-than (unordered, non-signaling)  */\n"
"#define _CMP_FALSE_OS 0x1b /* False (ordered, signaling)  */\n"
"#define _CMP_NEQ_OS   0x1c /* Not-equal (ordered, signaling)  */\n"
"#define _CMP_GE_OQ    0x1d /* Greater-than-or-equal (ordered, non-signaling)  */\n"
"#define _CMP_GT_OQ    0x1e /* Greater-than (ordered, non-signaling)  */\n"
"#define _CMP_TRUE_US  0x1f /* True (unordered, signaling)  */\n"
"\n"
"/// Compares each of the corresponding double-precision values of two\n"
"///    128-bit vectors of [2 x double], using the operation specified by the\n"
"///    immediate integer operand.\n"
"///\n"
"///    Returns a [2 x double] vector consisting of two doubles corresponding to\n"
"///    the two comparison results: zero if the comparison is false, and all 1's\n"
"///    if the comparison is true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_cmp_pd(__m128d a, __m128d b, const int c);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPPD </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param c\n"
"///    An immediate integer operand, with bits [4:0] specifying which comparison\n"
"///    operation to use: \\n\n"
"///    0x00: Equal (ordered, non-signaling) \\n\n"
"///    0x01: Less-than (ordered, signaling) \\n\n"
"///    0x02: Less-than-or-equal (ordered, signaling) \\n\n"
"///    0x03: Unordered (non-signaling) \\n\n"
"///    0x04: Not-equal (unordered, non-signaling) \\n\n"
"///    0x05: Not-less-than (unordered, signaling) \\n\n"
"///    0x06: Not-less-than-or-equal (unordered, signaling) \\n\n"
"///    0x07: Ordered (non-signaling) \\n\n"
"///    0x08: Equal (unordered, non-signaling) \\n\n"
"///    0x09: Not-greater-than-or-equal (unordered, signaling) \\n\n"
"///    0x0A: Not-greater-than (unordered, signaling) \\n\n"
"///    0x0B: False (ordered, non-signaling) \\n\n"
"///    0x0C: Not-equal (ordered, non-signaling) \\n\n"
"///    0x0D: Greater-than-or-equal (ordered, signaling) \\n\n"
"///    0x0E: Greater-than (ordered, signaling) \\n\n"
"///    0x0F: True (unordered, non-signaling) \\n\n"
"///    0x10: Equal (ordered, signaling) \\n\n"
"///    0x11: Less-than (ordered, non-signaling) \\n\n"
"///    0x12: Less-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x13: Unordered (signaling) \\n\n"
"///    0x14: Not-equal (unordered, signaling) \\n\n"
"///    0x15: Not-less-than (unordered, non-signaling) \\n\n"
"///    0x16: Not-less-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x17: Ordered (signaling) \\n\n"
"///    0x18: Equal (unordered, signaling) \\n\n"
"///    0x19: Not-greater-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x1A: Not-greater-than (unordered, non-signaling) \\n\n"
"///    0x1B: False (ordered, signaling) \\n\n"
"///    0x1C: Not-equal (ordered, signaling) \\n\n"
"///    0x1D: Greater-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x1E: Greater-than (ordered, non-signaling) \\n\n"
"///    0x1F: True (unordered, signaling)\n"
"/// \\returns A 128-bit vector of [2 x double] containing the comparison results.\n"
"#define _mm_cmp_pd(a, b, c) \\\n"
"  (__m128d)__builtin_ia32_cmppd((__v2df)(__m128d)(a), \\\n"
"                                (__v2df)(__m128d)(b), (c))\n"
"\n"
"/// Compares each of the corresponding values of two 128-bit vectors of\n"
"///    [4 x float], using the operation specified by the immediate integer\n"
"///    operand.\n"
"///\n"
"///    Returns a [4 x float] vector consisting of four floats corresponding to\n"
"///    the four comparison results: zero if the comparison is false, and all 1's\n"
"///    if the comparison is true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_cmp_ps(__m128 a, __m128 b, const int c);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPPS </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param c\n"
"///    An immediate integer operand, with bits [4:0] specifying which comparison\n"
"///    operation to use: \\n\n"
"///    0x00: Equal (ordered, non-signaling) \\n\n"
"///    0x01: Less-than (ordered, signaling) \\n\n"
"///    0x02: Less-than-or-equal (ordered, signaling) \\n\n"
"///    0x03: Unordered (non-signaling) \\n\n"
"///    0x04: Not-equal (unordered, non-signaling) \\n\n"
"///    0x05: Not-less-than (unordered, signaling) \\n\n"
"///    0x06: Not-less-than-or-equal (unordered, signaling) \\n\n"
"///    0x07: Ordered (non-signaling) \\n\n"
"///    0x08: Equal (unordered, non-signaling) \\n\n"
"///    0x09: Not-greater-than-or-equal (unordered, signaling) \\n\n"
"///    0x0A: Not-greater-than (unordered, signaling) \\n\n"
"///    0x0B: False (ordered, non-signaling) \\n\n"
"///    0x0C: Not-equal (ordered, non-signaling) \\n\n"
"///    0x0D: Greater-than-or-equal (ordered, signaling) \\n\n"
"///    0x0E: Greater-than (ordered, signaling) \\n\n"
"///    0x0F: True (unordered, non-signaling) \\n\n"
"///    0x10: Equal (ordered, signaling) \\n\n"
"///    0x11: Less-than (ordered, non-signaling) \\n\n"
"///    0x12: Less-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x13: Unordered (signaling) \\n\n"
"///    0x14: Not-equal (unordered, signaling) \\n\n"
"///    0x15: Not-less-than (unordered, non-signaling) \\n\n"
"///    0x16: Not-less-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x17: Ordered (signaling) \\n\n"
"///    0x18: Equal (unordered, signaling) \\n\n"
"///    0x19: Not-greater-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x1A: Not-greater-than (unordered, non-signaling) \\n\n"
"///    0x1B: False (ordered, signaling) \\n\n"
"///    0x1C: Not-equal (ordered, signaling) \\n\n"
"///    0x1D: Greater-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x1E: Greater-than (ordered, non-signaling) \\n\n"
"///    0x1F: True (unordered, signaling)\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"#define _mm_cmp_ps(a, b, c) \\\n"
"  (__m128)__builtin_ia32_cmpps((__v4sf)(__m128)(a), \\\n"
"                               (__v4sf)(__m128)(b), (c))\n"
"\n"
"/// Compares each of the corresponding double-precision values of two\n"
"///    256-bit vectors of [4 x double], using the operation specified by the\n"
"///    immediate integer operand.\n"
"///\n"
"///    Returns a [4 x double] vector consisting of four doubles corresponding to\n"
"///    the four comparison results: zero if the comparison is false, and all 1's\n"
"///    if the comparison is true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_cmp_pd(__m256d a, __m256d b, const int c);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPPD </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param b\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param c\n"
"///    An immediate integer operand, with bits [4:0] specifying which comparison\n"
"///    operation to use: \\n\n"
"///    0x00: Equal (ordered, non-signaling) \\n\n"
"///    0x01: Less-than (ordered, signaling) \\n\n"
"///    0x02: Less-than-or-equal (ordered, signaling) \\n\n"
"///    0x03: Unordered (non-signaling) \\n\n"
"///    0x04: Not-equal (unordered, non-signaling) \\n\n"
"///    0x05: Not-less-than (unordered, signaling) \\n\n"
"///    0x06: Not-less-than-or-equal (unordered, signaling) \\n\n"
"///    0x07: Ordered (non-signaling) \\n\n"
"///    0x08: Equal (unordered, non-signaling) \\n\n"
"///    0x09: Not-greater-than-or-equal (unordered, signaling) \\n\n"
"///    0x0A: Not-greater-than (unordered, signaling) \\n\n"
"///    0x0B: False (ordered, non-signaling) \\n\n"
"///    0x0C: Not-equal (ordered, non-signaling) \\n\n"
"///    0x0D: Greater-than-or-equal (ordered, signaling) \\n\n"
"///    0x0E: Greater-than (ordered, signaling) \\n\n"
"///    0x0F: True (unordered, non-signaling) \\n\n"
"///    0x10: Equal (ordered, signaling) \\n\n"
"///    0x11: Less-than (ordered, non-signaling) \\n\n"
"///    0x12: Less-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x13: Unordered (signaling) \\n\n"
"///    0x14: Not-equal (unordered, signaling) \\n\n"
"///    0x15: Not-less-than (unordered, non-signaling) \\n\n"
"///    0x16: Not-less-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x17: Ordered (signaling) \\n\n"
"///    0x18: Equal (unordered, signaling) \\n\n"
"///    0x19: Not-greater-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x1A: Not-greater-than (unordered, non-signaling) \\n\n"
"///    0x1B: False (ordered, signaling) \\n\n"
"///    0x1C: Not-equal (ordered, signaling) \\n\n"
"///    0x1D: Greater-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x1E: Greater-than (ordered, non-signaling) \\n\n"
"///    0x1F: True (unordered, signaling)\n"
"/// \\returns A 256-bit vector of [4 x double] containing the comparison results.\n"
"#define _mm256_cmp_pd(a, b, c) \\\n"
"  (__m256d)__builtin_ia32_cmppd256((__v4df)(__m256d)(a), \\\n"
"                                   (__v4df)(__m256d)(b), (c))\n"
"\n"
"/// Compares each of the corresponding values of two 256-bit vectors of\n"
"///    [8 x float], using the operation specified by the immediate integer\n"
"///    operand.\n"
"///\n"
"///    Returns a [8 x float] vector consisting of eight floats corresponding to\n"
"///    the eight comparison results: zero if the comparison is false, and all\n"
"///    1's if the comparison is true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_cmp_ps(__m256 a, __m256 b, const int c);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPPS </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param b\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param c\n"
"///    An immediate integer operand, with bits [4:0] specifying which comparison\n"
"///    operation to use: \\n\n"
"///    0x00: Equal (ordered, non-signaling) \\n\n"
"///    0x01: Less-than (ordered, signaling) \\n\n"
"///    0x02: Less-than-or-equal (ordered, signaling) \\n\n"
"///    0x03: Unordered (non-signaling) \\n\n"
"///    0x04: Not-equal (unordered, non-signaling) \\n\n"
"///    0x05: Not-less-than (unordered, signaling) \\n\n"
"///    0x06: Not-less-than-or-equal (unordered, signaling) \\n\n"
"///    0x07: Ordered (non-signaling) \\n\n"
"///    0x08: Equal (unordered, non-signaling) \\n\n"
"///    0x09: Not-greater-than-or-equal (unordered, signaling) \\n\n"
"///    0x0A: Not-greater-than (unordered, signaling) \\n\n"
"///    0x0B: False (ordered, non-signaling) \\n\n"
"///    0x0C: Not-equal (ordered, non-signaling) \\n\n"
"///    0x0D: Greater-than-or-equal (ordered, signaling) \\n\n"
"///    0x0E: Greater-than (ordered, signaling) \\n\n"
"///    0x0F: True (unordered, non-signaling) \\n\n"
"///    0x10: Equal (ordered, signaling) \\n\n"
"///    0x11: Less-than (ordered, non-signaling) \\n\n"
"///    0x12: Less-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x13: Unordered (signaling) \\n\n"
"///    0x14: Not-equal (unordered, signaling) \\n\n"
"///    0x15: Not-less-than (unordered, non-signaling) \\n\n"
"///    0x16: Not-less-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x17: Ordered (signaling) \\n\n"
"///    0x18: Equal (unordered, signaling) \\n\n"
"///    0x19: Not-greater-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x1A: Not-greater-than (unordered, non-signaling) \\n\n"
"///    0x1B: False (ordered, signaling) \\n\n"
"///    0x1C: Not-equal (ordered, signaling) \\n\n"
"///    0x1D: Greater-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x1E: Greater-than (ordered, non-signaling) \\n\n"
"///    0x1F: True (unordered, signaling)\n"
"/// \\returns A 256-bit vector of [8 x float] containing the comparison results.\n"
"#define _mm256_cmp_ps(a, b, c) \\\n"
"  (__m256)__builtin_ia32_cmpps256((__v8sf)(__m256)(a), \\\n"
"                                  (__v8sf)(__m256)(b), (c))\n"
"\n"
"/// Compares each of the corresponding scalar double-precision values of\n"
"///    two 128-bit vectors of [2 x double], using the operation specified by the\n"
"///    immediate integer operand.\n"
"///\n"
"///    If the result is true, all 64 bits of the destination vector are set;\n"
"///    otherwise they are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_cmp_sd(__m128d a, __m128d b, const int c);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPSD </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param c\n"
"///    An immediate integer operand, with bits [4:0] specifying which comparison\n"
"///    operation to use: \\n\n"
"///    0x00: Equal (ordered, non-signaling) \\n\n"
"///    0x01: Less-than (ordered, signaling) \\n\n"
"///    0x02: Less-than-or-equal (ordered, signaling) \\n\n"
"///    0x03: Unordered (non-signaling) \\n\n"
"///    0x04: Not-equal (unordered, non-signaling) \\n\n"
"///    0x05: Not-less-than (unordered, signaling) \\n\n"
"///    0x06: Not-less-than-or-equal (unordered, signaling) \\n\n"
"///    0x07: Ordered (non-signaling) \\n\n"
"///    0x08: Equal (unordered, non-signaling) \\n\n"
"///    0x09: Not-greater-than-or-equal (unordered, signaling) \\n\n"
"///    0x0A: Not-greater-than (unordered, signaling) \\n\n"
"///    0x0B: False (ordered, non-signaling) \\n\n"
"///    0x0C: Not-equal (ordered, non-signaling) \\n\n"
"///    0x0D: Greater-than-or-equal (ordered, signaling) \\n\n"
"///    0x0E: Greater-than (ordered, signaling) \\n\n"
"///    0x0F: True (unordered, non-signaling) \\n\n"
"///    0x10: Equal (ordered, signaling) \\n\n"
"///    0x11: Less-than (ordered, non-signaling) \\n\n"
"///    0x12: Less-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x13: Unordered (signaling) \\n\n"
"///    0x14: Not-equal (unordered, signaling) \\n\n"
"///    0x15: Not-less-than (unordered, non-signaling) \\n\n"
"///    0x16: Not-less-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x17: Ordered (signaling) \\n\n"
"///    0x18: Equal (unordered, signaling) \\n\n"
"///    0x19: Not-greater-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x1A: Not-greater-than (unordered, non-signaling) \\n\n"
"///    0x1B: False (ordered, signaling) \\n\n"
"///    0x1C: Not-equal (ordered, signaling) \\n\n"
"///    0x1D: Greater-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x1E: Greater-than (ordered, non-signaling) \\n\n"
"///    0x1F: True (unordered, signaling)\n"
"/// \\returns A 128-bit vector of [2 x double] containing the comparison results.\n"
"#define _mm_cmp_sd(a, b, c) \\\n"
"  (__m128d)__builtin_ia32_cmpsd((__v2df)(__m128d)(a), \\\n"
"                                (__v2df)(__m128d)(b), (c))\n"
"\n"
"/// Compares each of the corresponding scalar values of two 128-bit\n"
"///    vectors of [4 x float], using the operation specified by the immediate\n"
"///    integer operand.\n"
"///\n"
"///    If the result is true, all 32 bits of the destination vector are set;\n"
"///    otherwise they are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_cmp_ss(__m128 a, __m128 b, const int c);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPSS </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param c\n"
"///    An immediate integer operand, with bits [4:0] specifying which comparison\n"
"///    operation to use: \\n\n"
"///    0x00: Equal (ordered, non-signaling) \\n\n"
"///    0x01: Less-than (ordered, signaling) \\n\n"
"///    0x02: Less-than-or-equal (ordered, signaling) \\n\n"
"///    0x03: Unordered (non-signaling) \\n\n"
"///    0x04: Not-equal (unordered, non-signaling) \\n\n"
"///    0x05: Not-less-than (unordered, signaling) \\n\n"
"///    0x06: Not-less-than-or-equal (unordered, signaling) \\n\n"
"///    0x07: Ordered (non-signaling) \\n\n"
"///    0x08: Equal (unordered, non-signaling) \\n\n"
"///    0x09: Not-greater-than-or-equal (unordered, signaling) \\n\n"
"///    0x0A: Not-greater-than (unordered, signaling) \\n\n"
"///    0x0B: False (ordered, non-signaling) \\n\n"
"///    0x0C: Not-equal (ordered, non-signaling) \\n\n"
"///    0x0D: Greater-than-or-equal (ordered, signaling) \\n\n"
"///    0x0E: Greater-than (ordered, signaling) \\n\n"
"///    0x0F: True (unordered, non-signaling) \\n\n"
"///    0x10: Equal (ordered, signaling) \\n\n"
"///    0x11: Less-than (ordered, non-signaling) \\n\n"
"///    0x12: Less-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x13: Unordered (signaling) \\n\n"
"///    0x14: Not-equal (unordered, signaling) \\n\n"
"///    0x15: Not-less-than (unordered, non-signaling) \\n\n"
"///    0x16: Not-less-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x17: Ordered (signaling) \\n\n"
"///    0x18: Equal (unordered, signaling) \\n\n"
"///    0x19: Not-greater-than-or-equal (unordered, non-signaling) \\n\n"
"///    0x1A: Not-greater-than (unordered, non-signaling) \\n\n"
"///    0x1B: False (ordered, signaling) \\n\n"
"///    0x1C: Not-equal (ordered, signaling) \\n\n"
"///    0x1D: Greater-than-or-equal (ordered, non-signaling) \\n\n"
"///    0x1E: Greater-than (ordered, non-signaling) \\n\n"
"///    0x1F: True (unordered, signaling)\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"#define _mm_cmp_ss(a, b, c) \\\n"
"  (__m128)__builtin_ia32_cmpss((__v4sf)(__m128)(a), \\\n"
"                               (__v4sf)(__m128)(b), (c))\n"
"\n"
"/// Takes a [8 x i32] vector and returns the vector element value\n"
"///    indexed by the immediate constant operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x i32].\n"
"/// \\param __imm\n"
"///    An immediate integer operand with bits [2:0] determining which vector\n"
"///    element is extracted and returned.\n"
"/// \\returns A 32-bit integer containing the extracted 32 bits of extended\n"
"///    packed data.\n"
"#define _mm256_extract_epi32(X, N) \\\n"
"  (int)__builtin_ia32_vec_ext_v8si((__v8si)(__m256i)(X), (int)(N))\n"
"\n"
"/// Takes a [16 x i16] vector and returns the vector element value\n"
"///    indexed by the immediate constant operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector of [16 x i16].\n"
"/// \\param __imm\n"
"///    An immediate integer operand with bits [3:0] determining which vector\n"
"///    element is extracted and returned.\n"
"/// \\returns A 32-bit integer containing the extracted 16 bits of zero extended\n"
"///    packed data.\n"
"#define _mm256_extract_epi16(X, N) \\\n"
"  (int)(unsigned short)__builtin_ia32_vec_ext_v16hi((__v16hi)(__m256i)(X), \\\n"
"                                                    (int)(N))\n"
"\n"
"/// Takes a [32 x i8] vector and returns the vector element value\n"
"///    indexed by the immediate constant operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector of [32 x i8].\n"
"/// \\param __imm\n"
"///    An immediate integer operand with bits [4:0] determining which vector\n"
"///    element is extracted and returned.\n"
"/// \\returns A 32-bit integer containing the extracted 8 bits of zero extended\n"
"///    packed data.\n"
"#define _mm256_extract_epi8(X, N) \\\n"
"  (int)(unsigned char)__builtin_ia32_vec_ext_v32qi((__v32qi)(__m256i)(X), \\\n"
"                                                   (int)(N))\n"
"\n"
"#ifdef __x86_64__\n"
"/// Takes a [4 x i64] vector and returns the vector element value\n"
"///    indexed by the immediate constant operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector of [4 x i64].\n"
"/// \\param __imm\n"
"///    An immediate integer operand with bits [1:0] determining which vector\n"
"///    element is extracted and returned.\n"
"/// \\returns A 64-bit integer containing the extracted 64 bits of extended\n"
"///    packed data.\n"
"#define _mm256_extract_epi64(X, N) \\\n"
"  (long long)__builtin_ia32_vec_ext_v4di((__v4di)(__m256i)(X), (int)(N))\n"
"#endif\n"
"\n"
"/// Takes a [8 x i32] vector and replaces the vector element value\n"
"///    indexed by the immediate constant operand by a new value. Returns the\n"
"///    modified vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A vector of [8 x i32] to be used by the insert operation.\n"
"/// \\param __b\n"
"///    An integer value. The replacement value for the insert operation.\n"
"/// \\param __imm\n"
"///    An immediate integer specifying the index of the vector element to be\n"
"///    replaced.\n"
"/// \\returns A copy of vector \\a __a, after replacing its element indexed by\n"
"///    \\a __imm with \\a __b.\n"
"#define _mm256_insert_epi32(X, I, N) \\\n"
"  (__m256i)__builtin_ia32_vec_set_v8si((__v8si)(__m256i)(X), \\\n"
"                                       (int)(I), (int)(N))\n"
"\n"
"\n"
"/// Takes a [16 x i16] vector and replaces the vector element value\n"
"///    indexed by the immediate constant operand with a new value. Returns the\n"
"///    modified vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A vector of [16 x i16] to be used by the insert operation.\n"
"/// \\param __b\n"
"///    An i16 integer value. The replacement value for the insert operation.\n"
"/// \\param __imm\n"
"///    An immediate integer specifying the index of the vector element to be\n"
"///    replaced.\n"
"/// \\returns A copy of vector \\a __a, after replacing its element indexed by\n"
"///    \\a __imm with \\a __b.\n"
"#define _mm256_insert_epi16(X, I, N) \\\n"
"  (__m256i)__builtin_ia32_vec_set_v16hi((__v16hi)(__m256i)(X), \\\n"
"                                        (int)(I), (int)(N))\n"
"\n"
"/// Takes a [32 x i8] vector and replaces the vector element value\n"
"///    indexed by the immediate constant operand with a new value. Returns the\n"
"///    modified vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A vector of [32 x i8] to be used by the insert operation.\n"
"/// \\param __b\n"
"///    An i8 integer value. The replacement value for the insert operation.\n"
"/// \\param __imm\n"
"///    An immediate integer specifying the index of the vector element to be\n"
"///    replaced.\n"
"/// \\returns A copy of vector \\a __a, after replacing its element indexed by\n"
"///    \\a __imm with \\a __b.\n"
"#define _mm256_insert_epi8(X, I, N) \\\n"
"  (__m256i)__builtin_ia32_vec_set_v32qi((__v32qi)(__m256i)(X), \\\n"
"                                        (int)(I), (int)(N))\n"
"\n"
"#ifdef __x86_64__\n"
"/// Takes a [4 x i64] vector and replaces the vector element value\n"
"///    indexed by the immediate constant operand with a new value. Returns the\n"
"///    modified vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A vector of [4 x i64] to be used by the insert operation.\n"
"/// \\param __b\n"
"///    A 64-bit integer value. The replacement value for the insert operation.\n"
"/// \\param __imm\n"
"///    An immediate integer specifying the index of the vector element to be\n"
"///    replaced.\n"
"/// \\returns A copy of vector \\a __a, after replacing its element indexed by\n"
"///     \\a __imm with \\a __b.\n"
"#define _mm256_insert_epi64(X, I, N) \\\n"
"  (__m256i)__builtin_ia32_vec_set_v4di((__v4di)(__m256i)(X), \\\n"
"                                       (long long)(I), (int)(N))\n"
"#endif\n"
"\n"
"/* Conversion */\n"
"/// Converts a vector of [4 x i32] into a vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTDQ2PD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector of [4 x i32].\n"
"/// \\returns A 256-bit vector of [4 x double] containing the converted values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_cvtepi32_pd(__m128i __a)\n"
"{\n"
"  return (__m256d)__builtin_convertvector((__v4si)__a, __v4df);\n"
"}\n"
"\n"
"/// Converts a vector of [8 x i32] into a vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTDQ2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the converted values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_cvtepi32_ps(__m256i __a)\n"
"{\n"
"  return (__m256)__builtin_convertvector((__v8si)__a, __v8sf);\n"
"}\n"
"\n"
"/// Converts a 256-bit vector of [4 x double] into a 128-bit vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPD2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the converted values.\n"
"static __inline __m128 __DEFAULT_FN_ATTRS\n"
"_mm256_cvtpd_ps(__m256d __a)\n"
"{\n"
"  return (__m128)__builtin_ia32_cvtpd2ps256((__v4df) __a);\n"
"}\n"
"\n"
"/// Converts a vector of [8 x float] into a vector of [8 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPS2DQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 256-bit integer vector containing the converted values.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_cvtps_epi32(__m256 __a)\n"
"{\n"
"  return (__m256i)__builtin_ia32_cvtps2dq256((__v8sf) __a);\n"
"}\n"
"\n"
"/// Converts a 128-bit vector of [4 x float] into a 256-bit vector of [4\n"
"///    x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPS2PD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 256-bit vector of [4 x double] containing the converted values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_cvtps_pd(__m128 __a)\n"
"{\n"
"  return (__m256d)__builtin_convertvector((__v4sf)__a, __v4df);\n"
"}\n"
"\n"
"/// Converts a 256-bit vector of [4 x double] into a 128-bit vector of [4\n"
"///    x i32], truncating the result by rounding towards zero when it is\n"
"///    inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTPD2DQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns A 128-bit integer vector containing the converted values.\n"
"static __inline __m128i __DEFAULT_FN_ATTRS\n"
"_mm256_cvttpd_epi32(__m256d __a)\n"
"{\n"
"  return (__m128i)__builtin_ia32_cvttpd2dq256((__v4df) __a);\n"
"}\n"
"\n"
"/// Converts a 256-bit vector of [4 x double] into a 128-bit vector of [4\n"
"///    x i32]. When a conversion is inexact, the value returned is rounded\n"
"///    according to the rounding control bits in the MXCSR register.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPD2DQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns A 128-bit integer vector containing the converted values.\n"
"static __inline __m128i __DEFAULT_FN_ATTRS\n"
"_mm256_cvtpd_epi32(__m256d __a)\n"
"{\n"
"  return (__m128i)__builtin_ia32_cvtpd2dq256((__v4df) __a);\n"
"}\n"
"\n"
"/// Converts a vector of [8 x float] into a vector of [8 x i32],\n"
"///    truncating the result by rounding towards zero when it is inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTPS2DQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 256-bit integer vector containing the converted values.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_cvttps_epi32(__m256 __a)\n"
"{\n"
"  return (__m256i)__builtin_ia32_cvttps2dq256((__v8sf) __a);\n"
"}\n"
"\n"
"/// Returns the first element of the input vector of [4 x double].\n"
"///\n"
"/// \\headerfile <avxintrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns A 64 bit double containing the first element of the input vector.\n"
"static __inline double __DEFAULT_FN_ATTRS\n"
"_mm256_cvtsd_f64(__m256d __a)\n"
"{\n"
" return __a[0];\n"
"}\n"
"\n"
"/// Returns the first element of the input vector of [8 x i32].\n"
"///\n"
"/// \\headerfile <avxintrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x i32].\n"
"/// \\returns A 32 bit integer containing the first element of the input vector.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_cvtsi256_si32(__m256i __a)\n"
"{\n"
" __v8si __b = (__v8si)__a;\n"
" return __b[0];\n"
"}\n"
"\n"
"/// Returns the first element of the input vector of [8 x float].\n"
"///\n"
"/// \\headerfile <avxintrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns A 32 bit float containing the first element of the input vector.\n"
"static __inline float __DEFAULT_FN_ATTRS\n"
"_mm256_cvtss_f32(__m256 __a)\n"
"{\n"
" return __a[0];\n"
"}\n"
"\n"
"/* Vector replicate */\n"
"/// Moves and duplicates odd-indexed values from a 256-bit vector of\n"
"///    [8 x float] to float values in a 256-bit vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSHDUP </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float]. \\n\n"
"///    Bits [255:224] of \\a __a are written to bits [255:224] and [223:192] of\n"
"///    the return value. \\n\n"
"///    Bits [191:160] of \\a __a are written to bits [191:160] and [159:128] of\n"
"///    the return value. \\n\n"
"///    Bits [127:96] of \\a __a are written to bits [127:96] and [95:64] of the\n"
"///    return value. \\n\n"
"///    Bits [63:32] of \\a __a are written to bits [63:32] and [31:0] of the\n"
"///    return value.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the moved and duplicated\n"
"///    values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_movehdup_ps(__m256 __a)\n"
"{\n"
"  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 1, 1, 3, 3, 5, 5, 7, 7);\n"
"}\n"
"\n"
"/// Moves and duplicates even-indexed values from a 256-bit vector of\n"
"///    [8 x float] to float values in a 256-bit vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSLDUP </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float]. \\n\n"
"///    Bits [223:192] of \\a __a are written to bits [255:224] and [223:192] of\n"
"///    the return value. \\n\n"
"///    Bits [159:128] of \\a __a are written to bits [191:160] and [159:128] of\n"
"///    the return value. \\n\n"
"///    Bits [95:64] of \\a __a are written to bits [127:96] and [95:64] of the\n"
"///    return value. \\n\n"
"///    Bits [31:0] of \\a __a are written to bits [63:32] and [31:0] of the\n"
"///    return value.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the moved and duplicated\n"
"///    values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_moveldup_ps(__m256 __a)\n"
"{\n"
"  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 0, 2, 2, 4, 4, 6, 6);\n"
"}\n"
"\n"
"/// Moves and duplicates double-precision floating point values from a\n"
"///    256-bit vector of [4 x double] to double-precision values in a 256-bit\n"
"///    vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double]. \\n\n"
"///    Bits [63:0] of \\a __a are written to bits [127:64] and [63:0] of the\n"
"///    return value. \\n\n"
"///    Bits [191:128] of \\a __a are written to bits [255:192] and [191:128] of\n"
"///    the return value.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the moved and\n"
"///    duplicated values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_movedup_pd(__m256d __a)\n"
"{\n"
"  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 0, 2, 2);\n"
"}\n"
"\n"
"/* Unpack and Interleave */\n"
"/// Unpacks the odd-indexed vector elements from two 256-bit vectors of\n"
"///    [4 x double] and interleaves them into a 256-bit vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKHPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [4 x double]. \\n\n"
"///    Bits [127:64] are written to bits [63:0] of the return value. \\n\n"
"///    Bits [255:192] are written to bits [191:128] of the return value. \\n\n"
"/// \\param __b\n"
"///    A 256-bit floating-point vector of [4 x double]. \\n\n"
"///    Bits [127:64] are written to bits [127:64] of the return value. \\n\n"
"///    Bits [255:192] are written to bits [255:192] of the return value. \\n\n"
"/// \\returns A 256-bit vector of [4 x double] containing the interleaved values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_unpackhi_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 1, 5, 1+2, 5+2);\n"
"}\n"
"\n"
"/// Unpacks the even-indexed vector elements from two 256-bit vectors of\n"
"///    [4 x double] and interleaves them into a 256-bit vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [4 x double]. \\n\n"
"///    Bits [63:0] are written to bits [63:0] of the return value. \\n\n"
"///    Bits [191:128] are written to bits [191:128] of the return value.\n"
"/// \\param __b\n"
"///    A 256-bit floating-point vector of [4 x double]. \\n\n"
"///    Bits [63:0] are written to bits [127:64] of the return value. \\n\n"
"///    Bits [191:128] are written to bits [255:192] of the return value. \\n\n"
"/// \\returns A 256-bit vector of [4 x double] containing the interleaved values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_unpacklo_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 0, 4, 0+2, 4+2);\n"
"}\n"
"\n"
"/// Unpacks the 32-bit vector elements 2, 3, 6 and 7 from each of the\n"
"///    two 256-bit vectors of [8 x float] and interleaves them into a 256-bit\n"
"///    vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKHPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float]. \\n\n"
"///    Bits [95:64] are written to bits [31:0] of the return value. \\n\n"
"///    Bits [127:96] are written to bits [95:64] of the return value. \\n\n"
"///    Bits [223:192] are written to bits [159:128] of the return value. \\n\n"
"///    Bits [255:224] are written to bits [223:192] of the return value.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float]. \\n\n"
"///    Bits [95:64] are written to bits [63:32] of the return value. \\n\n"
"///    Bits [127:96] are written to bits [127:96] of the return value. \\n\n"
"///    Bits [223:192] are written to bits [191:160] of the return value. \\n\n"
"///    Bits [255:224] are written to bits [255:224] of the return value.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the interleaved values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_unpackhi_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 2, 10, 2+1, 10+1, 6, 14, 6+1, 14+1);\n"
"}\n"
"\n"
"/// Unpacks the 32-bit vector elements 0, 1, 4 and 5 from each of the\n"
"///    two 256-bit vectors of [8 x float] and interleaves them into a 256-bit\n"
"///    vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float]. \\n\n"
"///    Bits [31:0] are written to bits [31:0] of the return value. \\n\n"
"///    Bits [63:32] are written to bits [95:64] of the return value. \\n\n"
"///    Bits [159:128] are written to bits [159:128] of the return value. \\n\n"
"///    Bits [191:160] are written to bits [223:192] of the return value.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float]. \\n\n"
"///    Bits [31:0] are written to bits [63:32] of the return value. \\n\n"
"///    Bits [63:32] are written to bits [127:96] of the return value. \\n\n"
"///    Bits [159:128] are written to bits [191:160] of the return value. \\n\n"
"///    Bits [191:160] are written to bits [255:224] of the return value.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the interleaved values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_unpacklo_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 0, 8, 0+1, 8+1, 4, 12, 4+1, 12+1);\n"
"}\n"
"\n"
"/* Bit Test */\n"
"/// Given two 128-bit floating-point vectors of [2 x double], perform an\n"
"///    element-by-element comparison of the double-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the ZF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns the ZF flag in the EFLAGS register.\n"
"static __inline int __DEFAULT_FN_ATTRS128\n"
"_mm_testz_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_vtestzpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Given two 128-bit floating-point vectors of [2 x double], perform an\n"
"///    element-by-element comparison of the double-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the CF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns the CF flag in the EFLAGS register.\n"
"static __inline int __DEFAULT_FN_ATTRS128\n"
"_mm_testc_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_vtestcpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Given two 128-bit floating-point vectors of [2 x double], perform an\n"
"///    element-by-element comparison of the double-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,\n"
"///    otherwise it returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.\n"
"static __inline int __DEFAULT_FN_ATTRS128\n"
"_mm_testnzc_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_vtestnzcpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Given two 128-bit floating-point vectors of [4 x float], perform an\n"
"///    element-by-element comparison of the single-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the ZF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns the ZF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS128\n"
"_mm_testz_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_vtestzps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Given two 128-bit floating-point vectors of [4 x float], perform an\n"
"///    element-by-element comparison of the single-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the CF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns the CF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS128\n"
"_mm_testc_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_vtestcps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Given two 128-bit floating-point vectors of [4 x float], perform an\n"
"///    element-by-element comparison of the single-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,\n"
"///    otherwise it returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.\n"
"static __inline int __DEFAULT_FN_ATTRS128\n"
"_mm_testnzc_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_vtestnzcps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit floating-point vectors of [4 x double], perform an\n"
"///    element-by-element comparison of the double-precision elements in the\n"
"///    first source vector and the corresponding elements in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the ZF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns the ZF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testz_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return __builtin_ia32_vtestzpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit floating-point vectors of [4 x double], perform an\n"
"///    element-by-element comparison of the double-precision elements in the\n"
"///    first source vector and the corresponding elements in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the CF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns the CF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testc_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return __builtin_ia32_vtestcpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit floating-point vectors of [4 x double], perform an\n"
"///    element-by-element comparison of the double-precision elements in the\n"
"///    first source vector and the corresponding elements in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of double-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,\n"
"///    otherwise it returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testnzc_pd(__m256d __a, __m256d __b)\n"
"{\n"
"  return __builtin_ia32_vtestnzcpd256((__v4df)__a, (__v4df)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit floating-point vectors of [8 x float], perform an\n"
"///    element-by-element comparison of the single-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the ZF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns the ZF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testz_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return __builtin_ia32_vtestzps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit floating-point vectors of [8 x float], perform an\n"
"///    element-by-element comparison of the single-precision element in the\n"
"///    first source vector and the corresponding element in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the CF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns the CF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testc_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return __builtin_ia32_vtestcps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit floating-point vectors of [8 x float], perform an\n"
"///    element-by-element comparison of the single-precision elements in the\n"
"///    first source vector and the corresponding elements in the second source\n"
"///    vector.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the\n"
"///    ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of single-precision elements where the\n"
"///    sign-bit of the first element is 0 and the sign-bit of the second element\n"
"///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,\n"
"///    otherwise it returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param __b\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testnzc_ps(__m256 __a, __m256 __b)\n"
"{\n"
"  return __builtin_ia32_vtestnzcps256((__v8sf)__a, (__v8sf)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit integer vectors, perform a bit-by-bit comparison\n"
"///    of the two source vectors.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of bits where both bits are 1, the ZF flag\n"
"///    is set to 0. Otherwise the ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of bits where the bit from the first source\n"
"///    vector is 0 and the bit from the second source vector is 1, the CF flag\n"
"///    is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the ZF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"/// \\param __b\n"
"///    A 256-bit integer vector.\n"
"/// \\returns the ZF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testz_si256(__m256i __a, __m256i __b)\n"
"{\n"
"  return __builtin_ia32_ptestz256((__v4di)__a, (__v4di)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit integer vectors, perform a bit-by-bit comparison\n"
"///    of the two source vectors.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of bits where both bits are 1, the ZF flag\n"
"///    is set to 0. Otherwise the ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of bits where the bit from the first source\n"
"///    vector is 0 and the bit from the second source vector is 1, the CF flag\n"
"///    is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns the value of the CF flag.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"/// \\param __b\n"
"///    A 256-bit integer vector.\n"
"/// \\returns the CF flag.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testc_si256(__m256i __a, __m256i __b)\n"
"{\n"
"  return __builtin_ia32_ptestc256((__v4di)__a, (__v4di)__b);\n"
"}\n"
"\n"
"/// Given two 256-bit integer vectors, perform a bit-by-bit comparison\n"
"///    of the two source vectors.\n"
"///\n"
"///    The EFLAGS register is updated as follows: \\n\n"
"///    If there is at least one pair of bits where both bits are 1, the ZF flag\n"
"///    is set to 0. Otherwise the ZF flag is set to 1. \\n\n"
"///    If there is at least one pair of bits where the bit from the first source\n"
"///    vector is 0 and the bit from the second source vector is 1, the CF flag\n"
"///    is set to 0. Otherwise the CF flag is set to 1. \\n\n"
"///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,\n"
"///    otherwise it returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"/// \\param __b\n"
"///    A 256-bit integer vector.\n"
"/// \\returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_testnzc_si256(__m256i __a, __m256i __b)\n"
"{\n"
"  return __builtin_ia32_ptestnzc256((__v4di)__a, (__v4di)__b);\n"
"}\n"
"\n"
"/* Vector extract sign mask */\n"
"/// Extracts the sign bits of double-precision floating point elements\n"
"///    in a 256-bit vector of [4 x double] and writes them to the lower order\n"
"///    bits of the return value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVMSKPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the double-precision\n"
"///    floating point values with sign bits to be extracted.\n"
"/// \\returns The sign bits from the operand, written to bits [3:0].\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_movemask_pd(__m256d __a)\n"
"{\n"
"  return __builtin_ia32_movmskpd256((__v4df)__a);\n"
"}\n"
"\n"
"/// Extracts the sign bits of single-precision floating point elements\n"
"///    in a 256-bit vector of [8 x float] and writes them to the lower order\n"
"///    bits of the return value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVMSKPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the single-precision floating\n"
"///    point values with sign bits to be extracted.\n"
"/// \\returns The sign bits from the operand, written to bits [7:0].\n"
"static __inline int __DEFAULT_FN_ATTRS\n"
"_mm256_movemask_ps(__m256 __a)\n"
"{\n"
"  return __builtin_ia32_movmskps256((__v8sf)__a);\n"
"}\n"
"\n"
"/* Vector __zero */\n"
"/// Zeroes the contents of all XMM or YMM registers.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VZEROALL </c> instruction.\n"
"static __inline void __attribute__((__always_inline__, __nodebug__, __target__(\"avx\")))\n"
"_mm256_zeroall(void)\n"
"{\n"
"  __builtin_ia32_vzeroall();\n"
"}\n"
"\n"
"/// Zeroes the upper 128 bits (bits 255:128) of all YMM registers.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VZEROUPPER </c> instruction.\n"
"static __inline void __attribute__((__always_inline__, __nodebug__, __target__(\"avx\")))\n"
"_mm256_zeroupper(void)\n"
"{\n"
"  __builtin_ia32_vzeroupper();\n"
"}\n"
"\n"
"/* Vector load with broadcast */\n"
"/// Loads a scalar single-precision floating point value from the\n"
"///    specified address pointed to by \\a __a and broadcasts it to the elements\n"
"///    of a [4 x float] vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBROADCASTSS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    The single-precision floating point value to be broadcast.\n"
"/// \\returns A 128-bit vector of [4 x float] whose 32-bit elements are set\n"
"///    equal to the broadcast value.\n"
"static __inline __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_broadcast_ss(float const *__a)\n"
"{\n"
"  float __f = *__a;\n"
"  return __extension__ (__m128)(__v4sf){ __f, __f, __f, __f };\n"
"}\n"
"\n"
"/// Loads a scalar double-precision floating point value from the\n"
"///    specified address pointed to by \\a __a and broadcasts it to the elements\n"
"///    of a [4 x double] vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBROADCASTSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    The double-precision floating point value to be broadcast.\n"
"/// \\returns A 256-bit vector of [4 x double] whose 64-bit elements are set\n"
"///    equal to the broadcast value.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_broadcast_sd(double const *__a)\n"
"{\n"
"  double __d = *__a;\n"
"  return __extension__ (__m256d)(__v4df){ __d, __d, __d, __d };\n"
"}\n"
"\n"
"/// Loads a scalar single-precision floating point value from the\n"
"///    specified address pointed to by \\a __a and broadcasts it to the elements\n"
"///    of a [8 x float] vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBROADCASTSS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    The single-precision floating point value to be broadcast.\n"
"/// \\returns A 256-bit vector of [8 x float] whose 32-bit elements are set\n"
"///    equal to the broadcast value.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_broadcast_ss(float const *__a)\n"
"{\n"
"  float __f = *__a;\n"
"  return __extension__ (__m256)(__v8sf){ __f, __f, __f, __f, __f, __f, __f, __f };\n"
"}\n"
"\n"
"/// Loads the data from a 128-bit vector of [2 x double] from the\n"
"///    specified address pointed to by \\a __a and broadcasts it to 128-bit\n"
"///    elements in a 256-bit vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBROADCASTF128 </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    The 128-bit vector of [2 x double] to be broadcast.\n"
"/// \\returns A 256-bit vector of [4 x double] whose 128-bit elements are set\n"
"///    equal to the broadcast value.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_broadcast_pd(__m128d const *__a)\n"
"{\n"
"  __m128d __b = _mm_loadu_pd((const double *)__a);\n"
"  return (__m256d)__builtin_shufflevector((__v2df)__b, (__v2df)__b,\n"
"                                          0, 1, 0, 1);\n"
"}\n"
"\n"
"/// Loads the data from a 128-bit vector of [4 x float] from the\n"
"///    specified address pointed to by \\a __a and broadcasts it to 128-bit\n"
"///    elements in a 256-bit vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBROADCASTF128 </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    The 128-bit vector of [4 x float] to be broadcast.\n"
"/// \\returns A 256-bit vector of [8 x float] whose 128-bit elements are set\n"
"///    equal to the broadcast value.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_broadcast_ps(__m128 const *__a)\n"
"{\n"
"  __m128 __b = _mm_loadu_ps((const float *)__a);\n"
"  return (__m256)__builtin_shufflevector((__v4sf)__b, (__v4sf)__b,\n"
"                                         0, 1, 2, 3, 0, 1, 2, 3);\n"
"}\n"
"\n"
"/* SIMD load ops */\n"
"/// Loads 4 double-precision floating point values from a 32-byte aligned\n"
"///    memory location pointed to by \\a __p into a vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A 32-byte aligned pointer to a memory location containing\n"
"///    double-precision floating point values.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the moved values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_load_pd(double const *__p)\n"
"{\n"
"  return *(__m256d *)__p;\n"
"}\n"
"\n"
"/// Loads 8 single-precision floating point values from a 32-byte aligned\n"
"///    memory location pointed to by \\a __p into a vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A 32-byte aligned pointer to a memory location containing float values.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the moved values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_load_ps(float const *__p)\n"
"{\n"
"  return *(__m256 *)__p;\n"
"}\n"
"\n"
"/// Loads 4 double-precision floating point values from an unaligned\n"
"///    memory location pointed to by \\a __p into a vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location containing double-precision floating\n"
"///    point values.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the moved values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_loadu_pd(double const *__p)\n"
"{\n"
"  struct __loadu_pd {\n"
"    __m256d __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return ((struct __loadu_pd*)__p)->__v;\n"
"}\n"
"\n"
"/// Loads 8 single-precision floating point values from an unaligned\n"
"///    memory location pointed to by \\a __p into a vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location containing single-precision floating\n"
"///    point values.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the moved values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_loadu_ps(float const *__p)\n"
"{\n"
"  struct __loadu_ps {\n"
"    __m256 __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return ((struct __loadu_ps*)__p)->__v;\n"
"}\n"
"\n"
"/// Loads 256 bits of integer data from a 32-byte aligned memory\n"
"///    location pointed to by \\a __p into elements of a 256-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDQA </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A 32-byte aligned pointer to a 256-bit integer vector containing integer\n"
"///    values.\n"
"/// \\returns A 256-bit integer vector containing the moved values.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_load_si256(__m256i const *__p)\n"
"{\n"
"  return *__p;\n"
"}\n"
"\n"
"/// Loads 256 bits of integer data from an unaligned memory location\n"
"///    pointed to by \\a __p into a 256-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDQU </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 256-bit integer vector containing integer values.\n"
"/// \\returns A 256-bit integer vector containing the moved values.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_loadu_si256(__m256i const *__p)\n"
"{\n"
"  struct __loadu_si256 {\n"
"    __m256i __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return ((struct __loadu_si256*)__p)->__v;\n"
"}\n"
"\n"
"/// Loads 256 bits of integer data from an unaligned memory location\n"
"///    pointed to by \\a __p into a 256-bit integer vector. This intrinsic may\n"
"///    perform better than \\c _mm256_loadu_si256 when the data crosses a cache\n"
"///    line boundary.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VLDDQU </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 256-bit integer vector containing integer values.\n"
"/// \\returns A 256-bit integer vector containing the moved values.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_lddqu_si256(__m256i const *__p)\n"
"{\n"
"  return (__m256i)__builtin_ia32_lddqu256((char const *)__p);\n"
"}\n"
"\n"
"/* SIMD store ops */\n"
"/// Stores double-precision floating point values from a 256-bit vector\n"
"///    of [4 x double] to a 32-byte aligned memory location pointed to by\n"
"///    \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A 32-byte aligned pointer to a memory location that will receive the\n"
"///    double-precision floaing point values.\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_store_pd(double *__p, __m256d __a)\n"
"{\n"
"  *(__m256d *)__p = __a;\n"
"}\n"
"\n"
"/// Stores single-precision floating point values from a 256-bit vector\n"
"///    of [8 x float] to a 32-byte aligned memory location pointed to by \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A 32-byte aligned pointer to a memory location that will receive the\n"
"///    float values.\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_store_ps(float *__p, __m256 __a)\n"
"{\n"
"  *(__m256 *)__p = __a;\n"
"}\n"
"\n"
"/// Stores double-precision floating point values from a 256-bit vector\n"
"///    of [4 x double] to an unaligned memory location pointed to by \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the double-precision\n"
"///    floating point values.\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_storeu_pd(double *__p, __m256d __a)\n"
"{\n"
"  struct __storeu_pd {\n"
"    __m256d __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_pd*)__p)->__v = __a;\n"
"}\n"
"\n"
"/// Stores single-precision floating point values from a 256-bit vector\n"
"///    of [8 x float] to an unaligned memory location pointed to by \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the float values.\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_storeu_ps(float *__p, __m256 __a)\n"
"{\n"
"  struct __storeu_ps {\n"
"    __m256 __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_ps*)__p)->__v = __a;\n"
"}\n"
"\n"
"/// Stores integer values from a 256-bit integer vector to a 32-byte\n"
"///    aligned memory location pointed to by \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDQA </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A 32-byte aligned pointer to a memory location that will receive the\n"
"///    integer values.\n"
"/// \\param __a\n"
"///    A 256-bit integer vector containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_store_si256(__m256i *__p, __m256i __a)\n"
"{\n"
"  *__p = __a;\n"
"}\n"
"\n"
"/// Stores integer values from a 256-bit integer vector to an unaligned\n"
"///    memory location pointed to by \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDQU </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the integer values.\n"
"/// \\param __a\n"
"///    A 256-bit integer vector containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_storeu_si256(__m256i *__p, __m256i __a)\n"
"{\n"
"  struct __storeu_si256 {\n"
"    __m256i __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_si256*)__p)->__v = __a;\n"
"}\n"
"\n"
"/* Conditional load ops */\n"
"/// Conditionally loads double-precision floating point elements from a\n"
"///    memory location pointed to by \\a __p into a 128-bit vector of\n"
"///    [2 x double], depending on the mask bits associated with each data\n"
"///    element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that contains the double-precision\n"
"///    floating point values.\n"
"/// \\param __m\n"
"///    A 128-bit integer vector containing the mask. The most significant bit of\n"
"///    each data element represents the mask bits. If a mask bit is zero, the\n"
"///    corresponding value in the memory location is not loaded and the\n"
"///    corresponding field in the return value is set to zero.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the loaded values.\n"
"static __inline __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_maskload_pd(double const *__p, __m128i __m)\n"
"{\n"
"  return (__m128d)__builtin_ia32_maskloadpd((const __v2df *)__p, (__v2di)__m);\n"
"}\n"
"\n"
"/// Conditionally loads double-precision floating point elements from a\n"
"///    memory location pointed to by \\a __p into a 256-bit vector of\n"
"///    [4 x double], depending on the mask bits associated with each data\n"
"///    element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that contains the double-precision\n"
"///    floating point values.\n"
"/// \\param __m\n"
"///    A 256-bit integer vector of [4 x quadword] containing the mask. The most\n"
"///    significant bit of each quadword element represents the mask bits. If a\n"
"///    mask bit is zero, the corresponding value in the memory location is not\n"
"///    loaded and the corresponding field in the return value is set to zero.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the loaded values.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_maskload_pd(double const *__p, __m256i __m)\n"
"{\n"
"  return (__m256d)__builtin_ia32_maskloadpd256((const __v4df *)__p,\n"
"                                               (__v4di)__m);\n"
"}\n"
"\n"
"/// Conditionally loads single-precision floating point elements from a\n"
"///    memory location pointed to by \\a __p into a 128-bit vector of\n"
"///    [4 x float], depending on the mask bits associated with each data\n"
"///    element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that contains the single-precision\n"
"///    floating point values.\n"
"/// \\param __m\n"
"///    A 128-bit integer vector containing the mask. The most significant bit of\n"
"///    each data element represents the mask bits. If a mask bit is zero, the\n"
"///    corresponding value in the memory location is not loaded and the\n"
"///    corresponding field in the return value is set to zero.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the loaded values.\n"
"static __inline __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_maskload_ps(float const *__p, __m128i __m)\n"
"{\n"
"  return (__m128)__builtin_ia32_maskloadps((const __v4sf *)__p, (__v4si)__m);\n"
"}\n"
"\n"
"/// Conditionally loads single-precision floating point elements from a\n"
"///    memory location pointed to by \\a __p into a 256-bit vector of\n"
"///    [8 x float], depending on the mask bits associated with each data\n"
"///    element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that contains the single-precision\n"
"///    floating point values.\n"
"/// \\param __m\n"
"///    A 256-bit integer vector of [8 x dword] containing the mask. The most\n"
"///    significant bit of each dword element represents the mask bits. If a mask\n"
"///    bit is zero, the corresponding value in the memory location is not loaded\n"
"///    and the corresponding field in the return value is set to zero.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the loaded values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_maskload_ps(float const *__p, __m256i __m)\n"
"{\n"
"  return (__m256)__builtin_ia32_maskloadps256((const __v8sf *)__p, (__v8si)__m);\n"
"}\n"
"\n"
"/* Conditional store ops */\n"
"/// Moves single-precision floating point values from a 256-bit vector\n"
"///    of [8 x float] to a memory location pointed to by \\a __p, according to\n"
"///    the specified mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the float values.\n"
"/// \\param __m\n"
"///    A 256-bit integer vector of [8 x dword] containing the mask. The most\n"
"///    significant bit of each dword element in the mask vector represents the\n"
"///    mask bits. If a mask bit is zero, the corresponding value from vector\n"
"///    \\a __a is not stored and the corresponding field in the memory location\n"
"///    pointed to by \\a __p is not changed.\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the values to be stored.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_maskstore_ps(float *__p, __m256i __m, __m256 __a)\n"
"{\n"
"  __builtin_ia32_maskstoreps256((__v8sf *)__p, (__v8si)__m, (__v8sf)__a);\n"
"}\n"
"\n"
"/// Moves double-precision values from a 128-bit vector of [2 x double]\n"
"///    to a memory location pointed to by \\a __p, according to the specified\n"
"///    mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the float values.\n"
"/// \\param __m\n"
"///    A 128-bit integer vector containing the mask. The most significant bit of\n"
"///    each field in the mask vector represents the mask bits. If a mask bit is\n"
"///    zero, the corresponding value from vector \\a __a is not stored and the\n"
"///    corresponding field in the memory location pointed to by \\a __p is not\n"
"///    changed.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the values to be stored.\n"
"static __inline void __DEFAULT_FN_ATTRS128\n"
"_mm_maskstore_pd(double *__p, __m128i __m, __m128d __a)\n"
"{\n"
"  __builtin_ia32_maskstorepd((__v2df *)__p, (__v2di)__m, (__v2df)__a);\n"
"}\n"
"\n"
"/// Moves double-precision values from a 256-bit vector of [4 x double]\n"
"///    to a memory location pointed to by \\a __p, according to the specified\n"
"///    mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the float values.\n"
"/// \\param __m\n"
"///    A 256-bit integer vector of [4 x quadword] containing the mask. The most\n"
"///    significant bit of each quadword element in the mask vector represents\n"
"///    the mask bits. If a mask bit is zero, the corresponding value from vector\n"
"///    __a is not stored and the corresponding field in the memory location\n"
"///    pointed to by \\a __p is not changed.\n"
"/// \\param __a\n"
"///    A 256-bit vector of [4 x double] containing the values to be stored.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_maskstore_pd(double *__p, __m256i __m, __m256d __a)\n"
"{\n"
"  __builtin_ia32_maskstorepd256((__v4df *)__p, (__v4di)__m, (__v4df)__a);\n"
"}\n"
"\n"
"/// Moves single-precision floating point values from a 128-bit vector\n"
"///    of [4 x float] to a memory location pointed to by \\a __p, according to\n"
"///    the specified mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the float values.\n"
"/// \\param __m\n"
"///    A 128-bit integer vector containing the mask. The most significant bit of\n"
"///    each field in the mask vector represents the mask bits. If a mask bit is\n"
"///    zero, the corresponding value from vector __a is not stored and the\n"
"///    corresponding field in the memory location pointed to by \\a __p is not\n"
"///    changed.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the values to be stored.\n"
"static __inline void __DEFAULT_FN_ATTRS128\n"
"_mm_maskstore_ps(float *__p, __m128i __m, __m128 __a)\n"
"{\n"
"  __builtin_ia32_maskstoreps((__v4sf *)__p, (__v4si)__m, (__v4sf)__a);\n"
"}\n"
"\n"
"/* Cacheability support ops */\n"
"/// Moves integer data from a 256-bit integer vector to a 32-byte\n"
"///    aligned memory location. To minimize caching, the data is flagged as\n"
"///    non-temporal (unlikely to be used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVNTDQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A pointer to a 32-byte aligned memory location that will receive the\n"
"///    integer values.\n"
"/// \\param __b\n"
"///    A 256-bit integer vector containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_stream_si256(__m256i *__a, __m256i __b)\n"
"{\n"
"  typedef __v4di __v4di_aligned __attribute__((aligned(32)));\n"
"  __builtin_nontemporal_store((__v4di_aligned)__b, (__v4di_aligned*)__a);\n"
"}\n"
"\n"
"/// Moves double-precision values from a 256-bit vector of [4 x double]\n"
"///    to a 32-byte aligned memory location. To minimize caching, the data is\n"
"///    flagged as non-temporal (unlikely to be used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVNTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A pointer to a 32-byte aligned memory location that will receive the\n"
"///    double-precision floating-point values.\n"
"/// \\param __b\n"
"///    A 256-bit vector of [4 x double] containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_stream_pd(double *__a, __m256d __b)\n"
"{\n"
"  typedef __v4df __v4df_aligned __attribute__((aligned(32)));\n"
"  __builtin_nontemporal_store((__v4df_aligned)__b, (__v4df_aligned*)__a);\n"
"}\n"
"\n"
"/// Moves single-precision floating point values from a 256-bit vector\n"
"///    of [8 x float] to a 32-byte aligned memory location. To minimize\n"
"///    caching, the data is flagged as non-temporal (unlikely to be used again\n"
"///    soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVNTPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 32-byte aligned memory location that will receive the\n"
"///    single-precision floating point values.\n"
"/// \\param __a\n"
"///    A 256-bit vector of [8 x float] containing the values to be moved.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_stream_ps(float *__p, __m256 __a)\n"
"{\n"
"  typedef __v8sf __v8sf_aligned __attribute__((aligned(32)));\n"
"  __builtin_nontemporal_store((__v8sf_aligned)__a, (__v8sf_aligned*)__p);\n"
"}\n"
"\n"
"/* Create vectors */\n"
"/// Create a 256-bit vector of [4 x double] with undefined values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\returns A 256-bit vector of [4 x double] containing undefined values.\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_undefined_pd(void)\n"
"{\n"
"  return (__m256d)__builtin_ia32_undef256();\n"
"}\n"
"\n"
"/// Create a 256-bit vector of [8 x float] with undefined values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\returns A 256-bit vector of [8 x float] containing undefined values.\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_undefined_ps(void)\n"
"{\n"
"  return (__m256)__builtin_ia32_undef256();\n"
"}\n"
"\n"
"/// Create a 256-bit integer vector with undefined values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\returns A 256-bit integer vector containing undefined values.\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_undefined_si256(void)\n"
"{\n"
"  return (__m256i)__builtin_ia32_undef256();\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [4 x double]\n"
"///    initialized with the specified double-precision floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPD+VINSERTF128 </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A double-precision floating-point value used to initialize bits [255:192]\n"
"///    of the result.\n"
"/// \\param __b\n"
"///    A double-precision floating-point value used to initialize bits [191:128]\n"
"///    of the result.\n"
"/// \\param __c\n"
"///    A double-precision floating-point value used to initialize bits [127:64]\n"
"///    of the result.\n"
"/// \\param __d\n"
"///    A double-precision floating-point value used to initialize bits [63:0]\n"
"///    of the result.\n"
"/// \\returns An initialized 256-bit floating-point vector of [4 x double].\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_set_pd(double __a, double __b, double __c, double __d)\n"
"{\n"
"  return __extension__ (__m256d){ __d, __c, __b, __a };\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float] initialized\n"
"///    with the specified single-precision floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A single-precision floating-point value used to initialize bits [255:224]\n"
"///    of the result.\n"
"/// \\param __b\n"
"///    A single-precision floating-point value used to initialize bits [223:192]\n"
"///    of the result.\n"
"/// \\param __c\n"
"///    A single-precision floating-point value used to initialize bits [191:160]\n"
"///    of the result.\n"
"/// \\param __d\n"
"///    A single-precision floating-point value used to initialize bits [159:128]\n"
"///    of the result.\n"
"/// \\param __e\n"
"///    A single-precision floating-point value used to initialize bits [127:96]\n"
"///    of the result.\n"
"/// \\param __f\n"
"///    A single-precision floating-point value used to initialize bits [95:64]\n"
"///    of the result.\n"
"/// \\param __g\n"
"///    A single-precision floating-point value used to initialize bits [63:32]\n"
"///    of the result.\n"
"/// \\param __h\n"
"///    A single-precision floating-point value used to initialize bits [31:0]\n"
"///    of the result.\n"
"/// \\returns An initialized 256-bit floating-point vector of [8 x float].\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_set_ps(float __a, float __b, float __c, float __d,\n"
"              float __e, float __f, float __g, float __h)\n"
"{\n"
"  return __extension__ (__m256){ __h, __g, __f, __e, __d, __c, __b, __a };\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector initialized with the specified\n"
"///    32-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __i0\n"
"///    A 32-bit integral value used to initialize bits [255:224] of the result.\n"
"/// \\param __i1\n"
"///    A 32-bit integral value used to initialize bits [223:192] of the result.\n"
"/// \\param __i2\n"
"///    A 32-bit integral value used to initialize bits [191:160] of the result.\n"
"/// \\param __i3\n"
"///    A 32-bit integral value used to initialize bits [159:128] of the result.\n"
"/// \\param __i4\n"
"///    A 32-bit integral value used to initialize bits [127:96] of the result.\n"
"/// \\param __i5\n"
"///    A 32-bit integral value used to initialize bits [95:64] of the result.\n"
"/// \\param __i6\n"
"///    A 32-bit integral value used to initialize bits [63:32] of the result.\n"
"/// \\param __i7\n"
"///    A 32-bit integral value used to initialize bits [31:0] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set_epi32(int __i0, int __i1, int __i2, int __i3,\n"
"                 int __i4, int __i5, int __i6, int __i7)\n"
"{\n"
"  return __extension__ (__m256i)(__v8si){ __i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0 };\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector initialized with the specified\n"
"///    16-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __w15\n"
"///    A 16-bit integral value used to initialize bits [255:240] of the result.\n"
"/// \\param __w14\n"
"///    A 16-bit integral value used to initialize bits [239:224] of the result.\n"
"/// \\param __w13\n"
"///    A 16-bit integral value used to initialize bits [223:208] of the result.\n"
"/// \\param __w12\n"
"///    A 16-bit integral value used to initialize bits [207:192] of the result.\n"
"/// \\param __w11\n"
"///    A 16-bit integral value used to initialize bits [191:176] of the result.\n"
"/// \\param __w10\n"
"///    A 16-bit integral value used to initialize bits [175:160] of the result.\n"
"/// \\param __w09\n"
"///    A 16-bit integral value used to initialize bits [159:144] of the result.\n"
"/// \\param __w08\n"
"///    A 16-bit integral value used to initialize bits [143:128] of the result.\n"
"/// \\param __w07\n"
"///    A 16-bit integral value used to initialize bits [127:112] of the result.\n"
"/// \\param __w06\n"
"///    A 16-bit integral value used to initialize bits [111:96] of the result.\n"
"/// \\param __w05\n"
"///    A 16-bit integral value used to initialize bits [95:80] of the result.\n"
"/// \\param __w04\n"
"///    A 16-bit integral value used to initialize bits [79:64] of the result.\n"
"/// \\param __w03\n"
"///    A 16-bit integral value used to initialize bits [63:48] of the result.\n"
"/// \\param __w02\n"
"///    A 16-bit integral value used to initialize bits [47:32] of the result.\n"
"/// \\param __w01\n"
"///    A 16-bit integral value used to initialize bits [31:16] of the result.\n"
"/// \\param __w00\n"
"///    A 16-bit integral value used to initialize bits [15:0] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set_epi16(short __w15, short __w14, short __w13, short __w12,\n"
"                 short __w11, short __w10, short __w09, short __w08,\n"
"                 short __w07, short __w06, short __w05, short __w04,\n"
"                 short __w03, short __w02, short __w01, short __w00)\n"
"{\n"
"  return __extension__ (__m256i)(__v16hi){ __w00, __w01, __w02, __w03, __w04, __w05, __w06,\n"
"    __w07, __w08, __w09, __w10, __w11, __w12, __w13, __w14, __w15 };\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector initialized with the specified\n"
"///    8-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __b31\n"
"///    An 8-bit integral value used to initialize bits [255:248] of the result.\n"
"/// \\param __b30\n"
"///    An 8-bit integral value used to initialize bits [247:240] of the result.\n"
"/// \\param __b29\n"
"///    An 8-bit integral value used to initialize bits [239:232] of the result.\n"
"/// \\param __b28\n"
"///    An 8-bit integral value used to initialize bits [231:224] of the result.\n"
"/// \\param __b27\n"
"///    An 8-bit integral value used to initialize bits [223:216] of the result.\n"
"/// \\param __b26\n"
"///    An 8-bit integral value used to initialize bits [215:208] of the result.\n"
"/// \\param __b25\n"
"///    An 8-bit integral value used to initialize bits [207:200] of the result.\n"
"/// \\param __b24\n"
"///    An 8-bit integral value used to initialize bits [199:192] of the result.\n"
"/// \\param __b23\n"
"///    An 8-bit integral value used to initialize bits [191:184] of the result.\n"
"/// \\param __b22\n"
"///    An 8-bit integral value used to initialize bits [183:176] of the result.\n"
"/// \\param __b21\n"
"///    An 8-bit integral value used to initialize bits [175:168] of the result.\n"
"/// \\param __b20\n"
"///    An 8-bit integral value used to initialize bits [167:160] of the result.\n"
"/// \\param __b19\n"
"///    An 8-bit integral value used to initialize bits [159:152] of the result.\n"
"/// \\param __b18\n"
"///    An 8-bit integral value used to initialize bits [151:144] of the result.\n"
"/// \\param __b17\n"
"///    An 8-bit integral value used to initialize bits [143:136] of the result.\n"
"/// \\param __b16\n"
"///    An 8-bit integral value used to initialize bits [135:128] of the result.\n"
"/// \\param __b15\n"
"///    An 8-bit integral value used to initialize bits [127:120] of the result.\n"
"/// \\param __b14\n"
"///    An 8-bit integral value used to initialize bits [119:112] of the result.\n"
"/// \\param __b13\n"
"///    An 8-bit integral value used to initialize bits [111:104] of the result.\n"
"/// \\param __b12\n"
"///    An 8-bit integral value used to initialize bits [103:96] of the result.\n"
"/// \\param __b11\n"
"///    An 8-bit integral value used to initialize bits [95:88] of the result.\n"
"/// \\param __b10\n"
"///    An 8-bit integral value used to initialize bits [87:80] of the result.\n"
"/// \\param __b09\n"
"///    An 8-bit integral value used to initialize bits [79:72] of the result.\n"
"/// \\param __b08\n"
"///    An 8-bit integral value used to initialize bits [71:64] of the result.\n"
"/// \\param __b07\n"
"///    An 8-bit integral value used to initialize bits [63:56] of the result.\n"
"/// \\param __b06\n"
"///    An 8-bit integral value used to initialize bits [55:48] of the result.\n"
"/// \\param __b05\n"
"///    An 8-bit integral value used to initialize bits [47:40] of the result.\n"
"/// \\param __b04\n"
"///    An 8-bit integral value used to initialize bits [39:32] of the result.\n"
"/// \\param __b03\n"
"///    An 8-bit integral value used to initialize bits [31:24] of the result.\n"
"/// \\param __b02\n"
"///    An 8-bit integral value used to initialize bits [23:16] of the result.\n"
"/// \\param __b01\n"
"///    An 8-bit integral value used to initialize bits [15:8] of the result.\n"
"/// \\param __b00\n"
"///    An 8-bit integral value used to initialize bits [7:0] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set_epi8(char __b31, char __b30, char __b29, char __b28,\n"
"                char __b27, char __b26, char __b25, char __b24,\n"
"                char __b23, char __b22, char __b21, char __b20,\n"
"                char __b19, char __b18, char __b17, char __b16,\n"
"                char __b15, char __b14, char __b13, char __b12,\n"
"                char __b11, char __b10, char __b09, char __b08,\n"
"                char __b07, char __b06, char __b05, char __b04,\n"
"                char __b03, char __b02, char __b01, char __b00)\n"
"{\n"
"  return __extension__ (__m256i)(__v32qi){\n"
"    __b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,\n"
"    __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,\n"
"    __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,\n"
"    __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31\n"
"  };\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector initialized with the specified\n"
"///    64-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKLQDQ+VINSERTF128 </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integral value used to initialize bits [255:192] of the result.\n"
"/// \\param __b\n"
"///    A 64-bit integral value used to initialize bits [191:128] of the result.\n"
"/// \\param __c\n"
"///    A 64-bit integral value used to initialize bits [127:64] of the result.\n"
"/// \\param __d\n"
"///    A 64-bit integral value used to initialize bits [63:0] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set_epi64x(long long __a, long long __b, long long __c, long long __d)\n"
"{\n"
"  return __extension__ (__m256i)(__v4di){ __d, __c, __b, __a };\n"
"}\n"
"\n"
"/* Create vectors with elements in reverse order */\n"
"/// Constructs a 256-bit floating-point vector of [4 x double],\n"
"///    initialized in reverse order with the specified double-precision\n"
"///    floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPD+VINSERTF128 </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A double-precision floating-point value used to initialize bits [63:0]\n"
"///    of the result.\n"
"/// \\param __b\n"
"///    A double-precision floating-point value used to initialize bits [127:64]\n"
"///    of the result.\n"
"/// \\param __c\n"
"///    A double-precision floating-point value used to initialize bits [191:128]\n"
"///    of the result.\n"
"/// \\param __d\n"
"///    A double-precision floating-point value used to initialize bits [255:192]\n"
"///    of the result.\n"
"/// \\returns An initialized 256-bit floating-point vector of [4 x double].\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_setr_pd(double __a, double __b, double __c, double __d)\n"
"{\n"
"  return _mm256_set_pd(__d, __c, __b, __a);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float],\n"
"///    initialized in reverse order with the specified single-precision\n"
"///    float-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A single-precision floating-point value used to initialize bits [31:0]\n"
"///    of the result.\n"
"/// \\param __b\n"
"///    A single-precision floating-point value used to initialize bits [63:32]\n"
"///    of the result.\n"
"/// \\param __c\n"
"///    A single-precision floating-point value used to initialize bits [95:64]\n"
"///    of the result.\n"
"/// \\param __d\n"
"///    A single-precision floating-point value used to initialize bits [127:96]\n"
"///    of the result.\n"
"/// \\param __e\n"
"///    A single-precision floating-point value used to initialize bits [159:128]\n"
"///    of the result.\n"
"/// \\param __f\n"
"///    A single-precision floating-point value used to initialize bits [191:160]\n"
"///    of the result.\n"
"/// \\param __g\n"
"///    A single-precision floating-point value used to initialize bits [223:192]\n"
"///    of the result.\n"
"/// \\param __h\n"
"///    A single-precision floating-point value used to initialize bits [255:224]\n"
"///    of the result.\n"
"/// \\returns An initialized 256-bit floating-point vector of [8 x float].\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_setr_ps(float __a, float __b, float __c, float __d,\n"
"               float __e, float __f, float __g, float __h)\n"
"{\n"
"  return _mm256_set_ps(__h, __g, __f, __e, __d, __c, __b, __a);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector, initialized in reverse order\n"
"///    with the specified 32-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __i0\n"
"///    A 32-bit integral value used to initialize bits [31:0] of the result.\n"
"/// \\param __i1\n"
"///    A 32-bit integral value used to initialize bits [63:32] of the result.\n"
"/// \\param __i2\n"
"///    A 32-bit integral value used to initialize bits [95:64] of the result.\n"
"/// \\param __i3\n"
"///    A 32-bit integral value used to initialize bits [127:96] of the result.\n"
"/// \\param __i4\n"
"///    A 32-bit integral value used to initialize bits [159:128] of the result.\n"
"/// \\param __i5\n"
"///    A 32-bit integral value used to initialize bits [191:160] of the result.\n"
"/// \\param __i6\n"
"///    A 32-bit integral value used to initialize bits [223:192] of the result.\n"
"/// \\param __i7\n"
"///    A 32-bit integral value used to initialize bits [255:224] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_setr_epi32(int __i0, int __i1, int __i2, int __i3,\n"
"                  int __i4, int __i5, int __i6, int __i7)\n"
"{\n"
"  return _mm256_set_epi32(__i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector, initialized in reverse order\n"
"///    with the specified 16-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __w15\n"
"///    A 16-bit integral value used to initialize bits [15:0] of the result.\n"
"/// \\param __w14\n"
"///    A 16-bit integral value used to initialize bits [31:16] of the result.\n"
"/// \\param __w13\n"
"///    A 16-bit integral value used to initialize bits [47:32] of the result.\n"
"/// \\param __w12\n"
"///    A 16-bit integral value used to initialize bits [63:48] of the result.\n"
"/// \\param __w11\n"
"///    A 16-bit integral value used to initialize bits [79:64] of the result.\n"
"/// \\param __w10\n"
"///    A 16-bit integral value used to initialize bits [95:80] of the result.\n"
"/// \\param __w09\n"
"///    A 16-bit integral value used to initialize bits [111:96] of the result.\n"
"/// \\param __w08\n"
"///    A 16-bit integral value used to initialize bits [127:112] of the result.\n"
"/// \\param __w07\n"
"///    A 16-bit integral value used to initialize bits [143:128] of the result.\n"
"/// \\param __w06\n"
"///    A 16-bit integral value used to initialize bits [159:144] of the result.\n"
"/// \\param __w05\n"
"///    A 16-bit integral value used to initialize bits [175:160] of the result.\n"
"/// \\param __w04\n"
"///    A 16-bit integral value used to initialize bits [191:176] of the result.\n"
"/// \\param __w03\n"
"///    A 16-bit integral value used to initialize bits [207:192] of the result.\n"
"/// \\param __w02\n"
"///    A 16-bit integral value used to initialize bits [223:208] of the result.\n"
"/// \\param __w01\n"
"///    A 16-bit integral value used to initialize bits [239:224] of the result.\n"
"/// \\param __w00\n"
"///    A 16-bit integral value used to initialize bits [255:240] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_setr_epi16(short __w15, short __w14, short __w13, short __w12,\n"
"       short __w11, short __w10, short __w09, short __w08,\n"
"       short __w07, short __w06, short __w05, short __w04,\n"
"       short __w03, short __w02, short __w01, short __w00)\n"
"{\n"
"  return _mm256_set_epi16(__w00, __w01, __w02, __w03,\n"
"                          __w04, __w05, __w06, __w07,\n"
"                          __w08, __w09, __w10, __w11,\n"
"                          __w12, __w13, __w14, __w15);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector, initialized in reverse order\n"
"///    with the specified 8-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///   instruction.\n"
"///\n"
"/// \\param __b31\n"
"///    An 8-bit integral value used to initialize bits [7:0] of the result.\n"
"/// \\param __b30\n"
"///    An 8-bit integral value used to initialize bits [15:8] of the result.\n"
"/// \\param __b29\n"
"///    An 8-bit integral value used to initialize bits [23:16] of the result.\n"
"/// \\param __b28\n"
"///    An 8-bit integral value used to initialize bits [31:24] of the result.\n"
"/// \\param __b27\n"
"///    An 8-bit integral value used to initialize bits [39:32] of the result.\n"
"/// \\param __b26\n"
"///    An 8-bit integral value used to initialize bits [47:40] of the result.\n"
"/// \\param __b25\n"
"///    An 8-bit integral value used to initialize bits [55:48] of the result.\n"
"/// \\param __b24\n"
"///    An 8-bit integral value used to initialize bits [63:56] of the result.\n"
"/// \\param __b23\n"
"///    An 8-bit integral value used to initialize bits [71:64] of the result.\n"
"/// \\param __b22\n"
"///    An 8-bit integral value used to initialize bits [79:72] of the result.\n"
"/// \\param __b21\n"
"///    An 8-bit integral value used to initialize bits [87:80] of the result.\n"
"/// \\param __b20\n"
"///    An 8-bit integral value used to initialize bits [95:88] of the result.\n"
"/// \\param __b19\n"
"///    An 8-bit integral value used to initialize bits [103:96] of the result.\n"
"/// \\param __b18\n"
"///    An 8-bit integral value used to initialize bits [111:104] of the result.\n"
"/// \\param __b17\n"
"///    An 8-bit integral value used to initialize bits [119:112] of the result.\n"
"/// \\param __b16\n"
"///    An 8-bit integral value used to initialize bits [127:120] of the result.\n"
"/// \\param __b15\n"
"///    An 8-bit integral value used to initialize bits [135:128] of the result.\n"
"/// \\param __b14\n"
"///    An 8-bit integral value used to initialize bits [143:136] of the result.\n"
"/// \\param __b13\n"
"///    An 8-bit integral value used to initialize bits [151:144] of the result.\n"
"/// \\param __b12\n"
"///    An 8-bit integral value used to initialize bits [159:152] of the result.\n"
"/// \\param __b11\n"
"///    An 8-bit integral value used to initialize bits [167:160] of the result.\n"
"/// \\param __b10\n"
"///    An 8-bit integral value used to initialize bits [175:168] of the result.\n"
"/// \\param __b09\n"
"///    An 8-bit integral value used to initialize bits [183:176] of the result.\n"
"/// \\param __b08\n"
"///    An 8-bit integral value used to initialize bits [191:184] of the result.\n"
"/// \\param __b07\n"
"///    An 8-bit integral value used to initialize bits [199:192] of the result.\n"
"/// \\param __b06\n"
"///    An 8-bit integral value used to initialize bits [207:200] of the result.\n"
"/// \\param __b05\n"
"///    An 8-bit integral value used to initialize bits [215:208] of the result.\n"
"/// \\param __b04\n"
"///    An 8-bit integral value used to initialize bits [223:216] of the result.\n"
"/// \\param __b03\n"
"///    An 8-bit integral value used to initialize bits [231:224] of the result.\n"
"/// \\param __b02\n"
"///    An 8-bit integral value used to initialize bits [239:232] of the result.\n"
"/// \\param __b01\n"
"///    An 8-bit integral value used to initialize bits [247:240] of the result.\n"
"/// \\param __b00\n"
"///    An 8-bit integral value used to initialize bits [255:248] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_setr_epi8(char __b31, char __b30, char __b29, char __b28,\n"
"                 char __b27, char __b26, char __b25, char __b24,\n"
"                 char __b23, char __b22, char __b21, char __b20,\n"
"                 char __b19, char __b18, char __b17, char __b16,\n"
"                 char __b15, char __b14, char __b13, char __b12,\n"
"                 char __b11, char __b10, char __b09, char __b08,\n"
"                 char __b07, char __b06, char __b05, char __b04,\n"
"                 char __b03, char __b02, char __b01, char __b00)\n"
"{\n"
"  return _mm256_set_epi8(__b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,\n"
"                         __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,\n"
"                         __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,\n"
"                         __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector, initialized in reverse order\n"
"///    with the specified 64-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKLQDQ+VINSERTF128 </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integral value used to initialize bits [63:0] of the result.\n"
"/// \\param __b\n"
"///    A 64-bit integral value used to initialize bits [127:64] of the result.\n"
"/// \\param __c\n"
"///    A 64-bit integral value used to initialize bits [191:128] of the result.\n"
"/// \\param __d\n"
"///    A 64-bit integral value used to initialize bits [255:192] of the result.\n"
"/// \\returns An initialized 256-bit integer vector.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_setr_epi64x(long long __a, long long __b, long long __c, long long __d)\n"
"{\n"
"  return _mm256_set_epi64x(__d, __c, __b, __a);\n"
"}\n"
"\n"
"/* Create vectors with repeated elements */\n"
"/// Constructs a 256-bit floating-point vector of [4 x double], with each\n"
"///    of the four double-precision floating-point vector elements set to the\n"
"///    specified double-precision floating-point value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP+VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A double-precision floating-point value used to initialize each vector\n"
"///    element of the result.\n"
"/// \\returns An initialized 256-bit floating-point vector of [4 x double].\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_set1_pd(double __w)\n"
"{\n"
"  return _mm256_set_pd(__w, __w, __w, __w);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float], with each\n"
"///    of the eight single-precision floating-point vector elements set to the\n"
"///    specified single-precision floating-point value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS+VINSERTF128 </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A single-precision floating-point value used to initialize each vector\n"
"///    element of the result.\n"
"/// \\returns An initialized 256-bit floating-point vector of [8 x float].\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_set1_ps(float __w)\n"
"{\n"
"  return _mm256_set_ps(__w, __w, __w, __w, __w, __w, __w, __w);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector of [8 x i32], with each of the\n"
"///    32-bit integral vector elements set to the specified 32-bit integral\n"
"///    value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS+VINSERTF128 </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __i\n"
"///    A 32-bit integral value used to initialize each vector element of the\n"
"///    result.\n"
"/// \\returns An initialized 256-bit integer vector of [8 x i32].\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set1_epi32(int __i)\n"
"{\n"
"  return _mm256_set_epi32(__i, __i, __i, __i, __i, __i, __i, __i);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector of [16 x i16], with each of the\n"
"///    16-bit integral vector elements set to the specified 16-bit integral\n"
"///    value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSHUFB+VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A 16-bit integral value used to initialize each vector element of the\n"
"///    result.\n"
"/// \\returns An initialized 256-bit integer vector of [16 x i16].\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set1_epi16(short __w)\n"
"{\n"
"  return _mm256_set_epi16(__w, __w, __w, __w, __w, __w, __w, __w,\n"
"                          __w, __w, __w, __w, __w, __w, __w, __w);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector of [32 x i8], with each of the\n"
"///    8-bit integral vector elements set to the specified 8-bit integral value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSHUFB+VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __b\n"
"///    An 8-bit integral value used to initialize each vector element of the\n"
"///    result.\n"
"/// \\returns An initialized 256-bit integer vector of [32 x i8].\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set1_epi8(char __b)\n"
"{\n"
"  return _mm256_set_epi8(__b, __b, __b, __b, __b, __b, __b, __b,\n"
"                         __b, __b, __b, __b, __b, __b, __b, __b,\n"
"                         __b, __b, __b, __b, __b, __b, __b, __b,\n"
"                         __b, __b, __b, __b, __b, __b, __b, __b);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector of [4 x i64], with each of the\n"
"///    64-bit integral vector elements set to the specified 64-bit integral\n"
"///    value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP+VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __q\n"
"///    A 64-bit integral value used to initialize each vector element of the\n"
"///    result.\n"
"/// \\returns An initialized 256-bit integer vector of [4 x i64].\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set1_epi64x(long long __q)\n"
"{\n"
"  return _mm256_set_epi64x(__q, __q, __q, __q);\n"
"}\n"
"\n"
"/* Create __zeroed vectors */\n"
"/// Constructs a 256-bit floating-point vector of [4 x double] with all\n"
"///    vector elements initialized to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS </c> instruction.\n"
"///\n"
"/// \\returns A 256-bit vector of [4 x double] with all elements set to zero.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_setzero_pd(void)\n"
"{\n"
"  return __extension__ (__m256d){ 0, 0, 0, 0 };\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float] with all\n"
"///    vector elements initialized to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS </c> instruction.\n"
"///\n"
"/// \\returns A 256-bit vector of [8 x float] with all elements set to zero.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_setzero_ps(void)\n"
"{\n"
"  return __extension__ (__m256){ 0, 0, 0, 0, 0, 0, 0, 0 };\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector initialized to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS </c> instruction.\n"
"///\n"
"/// \\returns A 256-bit integer vector initialized to zero.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_setzero_si256(void)\n"
"{\n"
"  return __extension__ (__m256i)(__v4di){ 0, 0, 0, 0 };\n"
"}\n"
"\n"
"/* Cast between vector types */\n"
"/// Casts a 256-bit floating-point vector of [4 x double] into a 256-bit\n"
"///    floating-point vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [4 x double].\n"
"/// \\returns A 256-bit floating-point vector of [8 x float] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_castpd_ps(__m256d __a)\n"
"{\n"
"  return (__m256)__a;\n"
"}\n"
"\n"
"/// Casts a 256-bit floating-point vector of [4 x double] into a 256-bit\n"
"///    integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [4 x double].\n"
"/// \\returns A 256-bit integer vector containing the same bitwise pattern as the\n"
"///    parameter.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_castpd_si256(__m256d __a)\n"
"{\n"
"  return (__m256i)__a;\n"
"}\n"
"\n"
"/// Casts a 256-bit floating-point vector of [8 x float] into a 256-bit\n"
"///    floating-point vector of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [8 x float].\n"
"/// \\returns A 256-bit floating-point vector of [4 x double] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_castps_pd(__m256 __a)\n"
"{\n"
"  return (__m256d)__a;\n"
"}\n"
"\n"
"/// Casts a 256-bit floating-point vector of [8 x float] into a 256-bit\n"
"///    integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [8 x float].\n"
"/// \\returns A 256-bit integer vector containing the same bitwise pattern as the\n"
"///    parameter.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_castps_si256(__m256 __a)\n"
"{\n"
"  return (__m256i)__a;\n"
"}\n"
"\n"
"/// Casts a 256-bit integer vector into a 256-bit floating-point vector\n"
"///    of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"/// \\returns A 256-bit floating-point vector of [8 x float] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_castsi256_ps(__m256i __a)\n"
"{\n"
"  return (__m256)__a;\n"
"}\n"
"\n"
"/// Casts a 256-bit integer vector into a 256-bit floating-point vector\n"
"///    of [4 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"/// \\returns A 256-bit floating-point vector of [4 x double] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_castsi256_pd(__m256i __a)\n"
"{\n"
"  return (__m256d)__a;\n"
"}\n"
"\n"
"/// Returns the lower 128 bits of a 256-bit floating-point vector of\n"
"///    [4 x double] as a 128-bit floating-point vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [4 x double].\n"
"/// \\returns A 128-bit floating-point vector of [2 x double] containing the\n"
"///    lower 128 bits of the parameter.\n"
"static __inline __m128d __DEFAULT_FN_ATTRS\n"
"_mm256_castpd256_pd128(__m256d __a)\n"
"{\n"
"  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 1);\n"
"}\n"
"\n"
"/// Returns the lower 128 bits of a 256-bit floating-point vector of\n"
"///    [8 x float] as a 128-bit floating-point vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [8 x float].\n"
"/// \\returns A 128-bit floating-point vector of [4 x float] containing the\n"
"///    lower 128 bits of the parameter.\n"
"static __inline __m128 __DEFAULT_FN_ATTRS\n"
"_mm256_castps256_ps128(__m256 __a)\n"
"{\n"
"  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 1, 2, 3);\n"
"}\n"
"\n"
"/// Truncates a 256-bit integer vector into a 128-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the lower 128 bits of the\n"
"///    parameter.\n"
"static __inline __m128i __DEFAULT_FN_ATTRS\n"
"_mm256_castsi256_si128(__m256i __a)\n"
"{\n"
"  return __builtin_shufflevector((__v4di)__a, (__v4di)__a, 0, 1);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [4 x double] from a\n"
"///    128-bit floating-point vector of [2 x double].\n"
"///\n"
"///    The lower 128 bits contain the value of the source vector. The contents\n"
"///    of the upper 128 bits are undefined.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 256-bit floating-point vector of [4 x double]. The lower 128 bits\n"
"///    contain the value of the parameter. The contents of the upper 128 bits\n"
"///    are undefined.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_castpd128_pd256(__m128d __a)\n"
"{\n"
"  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 1, -1, -1);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float] from a\n"
"///    128-bit floating-point vector of [4 x float].\n"
"///\n"
"///    The lower 128 bits contain the value of the source vector. The contents\n"
"///    of the upper 128 bits are undefined.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 256-bit floating-point vector of [8 x float]. The lower 128 bits\n"
"///    contain the value of the parameter. The contents of the upper 128 bits\n"
"///    are undefined.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_castps128_ps256(__m128 __a)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 1, 2, 3, -1, -1, -1, -1);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector from a 128-bit integer vector.\n"
"///\n"
"///    The lower 128 bits contain the value of the source vector. The contents\n"
"///    of the upper 128 bits are undefined.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 256-bit integer vector. The lower 128 bits contain the value of\n"
"///    the parameter. The contents of the upper 128 bits are undefined.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_castsi128_si256(__m128i __a)\n"
"{\n"
"  return __builtin_shufflevector((__v2di)__a, (__v2di)__a, 0, 1, -1, -1);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [4 x double] from a\n"
"///    128-bit floating-point vector of [2 x double]. The lower 128 bits\n"
"///    contain the value of the source vector. The upper 128 bits are set\n"
"///    to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 256-bit floating-point vector of [4 x double]. The lower 128 bits\n"
"///    contain the value of the parameter. The upper 128 bits are set to zero.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_zextpd128_pd256(__m128d __a)\n"
"{\n"
"  return __builtin_shufflevector((__v2df)__a, (__v2df)_mm_setzero_pd(), 0, 1, 2, 3);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float] from a\n"
"///    128-bit floating-point vector of [4 x float]. The lower 128 bits contain\n"
"///    the value of the source vector. The upper 128 bits are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 256-bit floating-point vector of [8 x float]. The lower 128 bits\n"
"///    contain the value of the parameter. The upper 128 bits are set to zero.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_zextps128_ps256(__m128 __a)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)_mm_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector from a 128-bit integer vector.\n"
"///    The lower 128 bits contain the value of the source vector. The upper\n"
"///    128 bits are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 256-bit integer vector. The lower 128 bits contain the value of\n"
"///    the parameter. The upper 128 bits are set to zero.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_zextsi128_si256(__m128i __a)\n"
"{\n"
"  return __builtin_shufflevector((__v2di)__a, (__v2di)_mm_setzero_si128(), 0, 1, 2, 3);\n"
"}\n"
"\n"
"/*\n"
"   Vector insert.\n"
"   We use macros rather than inlines because we only want to accept\n"
"   invocations where the immediate M is a constant expression.\n"
"*/\n"
"/// Constructs a new 256-bit vector of [8 x float] by first duplicating\n"
"///    a 256-bit vector of [8 x float] given in the first parameter, and then\n"
"///    replacing either the upper or the lower 128 bits with the contents of a\n"
"///    128-bit vector of [4 x float] in the second parameter.\n"
"///\n"
"///    The immediate integer parameter determines between the upper or the lower\n"
"///    128 bits.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256 _mm256_insertf128_ps(__m256 V1, __m128 V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit vector of [8 x float]. This vector is copied to the result\n"
"///    first, and then either the upper or the lower 128 bits of the result will\n"
"///    be replaced by the contents of \\a V2.\n"
"/// \\param V2\n"
"///    A 128-bit vector of [4 x float]. The contents of this parameter are\n"
"///    written to either the upper or the lower 128 bits of the result depending\n"
"///    on the value of parameter \\a M.\n"
"/// \\param M\n"
"///    An immediate integer. The least significant bit determines how the values\n"
"///    from the two parameters are interleaved: \\n\n"
"///    If bit [0] of \\a M is 0, \\a V2 are copied to bits [127:0] of the result,\n"
"///    and bits [255:128] of \\a V1 are copied to bits [255:128] of the\n"
"///    result. \\n\n"
"///    If bit [0] of \\a M is 1, \\a V2 are copied to bits [255:128] of the\n"
"///    result, and bits [127:0] of \\a V1 are copied to bits [127:0] of the\n"
"///    result.\n"
"/// \\returns A 256-bit vector of [8 x float] containing the interleaved values.\n"
"#define _mm256_insertf128_ps(V1, V2, M) \\\n"
"  (__m256)__builtin_ia32_vinsertf128_ps256((__v8sf)(__m256)(V1), \\\n"
"                                           (__v4sf)(__m128)(V2), (int)(M))\n"
"\n"
"/// Constructs a new 256-bit vector of [4 x double] by first duplicating\n"
"///    a 256-bit vector of [4 x double] given in the first parameter, and then\n"
"///    replacing either the upper or the lower 128 bits with the contents of a\n"
"///    128-bit vector of [2 x double] in the second parameter.\n"
"///\n"
"///    The immediate integer parameter determines between the upper or the lower\n"
"///    128 bits.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256d _mm256_insertf128_pd(__m256d V1, __m128d V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit vector of [4 x double]. This vector is copied to the result\n"
"///    first, and then either the upper or the lower 128 bits of the result will\n"
"///    be replaced by the contents of \\a V2.\n"
"/// \\param V2\n"
"///    A 128-bit vector of [2 x double]. The contents of this parameter are\n"
"///    written to either the upper or the lower 128 bits of the result depending\n"
"///    on the value of parameter \\a M.\n"
"/// \\param M\n"
"///    An immediate integer. The least significant bit determines how the values\n"
"///    from the two parameters are interleaved: \\n\n"
"///    If bit [0] of \\a M is 0, \\a V2 are copied to bits [127:0] of the result,\n"
"///    and bits [255:128] of \\a V1 are copied to bits [255:128] of the\n"
"///    result. \\n\n"
"///    If bit [0] of \\a M is 1, \\a V2 are copied to bits [255:128] of the\n"
"///    result, and bits [127:0] of \\a V1 are copied to bits [127:0] of the\n"
"///    result.\n"
"/// \\returns A 256-bit vector of [4 x double] containing the interleaved values.\n"
"#define _mm256_insertf128_pd(V1, V2, M) \\\n"
"  (__m256d)__builtin_ia32_vinsertf128_pd256((__v4df)(__m256d)(V1), \\\n"
"                                            (__v2df)(__m128d)(V2), (int)(M))\n"
"\n"
"/// Constructs a new 256-bit integer vector by first duplicating a\n"
"///    256-bit integer vector given in the first parameter, and then replacing\n"
"///    either the upper or the lower 128 bits with the contents of a 128-bit\n"
"///    integer vector in the second parameter.\n"
"///\n"
"///    The immediate integer parameter determines between the upper or the lower\n"
"///    128 bits.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m256i _mm256_insertf128_si256(__m256i V1, __m128i V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 256-bit integer vector. This vector is copied to the result first, and\n"
"///    then either the upper or the lower 128 bits of the result will be\n"
"///    replaced by the contents of \\a V2.\n"
"/// \\param V2\n"
"///    A 128-bit integer vector. The contents of this parameter are written to\n"
"///    either the upper or the lower 128 bits of the result depending on the\n"
"///     value of parameter \\a M.\n"
"/// \\param M\n"
"///    An immediate integer. The least significant bit determines how the values\n"
"///    from the two parameters are interleaved: \\n\n"
"///    If bit [0] of \\a M is 0, \\a V2 are copied to bits [127:0] of the result,\n"
"///    and bits [255:128] of \\a V1 are copied to bits [255:128] of the\n"
"///    result. \\n\n"
"///    If bit [0] of \\a M is 1, \\a V2 are copied to bits [255:128] of the\n"
"///    result, and bits [127:0] of \\a V1 are copied to bits [127:0] of the\n"
"///    result.\n"
"/// \\returns A 256-bit integer vector containing the interleaved values.\n"
"#define _mm256_insertf128_si256(V1, V2, M) \\\n"
"  (__m256i)__builtin_ia32_vinsertf128_si256((__v8si)(__m256i)(V1), \\\n"
"                                            (__v4si)(__m128i)(V2), (int)(M))\n"
"\n"
"/*\n"
"   Vector extract.\n"
"   We use macros rather than inlines because we only want to accept\n"
"   invocations where the immediate M is a constant expression.\n"
"*/\n"
"/// Extracts either the upper or the lower 128 bits from a 256-bit vector\n"
"///    of [8 x float], as determined by the immediate integer parameter, and\n"
"///    returns the extracted bits as a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm256_extractf128_ps(__m256 V, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [8 x float].\n"
"/// \\param M\n"
"///    An immediate integer. The least significant bit determines which bits are\n"
"///    extracted from the first parameter: \\n\n"
"///    If bit [0] of \\a M is 0, bits [127:0] of \\a V are copied to the\n"
"///    result. \\n\n"
"///    If bit [0] of \\a M is 1, bits [255:128] of \\a V are copied to the result.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the extracted bits.\n"
"#define _mm256_extractf128_ps(V, M) \\\n"
"  (__m128)__builtin_ia32_vextractf128_ps256((__v8sf)(__m256)(V), (int)(M))\n"
"\n"
"/// Extracts either the upper or the lower 128 bits from a 256-bit vector\n"
"///    of [4 x double], as determined by the immediate integer parameter, and\n"
"///    returns the extracted bits as a 128-bit vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm256_extractf128_pd(__m256d V, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit vector of [4 x double].\n"
"/// \\param M\n"
"///    An immediate integer. The least significant bit determines which bits are\n"
"///    extracted from the first parameter: \\n\n"
"///    If bit [0] of \\a M is 0, bits [127:0] of \\a V are copied to the\n"
"///    result. \\n\n"
"///    If bit [0] of \\a M is 1, bits [255:128] of \\a V are copied to the result.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the extracted bits.\n"
"#define _mm256_extractf128_pd(V, M) \\\n"
"  (__m128d)__builtin_ia32_vextractf128_pd256((__v4df)(__m256d)(V), (int)(M))\n"
"\n"
"/// Extracts either the upper or the lower 128 bits from a 256-bit\n"
"///    integer vector, as determined by the immediate integer parameter, and\n"
"///    returns the extracted bits as a 128-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm256_extractf128_si256(__m256i V, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 256-bit integer vector.\n"
"/// \\param M\n"
"///    An immediate integer. The least significant bit determines which bits are\n"
"///    extracted from the first parameter:  \\n\n"
"///    If bit [0] of \\a M is 0, bits [127:0] of \\a V are copied to the\n"
"///    result. \\n\n"
"///    If bit [0] of \\a M is 1, bits [255:128] of \\a V are copied to the result.\n"
"/// \\returns A 128-bit integer vector containing the extracted bits.\n"
"#define _mm256_extractf128_si256(V, M) \\\n"
"  (__m128i)__builtin_ia32_vextractf128_si256((__v8si)(__m256i)(V), (int)(M))\n"
"\n"
"/* SIMD load ops (unaligned) */\n"
"/// Loads two 128-bit floating-point vectors of [4 x float] from\n"
"///    unaligned memory locations and constructs a 256-bit floating-point vector\n"
"///    of [8 x float] by concatenating the two 128-bit vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to load instructions followed by the\n"
"///   <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __addr_hi\n"
"///    A pointer to a 128-bit memory location containing 4 consecutive\n"
"///    single-precision floating-point values. These values are to be copied to\n"
"///    bits[255:128] of the result. The address of the memory location does not\n"
"///    have to be aligned.\n"
"/// \\param __addr_lo\n"
"///    A pointer to a 128-bit memory location containing 4 consecutive\n"
"///    single-precision floating-point values. These values are to be copied to\n"
"///    bits[127:0] of the result. The address of the memory location does not\n"
"///    have to be aligned.\n"
"/// \\returns A 256-bit floating-point vector of [8 x float] containing the\n"
"///    concatenated result.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_loadu2_m128(float const *__addr_hi, float const *__addr_lo)\n"
"{\n"
"  __m256 __v256 = _mm256_castps128_ps256(_mm_loadu_ps(__addr_lo));\n"
"  return _mm256_insertf128_ps(__v256, _mm_loadu_ps(__addr_hi), 1);\n"
"}\n"
"\n"
"/// Loads two 128-bit floating-point vectors of [2 x double] from\n"
"///    unaligned memory locations and constructs a 256-bit floating-point vector\n"
"///    of [4 x double] by concatenating the two 128-bit vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to load instructions followed by the\n"
"///   <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __addr_hi\n"
"///    A pointer to a 128-bit memory location containing two consecutive\n"
"///    double-precision floating-point values. These values are to be copied to\n"
"///    bits[255:128] of the result. The address of the memory location does not\n"
"///    have to be aligned.\n"
"/// \\param __addr_lo\n"
"///    A pointer to a 128-bit memory location containing two consecutive\n"
"///    double-precision floating-point values. These values are to be copied to\n"
"///    bits[127:0] of the result. The address of the memory location does not\n"
"///    have to be aligned.\n"
"/// \\returns A 256-bit floating-point vector of [4 x double] containing the\n"
"///    concatenated result.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_loadu2_m128d(double const *__addr_hi, double const *__addr_lo)\n"
"{\n"
"  __m256d __v256 = _mm256_castpd128_pd256(_mm_loadu_pd(__addr_lo));\n"
"  return _mm256_insertf128_pd(__v256, _mm_loadu_pd(__addr_hi), 1);\n"
"}\n"
"\n"
"/// Loads two 128-bit integer vectors from unaligned memory locations and\n"
"///    constructs a 256-bit integer vector by concatenating the two 128-bit\n"
"///    vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to load instructions followed by the\n"
"///   <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __addr_hi\n"
"///    A pointer to a 128-bit memory location containing a 128-bit integer\n"
"///    vector. This vector is to be copied to bits[255:128] of the result. The\n"
"///    address of the memory location does not have to be aligned.\n"
"/// \\param __addr_lo\n"
"///    A pointer to a 128-bit memory location containing a 128-bit integer\n"
"///    vector. This vector is to be copied to bits[127:0] of the result. The\n"
"///    address of the memory location does not have to be aligned.\n"
"/// \\returns A 256-bit integer vector containing the concatenated result.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_loadu2_m128i(__m128i const *__addr_hi, __m128i const *__addr_lo)\n"
"{\n"
"  __m256i __v256 = _mm256_castsi128_si256(_mm_loadu_si128(__addr_lo));\n"
"  return _mm256_insertf128_si256(__v256, _mm_loadu_si128(__addr_hi), 1);\n"
"}\n"
"\n"
"/* SIMD store ops (unaligned) */\n"
"/// Stores the upper and lower 128 bits of a 256-bit floating-point\n"
"///    vector of [8 x float] into two different unaligned memory locations.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction and the\n"
"///   store instructions.\n"
"///\n"
"/// \\param __addr_hi\n"
"///    A pointer to a 128-bit memory location. Bits[255:128] of \\a __a are to be\n"
"///    copied to this memory location. The address of this memory location does\n"
"///    not have to be aligned.\n"
"/// \\param __addr_lo\n"
"///    A pointer to a 128-bit memory location. Bits[127:0] of \\a __a are to be\n"
"///    copied to this memory location. The address of this memory location does\n"
"///    not have to be aligned.\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [8 x float].\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_storeu2_m128(float *__addr_hi, float *__addr_lo, __m256 __a)\n"
"{\n"
"  __m128 __v128;\n"
"\n"
"  __v128 = _mm256_castps256_ps128(__a);\n"
"  _mm_storeu_ps(__addr_lo, __v128);\n"
"  __v128 = _mm256_extractf128_ps(__a, 1);\n"
"  _mm_storeu_ps(__addr_hi, __v128);\n"
"}\n"
"\n"
"/// Stores the upper and lower 128 bits of a 256-bit floating-point\n"
"///    vector of [4 x double] into two different unaligned memory locations.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction and the\n"
"///   store instructions.\n"
"///\n"
"/// \\param __addr_hi\n"
"///    A pointer to a 128-bit memory location. Bits[255:128] of \\a __a are to be\n"
"///    copied to this memory location. The address of this memory location does\n"
"///    not have to be aligned.\n"
"/// \\param __addr_lo\n"
"///    A pointer to a 128-bit memory location. Bits[127:0] of \\a __a are to be\n"
"///    copied to this memory location. The address of this memory location does\n"
"///    not have to be aligned.\n"
"/// \\param __a\n"
"///    A 256-bit floating-point vector of [4 x double].\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_storeu2_m128d(double *__addr_hi, double *__addr_lo, __m256d __a)\n"
"{\n"
"  __m128d __v128;\n"
"\n"
"  __v128 = _mm256_castpd256_pd128(__a);\n"
"  _mm_storeu_pd(__addr_lo, __v128);\n"
"  __v128 = _mm256_extractf128_pd(__a, 1);\n"
"  _mm_storeu_pd(__addr_hi, __v128);\n"
"}\n"
"\n"
"/// Stores the upper and lower 128 bits of a 256-bit integer vector into\n"
"///    two different unaligned memory locations.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction and the\n"
"///   store instructions.\n"
"///\n"
"/// \\param __addr_hi\n"
"///    A pointer to a 128-bit memory location. Bits[255:128] of \\a __a are to be\n"
"///    copied to this memory location. The address of this memory location does\n"
"///    not have to be aligned.\n"
"/// \\param __addr_lo\n"
"///    A pointer to a 128-bit memory location. Bits[127:0] of \\a __a are to be\n"
"///    copied to this memory location. The address of this memory location does\n"
"///    not have to be aligned.\n"
"/// \\param __a\n"
"///    A 256-bit integer vector.\n"
"static __inline void __DEFAULT_FN_ATTRS\n"
"_mm256_storeu2_m128i(__m128i *__addr_hi, __m128i *__addr_lo, __m256i __a)\n"
"{\n"
"  __m128i __v128;\n"
"\n"
"  __v128 = _mm256_castsi256_si128(__a);\n"
"  _mm_storeu_si128(__addr_lo, __v128);\n"
"  __v128 = _mm256_extractf128_si256(__a, 1);\n"
"  _mm_storeu_si128(__addr_hi, __v128);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float] by\n"
"///    concatenating two 128-bit floating-point vectors of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __hi\n"
"///    A 128-bit floating-point vector of [4 x float] to be copied to the upper\n"
"///    128 bits of the result.\n"
"/// \\param __lo\n"
"///    A 128-bit floating-point vector of [4 x float] to be copied to the lower\n"
"///    128 bits of the result.\n"
"/// \\returns A 256-bit floating-point vector of [8 x float] containing the\n"
"///    concatenated result.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_set_m128 (__m128 __hi, __m128 __lo)\n"
"{\n"
"  return (__m256) __builtin_shufflevector((__v4sf)__lo, (__v4sf)__hi, 0, 1, 2, 3, 4, 5, 6, 7);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [4 x double] by\n"
"///    concatenating two 128-bit floating-point vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __hi\n"
"///    A 128-bit floating-point vector of [2 x double] to be copied to the upper\n"
"///    128 bits of the result.\n"
"/// \\param __lo\n"
"///    A 128-bit floating-point vector of [2 x double] to be copied to the lower\n"
"///    128 bits of the result.\n"
"/// \\returns A 256-bit floating-point vector of [4 x double] containing the\n"
"///    concatenated result.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_set_m128d (__m128d __hi, __m128d __lo)\n"
"{\n"
"  return (__m256d) __builtin_shufflevector((__v2df)__lo, (__v2df)__hi, 0, 1, 2, 3);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector by concatenating two 128-bit\n"
"///    integer vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __hi\n"
"///    A 128-bit integer vector to be copied to the upper 128 bits of the\n"
"///    result.\n"
"/// \\param __lo\n"
"///    A 128-bit integer vector to be copied to the lower 128 bits of the\n"
"///    result.\n"
"/// \\returns A 256-bit integer vector containing the concatenated result.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_set_m128i (__m128i __hi, __m128i __lo)\n"
"{\n"
"  return (__m256i) __builtin_shufflevector((__v2di)__lo, (__v2di)__hi, 0, 1, 2, 3);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [8 x float] by\n"
"///    concatenating two 128-bit floating-point vectors of [4 x float]. This is\n"
"///    similar to _mm256_set_m128, but the order of the input parameters is\n"
"///    swapped.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __lo\n"
"///    A 128-bit floating-point vector of [4 x float] to be copied to the lower\n"
"///    128 bits of the result.\n"
"/// \\param __hi\n"
"///    A 128-bit floating-point vector of [4 x float] to be copied to the upper\n"
"///    128 bits of the result.\n"
"/// \\returns A 256-bit floating-point vector of [8 x float] containing the\n"
"///    concatenated result.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS\n"
"_mm256_setr_m128 (__m128 __lo, __m128 __hi)\n"
"{\n"
"  return _mm256_set_m128(__hi, __lo);\n"
"}\n"
"\n"
"/// Constructs a 256-bit floating-point vector of [4 x double] by\n"
"///    concatenating two 128-bit floating-point vectors of [2 x double]. This is\n"
"///    similar to _mm256_set_m128d, but the order of the input parameters is\n"
"///    swapped.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __lo\n"
"///    A 128-bit floating-point vector of [2 x double] to be copied to the lower\n"
"///    128 bits of the result.\n"
"/// \\param __hi\n"
"///    A 128-bit floating-point vector of [2 x double] to be copied to the upper\n"
"///    128 bits of the result.\n"
"/// \\returns A 256-bit floating-point vector of [4 x double] containing the\n"
"///    concatenated result.\n"
"static __inline __m256d __DEFAULT_FN_ATTRS\n"
"_mm256_setr_m128d (__m128d __lo, __m128d __hi)\n"
"{\n"
"  return (__m256d)_mm256_set_m128d(__hi, __lo);\n"
"}\n"
"\n"
"/// Constructs a 256-bit integer vector by concatenating two 128-bit\n"
"///    integer vectors. This is similar to _mm256_set_m128i, but the order of\n"
"///    the input parameters is swapped.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.\n"
"///\n"
"/// \\param __lo\n"
"///    A 128-bit integer vector to be copied to the lower 128 bits of the\n"
"///    result.\n"
"/// \\param __hi\n"
"///    A 128-bit integer vector to be copied to the upper 128 bits of the\n"
"///    result.\n"
"/// \\returns A 256-bit integer vector containing the concatenated result.\n"
"static __inline __m256i __DEFAULT_FN_ATTRS\n"
"_mm256_setr_m128i (__m128i __lo, __m128i __hi)\n"
"{\n"
"  return (__m256i)_mm256_set_m128i(__hi, __lo);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __DEFAULT_FN_ATTRS128\n"
"\n"
"#endif /* __AVXINTRIN_H */\n"
"" } , 
 { "/builtins/bmi2intrin.h" , "/*===---- bmi2intrin.h - BMI2 intrinsics -----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <bmi2intrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __BMI2INTRIN_H\n"
"#define __BMI2INTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"bmi2\")))\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_bzhi_u32(unsigned int __X, unsigned int __Y)\n"
"{\n"
"  return __builtin_ia32_bzhi_si(__X, __Y);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_pdep_u32(unsigned int __X, unsigned int __Y)\n"
"{\n"
"  return __builtin_ia32_pdep_si(__X, __Y);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_pext_u32(unsigned int __X, unsigned int __Y)\n"
"{\n"
"  return __builtin_ia32_pext_si(__X, __Y);\n"
"}\n"
"\n"
"#ifdef  __x86_64__\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"_bzhi_u64(unsigned long long __X, unsigned long long __Y)\n"
"{\n"
"  return __builtin_ia32_bzhi_di(__X, __Y);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"_pdep_u64(unsigned long long __X, unsigned long long __Y)\n"
"{\n"
"  return __builtin_ia32_pdep_di(__X, __Y);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"_pext_u64(unsigned long long __X, unsigned long long __Y)\n"
"{\n"
"  return __builtin_ia32_pext_di(__X, __Y);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"_mulx_u64 (unsigned long long __X, unsigned long long __Y,\n"
"	   unsigned long long *__P)\n"
"{\n"
"  unsigned __int128 __res = (unsigned __int128) __X * __Y;\n"
"  *__P = (unsigned long long) (__res >> 64);\n"
"  return (unsigned long long) __res;\n"
"}\n"
"\n"
"#else /* !__x86_64__ */\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_mulx_u32 (unsigned int __X, unsigned int __Y, unsigned int *__P)\n"
"{\n"
"  unsigned long long __res = (unsigned long long) __X * __Y;\n"
"  *__P = (unsigned int) (__res >> 32);\n"
"  return (unsigned int) __res;\n"
"}\n"
"\n"
"#endif /* !__x86_64__  */\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __BMI2INTRIN_H */\n"
"" } , 
 { "/builtins/bmiintrin.h" , "/*===---- bmiintrin.h - BMI intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <bmiintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __BMIINTRIN_H\n"
"#define __BMIINTRIN_H\n"
"\n"
"#define _tzcnt_u16(a)     (__tzcnt_u16((a)))\n"
"\n"
"#define _andn_u32(a, b)   (__andn_u32((a), (b)))\n"
"\n"
"/* _bextr_u32 != __bextr_u32 */\n"
"#define _blsi_u32(a)      (__blsi_u32((a)))\n"
"\n"
"#define _blsmsk_u32(a)    (__blsmsk_u32((a)))\n"
"\n"
"#define _blsr_u32(a)      (__blsr_u32((a)))\n"
"\n"
"#define _tzcnt_u32(a)     (__tzcnt_u32((a)))\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"bmi\")))\n"
"\n"
"/* Allow using the tzcnt intrinsics even for non-BMI targets. Since the TZCNT\n"
"   instruction behaves as BSF on non-BMI targets, there is code that expects\n"
"   to use it as a potentially faster version of BSF. */\n"
"#define __RELAXED_FN_ATTRS __attribute__((__always_inline__, __nodebug__))\n"
"\n"
"/// Counts the number of trailing zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> TZCNT </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 16-bit integer whose trailing zeros are to be counted.\n"
"/// \\returns An unsigned 16-bit integer containing the number of trailing zero\n"
"///    bits in the operand.\n"
"static __inline__ unsigned short __RELAXED_FN_ATTRS\n"
"__tzcnt_u16(unsigned short __X)\n"
"{\n"
"  return __builtin_ia32_tzcnt_u16(__X);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of the second operand with the one's\n"
"///    complement of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> ANDN </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned integer containing one of the operands.\n"
"/// \\param __Y\n"
"///    An unsigned integer containing one of the operands.\n"
"/// \\returns An unsigned integer containing the bitwise AND of the second\n"
"///    operand with the one's complement of the first operand.\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__andn_u32(unsigned int __X, unsigned int __Y)\n"
"{\n"
"  return ~__X & __Y;\n"
"}\n"
"\n"
"/* AMD-specified, double-leading-underscore version of BEXTR */\n"
"/// Extracts the specified bits from the first operand and returns them\n"
"///    in the least significant bits of the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BEXTR </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned integer whose bits are to be extracted.\n"
"/// \\param __Y\n"
"///    An unsigned integer used to specify which bits are extracted. Bits [7:0]\n"
"///    specify the index of the least significant bit. Bits [15:8] specify the\n"
"///    number of bits to be extracted.\n"
"/// \\returns An unsigned integer whose least significant bits contain the\n"
"///    extracted bits.\n"
"/// \\see _bextr_u32\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__bextr_u32(unsigned int __X, unsigned int __Y)\n"
"{\n"
"  return __builtin_ia32_bextr_u32(__X, __Y);\n"
"}\n"
"\n"
"/* Intel-specified, single-leading-underscore version of BEXTR */\n"
"/// Extracts the specified bits from the first operand and returns them\n"
"///    in the least significant bits of the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BEXTR </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned integer whose bits are to be extracted.\n"
"/// \\param __Y\n"
"///    An unsigned integer used to specify the index of the least significant\n"
"///    bit for the bits to be extracted. Bits [7:0] specify the index.\n"
"/// \\param __Z\n"
"///    An unsigned integer used to specify the number of bits to be extracted.\n"
"///    Bits [7:0] specify the number of bits.\n"
"/// \\returns An unsigned integer whose least significant bits contain the\n"
"///    extracted bits.\n"
"/// \\see __bextr_u32\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_bextr_u32(unsigned int __X, unsigned int __Y, unsigned int __Z)\n"
"{\n"
"  return __builtin_ia32_bextr_u32 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));\n"
"}\n"
"\n"
"/// Clears all bits in the source except for the least significant bit\n"
"///    containing a value of 1 and returns the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BLSI </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned integer whose bits are to be cleared.\n"
"/// \\returns An unsigned integer containing the result of clearing the bits from\n"
"///    the source operand.\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blsi_u32(unsigned int __X)\n"
"{\n"
"  return __X & -__X;\n"
"}\n"
"\n"
"/// Creates a mask whose bits are set to 1, using bit 0 up to and\n"
"///    including the least significant bit that is set to 1 in the source\n"
"///    operand and returns the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BLSMSK </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned integer used to create the mask.\n"
"/// \\returns An unsigned integer containing the newly created mask.\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blsmsk_u32(unsigned int __X)\n"
"{\n"
"  return __X ^ (__X - 1);\n"
"}\n"
"\n"
"/// Clears the least significant bit that is set to 1 in the source\n"
"///    operand and returns the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BLSR </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned integer containing the operand to be cleared.\n"
"/// \\returns An unsigned integer containing the result of clearing the source\n"
"///    operand.\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blsr_u32(unsigned int __X)\n"
"{\n"
"  return __X & (__X - 1);\n"
"}\n"
"\n"
"/// Counts the number of trailing zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> TZCNT </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 32-bit integer whose trailing zeros are to be counted.\n"
"/// \\returns An unsigned 32-bit integer containing the number of trailing zero\n"
"///    bits in the operand.\n"
"static __inline__ unsigned int __RELAXED_FN_ATTRS\n"
"__tzcnt_u32(unsigned int __X)\n"
"{\n"
"  return __builtin_ia32_tzcnt_u32(__X);\n"
"}\n"
"\n"
"/// Counts the number of trailing zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> TZCNT </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 32-bit integer whose trailing zeros are to be counted.\n"
"/// \\returns An 32-bit integer containing the number of trailing zero bits in\n"
"///    the operand.\n"
"static __inline__ int __RELAXED_FN_ATTRS\n"
"_mm_tzcnt_32(unsigned int __X)\n"
"{\n"
"  return __builtin_ia32_tzcnt_u32(__X);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"\n"
"#define _andn_u64(a, b)   (__andn_u64((a), (b)))\n"
"\n"
"/* _bextr_u64 != __bextr_u64 */\n"
"#define _blsi_u64(a)      (__blsi_u64((a)))\n"
"\n"
"#define _blsmsk_u64(a)    (__blsmsk_u64((a)))\n"
"\n"
"#define _blsr_u64(a)      (__blsr_u64((a)))\n"
"\n"
"#define _tzcnt_u64(a)     (__tzcnt_u64((a)))\n"
"\n"
"/// Performs a bitwise AND of the second operand with the one's\n"
"///    complement of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> ANDN </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer containing one of the operands.\n"
"/// \\param __Y\n"
"///    An unsigned 64-bit integer containing one of the operands.\n"
"/// \\returns An unsigned 64-bit integer containing the bitwise AND of the second\n"
"///    operand with the one's complement of the first operand.\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__andn_u64 (unsigned long long __X, unsigned long long __Y)\n"
"{\n"
"  return ~__X & __Y;\n"
"}\n"
"\n"
"/* AMD-specified, double-leading-underscore version of BEXTR */\n"
"/// Extracts the specified bits from the first operand and returns them\n"
"///    in the least significant bits of the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BEXTR </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer whose bits are to be extracted.\n"
"/// \\param __Y\n"
"///    An unsigned 64-bit integer used to specify which bits are extracted. Bits\n"
"///    [7:0] specify the index of the least significant bit. Bits [15:8] specify\n"
"///    the number of bits to be extracted.\n"
"/// \\returns An unsigned 64-bit integer whose least significant bits contain the\n"
"///    extracted bits.\n"
"/// \\see _bextr_u64\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__bextr_u64(unsigned long long __X, unsigned long long __Y)\n"
"{\n"
"  return __builtin_ia32_bextr_u64(__X, __Y);\n"
"}\n"
"\n"
"/* Intel-specified, single-leading-underscore version of BEXTR */\n"
"/// Extracts the specified bits from the first operand and returns them\n"
"///     in the least significant bits of the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BEXTR </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer whose bits are to be extracted.\n"
"/// \\param __Y\n"
"///    An unsigned integer used to specify the index of the least significant\n"
"///    bit for the bits to be extracted. Bits [7:0] specify the index.\n"
"/// \\param __Z\n"
"///    An unsigned integer used to specify the number of bits to be extracted.\n"
"///    Bits [7:0] specify the number of bits.\n"
"/// \\returns An unsigned 64-bit integer whose least significant bits contain the\n"
"///    extracted bits.\n"
"/// \\see __bextr_u64\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"_bextr_u64(unsigned long long __X, unsigned int __Y, unsigned int __Z)\n"
"{\n"
"  return __builtin_ia32_bextr_u64 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));\n"
"}\n"
"\n"
"/// Clears all bits in the source except for the least significant bit\n"
"///    containing a value of 1 and returns the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BLSI </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer whose bits are to be cleared.\n"
"/// \\returns An unsigned 64-bit integer containing the result of clearing the\n"
"///    bits from the source operand.\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blsi_u64(unsigned long long __X)\n"
"{\n"
"  return __X & -__X;\n"
"}\n"
"\n"
"/// Creates a mask whose bits are set to 1, using bit 0 up to and\n"
"///    including the least significant bit that is set to 1 in the source\n"
"///    operand and returns the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BLSMSK </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer used to create the mask.\n"
"/// \\returns An unsigned 64-bit integer containing the newly created mask.\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blsmsk_u64(unsigned long long __X)\n"
"{\n"
"  return __X ^ (__X - 1);\n"
"}\n"
"\n"
"/// Clears the least significant bit that is set to 1 in the source\n"
"///    operand and returns the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> BLSR </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer containing the operand to be cleared.\n"
"/// \\returns An unsigned 64-bit integer containing the result of clearing the\n"
"///    source operand.\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blsr_u64(unsigned long long __X)\n"
"{\n"
"  return __X & (__X - 1);\n"
"}\n"
"\n"
"/// Counts the number of trailing zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> TZCNT </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer whose trailing zeros are to be counted.\n"
"/// \\returns An unsigned 64-bit integer containing the number of trailing zero\n"
"///    bits in the operand.\n"
"static __inline__ unsigned long long __RELAXED_FN_ATTRS\n"
"__tzcnt_u64(unsigned long long __X)\n"
"{\n"
"  return __builtin_ia32_tzcnt_u64(__X);\n"
"}\n"
"\n"
"/// Counts the number of trailing zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> TZCNT </c> instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer whose trailing zeros are to be counted.\n"
"/// \\returns An 64-bit integer containing the number of trailing zero bits in\n"
"///    the operand.\n"
"static __inline__ long long __RELAXED_FN_ATTRS\n"
"_mm_tzcnt_64(unsigned long long __X)\n"
"{\n"
"  return __builtin_ia32_tzcnt_u64(__X);\n"
"}\n"
"\n"
"#endif /* __x86_64__ */\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __RELAXED_FN_ATTRS\n"
"\n"
"#endif /* __BMIINTRIN_H */\n"
"" } , 
 { "/builtins/cetintrin.h" , "/*===---- cetintrin.h - CET intrinsic --------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <cetintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __CETINTRIN_H\n"
"#define __CETINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS                                                     \\\n"
"  __attribute__((__always_inline__, __nodebug__, __target__(\"shstk\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS _incsspd(int __a) {\n"
"  __builtin_ia32_incsspd(__a);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS _incsspq(unsigned long long __a) {\n"
"  __builtin_ia32_incsspq(__a);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS _inc_ssp(unsigned int __a) {\n"
"  __builtin_ia32_incsspq(__a);\n"
"}\n"
"#else /* __x86_64__ */\n"
"static __inline__ void __DEFAULT_FN_ATTRS _inc_ssp(unsigned int __a) {\n"
"  __builtin_ia32_incsspd((int)__a);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS _rdsspd(unsigned int __a) {\n"
"  return __builtin_ia32_rdsspd(__a);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS _rdsspq(unsigned long long __a) {\n"
"  return __builtin_ia32_rdsspq(__a);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS _get_ssp(void) {\n"
"  return __builtin_ia32_rdsspq(0);\n"
"}\n"
"#else /* __x86_64__ */\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS _get_ssp(void) {\n"
"  return __builtin_ia32_rdsspd(0);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS _saveprevssp() {\n"
"  __builtin_ia32_saveprevssp();\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS _rstorssp(void * __p) {\n"
"  __builtin_ia32_rstorssp(__p);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS _wrssd(unsigned int __a, void * __p) {\n"
"  __builtin_ia32_wrssd(__a, __p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS _wrssq(unsigned long long __a, void * __p) {\n"
"  __builtin_ia32_wrssq(__a, __p);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS _wrussd(unsigned int __a, void * __p) {\n"
"  __builtin_ia32_wrussd(__a, __p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS _wrussq(unsigned long long __a, void * __p) {\n"
"  __builtin_ia32_wrussq(__a, __p);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS _setssbsy() {\n"
"  __builtin_ia32_setssbsy();\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS _clrssbsy(void * __p) {\n"
"  __builtin_ia32_clrssbsy(__p);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __CETINTRIN_H */\n"
"" } , 
 { "/builtins/cldemoteintrin.h" , "/*===---- cldemoteintrin.h - CLDEMOTE intrinsic ----------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <cldemoteintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __CLDEMOTEINTRIN_H\n"
"#define __CLDEMOTEINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS \\\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"cldemote\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_cldemote(const void * __P) {\n"
"  __builtin_ia32_cldemote(__P);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/clflushoptintrin.h" , "/*===---- clflushoptintrin.h - CLFLUSHOPT intrinsic ------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <clflushoptintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __CLFLUSHOPTINTRIN_H\n"
"#define __CLFLUSHOPTINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"clflushopt\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_clflushopt(void const * __m) {\n"
"  __builtin_ia32_clflushopt(__m);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/clwbintrin.h" , "/*===---- clwbintrin.h - CLWB intrinsic ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <clwbintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __CLWBINTRIN_H\n"
"#define __CLWBINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"clwb\")))\n"
"\n"
"/// Writes back to memory the cache line (if modified) that contains the\n"
"/// linear address specified in \\a __p from any level of the cache hierarchy in\n"
"/// the cache coherence domain\n"
"///\n"
"/// \\headerfile <immintrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CLWB </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to the memory location used to identify the cache line to be\n"
"///    written back.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_clwb(void const *__p) {\n"
"  __builtin_ia32_clwb(__p);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/clzerointrin.h" , "/*===----------------------- clzerointrin.h - CLZERO ----------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <clzerointrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __CLZEROINTRIN_H\n"
"#define __CLZEROINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS \\\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"clzero\")))\n"
"\n"
"/// Loads the cache line address and zero's out the cacheline\n"
"///\n"
"/// \\headerfile <clzerointrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CLZERO </c> instruction.\n"
"///\n"
"/// \\param __line\n"
"///    A pointer to a cacheline which needs to be zeroed out.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_clzero (void * __line)\n"
"{\n"
"  __builtin_ia32_clzero ((void *)__line);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __CLZEROINTRIN_H */\n"
"" } , 
 { "/builtins/cpuid.h" , "/*===---- cpuid.h - X86 cpu model detection --------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !(__x86_64__ || __i386__)\n"
"#error this header is for x86 only\n"
"#endif\n"
"\n"
"/* Responses identification request with %eax 0 */\n"
"/* AMD:     \"AuthenticAMD\" */\n"
"#define signature_AMD_ebx 0x68747541\n"
"#define signature_AMD_edx 0x69746e65\n"
"#define signature_AMD_ecx 0x444d4163\n"
"/* CENTAUR: \"CentaurHauls\" */\n"
"#define signature_CENTAUR_ebx 0x746e6543\n"
"#define signature_CENTAUR_edx 0x48727561\n"
"#define signature_CENTAUR_ecx 0x736c7561\n"
"/* CYRIX:   \"CyrixInstead\" */\n"
"#define signature_CYRIX_ebx 0x69727943\n"
"#define signature_CYRIX_edx 0x736e4978\n"
"#define signature_CYRIX_ecx 0x64616574\n"
"/* INTEL:   \"GenuineIntel\" */\n"
"#define signature_INTEL_ebx 0x756e6547\n"
"#define signature_INTEL_edx 0x49656e69\n"
"#define signature_INTEL_ecx 0x6c65746e\n"
"/* TM1:     \"TransmetaCPU\" */\n"
"#define signature_TM1_ebx 0x6e617254\n"
"#define signature_TM1_edx 0x74656d73\n"
"#define signature_TM1_ecx 0x55504361\n"
"/* TM2:     \"GenuineTMx86\" */\n"
"#define signature_TM2_ebx 0x756e6547\n"
"#define signature_TM2_edx 0x54656e69\n"
"#define signature_TM2_ecx 0x3638784d\n"
"/* NSC:     \"Geode by NSC\" */\n"
"#define signature_NSC_ebx 0x646f6547\n"
"#define signature_NSC_edx 0x43534e20\n"
"#define signature_NSC_ecx 0x79622065\n"
"/* NEXGEN:  \"NexGenDriven\" */\n"
"#define signature_NEXGEN_ebx 0x4778654e\n"
"#define signature_NEXGEN_edx 0x72446e65\n"
"#define signature_NEXGEN_ecx 0x6e657669\n"
"/* RISE:    \"RiseRiseRise\" */\n"
"#define signature_RISE_ebx 0x65736952\n"
"#define signature_RISE_edx 0x65736952\n"
"#define signature_RISE_ecx 0x65736952\n"
"/* SIS:     \"SiS SiS SiS \" */\n"
"#define signature_SIS_ebx 0x20536953\n"
"#define signature_SIS_edx 0x20536953\n"
"#define signature_SIS_ecx 0x20536953\n"
"/* UMC:     \"UMC UMC UMC \" */\n"
"#define signature_UMC_ebx 0x20434d55\n"
"#define signature_UMC_edx 0x20434d55\n"
"#define signature_UMC_ecx 0x20434d55\n"
"/* VIA:     \"VIA VIA VIA \" */\n"
"#define signature_VIA_ebx 0x20414956\n"
"#define signature_VIA_edx 0x20414956\n"
"#define signature_VIA_ecx 0x20414956\n"
"/* VORTEX:  \"Vortex86 SoC\" */\n"
"#define signature_VORTEX_ebx 0x74726f56\n"
"#define signature_VORTEX_edx 0x36387865\n"
"#define signature_VORTEX_ecx 0x436f5320\n"
"\n"
"/* Features in %ecx for leaf 1 */\n"
"#define bit_SSE3        0x00000001\n"
"#define bit_PCLMULQDQ   0x00000002\n"
"#define bit_PCLMUL      bit_PCLMULQDQ   /* for gcc compat */\n"
"#define bit_DTES64      0x00000004\n"
"#define bit_MONITOR     0x00000008\n"
"#define bit_DSCPL       0x00000010\n"
"#define bit_VMX         0x00000020\n"
"#define bit_SMX         0x00000040\n"
"#define bit_EIST        0x00000080\n"
"#define bit_TM2         0x00000100\n"
"#define bit_SSSE3       0x00000200\n"
"#define bit_CNXTID      0x00000400\n"
"#define bit_FMA         0x00001000\n"
"#define bit_CMPXCHG16B  0x00002000\n"
"#define bit_xTPR        0x00004000\n"
"#define bit_PDCM        0x00008000\n"
"#define bit_PCID        0x00020000\n"
"#define bit_DCA         0x00040000\n"
"#define bit_SSE41       0x00080000\n"
"#define bit_SSE4_1      bit_SSE41       /* for gcc compat */\n"
"#define bit_SSE42       0x00100000\n"
"#define bit_SSE4_2      bit_SSE42       /* for gcc compat */\n"
"#define bit_x2APIC      0x00200000\n"
"#define bit_MOVBE       0x00400000\n"
"#define bit_POPCNT      0x00800000\n"
"#define bit_TSCDeadline 0x01000000\n"
"#define bit_AESNI       0x02000000\n"
"#define bit_AES         bit_AESNI       /* for gcc compat */\n"
"#define bit_XSAVE       0x04000000\n"
"#define bit_OSXSAVE     0x08000000\n"
"#define bit_AVX         0x10000000\n"
"#define bit_F16C        0x20000000\n"
"#define bit_RDRND       0x40000000\n"
"\n"
"/* Features in %edx for leaf 1 */\n"
"#define bit_FPU         0x00000001\n"
"#define bit_VME         0x00000002\n"
"#define bit_DE          0x00000004\n"
"#define bit_PSE         0x00000008\n"
"#define bit_TSC         0x00000010\n"
"#define bit_MSR         0x00000020\n"
"#define bit_PAE         0x00000040\n"
"#define bit_MCE         0x00000080\n"
"#define bit_CX8         0x00000100\n"
"#define bit_CMPXCHG8B   bit_CX8         /* for gcc compat */\n"
"#define bit_APIC        0x00000200\n"
"#define bit_SEP         0x00000800\n"
"#define bit_MTRR        0x00001000\n"
"#define bit_PGE         0x00002000\n"
"#define bit_MCA         0x00004000\n"
"#define bit_CMOV        0x00008000\n"
"#define bit_PAT         0x00010000\n"
"#define bit_PSE36       0x00020000\n"
"#define bit_PSN         0x00040000\n"
"#define bit_CLFSH       0x00080000\n"
"#define bit_DS          0x00200000\n"
"#define bit_ACPI        0x00400000\n"
"#define bit_MMX         0x00800000\n"
"#define bit_FXSR        0x01000000\n"
"#define bit_FXSAVE      bit_FXSR        /* for gcc compat */\n"
"#define bit_SSE         0x02000000\n"
"#define bit_SSE2        0x04000000\n"
"#define bit_SS          0x08000000\n"
"#define bit_HTT         0x10000000\n"
"#define bit_TM          0x20000000\n"
"#define bit_PBE         0x80000000\n"
"\n"
"/* Features in %ebx for leaf 7 sub-leaf 0 */\n"
"#define bit_FSGSBASE    0x00000001\n"
"#define bit_SGX         0x00000004\n"
"#define bit_BMI         0x00000008\n"
"#define bit_HLE         0x00000010\n"
"#define bit_AVX2        0x00000020\n"
"#define bit_SMEP        0x00000080\n"
"#define bit_BMI2        0x00000100\n"
"#define bit_ENH_MOVSB   0x00000200\n"
"#define bit_INVPCID     0x00000400\n"
"#define bit_RTM         0x00000800\n"
"#define bit_MPX         0x00004000\n"
"#define bit_AVX512F     0x00010000\n"
"#define bit_AVX512DQ    0x00020000\n"
"#define bit_RDSEED      0x00040000\n"
"#define bit_ADX         0x00080000\n"
"#define bit_AVX512IFMA  0x00200000\n"
"#define bit_CLFLUSHOPT  0x00800000\n"
"#define bit_CLWB        0x01000000\n"
"#define bit_AVX512PF    0x04000000\n"
"#define bit_AVX512ER    0x08000000\n"
"#define bit_AVX512CD    0x10000000\n"
"#define bit_SHA         0x20000000\n"
"#define bit_AVX512BW    0x40000000\n"
"#define bit_AVX512VL    0x80000000\n"
"\n"
"/* Features in %ecx for leaf 7 sub-leaf 0 */\n"
"#define bit_PREFTCHWT1       0x00000001\n"
"#define bit_AVX512VBMI       0x00000002\n"
"#define bit_PKU              0x00000004\n"
"#define bit_OSPKE            0x00000010\n"
"#define bit_WAITPKG          0x00000020\n"
"#define bit_AVX512VBMI2      0x00000040\n"
"#define bit_SHSTK            0x00000080\n"
"#define bit_GFNI             0x00000100\n"
"#define bit_VAES             0x00000200\n"
"#define bit_VPCLMULQDQ       0x00000400\n"
"#define bit_AVX512VNNI       0x00000800\n"
"#define bit_AVX512BITALG     0x00001000\n"
"#define bit_AVX512VPOPCNTDQ  0x00004000\n"
"#define bit_RDPID            0x00400000\n"
"#define bit_CLDEMOTE         0x02000000\n"
"#define bit_MOVDIRI          0x08000000\n"
"#define bit_MOVDIR64B        0x10000000\n"
"\n"
"/* Features in %edx for leaf 7 sub-leaf 0 */\n"
"#define bit_AVX5124VNNIW  0x00000004\n"
"#define bit_AVX5124FMAPS  0x00000008\n"
"#define bit_PCONFIG       0x00040000\n"
"#define bit_IBT           0x00100000\n"
"\n"
"/* Features in %eax for leaf 13 sub-leaf 1 */\n"
"#define bit_XSAVEOPT    0x00000001\n"
"#define bit_XSAVEC      0x00000002\n"
"#define bit_XSAVES      0x00000008\n"
"\n"
"/* Features in %eax for leaf 0x14 sub-leaf 0 */\n"
"#define bit_PTWRITE     0x00000010\n"
"\n"
"/* Features in %ecx for leaf 0x80000001 */\n"
"#define bit_LAHF_LM     0x00000001\n"
"#define bit_ABM         0x00000020\n"
"#define bit_LZCNT       bit_ABM        /* for gcc compat */\n"
"#define bit_SSE4a       0x00000040\n"
"#define bit_PRFCHW      0x00000100\n"
"#define bit_XOP         0x00000800\n"
"#define bit_LWP         0x00008000\n"
"#define bit_FMA4        0x00010000\n"
"#define bit_TBM         0x00200000\n"
"#define bit_MWAITX      0x20000000\n"
"\n"
"/* Features in %edx for leaf 0x80000001 */\n"
"#define bit_MMXEXT      0x00400000\n"
"#define bit_LM          0x20000000\n"
"#define bit_3DNOWP      0x40000000\n"
"#define bit_3DNOW       0x80000000\n"
"\n"
"/* Features in %ebx for leaf 0x80000008 */\n"
"#define bit_CLZERO      0x00000001\n"
"#define bit_WBNOINVD    0x00000200\n"
"\n"
"\n"
"#if __i386__\n"
"#define __cpuid(__leaf, __eax, __ebx, __ecx, __edx) \\\n"
"    __asm(\"cpuid\" : \"=a\"(__eax), \"=b\" (__ebx), \"=c\"(__ecx), \"=d\"(__edx) \\\n"
"                  : \"0\"(__leaf))\n"
"\n"
"#define __cpuid_count(__leaf, __count, __eax, __ebx, __ecx, __edx) \\\n"
"    __asm(\"cpuid\" : \"=a\"(__eax), \"=b\" (__ebx), \"=c\"(__ecx), \"=d\"(__edx) \\\n"
"                  : \"0\"(__leaf), \"2\"(__count))\n"
"#else\n"
"/* x86-64 uses %rbx as the base register, so preserve it. */\n"
"#define __cpuid(__leaf, __eax, __ebx, __ecx, __edx) \\\n"
"    __asm(\"  xchgq  %%rbx,%q1\\n\" \\\n"
"          \"  cpuid\\n\" \\\n"
"          \"  xchgq  %%rbx,%q1\" \\\n"
"        : \"=a\"(__eax), \"=r\" (__ebx), \"=c\"(__ecx), \"=d\"(__edx) \\\n"
"        : \"0\"(__leaf))\n"
"\n"
"#define __cpuid_count(__leaf, __count, __eax, __ebx, __ecx, __edx) \\\n"
"    __asm(\"  xchgq  %%rbx,%q1\\n\" \\\n"
"          \"  cpuid\\n\" \\\n"
"          \"  xchgq  %%rbx,%q1\" \\\n"
"        : \"=a\"(__eax), \"=r\" (__ebx), \"=c\"(__ecx), \"=d\"(__edx) \\\n"
"        : \"0\"(__leaf), \"2\"(__count))\n"
"#endif\n"
"\n"
"static __inline int __get_cpuid_max (unsigned int __leaf, unsigned int *__sig)\n"
"{\n"
"    unsigned int __eax, __ebx, __ecx, __edx;\n"
"#if __i386__\n"
"    int __cpuid_supported;\n"
"\n"
"    __asm(\"  pushfl\\n\"\n"
"          \"  popl   %%eax\\n\"\n"
"          \"  movl   %%eax,%%ecx\\n\"\n"
"          \"  xorl   $0x00200000,%%eax\\n\"\n"
"          \"  pushl  %%eax\\n\"\n"
"          \"  popfl\\n\"\n"
"          \"  pushfl\\n\"\n"
"          \"  popl   %%eax\\n\"\n"
"          \"  movl   $0,%0\\n\"\n"
"          \"  cmpl   %%eax,%%ecx\\n\"\n"
"          \"  je     1f\\n\"\n"
"          \"  movl   $1,%0\\n\"\n"
"          \"1:\"\n"
"        : \"=r\" (__cpuid_supported) : : \"eax\", \"ecx\");\n"
"    if (!__cpuid_supported)\n"
"        return 0;\n"
"#endif\n"
"\n"
"    __cpuid(__leaf, __eax, __ebx, __ecx, __edx);\n"
"    if (__sig)\n"
"        *__sig = __ebx;\n"
"    return __eax;\n"
"}\n"
"\n"
"static __inline int __get_cpuid (unsigned int __leaf, unsigned int *__eax,\n"
"                                 unsigned int *__ebx, unsigned int *__ecx,\n"
"                                 unsigned int *__edx)\n"
"{\n"
"    unsigned int __max_leaf = __get_cpuid_max(__leaf & 0x80000000, 0);\n"
"\n"
"    if (__max_leaf == 0 || __max_leaf < __leaf)\n"
"        return 0;\n"
"\n"
"    __cpuid(__leaf, *__eax, *__ebx, *__ecx, *__edx);\n"
"    return 1;\n"
"}\n"
"\n"
"static __inline int __get_cpuid_count (unsigned int __leaf,\n"
"                                       unsigned int __subleaf,\n"
"                                       unsigned int *__eax, unsigned int *__ebx,\n"
"                                       unsigned int *__ecx, unsigned int *__edx)\n"
"{\n"
"    unsigned int __max_leaf = __get_cpuid_max(__leaf & 0x80000000, 0);\n"
"\n"
"    if (__max_leaf == 0 || __max_leaf < __leaf)\n"
"        return 0;\n"
"\n"
"    __cpuid_count(__leaf, __subleaf, *__eax, *__ebx, *__ecx, *__edx);\n"
"    return 1;\n"
"}\n"
"" } , 
 { "/builtins/emmintrin.h" , "/*===---- emmintrin.h - SSE2 intrinsics ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __EMMINTRIN_H\n"
"#define __EMMINTRIN_H\n"
"\n"
"#include <xmmintrin.h>\n"
"\n"
"typedef double __m128d __attribute__((__vector_size__(16)));\n"
"typedef long long __m128i __attribute__((__vector_size__(16)));\n"
"\n"
"/* Type defines.  */\n"
"typedef double __v2df __attribute__ ((__vector_size__ (16)));\n"
"typedef long long __v2di __attribute__ ((__vector_size__ (16)));\n"
"typedef short __v8hi __attribute__((__vector_size__(16)));\n"
"typedef char __v16qi __attribute__((__vector_size__(16)));\n"
"\n"
"/* Unsigned types */\n"
"typedef unsigned long long __v2du __attribute__ ((__vector_size__ (16)));\n"
"typedef unsigned short __v8hu __attribute__((__vector_size__(16)));\n"
"typedef unsigned char __v16qu __attribute__((__vector_size__(16)));\n"
"\n"
"/* We need an explicitly signed variant for char. Note that this shouldn't\n"
" * appear in the interface though. */\n"
"typedef signed char __v16qs __attribute__((__vector_size__(16)));\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"sse2\"), __min_vector_width__(128)))\n"
"#define __DEFAULT_FN_ATTRS_MMX __attribute__((__always_inline__, __nodebug__, __target__(\"mmx,sse2\"), __min_vector_width__(64)))\n"
"\n"
"/// Adds lower double-precision values in both operands and returns the\n"
"///    sum in the lower 64 bits of the result. The upper 64 bits of the result\n"
"///    are copied from the upper double-precision value of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDSD / ADDSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    sum of the lower 64 bits of both operands. The upper 64 bits are copied\n"
"///    from the upper 64 bits of the first source operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_add_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __a[0] += __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Adds two 128-bit vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDPD / ADDPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the sums of both\n"
"///    operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_add_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)((__v2df)__a + (__v2df)__b);\n"
"}\n"
"\n"
"/// Subtracts the lower double-precision value of the second operand\n"
"///    from the lower double-precision value of the first operand and returns\n"
"///    the difference in the lower 64 bits of the result. The upper 64 bits of\n"
"///    the result are copied from the upper double-precision value of the first\n"
"///    operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSUBSD / SUBSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the minuend.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing the subtrahend.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    difference of the lower 64 bits of both operands. The upper 64 bits are\n"
"///    copied from the upper 64 bits of the first source operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_sub_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __a[0] -= __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Subtracts two 128-bit vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSUBPD / SUBPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the minuend.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing the subtrahend.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the differences between\n"
"///    both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_sub_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)((__v2df)__a - (__v2df)__b);\n"
"}\n"
"\n"
"/// Multiplies lower double-precision values in both operands and returns\n"
"///    the product in the lower 64 bits of the result. The upper 64 bits of the\n"
"///    result are copied from the upper double-precision value of the first\n"
"///    operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMULSD / MULSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    product of the lower 64 bits of both operands. The upper 64 bits are\n"
"///    copied from the upper 64 bits of the first source operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_mul_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __a[0] *= __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Multiplies two 128-bit vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMULPD / MULPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the operands.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the products of both\n"
"///    operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_mul_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)((__v2df)__a * (__v2df)__b);\n"
"}\n"
"\n"
"/// Divides the lower double-precision value of the first operand by the\n"
"///    lower double-precision value of the second operand and returns the\n"
"///    quotient in the lower 64 bits of the result. The upper 64 bits of the\n"
"///    result are copied from the upper double-precision value of the first\n"
"///    operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDIVSD / DIVSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the dividend.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing divisor.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    quotient of the lower 64 bits of both operands. The upper 64 bits are\n"
"///    copied from the upper 64 bits of the first source operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_div_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __a[0] /= __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Performs an element-by-element division of two 128-bit vectors of\n"
"///    [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDIVPD / DIVPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the dividend.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing the divisor.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the quotients of both\n"
"///    operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_div_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)((__v2df)__a / (__v2df)__b);\n"
"}\n"
"\n"
"/// Calculates the square root of the lower double-precision value of\n"
"///    the second operand and returns it in the lower 64 bits of the result.\n"
"///    The upper 64 bits of the result are copied from the upper\n"
"///    double-precision value of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSQRTSD / SQRTSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the operands. The\n"
"///    upper 64 bits of this operand are copied to the upper 64 bits of the\n"
"///    result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the operands. The\n"
"///    square root is calculated using the lower 64 bits of this operand.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    square root of the lower 64 bits of operand \\a __b, and whose upper 64\n"
"///    bits are copied from the upper 64 bits of operand \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_sqrt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __m128d __c = __builtin_ia32_sqrtsd((__v2df)__b);\n"
"  return __extension__ (__m128d) { __c[0], __a[1] };\n"
"}\n"
"\n"
"/// Calculates the square root of the each of two values stored in a\n"
"///    128-bit vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSQRTPD / SQRTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector of [2 x double] containing the square roots of the\n"
"///    values in the operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_sqrt_pd(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_sqrtpd((__v2df)__a);\n"
"}\n"
"\n"
"/// Compares lower 64-bit double-precision values of both operands, and\n"
"///    returns the lesser of the pair of values in the lower 64-bits of the\n"
"///    result. The upper 64 bits of the result are copied from the upper\n"
"///    double-precision value of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMINSD / MINSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the operands. The\n"
"///    lower 64 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the operands. The\n"
"///    lower 64 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    minimum value between both operands. The upper 64 bits are copied from\n"
"///    the upper 64 bits of the first source operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_min_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_minsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Performs element-by-element comparison of the two 128-bit vectors of\n"
"///    [2 x double] and returns the vector containing the lesser of each pair of\n"
"///    values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMINPD / MINPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the operands.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the minimum values\n"
"///    between both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_min_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_minpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares lower 64-bit double-precision values of both operands, and\n"
"///    returns the greater of the pair of values in the lower 64-bits of the\n"
"///    result. The upper 64 bits of the result are copied from the upper\n"
"///    double-precision value of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMAXSD / MAXSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the operands. The\n"
"///    lower 64 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the operands. The\n"
"///    lower 64 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    maximum value between both operands. The upper 64 bits are copied from\n"
"///    the upper 64 bits of the first source operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_max_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_maxsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Performs element-by-element comparison of the two 128-bit vectors of\n"
"///    [2 x double] and returns the vector containing the greater of each pair\n"
"///    of values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMAXPD / MAXPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the operands.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the maximum values\n"
"///    between both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_max_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_maxpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 128-bit vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPAND / PAND </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the bitwise AND of the\n"
"///    values between both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_and_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)((__v2du)__a & (__v2du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 128-bit vectors of [2 x double], using\n"
"///    the one's complement of the values contained in the first source operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPANDN / PANDN </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the left source operand. The\n"
"///    one's complement of this value is used in the bitwise AND.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing the right source operand.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the bitwise AND of the\n"
"///    values in the second operand and the one's complement of the first\n"
"///    operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_andnot_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)(~(__v2du)__a & (__v2du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise OR of two 128-bit vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPOR / POR </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the bitwise OR of the\n"
"///    values between both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_or_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)((__v2du)__a | (__v2du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise XOR of two 128-bit vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPXOR / PXOR </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the bitwise XOR of the\n"
"///    values between both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_xor_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)((__v2du)__a ^ (__v2du)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] for equality. Each comparison yields 0x0\n"
"///    for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPEQPD / CMPEQPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpeqpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are less than those in the second operand. Each comparison\n"
"///    yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTPD / CMPLTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmplt_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are less than or equal to those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLEPD / CMPLEPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmple_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmplepd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are greater than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTPD / CMPLTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__b, (__v2df)__a);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are greater than or equal to those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLEPD / CMPLEPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpge_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmplepd((__v2df)__b, (__v2df)__a);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are ordered with respect to those in the second operand.\n"
"///\n"
"///    A pair of double-precision values are \"ordered\" with respect to each\n"
"///    other if neither value is a NaN. Each comparison yields 0x0 for false,\n"
"///    0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPORDPD / CMPORDPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpord_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpordpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are unordered with respect to those in the second operand.\n"
"///\n"
"///    A pair of double-precision values are \"unordered\" with respect to each\n"
"///    other if one or both values are NaN. Each comparison yields 0x0 for\n"
"///    false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPUNORDPD / CMPUNORDPD </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpunord_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpunordpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are unequal to those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNEQPD / CMPNEQPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpneq_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpneqpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are not less than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTPD / CMPNLTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpnlt_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are not less than or equal to those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLEPD / CMPNLEPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpnle_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are not greater than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTPD / CMPNLTPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpngt_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__b, (__v2df)__a);\n"
"}\n"
"\n"
"/// Compares each of the corresponding double-precision values of the\n"
"///    128-bit vectors of [2 x double] to determine if the values in the first\n"
"///    operand are not greater than or equal to those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLEPD / CMPNLEPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector containing the comparison results.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpnge_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__b, (__v2df)__a);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] for equality.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPEQSD / CMPEQSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpeqsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is less than the corresponding value in\n"
"///    the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTSD / CMPLTSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmplt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpltsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is less than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLESD / CMPLESD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmple_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmplesd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is greater than the corresponding value\n"
"///    in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTSD / CMPLTSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///     A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///     compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///     A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///     compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///     results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __m128d __c = __builtin_ia32_cmpltsd((__v2df)__b, (__v2df)__a);\n"
"  return __extension__ (__m128d) { __c[0], __a[1] };\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is greater than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLESD / CMPLESD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpge_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __m128d __c = __builtin_ia32_cmplesd((__v2df)__b, (__v2df)__a);\n"
"  return __extension__ (__m128d) { __c[0], __a[1] };\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is \"ordered\" with respect to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true. A pair\n"
"///    of double-precision values are \"ordered\" with respect to each other if\n"
"///    neither value is a NaN.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPORDSD / CMPORDSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpord_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpordsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is \"unordered\" with respect to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true. A pair\n"
"///    of double-precision values are \"unordered\" with respect to each other if\n"
"///    one or both values are NaN.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPUNORDSD / CMPUNORDSD </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpunord_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpunordsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is unequal to the corresponding value in\n"
"///    the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNEQSD / CMPNEQSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpneq_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpneqsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is not less than the corresponding\n"
"///    value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTSD / CMPNLTSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpnlt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpnltsd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is not less than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLESD / CMPNLESD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns  A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpnle_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return (__m128d)__builtin_ia32_cmpnlesd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is not greater than the corresponding\n"
"///    value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTSD / CMPNLTSD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpngt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __m128d __c = __builtin_ia32_cmpnltsd((__v2df)__b, (__v2df)__a);\n"
"  return __extension__ (__m128d) { __c[0], __a[1] };\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is not greater than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0x0 for false, 0xFFFFFFFFFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLESD / CMPNLESD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns A 128-bit vector. The lower 64 bits contains the comparison\n"
"///    results. The upper 64 bits are copied from the upper 64 bits of \\a __a.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cmpnge_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __m128d __c = __builtin_ia32_cmpnlesd((__v2df)__b, (__v2df)__a);\n"
"  return __extension__ (__m128d) { __c[0], __a[1] };\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] for equality.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comieq_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_comisdeq((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is less than the corresponding value in\n"
"///    the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comilt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_comisdlt((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is less than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///     A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///     compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comile_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_comisdle((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is greater than the corresponding value\n"
"///    in the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comigt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_comisdgt((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is greater than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comige_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_comisdge((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is unequal to the corresponding value in\n"
"///    the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two\n"
"///    lower double-precision values is NaN, 1 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower double-precision values is NaN, 1 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comineq_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_comisdneq((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] for equality. The\n"
"///    comparison yields 0 for false, 1 for true.\n"
"///\n"
"///    If either of the two lower double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomieq_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_ucomisdeq((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is less than the corresponding value in\n"
"///    the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two lower\n"
"///    double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomilt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_ucomisdlt((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is less than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two lower\n"
"///    double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///     A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///     compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomile_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_ucomisdle((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is greater than the corresponding value\n"
"///    in the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two lower\n"
"///    double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///     A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///     compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomigt_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_ucomisdgt((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is greater than or equal to the\n"
"///    corresponding value in the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true.  If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower double-precision values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomige_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_ucomisdge((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Compares the lower double-precision floating-point values in each of\n"
"///    the two 128-bit floating-point vectors of [2 x double] to determine if\n"
"///    the value in the first parameter is unequal to the corresponding value in\n"
"///    the second parameter.\n"
"///\n"
"///    The comparison yields 0 for false, 1 for true. If either of the two lower\n"
"///    double-precision values is NaN, 1 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __b.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision value is\n"
"///    compared to the lower double-precision value of \\a __a.\n"
"/// \\returns An integer containing the comparison result. If either of the two\n"
"///    lower double-precision values is NaN, 1 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomineq_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_ucomisdneq((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Converts the two double-precision floating-point elements of a\n"
"///    128-bit vector of [2 x double] into two single-precision floating-point\n"
"///    values, returned in the lower 64 bits of a 128-bit vector of [4 x float].\n"
"///    The upper 64 bits of the result vector are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPD2PS / CVTPD2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 64 bits contain the\n"
"///    converted values. The upper 64 bits are set to zero.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cvtpd_ps(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_cvtpd2ps((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the lower two single-precision floating-point elements of a\n"
"///    128-bit vector of [4 x float] into two double-precision floating-point\n"
"///    values, returned in a 128-bit vector of [2 x double]. The upper two\n"
"///    elements of the input vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPS2PD / CVTPS2PD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower two single-precision\n"
"///    floating-point elements are converted to double-precision values. The\n"
"///    upper two elements are unused.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the converted values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cvtps_pd(__m128 __a)\n"
"{\n"
"  return (__m128d) __builtin_convertvector(\n"
"      __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 1), __v2df);\n"
"}\n"
"\n"
"/// Converts the lower two integer elements of a 128-bit vector of\n"
"///    [4 x i32] into two double-precision floating-point values, returned in a\n"
"///    128-bit vector of [2 x double].\n"
"///\n"
"///    The upper two elements of the input vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTDQ2PD / CVTDQ2PD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector of [4 x i32]. The lower two integer elements are\n"
"///    converted to double-precision values.\n"
"///\n"
"///    The upper two elements are unused.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the converted values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi32_pd(__m128i __a)\n"
"{\n"
"  return (__m128d) __builtin_convertvector(\n"
"      __builtin_shufflevector((__v4si)__a, (__v4si)__a, 0, 1), __v2df);\n"
"}\n"
"\n"
"/// Converts the two double-precision floating-point elements of a\n"
"///    128-bit vector of [2 x double] into two signed 32-bit integer values,\n"
"///    returned in the lower 64 bits of a 128-bit vector of [4 x i32]. The upper\n"
"///    64 bits of the result vector are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPD2DQ / CVTPD2DQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector of [4 x i32] whose lower 64 bits contain the\n"
"///    converted values. The upper 64 bits are set to zero.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtpd_epi32(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_cvtpd2dq((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the low-order element of a 128-bit vector of [2 x double]\n"
"///    into a 32-bit signed integer value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSD2SI / CVTSD2SI </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the\n"
"///    conversion.\n"
"/// \\returns A 32-bit signed integer containing the converted value.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvtsd_si32(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_cvtsd2si((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the lower double-precision floating-point element of a\n"
"///    128-bit vector of [2 x double], in the second parameter, into a\n"
"///    single-precision floating-point value, returned in the lower 32 bits of a\n"
"///    128-bit vector of [4 x float]. The upper 96 bits of the result vector are\n"
"///    copied from the upper 96 bits of the first parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSD2SS / CVTSD2SS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The upper 96 bits of this parameter are\n"
"///    copied to the upper 96 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower double-precision\n"
"///    floating-point element is used in the conversion.\n"
"/// \\returns A 128-bit vector of [4 x float]. The lower 32 bits contain the\n"
"///    converted value from the second parameter. The upper 96 bits are copied\n"
"///    from the upper 96 bits of the first parameter.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cvtsd_ss(__m128 __a, __m128d __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cvtsd2ss((__v4sf)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Converts a 32-bit signed integer value, in the second parameter, into\n"
"///    a double-precision floating-point value, returned in the lower 64 bits of\n"
"///    a 128-bit vector of [2 x double]. The upper 64 bits of the result vector\n"
"///    are copied from the upper 64 bits of the first parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSI2SD / CVTSI2SD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The upper 64 bits of this parameter are\n"
"///    copied to the upper 64 bits of the result.\n"
"/// \\param __b\n"
"///    A 32-bit signed integer containing the value to be converted.\n"
"/// \\returns A 128-bit vector of [2 x double]. The lower 64 bits contain the\n"
"///    converted value from the second parameter. The upper 64 bits are copied\n"
"///    from the upper 64 bits of the first parameter.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi32_sd(__m128d __a, int __b)\n"
"{\n"
"  __a[0] = __b;\n"
"  return __a;\n"
"}\n"
"\n"
"/// Converts the lower single-precision floating-point element of a\n"
"///    128-bit vector of [4 x float], in the second parameter, into a\n"
"///    double-precision floating-point value, returned in the lower 64 bits of\n"
"///    a 128-bit vector of [2 x double]. The upper 64 bits of the result vector\n"
"///    are copied from the upper 64 bits of the first parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSS2SD / CVTSS2SD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The upper 64 bits of this parameter are\n"
"///    copied to the upper 64 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower single-precision\n"
"///    floating-point element is used in the conversion.\n"
"/// \\returns A 128-bit vector of [2 x double]. The lower 64 bits contain the\n"
"///    converted value from the second parameter. The upper 64 bits are copied\n"
"///    from the upper 64 bits of the first parameter.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cvtss_sd(__m128d __a, __m128 __b)\n"
"{\n"
"  __a[0] = __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Converts the two double-precision floating-point elements of a\n"
"///    128-bit vector of [2 x double] into two signed 32-bit integer values,\n"
"///    returned in the lower 64 bits of a 128-bit vector of [4 x i32].\n"
"///\n"
"///    If the result of either conversion is inexact, the result is truncated\n"
"///    (rounded towards zero) regardless of the current MXCSR setting. The upper\n"
"///    64 bits of the result vector are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTPD2DQ / CVTTPD2DQ </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector of [4 x i32] whose lower 64 bits contain the\n"
"///    converted values. The upper 64 bits are set to zero.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvttpd_epi32(__m128d __a)\n"
"{\n"
"  return (__m128i)__builtin_ia32_cvttpd2dq((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the low-order element of a [2 x double] vector into a 32-bit\n"
"///    signed integer value, truncating the result when it is inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTSD2SI / CVTTSD2SI </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the\n"
"///    conversion.\n"
"/// \\returns A 32-bit signed integer containing the converted value.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvttsd_si32(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_cvttsd2si((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the two double-precision floating-point elements of a\n"
"///    128-bit vector of [2 x double] into two signed 32-bit integer values,\n"
"///    returned in a 64-bit vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPD2PI </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 64-bit vector of [2 x i32] containing the converted values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpd_pi32(__m128d __a)\n"
"{\n"
"  return (__m64)__builtin_ia32_cvtpd2pi((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the two double-precision floating-point elements of a\n"
"///    128-bit vector of [2 x double] into two signed 32-bit integer values,\n"
"///    returned in a 64-bit vector of [2 x i32].\n"
"///\n"
"///    If the result of either conversion is inexact, the result is truncated\n"
"///    (rounded towards zero) regardless of the current MXCSR setting.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTTPD2PI </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 64-bit vector of [2 x i32] containing the converted values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvttpd_pi32(__m128d __a)\n"
"{\n"
"  return (__m64)__builtin_ia32_cvttpd2pi((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the two signed 32-bit integer elements of a 64-bit vector of\n"
"///    [2 x i32] into two double-precision floating-point values, returned in a\n"
"///    128-bit vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [2 x i32].\n"
"/// \\returns A 128-bit vector of [2 x double] containing the converted values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpi32_pd(__m64 __a)\n"
"{\n"
"  return __builtin_ia32_cvtpi2pd((__v2si)__a);\n"
"}\n"
"\n"
"/// Returns the low-order element of a 128-bit vector of [2 x double] as\n"
"///    a double-precision floating-point value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower 64 bits are returned.\n"
"/// \\returns A double-precision floating-point value copied from the lower 64\n"
"///    bits of \\a __a.\n"
"static __inline__ double __DEFAULT_FN_ATTRS\n"
"_mm_cvtsd_f64(__m128d __a)\n"
"{\n"
"  return __a[0];\n"
"}\n"
"\n"
"/// Loads a 128-bit floating-point vector of [2 x double] from an aligned\n"
"///    memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPD / MOVAPD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location has to be 16-byte aligned.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the loaded values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_load_pd(double const *__dp)\n"
"{\n"
"  return *(__m128d*)__dp;\n"
"}\n"
"\n"
"/// Loads a double-precision floating-point value from a specified memory\n"
"///    location and duplicates it to both vector elements of a 128-bit vector of\n"
"///    [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP / MOVDDUP </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a memory location containing a double-precision value.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the loaded and\n"
"///    duplicated values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_load1_pd(double const *__dp)\n"
"{\n"
"  struct __mm_load1_pd_struct {\n"
"    double __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  double __u = ((struct __mm_load1_pd_struct*)__dp)->__u;\n"
"  return __extension__ (__m128d){ __u, __u };\n"
"}\n"
"\n"
"#define        _mm_load_pd1(dp)        _mm_load1_pd(dp)\n"
"\n"
"/// Loads two double-precision values, in reverse order, from an aligned\n"
"///    memory location into a 128-bit vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPD / MOVAPD </c> instruction +\n"
"/// needed shuffling instructions. In AVX mode, the shuffling may be combined\n"
"/// with the \\c VMOVAPD, resulting in only a \\c VPERMILPD instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A 16-byte aligned pointer to an array of double-precision values to be\n"
"///    loaded in reverse order.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the reversed loaded\n"
"///    values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_loadr_pd(double const *__dp)\n"
"{\n"
"  __m128d __u = *(__m128d*)__dp;\n"
"  return __builtin_shufflevector((__v2df)__u, (__v2df)__u, 1, 0);\n"
"}\n"
"\n"
"/// Loads a 128-bit floating-point vector of [2 x double] from an\n"
"///    unaligned memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPD / MOVUPD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the loaded values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_loadu_pd(double const *__dp)\n"
"{\n"
"  struct __loadu_pd {\n"
"    __m128d __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return ((struct __loadu_pd*)__dp)->__v;\n"
"}\n"
"\n"
"/// Loads a 64-bit integer value to the low element of a 128-bit integer\n"
"///    vector and clears the upper element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A pointer to a 64-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the loaded value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_loadu_si64(void const *__a)\n"
"{\n"
"  struct __loadu_si64 {\n"
"    long long __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  long long __u = ((struct __loadu_si64*)__a)->__v;\n"
"  return __extension__ (__m128i)(__v2di){__u, 0LL};\n"
"}\n"
"\n"
"/// Loads a 32-bit integer value to the low element of a 128-bit integer\n"
"///    vector and clears the upper element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A pointer to a 32-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the loaded value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_loadu_si32(void const *__a)\n"
"{\n"
"  struct __loadu_si32 {\n"
"    int __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  int __u = ((struct __loadu_si32*)__a)->__v;\n"
"  return __extension__ (__m128i)(__v4si){__u, 0, 0, 0};\n"
"}\n"
"\n"
"/// Loads a 16-bit integer value to the low element of a 128-bit integer\n"
"///    vector and clears the upper element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic does not correspond to a specific instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A pointer to a 16-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the loaded value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_loadu_si16(void const *__a)\n"
"{\n"
"  struct __loadu_si16 {\n"
"    short __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  short __u = ((struct __loadu_si16*)__a)->__v;\n"
"  return __extension__ (__m128i)(__v8hi){__u, 0, 0, 0, 0, 0, 0, 0};\n"
"}\n"
"\n"
"/// Loads a 64-bit double-precision value to the low element of a\n"
"///    128-bit integer vector and clears the upper element.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSD / MOVSD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a memory location containing a double-precision value.\n"
"///    The address of the memory location does not have to be aligned.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the loaded value.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_load_sd(double const *__dp)\n"
"{\n"
"  struct __mm_load_sd_struct {\n"
"    double __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  double __u = ((struct __mm_load_sd_struct*)__dp)->__u;\n"
"  return __extension__ (__m128d){ __u, 0 };\n"
"}\n"
"\n"
"/// Loads a double-precision value into the high-order bits of a 128-bit\n"
"///    vector of [2 x double]. The low-order bits are copied from the low-order\n"
"///    bits of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVHPD / MOVHPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. \\n\n"
"///    Bits [63:0] are written to bits [63:0] of the result.\n"
"/// \\param __dp\n"
"///    A pointer to a 64-bit memory location containing a double-precision\n"
"///    floating-point value that is loaded. The loaded value is written to bits\n"
"///    [127:64] of the result. The address of the memory location does not have\n"
"///    to be aligned.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the moved values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_loadh_pd(__m128d __a, double const *__dp)\n"
"{\n"
"  struct __mm_loadh_pd_struct {\n"
"    double __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  double __u = ((struct __mm_loadh_pd_struct*)__dp)->__u;\n"
"  return __extension__ (__m128d){ __a[0], __u };\n"
"}\n"
"\n"
"/// Loads a double-precision value into the low-order bits of a 128-bit\n"
"///    vector of [2 x double]. The high-order bits are copied from the\n"
"///    high-order bits of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVLPD / MOVLPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. \\n\n"
"///    Bits [127:64] are written to bits [127:64] of the result.\n"
"/// \\param __dp\n"
"///    A pointer to a 64-bit memory location containing a double-precision\n"
"///    floating-point value that is loaded. The loaded value is written to bits\n"
"///    [63:0] of the result. The address of the memory location does not have to\n"
"///    be aligned.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the moved values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_loadl_pd(__m128d __a, double const *__dp)\n"
"{\n"
"  struct __mm_loadl_pd_struct {\n"
"    double __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  double __u = ((struct __mm_loadl_pd_struct*)__dp)->__u;\n"
"  return __extension__ (__m128d){ __u, __a[1] };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double] with\n"
"///    unspecified content. This could be used as an argument to another\n"
"///    intrinsic function where the argument is required but the value is not\n"
"///    actually used.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\returns A 128-bit floating-point vector of [2 x double] with unspecified\n"
"///    content.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_undefined_pd(void)\n"
"{\n"
"  return (__m128d)__builtin_ia32_undef128();\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double]. The lower\n"
"///    64 bits of the vector are initialized with the specified double-precision\n"
"///    floating-point value. The upper 64 bits are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A double-precision floating-point value used to initialize the lower 64\n"
"///    bits of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [2 x double]. The\n"
"///    lower 64 bits contain the value of the parameter. The upper 64 bits are\n"
"///    set to zero.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_set_sd(double __w)\n"
"{\n"
"  return __extension__ (__m128d){ __w, 0 };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double], with each\n"
"///    of the two double-precision floating-point vector elements set to the\n"
"///    specified double-precision floating-point value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP / MOVLHPS </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A double-precision floating-point value used to initialize each vector\n"
"///    element of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [2 x double].\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_set1_pd(double __w)\n"
"{\n"
"  return __extension__ (__m128d){ __w, __w };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double], with each\n"
"///    of the two double-precision floating-point vector elements set to the\n"
"///    specified double-precision floating-point value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP / MOVLHPS </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A double-precision floating-point value used to initialize each vector\n"
"///    element of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [2 x double].\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_set_pd1(double __w)\n"
"{\n"
"  return _mm_set1_pd(__w);\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double]\n"
"///    initialized with the specified double-precision floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A double-precision floating-point value used to initialize the upper 64\n"
"///    bits of the result.\n"
"/// \\param __x\n"
"///    A double-precision floating-point value used to initialize the lower 64\n"
"///    bits of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [2 x double].\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_set_pd(double __w, double __x)\n"
"{\n"
"  return __extension__ (__m128d){ __x, __w };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double],\n"
"///    initialized in reverse order with the specified double-precision\n"
"///    floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A double-precision floating-point value used to initialize the lower 64\n"
"///    bits of the result.\n"
"/// \\param __x\n"
"///    A double-precision floating-point value used to initialize the upper 64\n"
"///    bits of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [2 x double].\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_setr_pd(double __w, double __x)\n"
"{\n"
"  return __extension__ (__m128d){ __w, __x };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double]\n"
"///    initialized to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instruction.\n"
"///\n"
"/// \\returns An initialized 128-bit floating-point vector of [2 x double] with\n"
"///    all elements set to zero.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_setzero_pd(void)\n"
"{\n"
"  return __extension__ (__m128d){ 0, 0 };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double]. The lower\n"
"///    64 bits are set to the lower 64 bits of the second parameter. The upper\n"
"///    64 bits are set to the upper 64 bits of the first parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDPD / BLENDPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The upper 64 bits are written to the\n"
"///    upper 64 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. The lower 64 bits are written to the\n"
"///    lower 64 bits of the result.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the moved values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_move_sd(__m128d __a, __m128d __b)\n"
"{\n"
"  __a[0] = __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Stores the lower 64 bits of a 128-bit vector of [2 x double] to a\n"
"///    memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSD / MOVSD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a 64-bit memory location.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store_sd(double *__dp, __m128d __a)\n"
"{\n"
"  struct __mm_store_sd_struct {\n"
"    double __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __mm_store_sd_struct*)__dp)->__u = __a[0];\n"
"}\n"
"\n"
"/// Moves packed double-precision values from a 128-bit vector of\n"
"///    [2 x double] to a memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c>VMOVAPD / MOVAPS</c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to an aligned memory location that can store two\n"
"///    double-precision values.\n"
"/// \\param __a\n"
"///    A packed 128-bit vector of [2 x double] containing the values to be\n"
"///    moved.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store_pd(double *__dp, __m128d __a)\n"
"{\n"
"  *(__m128d*)__dp = __a;\n"
"}\n"
"\n"
"/// Moves the lower 64 bits of a 128-bit vector of [2 x double] twice to\n"
"///    the upper and lower 64 bits of a memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the\n"
"///   <c> VMOVDDUP + VMOVAPD / MOVLHPS + MOVAPS </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a memory location that can store two double-precision\n"
"///    values.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] whose lower 64 bits are copied to each\n"
"///    of the values in \\a __dp.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store1_pd(double *__dp, __m128d __a)\n"
"{\n"
"  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);\n"
"  _mm_store_pd(__dp, __a);\n"
"}\n"
"\n"
"/// Moves the lower 64 bits of a 128-bit vector of [2 x double] twice to\n"
"///    the upper and lower 64 bits of a memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the\n"
"///   <c> VMOVDDUP + VMOVAPD / MOVLHPS + MOVAPS </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a memory location that can store two double-precision\n"
"///    values.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] whose lower 64 bits are copied to each\n"
"///    of the values in \\a __dp.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store_pd1(double *__dp, __m128d __a)\n"
"{\n"
"  _mm_store1_pd(__dp, __a);\n"
"}\n"
"\n"
"/// Stores a 128-bit vector of [2 x double] into an unaligned memory\n"
"///    location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPD / MOVUPD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeu_pd(double *__dp, __m128d __a)\n"
"{\n"
"  struct __storeu_pd {\n"
"    __m128d __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_pd*)__dp)->__v = __a;\n"
"}\n"
"\n"
"/// Stores two double-precision values, in reverse order, from a 128-bit\n"
"///    vector of [2 x double] to a 16-byte aligned memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to a shuffling instruction followed by a\n"
"/// <c> VMOVAPD / MOVAPD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a 16-byte aligned memory location that can store two\n"
"///    double-precision values.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the values to be reversed and\n"
"///    stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storer_pd(double *__dp, __m128d __a)\n"
"{\n"
"  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 1, 0);\n"
"  *(__m128d *)__dp = __a;\n"
"}\n"
"\n"
"/// Stores the upper 64 bits of a 128-bit vector of [2 x double] to a\n"
"///    memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVHPD / MOVHPD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a 64-bit memory location.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeh_pd(double *__dp, __m128d __a)\n"
"{\n"
"  struct __mm_storeh_pd_struct {\n"
"    double __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[1];\n"
"}\n"
"\n"
"/// Stores the lower 64 bits of a 128-bit vector of [2 x double] to a\n"
"///    memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVLPD / MOVLPD </c> instruction.\n"
"///\n"
"/// \\param __dp\n"
"///    A pointer to a 64-bit memory location.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storel_pd(double *__dp, __m128d __a)\n"
"{\n"
"  struct __mm_storeh_pd_struct {\n"
"    double __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[0];\n"
"}\n"
"\n"
"/// Adds the corresponding elements of two 128-bit vectors of [16 x i8],\n"
"///    saving the lower 8 bits of each sum in the corresponding element of a\n"
"///    128-bit result vector of [16 x i8].\n"
"///\n"
"///    The integer elements of both parameters can be either signed or unsigned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDB / PADDB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the sums of both\n"
"///    parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_add_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v16qu)__a + (__v16qu)__b);\n"
"}\n"
"\n"
"/// Adds the corresponding elements of two 128-bit vectors of [8 x i16],\n"
"///    saving the lower 16 bits of each sum in the corresponding element of a\n"
"///    128-bit result vector of [8 x i16].\n"
"///\n"
"///    The integer elements of both parameters can be either signed or unsigned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDW / PADDW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16].\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the sums of both\n"
"///    parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_add_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v8hu)__a + (__v8hu)__b);\n"
"}\n"
"\n"
"/// Adds the corresponding elements of two 128-bit vectors of [4 x i32],\n"
"///    saving the lower 32 bits of each sum in the corresponding element of a\n"
"///    128-bit result vector of [4 x i32].\n"
"///\n"
"///    The integer elements of both parameters can be either signed or unsigned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDD / PADDD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the sums of both\n"
"///    parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_add_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v4su)__a + (__v4su)__b);\n"
"}\n"
"\n"
"/// Adds two signed or unsigned 64-bit integer values, returning the\n"
"///    lower 64 bits of the sum.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer.\n"
"/// \\param __b\n"
"///    A 64-bit integer.\n"
"/// \\returns A 64-bit integer containing the sum of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_add_si64(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_paddq((__v1di)__a, (__v1di)__b);\n"
"}\n"
"\n"
"/// Adds the corresponding elements of two 128-bit vectors of [2 x i64],\n"
"///    saving the lower 64 bits of each sum in the corresponding element of a\n"
"///    128-bit result vector of [2 x i64].\n"
"///\n"
"///    The integer elements of both parameters can be either signed or unsigned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDQ / PADDQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x i64].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x i64].\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the sums of both\n"
"///    parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_add_epi64(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v2du)__a + (__v2du)__b);\n"
"}\n"
"\n"
"/// Adds, with saturation, the corresponding elements of two 128-bit\n"
"///    signed [16 x i8] vectors, saving each sum in the corresponding element of\n"
"///    a 128-bit result vector of [16 x i8]. Positive sums greater than 0x7F are\n"
"///    saturated to 0x7F. Negative sums less than 0x80 are saturated to 0x80.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDSB / PADDSB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit signed [16 x i8] vector.\n"
"/// \\param __b\n"
"///    A 128-bit signed [16 x i8] vector.\n"
"/// \\returns A 128-bit signed [16 x i8] vector containing the saturated sums of\n"
"///    both parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_adds_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_paddsb128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Adds, with saturation, the corresponding elements of two 128-bit\n"
"///    signed [8 x i16] vectors, saving each sum in the corresponding element of\n"
"///    a 128-bit result vector of [8 x i16]. Positive sums greater than 0x7FFF\n"
"///    are saturated to 0x7FFF. Negative sums less than 0x8000 are saturated to\n"
"///    0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDSW / PADDSW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\returns A 128-bit signed [8 x i16] vector containing the saturated sums of\n"
"///    both parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_adds_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_paddsw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Adds, with saturation, the corresponding elements of two 128-bit\n"
"///    unsigned [16 x i8] vectors, saving each sum in the corresponding element\n"
"///    of a 128-bit result vector of [16 x i8]. Positive sums greater than 0xFF\n"
"///    are saturated to 0xFF. Negative sums are saturated to 0x00.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDUSB / PADDUSB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\param __b\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\returns A 128-bit unsigned [16 x i8] vector containing the saturated sums\n"
"///    of both parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_adds_epu8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_paddusb128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Adds, with saturation, the corresponding elements of two 128-bit\n"
"///    unsigned [8 x i16] vectors, saving each sum in the corresponding element\n"
"///    of a 128-bit result vector of [8 x i16]. Positive sums greater than\n"
"///    0xFFFF are saturated to 0xFFFF. Negative sums are saturated to 0x0000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPADDUSB / PADDUSB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit unsigned [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit unsigned [8 x i16] vector.\n"
"/// \\returns A 128-bit unsigned [8 x i16] vector containing the saturated sums\n"
"///    of both parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_adds_epu16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_paddusw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Computes the rounded avarages of corresponding elements of two\n"
"///    128-bit unsigned [16 x i8] vectors, saving each result in the\n"
"///    corresponding element of a 128-bit result vector of [16 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPAVGB / PAVGB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\param __b\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\returns A 128-bit unsigned [16 x i8] vector containing the rounded\n"
"///    averages of both parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_avg_epu8(__m128i __a, __m128i __b)\n"
"{\n"
"  typedef unsigned short __v16hu __attribute__ ((__vector_size__ (32)));\n"
"  return (__m128i)__builtin_convertvector(\n"
"               ((__builtin_convertvector((__v16qu)__a, __v16hu) +\n"
"                 __builtin_convertvector((__v16qu)__b, __v16hu)) + 1)\n"
"                 >> 1, __v16qu);\n"
"}\n"
"\n"
"/// Computes the rounded avarages of corresponding elements of two\n"
"///    128-bit unsigned [8 x i16] vectors, saving each result in the\n"
"///    corresponding element of a 128-bit result vector of [8 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPAVGW / PAVGW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit unsigned [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit unsigned [8 x i16] vector.\n"
"/// \\returns A 128-bit unsigned [8 x i16] vector containing the rounded\n"
"///    averages of both parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_avg_epu16(__m128i __a, __m128i __b)\n"
"{\n"
"  typedef unsigned int __v8su __attribute__ ((__vector_size__ (32)));\n"
"  return (__m128i)__builtin_convertvector(\n"
"               ((__builtin_convertvector((__v8hu)__a, __v8su) +\n"
"                 __builtin_convertvector((__v8hu)__b, __v8su)) + 1)\n"
"                 >> 1, __v8hu);\n"
"}\n"
"\n"
"/// Multiplies the corresponding elements of two 128-bit signed [8 x i16]\n"
"///    vectors, producing eight intermediate 32-bit signed integer products, and\n"
"///    adds the consecutive pairs of 32-bit products to form a 128-bit signed\n"
"///    [4 x i32] vector.\n"
"///\n"
"///    For example, bits [15:0] of both parameters are multiplied producing a\n"
"///    32-bit product, bits [31:16] of both parameters are multiplied producing\n"
"///    a 32-bit product, and the sum of those two products becomes bits [31:0]\n"
"///    of the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMADDWD / PMADDWD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\returns A 128-bit signed [4 x i32] vector containing the sums of products\n"
"///    of both parameters.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_madd_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pmaddwd128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Compares corresponding elements of two 128-bit signed [8 x i16]\n"
"///    vectors, saving the greater value from each comparison in the\n"
"///    corresponding element of a 128-bit result vector of [8 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMAXSW / PMAXSW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\returns A 128-bit signed [8 x i16] vector containing the greater value of\n"
"///    each comparison.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_max_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pmaxsw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Compares corresponding elements of two 128-bit unsigned [16 x i8]\n"
"///    vectors, saving the greater value from each comparison in the\n"
"///    corresponding element of a 128-bit result vector of [16 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMAXUB / PMAXUB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\param __b\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\returns A 128-bit unsigned [16 x i8] vector containing the greater value of\n"
"///    each comparison.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_max_epu8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pmaxub128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Compares corresponding elements of two 128-bit signed [8 x i16]\n"
"///    vectors, saving the smaller value from each comparison in the\n"
"///    corresponding element of a 128-bit result vector of [8 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMINSW / PMINSW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\returns A 128-bit signed [8 x i16] vector containing the smaller value of\n"
"///    each comparison.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_min_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pminsw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Compares corresponding elements of two 128-bit unsigned [16 x i8]\n"
"///    vectors, saving the smaller value from each comparison in the\n"
"///    corresponding element of a 128-bit result vector of [16 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMINUB / PMINUB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\param __b\n"
"///    A 128-bit unsigned [16 x i8] vector.\n"
"/// \\returns A 128-bit unsigned [16 x i8] vector containing the smaller value of\n"
"///    each comparison.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_min_epu8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pminub128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Multiplies the corresponding elements of two signed [8 x i16]\n"
"///    vectors, saving the upper 16 bits of each 32-bit product in the\n"
"///    corresponding element of a 128-bit signed [8 x i16] result vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMULHW / PMULHW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\returns A 128-bit signed [8 x i16] vector containing the upper 16 bits of\n"
"///    each of the eight 32-bit products.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_mulhi_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pmulhw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Multiplies the corresponding elements of two unsigned [8 x i16]\n"
"///    vectors, saving the upper 16 bits of each 32-bit product in the\n"
"///    corresponding element of a 128-bit unsigned [8 x i16] result vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMULHUW / PMULHUW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit unsigned [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit unsigned [8 x i16] vector.\n"
"/// \\returns A 128-bit unsigned [8 x i16] vector containing the upper 16 bits\n"
"///    of each of the eight 32-bit products.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_mulhi_epu16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pmulhuw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Multiplies the corresponding elements of two signed [8 x i16]\n"
"///    vectors, saving the lower 16 bits of each 32-bit product in the\n"
"///    corresponding element of a 128-bit signed [8 x i16] result vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMULLW / PMULLW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\param __b\n"
"///    A 128-bit signed [8 x i16] vector.\n"
"/// \\returns A 128-bit signed [8 x i16] vector containing the lower 16 bits of\n"
"///    each of the eight 32-bit products.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_mullo_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v8hu)__a * (__v8hu)__b);\n"
"}\n"
"\n"
"/// Multiplies 32-bit unsigned integer values contained in the lower bits\n"
"///    of the two 64-bit integer vectors and returns the 64-bit unsigned\n"
"///    product.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMULUDQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the product of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_mul_su32(__m64 __a, __m64 __b)\n"
"{\n"
"  return __builtin_ia32_pmuludq((__v2si)__a, (__v2si)__b);\n"
"}\n"
"\n"
"/// Multiplies 32-bit unsigned integer values contained in the lower\n"
"///    bits of the corresponding elements of two [2 x i64] vectors, and returns\n"
"///    the 64-bit products in the corresponding elements of a [2 x i64] vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMULUDQ / PMULUDQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A [2 x i64] vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A [2 x i64] vector containing one of the source operands.\n"
"/// \\returns A [2 x i64] vector containing the product of both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_mul_epu32(__m128i __a, __m128i __b)\n"
"{\n"
"  return __builtin_ia32_pmuludq128((__v4si)__a, (__v4si)__b);\n"
"}\n"
"\n"
"/// Computes the absolute differences of corresponding 8-bit integer\n"
"///    values in two 128-bit vectors. Sums the first 8 absolute differences, and\n"
"///    separately sums the second 8 absolute differences. Packs these two\n"
"///    unsigned 16-bit integer sums into the upper and lower elements of a\n"
"///    [2 x i64] vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSADBW / PSADBW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\returns A [2 x i64] vector containing the sums of the sets of absolute\n"
"///    differences between both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sad_epu8(__m128i __a, __m128i __b)\n"
"{\n"
"  return __builtin_ia32_psadbw128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Subtracts the corresponding 8-bit integer values in the operands.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBB / PSUBB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the differences of the values\n"
"///    in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sub_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v16qu)__a - (__v16qu)__b);\n"
"}\n"
"\n"
"/// Subtracts the corresponding 16-bit integer values in the operands.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBW / PSUBW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the differences of the values\n"
"///    in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sub_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v8hu)__a - (__v8hu)__b);\n"
"}\n"
"\n"
"/// Subtracts the corresponding 32-bit integer values in the operands.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBD / PSUBD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the differences of the values\n"
"///    in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sub_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v4su)__a - (__v4su)__b);\n"
"}\n"
"\n"
"/// Subtracts signed or unsigned 64-bit integer values and writes the\n"
"///    difference to the corresponding bits in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing the minuend.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing the subtrahend.\n"
"/// \\returns A 64-bit integer vector containing the difference of the values in\n"
"///    the operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_sub_si64(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_psubq((__v1di)__a, (__v1di)__b);\n"
"}\n"
"\n"
"/// Subtracts the corresponding elements of two [2 x i64] vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBQ / PSUBQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the differences of the values\n"
"///    in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sub_epi64(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v2du)__a - (__v2du)__b);\n"
"}\n"
"\n"
"/// Subtracts corresponding 8-bit signed integer values in the input and\n"
"///    returns the differences in the corresponding bytes in the destination.\n"
"///    Differences greater than 0x7F are saturated to 0x7F, and differences less\n"
"///    than 0x80 are saturated to 0x80.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBSB / PSUBSB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the differences of the values\n"
"///    in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_subs_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psubsb128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Subtracts corresponding 16-bit signed integer values in the input and\n"
"///    returns the differences in the corresponding bytes in the destination.\n"
"///    Differences greater than 0x7FFF are saturated to 0x7FFF, and values less\n"
"///    than 0x8000 are saturated to 0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBSW / PSUBSW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the differences of the values\n"
"///    in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_subs_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psubsw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Subtracts corresponding 8-bit unsigned integer values in the input\n"
"///    and returns the differences in the corresponding bytes in the\n"
"///    destination. Differences less than 0x00 are saturated to 0x00.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBUSB / PSUBUSB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the unsigned integer\n"
"///    differences of the values in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_subs_epu8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psubusb128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Subtracts corresponding 16-bit unsigned integer values in the input\n"
"///    and returns the differences in the corresponding bytes in the\n"
"///    destination. Differences less than 0x0000 are saturated to 0x0000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSUBUSW / PSUBUSW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the minuends.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the subtrahends.\n"
"/// \\returns A 128-bit integer vector containing the unsigned integer\n"
"///    differences of the values in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_subs_epu16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psubusw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 128-bit integer vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPAND / PAND </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\returns A 128-bit integer vector containing the bitwise AND of the values\n"
"///    in both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_and_si128(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v2du)__a & (__v2du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 128-bit integer vectors, using the\n"
"///    one's complement of the values contained in the first source operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPANDN / PANDN </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector containing the left source operand. The one's complement\n"
"///    of this value is used in the bitwise AND.\n"
"/// \\param __b\n"
"///    A 128-bit vector containing the right source operand.\n"
"/// \\returns A 128-bit integer vector containing the bitwise AND of the one's\n"
"///    complement of the first operand and the values in the second operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_andnot_si128(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)(~(__v2du)__a & (__v2du)__b);\n"
"}\n"
"/// Performs a bitwise OR of two 128-bit integer vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPOR / POR </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\returns A 128-bit integer vector containing the bitwise OR of the values\n"
"///    in both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_or_si128(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v2du)__a | (__v2du)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise exclusive OR of two 128-bit integer vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPXOR / PXOR </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing one of the source operands.\n"
"/// \\returns A 128-bit integer vector containing the bitwise exclusive OR of the\n"
"///    values in both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_xor_si128(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v2du)__a ^ (__v2du)__b);\n"
"}\n"
"\n"
"/// Left-shifts the 128-bit integer vector operand by the specified\n"
"///    number of bytes. Low-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_slli_si128(__m128i a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSLLDQ / PSLLDQ </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param imm\n"
"///    An immediate value specifying the number of bytes to left-shift operand\n"
"///    \\a a.\n"
"/// \\returns A 128-bit integer vector containing the left-shifted value.\n"
"#define _mm_slli_si128(a, imm) \\\n"
"  (__m128i)__builtin_ia32_pslldqi128_byteshift((__v2di)(__m128i)(a), (int)(imm))\n"
"\n"
"#define _mm_bslli_si128(a, imm) \\\n"
"  (__m128i)__builtin_ia32_pslldqi128_byteshift((__v2di)(__m128i)(a), (int)(imm))\n"
"\n"
"/// Left-shifts each 16-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. Low-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSLLW / PSLLW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to left-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the left-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_slli_epi16(__m128i __a, int __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psllwi128((__v8hi)__a, __count);\n"
"}\n"
"\n"
"/// Left-shifts each 16-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. Low-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSLLW / PSLLW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to left-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the left-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sll_epi16(__m128i __a, __m128i __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psllw128((__v8hi)__a, (__v8hi)__count);\n"
"}\n"
"\n"
"/// Left-shifts each 32-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. Low-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSLLD / PSLLD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to left-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the left-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_slli_epi32(__m128i __a, int __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pslldi128((__v4si)__a, __count);\n"
"}\n"
"\n"
"/// Left-shifts each 32-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. Low-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSLLD / PSLLD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to left-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the left-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sll_epi32(__m128i __a, __m128i __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_pslld128((__v4si)__a, (__v4si)__count);\n"
"}\n"
"\n"
"/// Left-shifts each 64-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. Low-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSLLQ / PSLLQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to left-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the left-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_slli_epi64(__m128i __a, int __count)\n"
"{\n"
"  return __builtin_ia32_psllqi128((__v2di)__a, __count);\n"
"}\n"
"\n"
"/// Left-shifts each 64-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. Low-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSLLQ / PSLLQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to left-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the left-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sll_epi64(__m128i __a, __m128i __count)\n"
"{\n"
"  return __builtin_ia32_psllq128((__v2di)__a, (__v2di)__count);\n"
"}\n"
"\n"
"/// Right-shifts each 16-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. High-order bits are filled with the sign\n"
"///    bit of the initial value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRAW / PSRAW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to right-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srai_epi16(__m128i __a, int __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrawi128((__v8hi)__a, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 16-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. High-order bits are filled with the sign\n"
"///    bit of the initial value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRAW / PSRAW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to right-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sra_epi16(__m128i __a, __m128i __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psraw128((__v8hi)__a, (__v8hi)__count);\n"
"}\n"
"\n"
"/// Right-shifts each 32-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. High-order bits are filled with the sign\n"
"///    bit of the initial value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRAD / PSRAD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to right-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srai_epi32(__m128i __a, int __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psradi128((__v4si)__a, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 32-bit value in the 128-bit integer vector operand\n"
"///    by the specified number of bits. High-order bits are filled with the sign\n"
"///    bit of the initial value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRAD / PSRAD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to right-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sra_epi32(__m128i __a, __m128i __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrad128((__v4si)__a, (__v4si)__count);\n"
"}\n"
"\n"
"/// Right-shifts the 128-bit integer vector operand by the specified\n"
"///    number of bytes. High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_srli_si128(__m128i a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRLDQ / PSRLDQ </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param imm\n"
"///    An immediate value specifying the number of bytes to right-shift operand\n"
"///    \\a a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted value.\n"
"#define _mm_srli_si128(a, imm) \\\n"
"  (__m128i)__builtin_ia32_psrldqi128_byteshift((__v2di)(__m128i)(a), (int)(imm))\n"
"\n"
"#define _mm_bsrli_si128(a, imm) \\\n"
"  (__m128i)__builtin_ia32_psrldqi128_byteshift((__v2di)(__m128i)(a), (int)(imm))\n"
"\n"
"/// Right-shifts each of 16-bit values in the 128-bit integer vector\n"
"///    operand by the specified number of bits. High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRLW / PSRLW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to right-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srli_epi16(__m128i __a, int __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrlwi128((__v8hi)__a, __count);\n"
"}\n"
"\n"
"/// Right-shifts each of 16-bit values in the 128-bit integer vector\n"
"///    operand by the specified number of bits. High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRLW / PSRLW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to right-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srl_epi16(__m128i __a, __m128i __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrlw128((__v8hi)__a, (__v8hi)__count);\n"
"}\n"
"\n"
"/// Right-shifts each of 32-bit values in the 128-bit integer vector\n"
"///    operand by the specified number of bits. High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRLD / PSRLD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to right-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srli_epi32(__m128i __a, int __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrldi128((__v4si)__a, __count);\n"
"}\n"
"\n"
"/// Right-shifts each of 32-bit values in the 128-bit integer vector\n"
"///    operand by the specified number of bits. High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRLD / PSRLD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to right-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srl_epi32(__m128i __a, __m128i __count)\n"
"{\n"
"  return (__m128i)__builtin_ia32_psrld128((__v4si)__a, (__v4si)__count);\n"
"}\n"
"\n"
"/// Right-shifts each of 64-bit values in the 128-bit integer vector\n"
"///    operand by the specified number of bits. High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRLQ / PSRLQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    An integer value specifying the number of bits to right-shift each value\n"
"///    in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srli_epi64(__m128i __a, int __count)\n"
"{\n"
"  return __builtin_ia32_psrlqi128((__v2di)__a, __count);\n"
"}\n"
"\n"
"/// Right-shifts each of 64-bit values in the 128-bit integer vector\n"
"///    operand by the specified number of bits. High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSRLQ / PSRLQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the source operand.\n"
"/// \\param __count\n"
"///    A 128-bit integer vector in which bits [63:0] specify the number of bits\n"
"///    to right-shift each value in operand \\a __a.\n"
"/// \\returns A 128-bit integer vector containing the right-shifted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_srl_epi64(__m128i __a, __m128i __count)\n"
"{\n"
"  return __builtin_ia32_psrlq128((__v2di)__a, (__v2di)__count);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 8-bit values of the 128-bit\n"
"///    integer vectors for equality. Each comparison yields 0x0 for false, 0xFF\n"
"///    for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPEQB / PCMPEQB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v16qi)__a == (__v16qi)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 16-bit values of the 128-bit\n"
"///    integer vectors for equality. Each comparison yields 0x0 for false,\n"
"///    0xFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPEQW / PCMPEQW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v8hi)__a == (__v8hi)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit values of the 128-bit\n"
"///    integer vectors for equality. Each comparison yields 0x0 for false,\n"
"///    0xFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPEQD / PCMPEQD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v4si)__a == (__v4si)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding signed 8-bit values of the 128-bit\n"
"///    integer vectors to determine if the values in the first operand are\n"
"///    greater than those in the second operand. Each comparison yields 0x0 for\n"
"///    false, 0xFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPGTB / PCMPGTB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  /* This function always performs a signed comparison, but __v16qi is a char\n"
"     which may be signed or unsigned, so use __v16qs. */\n"
"  return (__m128i)((__v16qs)__a > (__v16qs)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding signed 16-bit values of the\n"
"///    128-bit integer vectors to determine if the values in the first operand\n"
"///    are greater than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPGTW / PCMPGTW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v8hi)__a > (__v8hi)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding signed 32-bit values of the\n"
"///    128-bit integer vectors to determine if the values in the first operand\n"
"///    are greater than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPGTD / PCMPGTD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)((__v4si)__a > (__v4si)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding signed 8-bit values of the 128-bit\n"
"///    integer vectors to determine if the values in the first operand are less\n"
"///    than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPGTB / PCMPGTB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmplt_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return _mm_cmpgt_epi8(__b, __a);\n"
"}\n"
"\n"
"/// Compares each of the corresponding signed 16-bit values of the\n"
"///    128-bit integer vectors to determine if the values in the first operand\n"
"///    are less than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPGTW / PCMPGTW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmplt_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return _mm_cmpgt_epi16(__b, __a);\n"
"}\n"
"\n"
"/// Compares each of the corresponding signed 32-bit values of the\n"
"///    128-bit integer vectors to determine if the values in the first operand\n"
"///    are less than those in the second operand.\n"
"///\n"
"///    Each comparison yields 0x0 for false, 0xFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPGTD / PCMPGTD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmplt_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return _mm_cmpgt_epi32(__b, __a);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Converts a 64-bit signed integer value from the second operand into a\n"
"///    double-precision value and returns it in the lower element of a [2 x\n"
"///    double] vector; the upper element of the returned vector is copied from\n"
"///    the upper element of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSI2SD / CVTSI2SD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The upper 64 bits of this operand are\n"
"///    copied to the upper 64 bits of the destination.\n"
"/// \\param __b\n"
"///    A 64-bit signed integer operand containing the value to be converted.\n"
"/// \\returns A 128-bit vector of [2 x double] whose lower 64 bits contain the\n"
"///    converted value of the second operand. The upper 64 bits are copied from\n"
"///    the upper 64 bits of the first operand.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi64_sd(__m128d __a, long long __b)\n"
"{\n"
"  __a[0] = __b;\n"
"  return __a;\n"
"}\n"
"\n"
"/// Converts the first (lower) element of a vector of [2 x double] into a\n"
"///    64-bit signed integer value, according to the current rounding mode.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSD2SI / CVTSD2SI </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the\n"
"///    conversion.\n"
"/// \\returns A 64-bit signed integer containing the converted value.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_mm_cvtsd_si64(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_cvtsd2si64((__v2df)__a);\n"
"}\n"
"\n"
"/// Converts the first (lower) element of a vector of [2 x double] into a\n"
"///    64-bit signed integer value, truncating the result when it is inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTSD2SI / CVTTSD2SI </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the\n"
"///    conversion.\n"
"/// \\returns A 64-bit signed integer containing the converted value.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_mm_cvttsd_si64(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_cvttsd2si64((__v2df)__a);\n"
"}\n"
"#endif\n"
"\n"
"/// Converts a vector of [4 x i32] into a vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTDQ2PS / CVTDQ2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the converted values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi32_ps(__m128i __a)\n"
"{\n"
"  return (__m128)__builtin_convertvector((__v4si)__a, __v4sf);\n"
"}\n"
"\n"
"/// Converts a vector of [4 x float] into a vector of [4 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPS2DQ / CVTPS2DQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit integer vector of [4 x i32] containing the converted\n"
"///    values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtps_epi32(__m128 __a)\n"
"{\n"
"  return (__m128i)__builtin_ia32_cvtps2dq((__v4sf)__a);\n"
"}\n"
"\n"
"/// Converts a vector of [4 x float] into a vector of [4 x i32],\n"
"///    truncating the result when it is inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTPS2DQ / CVTTPS2DQ </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the converted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvttps_epi32(__m128 __a)\n"
"{\n"
"  return (__m128i)__builtin_ia32_cvttps2dq((__v4sf)__a);\n"
"}\n"
"\n"
"/// Returns a vector of [4 x i32] where the lowest element is the input\n"
"///    operand and the remaining elements are zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 32-bit signed integer operand.\n"
"/// \\returns A 128-bit vector of [4 x i32].\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi32_si128(int __a)\n"
"{\n"
"  return __extension__ (__m128i)(__v4si){ __a, 0, 0, 0 };\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Returns a vector of [2 x i64] where the lower element is the input\n"
"///    operand and the upper element is zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit signed integer operand containing the value to be converted.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the converted value.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi64_si128(long long __a)\n"
"{\n"
"  return __extension__ (__m128i)(__v2di){ __a, 0 };\n"
"}\n"
"#endif\n"
"\n"
"/// Moves the least significant 32 bits of a vector of [4 x i32] to a\n"
"///    32-bit signed integer value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A vector of [4 x i32]. The least significant 32 bits are moved to the\n"
"///    destination.\n"
"/// \\returns A 32-bit signed integer containing the moved value.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi128_si32(__m128i __a)\n"
"{\n"
"  __v4si __b = (__v4si)__a;\n"
"  return __b[0];\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Moves the least significant 64 bits of a vector of [2 x i64] to a\n"
"///    64-bit signed integer value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A vector of [2 x i64]. The least significant 64 bits are moved to the\n"
"///    destination.\n"
"/// \\returns A 64-bit signed integer containing the moved value.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi128_si64(__m128i __a)\n"
"{\n"
"  return __a[0];\n"
"}\n"
"#endif\n"
"\n"
"/// Moves packed integer values from an aligned 128-bit memory location\n"
"///    to elements in a 128-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDQA / MOVDQA </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    An aligned pointer to a memory location containing integer values.\n"
"/// \\returns A 128-bit integer vector containing the moved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_load_si128(__m128i const *__p)\n"
"{\n"
"  return *__p;\n"
"}\n"
"\n"
"/// Moves packed integer values from an unaligned 128-bit memory location\n"
"///    to elements in a 128-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDQU / MOVDQU </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location containing integer values.\n"
"/// \\returns A 128-bit integer vector containing the moved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_loadu_si128(__m128i const *__p)\n"
"{\n"
"  struct __loadu_si128 {\n"
"    __m128i __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return ((struct __loadu_si128*)__p)->__v;\n"
"}\n"
"\n"
"/// Returns a vector of [2 x i64] where the lower element is taken from\n"
"///    the lower element of the operand, and the upper element is zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A 128-bit vector of [2 x i64]. Bits [63:0] are written to bits [63:0] of\n"
"///    the destination.\n"
"/// \\returns A 128-bit vector of [2 x i64]. The lower order bits contain the\n"
"///    moved value. The higher order bits are cleared.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_loadl_epi64(__m128i const *__p)\n"
"{\n"
"  struct __mm_loadl_epi64_struct {\n"
"    long long __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return __extension__ (__m128i) { ((struct __mm_loadl_epi64_struct*)__p)->__u, 0};\n"
"}\n"
"\n"
"/// Generates a 128-bit vector of [4 x i32] with unspecified content.\n"
"///    This could be used as an argument to another intrinsic function where the\n"
"///    argument is required but the value is not actually used.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\returns A 128-bit vector of [4 x i32] with unspecified content.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_undefined_si128(void)\n"
"{\n"
"  return (__m128i)__builtin_ia32_undef128();\n"
"}\n"
"\n"
"/// Initializes both 64-bit values in a 128-bit vector of [2 x i64] with\n"
"///    the specified 64-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __q1\n"
"///    A 64-bit integer value used to initialize the upper 64 bits of the\n"
"///    destination vector of [2 x i64].\n"
"/// \\param __q0\n"
"///    A 64-bit integer value used to initialize the lower 64 bits of the\n"
"///    destination vector of [2 x i64].\n"
"/// \\returns An initialized 128-bit vector of [2 x i64] containing the values\n"
"///    provided in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set_epi64x(long long __q1, long long __q0)\n"
"{\n"
"  return __extension__ (__m128i)(__v2di){ __q0, __q1 };\n"
"}\n"
"\n"
"/// Initializes both 64-bit values in a 128-bit vector of [2 x i64] with\n"
"///    the specified 64-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __q1\n"
"///    A 64-bit integer value used to initialize the upper 64 bits of the\n"
"///    destination vector of [2 x i64].\n"
"/// \\param __q0\n"
"///    A 64-bit integer value used to initialize the lower 64 bits of the\n"
"///    destination vector of [2 x i64].\n"
"/// \\returns An initialized 128-bit vector of [2 x i64] containing the values\n"
"///    provided in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set_epi64(__m64 __q1, __m64 __q0)\n"
"{\n"
"  return _mm_set_epi64x((long long)__q1, (long long)__q0);\n"
"}\n"
"\n"
"/// Initializes the 32-bit values in a 128-bit vector of [4 x i32] with\n"
"///    the specified 32-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __i3\n"
"///    A 32-bit integer value used to initialize bits [127:96] of the\n"
"///    destination vector.\n"
"/// \\param __i2\n"
"///    A 32-bit integer value used to initialize bits [95:64] of the destination\n"
"///    vector.\n"
"/// \\param __i1\n"
"///    A 32-bit integer value used to initialize bits [63:32] of the destination\n"
"///    vector.\n"
"/// \\param __i0\n"
"///    A 32-bit integer value used to initialize bits [31:0] of the destination\n"
"///    vector.\n"
"/// \\returns An initialized 128-bit vector of [4 x i32] containing the values\n"
"///    provided in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set_epi32(int __i3, int __i2, int __i1, int __i0)\n"
"{\n"
"  return __extension__ (__m128i)(__v4si){ __i0, __i1, __i2, __i3};\n"
"}\n"
"\n"
"/// Initializes the 16-bit values in a 128-bit vector of [8 x i16] with\n"
"///    the specified 16-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __w7\n"
"///    A 16-bit integer value used to initialize bits [127:112] of the\n"
"///    destination vector.\n"
"/// \\param __w6\n"
"///    A 16-bit integer value used to initialize bits [111:96] of the\n"
"///    destination vector.\n"
"/// \\param __w5\n"
"///    A 16-bit integer value used to initialize bits [95:80] of the destination\n"
"///    vector.\n"
"/// \\param __w4\n"
"///    A 16-bit integer value used to initialize bits [79:64] of the destination\n"
"///    vector.\n"
"/// \\param __w3\n"
"///    A 16-bit integer value used to initialize bits [63:48] of the destination\n"
"///    vector.\n"
"/// \\param __w2\n"
"///    A 16-bit integer value used to initialize bits [47:32] of the destination\n"
"///    vector.\n"
"/// \\param __w1\n"
"///    A 16-bit integer value used to initialize bits [31:16] of the destination\n"
"///    vector.\n"
"/// \\param __w0\n"
"///    A 16-bit integer value used to initialize bits [15:0] of the destination\n"
"///    vector.\n"
"/// \\returns An initialized 128-bit vector of [8 x i16] containing the values\n"
"///    provided in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set_epi16(short __w7, short __w6, short __w5, short __w4, short __w3, short __w2, short __w1, short __w0)\n"
"{\n"
"  return __extension__ (__m128i)(__v8hi){ __w0, __w1, __w2, __w3, __w4, __w5, __w6, __w7 };\n"
"}\n"
"\n"
"/// Initializes the 8-bit values in a 128-bit vector of [16 x i8] with\n"
"///    the specified 8-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __b15\n"
"///    Initializes bits [127:120] of the destination vector.\n"
"/// \\param __b14\n"
"///    Initializes bits [119:112] of the destination vector.\n"
"/// \\param __b13\n"
"///    Initializes bits [111:104] of the destination vector.\n"
"/// \\param __b12\n"
"///    Initializes bits [103:96] of the destination vector.\n"
"/// \\param __b11\n"
"///    Initializes bits [95:88] of the destination vector.\n"
"/// \\param __b10\n"
"///    Initializes bits [87:80] of the destination vector.\n"
"/// \\param __b9\n"
"///    Initializes bits [79:72] of the destination vector.\n"
"/// \\param __b8\n"
"///    Initializes bits [71:64] of the destination vector.\n"
"/// \\param __b7\n"
"///    Initializes bits [63:56] of the destination vector.\n"
"/// \\param __b6\n"
"///    Initializes bits [55:48] of the destination vector.\n"
"/// \\param __b5\n"
"///    Initializes bits [47:40] of the destination vector.\n"
"/// \\param __b4\n"
"///    Initializes bits [39:32] of the destination vector.\n"
"/// \\param __b3\n"
"///    Initializes bits [31:24] of the destination vector.\n"
"/// \\param __b2\n"
"///    Initializes bits [23:16] of the destination vector.\n"
"/// \\param __b1\n"
"///    Initializes bits [15:8] of the destination vector.\n"
"/// \\param __b0\n"
"///    Initializes bits [7:0] of the destination vector.\n"
"/// \\returns An initialized 128-bit vector of [16 x i8] containing the values\n"
"///    provided in the operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set_epi8(char __b15, char __b14, char __b13, char __b12, char __b11, char __b10, char __b9, char __b8, char __b7, char __b6, char __b5, char __b4, char __b3, char __b2, char __b1, char __b0)\n"
"{\n"
"  return __extension__ (__m128i)(__v16qi){ __b0, __b1, __b2, __b3, __b4, __b5, __b6, __b7, __b8, __b9, __b10, __b11, __b12, __b13, __b14, __b15 };\n"
"}\n"
"\n"
"/// Initializes both values in a 128-bit integer vector with the\n"
"///    specified 64-bit integer value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __q\n"
"///    Integer value used to initialize the elements of the destination integer\n"
"///    vector.\n"
"/// \\returns An initialized 128-bit integer vector of [2 x i64] with both\n"
"///    elements containing the value provided in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set1_epi64x(long long __q)\n"
"{\n"
"  return _mm_set_epi64x(__q, __q);\n"
"}\n"
"\n"
"/// Initializes both values in a 128-bit vector of [2 x i64] with the\n"
"///    specified 64-bit value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __q\n"
"///    A 64-bit value used to initialize the elements of the destination integer\n"
"///    vector.\n"
"/// \\returns An initialized 128-bit vector of [2 x i64] with all elements\n"
"///    containing the value provided in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set1_epi64(__m64 __q)\n"
"{\n"
"  return _mm_set_epi64(__q, __q);\n"
"}\n"
"\n"
"/// Initializes all values in a 128-bit vector of [4 x i32] with the\n"
"///    specified 32-bit value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __i\n"
"///    A 32-bit value used to initialize the elements of the destination integer\n"
"///    vector.\n"
"/// \\returns An initialized 128-bit vector of [4 x i32] with all elements\n"
"///    containing the value provided in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set1_epi32(int __i)\n"
"{\n"
"  return _mm_set_epi32(__i, __i, __i, __i);\n"
"}\n"
"\n"
"/// Initializes all values in a 128-bit vector of [8 x i16] with the\n"
"///    specified 16-bit value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A 16-bit value used to initialize the elements of the destination integer\n"
"///    vector.\n"
"/// \\returns An initialized 128-bit vector of [8 x i16] with all elements\n"
"///    containing the value provided in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set1_epi16(short __w)\n"
"{\n"
"  return _mm_set_epi16(__w, __w, __w, __w, __w, __w, __w, __w);\n"
"}\n"
"\n"
"/// Initializes all values in a 128-bit vector of [16 x i8] with the\n"
"///    specified 8-bit value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __b\n"
"///    An 8-bit value used to initialize the elements of the destination integer\n"
"///    vector.\n"
"/// \\returns An initialized 128-bit vector of [16 x i8] with all elements\n"
"///    containing the value provided in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_set1_epi8(char __b)\n"
"{\n"
"  return _mm_set_epi8(__b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b);\n"
"}\n"
"\n"
"/// Constructs a 128-bit integer vector, initialized in reverse order\n"
"///     with the specified 64-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic does not correspond to a specific instruction.\n"
"///\n"
"/// \\param __q0\n"
"///    A 64-bit integral value used to initialize the lower 64 bits of the\n"
"///    result.\n"
"/// \\param __q1\n"
"///    A 64-bit integral value used to initialize the upper 64 bits of the\n"
"///    result.\n"
"/// \\returns An initialized 128-bit integer vector.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_setr_epi64(__m64 __q0, __m64 __q1)\n"
"{\n"
"  return _mm_set_epi64(__q1, __q0);\n"
"}\n"
"\n"
"/// Constructs a 128-bit integer vector, initialized in reverse order\n"
"///     with the specified 32-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __i0\n"
"///    A 32-bit integral value used to initialize bits [31:0] of the result.\n"
"/// \\param __i1\n"
"///    A 32-bit integral value used to initialize bits [63:32] of the result.\n"
"/// \\param __i2\n"
"///    A 32-bit integral value used to initialize bits [95:64] of the result.\n"
"/// \\param __i3\n"
"///    A 32-bit integral value used to initialize bits [127:96] of the result.\n"
"/// \\returns An initialized 128-bit integer vector.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_setr_epi32(int __i0, int __i1, int __i2, int __i3)\n"
"{\n"
"  return _mm_set_epi32(__i3, __i2, __i1, __i0);\n"
"}\n"
"\n"
"/// Constructs a 128-bit integer vector, initialized in reverse order\n"
"///     with the specified 16-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __w0\n"
"///    A 16-bit integral value used to initialize bits [15:0] of the result.\n"
"/// \\param __w1\n"
"///    A 16-bit integral value used to initialize bits [31:16] of the result.\n"
"/// \\param __w2\n"
"///    A 16-bit integral value used to initialize bits [47:32] of the result.\n"
"/// \\param __w3\n"
"///    A 16-bit integral value used to initialize bits [63:48] of the result.\n"
"/// \\param __w4\n"
"///    A 16-bit integral value used to initialize bits [79:64] of the result.\n"
"/// \\param __w5\n"
"///    A 16-bit integral value used to initialize bits [95:80] of the result.\n"
"/// \\param __w6\n"
"///    A 16-bit integral value used to initialize bits [111:96] of the result.\n"
"/// \\param __w7\n"
"///    A 16-bit integral value used to initialize bits [127:112] of the result.\n"
"/// \\returns An initialized 128-bit integer vector.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_setr_epi16(short __w0, short __w1, short __w2, short __w3, short __w4, short __w5, short __w6, short __w7)\n"
"{\n"
"  return _mm_set_epi16(__w7, __w6, __w5, __w4, __w3, __w2, __w1, __w0);\n"
"}\n"
"\n"
"/// Constructs a 128-bit integer vector, initialized in reverse order\n"
"///     with the specified 8-bit integral values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __b0\n"
"///    An 8-bit integral value used to initialize bits [7:0] of the result.\n"
"/// \\param __b1\n"
"///    An 8-bit integral value used to initialize bits [15:8] of the result.\n"
"/// \\param __b2\n"
"///    An 8-bit integral value used to initialize bits [23:16] of the result.\n"
"/// \\param __b3\n"
"///    An 8-bit integral value used to initialize bits [31:24] of the result.\n"
"/// \\param __b4\n"
"///    An 8-bit integral value used to initialize bits [39:32] of the result.\n"
"/// \\param __b5\n"
"///    An 8-bit integral value used to initialize bits [47:40] of the result.\n"
"/// \\param __b6\n"
"///    An 8-bit integral value used to initialize bits [55:48] of the result.\n"
"/// \\param __b7\n"
"///    An 8-bit integral value used to initialize bits [63:56] of the result.\n"
"/// \\param __b8\n"
"///    An 8-bit integral value used to initialize bits [71:64] of the result.\n"
"/// \\param __b9\n"
"///    An 8-bit integral value used to initialize bits [79:72] of the result.\n"
"/// \\param __b10\n"
"///    An 8-bit integral value used to initialize bits [87:80] of the result.\n"
"/// \\param __b11\n"
"///    An 8-bit integral value used to initialize bits [95:88] of the result.\n"
"/// \\param __b12\n"
"///    An 8-bit integral value used to initialize bits [103:96] of the result.\n"
"/// \\param __b13\n"
"///    An 8-bit integral value used to initialize bits [111:104] of the result.\n"
"/// \\param __b14\n"
"///    An 8-bit integral value used to initialize bits [119:112] of the result.\n"
"/// \\param __b15\n"
"///    An 8-bit integral value used to initialize bits [127:120] of the result.\n"
"/// \\returns An initialized 128-bit integer vector.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_setr_epi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5, char __b6, char __b7, char __b8, char __b9, char __b10, char __b11, char __b12, char __b13, char __b14, char __b15)\n"
"{\n"
"  return _mm_set_epi8(__b15, __b14, __b13, __b12, __b11, __b10, __b9, __b8, __b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);\n"
"}\n"
"\n"
"/// Creates a 128-bit integer vector initialized to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instruction.\n"
"///\n"
"/// \\returns An initialized 128-bit integer vector with all elements set to\n"
"///    zero.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_setzero_si128(void)\n"
"{\n"
"  return __extension__ (__m128i)(__v2di){ 0LL, 0LL };\n"
"}\n"
"\n"
"/// Stores a 128-bit integer vector to a memory location aligned on a\n"
"///    128-bit boundary.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to an aligned memory location that will receive the integer\n"
"///    values.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the values to be moved.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store_si128(__m128i *__p, __m128i __b)\n"
"{\n"
"  *__p = __b;\n"
"}\n"
"\n"
"/// Stores a 128-bit integer vector to an unaligned memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPS / MOVUPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the integer values.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the values to be moved.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeu_si128(__m128i *__p, __m128i __b)\n"
"{\n"
"  struct __storeu_si128 {\n"
"    __m128i __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_si128*)__p)->__v = __b;\n"
"}\n"
"\n"
"/// Stores a 64-bit integer value from the low element of a 128-bit integer\n"
"///    vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 64-bit memory location. The address of the memory\n"
"///    location does not have to be algned.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeu_si64(void const *__p, __m128i __b)\n"
"{\n"
"  struct __storeu_si64 {\n"
"    long long __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_si64*)__p)->__v = ((__v2di)__b)[0];\n"
"}\n"
"\n"
"/// Stores a 32-bit integer value from the low element of a 128-bit integer\n"
"///    vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 32-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeu_si32(void const *__p, __m128i __b)\n"
"{\n"
"  struct __storeu_si32 {\n"
"    int __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_si32*)__p)->__v = ((__v4si)__b)[0];\n"
"}\n"
"\n"
"/// Stores a 16-bit integer value from the low element of a 128-bit integer\n"
"///    vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic does not correspond to a specific instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 16-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeu_si16(void const *__p, __m128i __b)\n"
"{\n"
"  struct __storeu_si16 {\n"
"    short __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_si16*)__p)->__v = ((__v8hi)__b)[0];\n"
"}\n"
"\n"
"/// Moves bytes selected by the mask from the first operand to the\n"
"///    specified unaligned memory location. When a mask bit is 1, the\n"
"///    corresponding byte is written, otherwise it is not written.\n"
"///\n"
"///    To minimize caching, the data is flagged as non-temporal (unlikely to be\n"
"///    used again soon). Exception and trap behavior for elements not selected\n"
"///    for storage to memory are implementation dependent.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMASKMOVDQU / MASKMOVDQU </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __d\n"
"///    A 128-bit integer vector containing the values to be moved.\n"
"/// \\param __n\n"
"///    A 128-bit integer vector containing the mask. The most significant bit of\n"
"///    each byte represents the mask bits.\n"
"/// \\param __p\n"
"///    A pointer to an unaligned 128-bit memory location where the specified\n"
"///    values are moved.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_maskmoveu_si128(__m128i __d, __m128i __n, char *__p)\n"
"{\n"
"  __builtin_ia32_maskmovdqu((__v16qi)__d, (__v16qi)__n, __p);\n"
"}\n"
"\n"
"/// Stores the lower 64 bits of a 128-bit integer vector of [2 x i64] to\n"
"///    a memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVLPS / MOVLPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 64-bit memory location that will receive the lower 64 bits\n"
"///    of the integer vector parameter.\n"
"/// \\param __a\n"
"///    A 128-bit integer vector of [2 x i64]. The lower 64 bits contain the\n"
"///    value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storel_epi64(__m128i *__p, __m128i __a)\n"
"{\n"
"  struct __mm_storel_epi64_struct {\n"
"    long long __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __mm_storel_epi64_struct*)__p)->__u = __a[0];\n"
"}\n"
"\n"
"/// Stores a 128-bit floating point vector of [2 x double] to a 128-bit\n"
"///    aligned memory location.\n"
"///\n"
"///    To minimize caching, the data is flagged as non-temporal (unlikely to be\n"
"///    used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVNTPS / MOVNTPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to the 128-bit aligned memory location used to store the value.\n"
"/// \\param __a\n"
"///    A vector of [2 x double] containing the 64-bit values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_stream_pd(double *__p, __m128d __a)\n"
"{\n"
"  __builtin_nontemporal_store((__v2df)__a, (__v2df*)__p);\n"
"}\n"
"\n"
"/// Stores a 128-bit integer vector to a 128-bit aligned memory location.\n"
"///\n"
"///    To minimize caching, the data is flagged as non-temporal (unlikely to be\n"
"///    used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVNTPS / MOVNTPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to the 128-bit aligned memory location used to store the value.\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_stream_si128(__m128i *__p, __m128i __a)\n"
"{\n"
"  __builtin_nontemporal_store((__v2di)__a, (__v2di*)__p);\n"
"}\n"
"\n"
"/// Stores a 32-bit integer value in the specified memory location.\n"
"///\n"
"///    To minimize caching, the data is flagged as non-temporal (unlikely to be\n"
"///    used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVNTI </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to the 32-bit memory location used to store the value.\n"
"/// \\param __a\n"
"///    A 32-bit integer containing the value to be stored.\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"sse2\")))\n"
"_mm_stream_si32(int *__p, int __a)\n"
"{\n"
"  __builtin_ia32_movnti(__p, __a);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Stores a 64-bit integer value in the specified memory location.\n"
"///\n"
"///    To minimize caching, the data is flagged as non-temporal (unlikely to be\n"
"///    used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVNTIQ </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to the 64-bit memory location used to store the value.\n"
"/// \\param __a\n"
"///    A 64-bit integer containing the value to be stored.\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"sse2\")))\n"
"_mm_stream_si64(long long *__p, long long __a)\n"
"{\n"
"  __builtin_ia32_movnti64(__p, __a);\n"
"}\n"
"#endif\n"
"\n"
"#if defined(__cplusplus)\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"/// The cache line containing \\a __p is flushed and invalidated from all\n"
"///    caches in the coherency domain.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CLFLUSH </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to the memory location used to identify the cache line to be\n"
"///    flushed.\n"
"void _mm_clflush(void const * __p);\n"
"\n"
"/// Forces strong memory ordering (serialization) between load\n"
"///    instructions preceding this instruction and load instructions following\n"
"///    this instruction, ensuring the system completes all previous loads before\n"
"///    executing subsequent loads.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> LFENCE </c> instruction.\n"
"///\n"
"void _mm_lfence(void);\n"
"\n"
"/// Forces strong memory ordering (serialization) between load and store\n"
"///    instructions preceding this instruction and load and store instructions\n"
"///    following this instruction, ensuring that the system completes all\n"
"///    previous memory accesses before executing subsequent memory accesses.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MFENCE </c> instruction.\n"
"///\n"
"void _mm_mfence(void);\n"
"\n"
"#if defined(__cplusplus)\n"
"} // extern \"C\"\n"
"#endif\n"
"\n"
"/// Converts 16-bit signed integers from both 128-bit integer vector\n"
"///    operands into 8-bit signed integers, and packs the results into the\n"
"///    destination. Positive values greater than 0x7F are saturated to 0x7F.\n"
"///    Negative values less than 0x80 are saturated to 0x80.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPACKSSWB / PACKSSWB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///   A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as\n"
"///   a signed integer and is converted to a 8-bit signed integer with\n"
"///   saturation. Values greater than 0x7F are saturated to 0x7F. Values less\n"
"///   than 0x80 are saturated to 0x80. The converted [8 x i8] values are\n"
"///   written to the lower 64 bits of the result.\n"
"/// \\param __b\n"
"///   A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as\n"
"///   a signed integer and is converted to a 8-bit signed integer with\n"
"///   saturation. Values greater than 0x7F are saturated to 0x7F. Values less\n"
"///   than 0x80 are saturated to 0x80. The converted [8 x i8] values are\n"
"///   written to the higher 64 bits of the result.\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the converted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_packs_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_packsswb128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Converts 32-bit signed integers from both 128-bit integer vector\n"
"///    operands into 16-bit signed integers, and packs the results into the\n"
"///    destination. Positive values greater than 0x7FFF are saturated to 0x7FFF.\n"
"///    Negative values less than 0x8000 are saturated to 0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPACKSSDW / PACKSSDW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector of [4 x i32]. Each 32-bit element is treated as\n"
"///    a signed integer and is converted to a 16-bit signed integer with\n"
"///    saturation. Values greater than 0x7FFF are saturated to 0x7FFF. Values\n"
"///    less than 0x8000 are saturated to 0x8000. The converted [4 x i16] values\n"
"///    are written to the lower 64 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector of [4 x i32]. Each 32-bit element is treated as\n"
"///    a signed integer and is converted to a 16-bit signed integer with\n"
"///    saturation. Values greater than 0x7FFF are saturated to 0x7FFF. Values\n"
"///    less than 0x8000 are saturated to 0x8000. The converted [4 x i16] values\n"
"///    are written to the higher 64 bits of the result.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the converted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_packs_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_packssdw128((__v4si)__a, (__v4si)__b);\n"
"}\n"
"\n"
"/// Converts 16-bit signed integers from both 128-bit integer vector\n"
"///    operands into 8-bit unsigned integers, and packs the results into the\n"
"///    destination. Values greater than 0xFF are saturated to 0xFF. Values less\n"
"///    than 0x00 are saturated to 0x00.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPACKUSWB / PACKUSWB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as\n"
"///    a signed integer and is converted to an 8-bit unsigned integer with\n"
"///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less\n"
"///    than 0x00 are saturated to 0x00. The converted [8 x i8] values are\n"
"///    written to the lower 64 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as\n"
"///    a signed integer and is converted to an 8-bit unsigned integer with\n"
"///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less\n"
"///    than 0x00 are saturated to 0x00. The converted [8 x i8] values are\n"
"///    written to the higher 64 bits of the result.\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the converted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_packus_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_ia32_packuswb128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Extracts 16 bits from a 128-bit integer vector of [8 x i16], using\n"
"///    the immediate-value parameter as a selector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPEXTRW / PEXTRW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\param __imm\n"
"///    An immediate value. Bits [2:0] selects values from \\a __a to be assigned\n"
"///    to bits[15:0] of the result. \\n\n"
"///    000: assign values from bits [15:0] of \\a __a. \\n\n"
"///    001: assign values from bits [31:16] of \\a __a. \\n\n"
"///    010: assign values from bits [47:32] of \\a __a. \\n\n"
"///    011: assign values from bits [63:48] of \\a __a. \\n\n"
"///    100: assign values from bits [79:64] of \\a __a. \\n\n"
"///    101: assign values from bits [95:80] of \\a __a. \\n\n"
"///    110: assign values from bits [111:96] of \\a __a. \\n\n"
"///    111: assign values from bits [127:112] of \\a __a.\n"
"/// \\returns An integer, whose lower 16 bits are selected from the 128-bit\n"
"///    integer vector parameter and the remaining bits are assigned zeros.\n"
"#define _mm_extract_epi16(a, imm) \\\n"
"  (int)(unsigned short)__builtin_ia32_vec_ext_v8hi((__v8hi)(__m128i)(a), \\\n"
"                                                   (int)(imm))\n"
"\n"
"/// Constructs a 128-bit integer vector by first making a copy of the\n"
"///    128-bit integer vector parameter, and then inserting the lower 16 bits\n"
"///    of an integer parameter into an offset specified by the immediate-value\n"
"///    parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPINSRW / PINSRW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector of [8 x i16]. This vector is copied to the\n"
"///    result and then one of the eight elements in the result is replaced by\n"
"///    the lower 16 bits of \\a __b.\n"
"/// \\param __b\n"
"///    An integer. The lower 16 bits of this parameter are written to the\n"
"///    result beginning at an offset specified by \\a __imm.\n"
"/// \\param __imm\n"
"///    An immediate value specifying the bit offset in the result at which the\n"
"///    lower 16 bits of \\a __b are written.\n"
"/// \\returns A 128-bit integer vector containing the constructed values.\n"
"#define _mm_insert_epi16(a, b, imm) \\\n"
"  (__m128i)__builtin_ia32_vec_set_v8hi((__v8hi)(__m128i)(a), (int)(b), \\\n"
"                                       (int)(imm))\n"
"\n"
"/// Copies the values of the most significant bits from each 8-bit\n"
"///    element in a 128-bit integer vector of [16 x i8] to create a 16-bit mask\n"
"///    value, zero-extends the value, and writes it to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVMSKB / PMOVMSKB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the values with bits to be extracted.\n"
"/// \\returns The most significant bits from each 8-bit element in \\a __a,\n"
"///    written to bits [15:0]. The other bits are assigned zeros.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_movemask_epi8(__m128i __a)\n"
"{\n"
"  return __builtin_ia32_pmovmskb128((__v16qi)__a);\n"
"}\n"
"\n"
"/// Constructs a 128-bit integer vector by shuffling four 32-bit\n"
"///    elements of a 128-bit integer vector parameter, using the immediate-value\n"
"///    parameter as a specifier.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_shuffle_epi32(__m128i a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSHUFD / PSHUFD </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit integer vector containing the values to be copied.\n"
"/// \\param imm\n"
"///    An immediate value containing an 8-bit value specifying which elements to\n"
"///    copy from a. The destinations within the 128-bit destination are assigned\n"
"///    values as follows: \\n\n"
"///    Bits [1:0] are used to assign values to bits [31:0] of the result. \\n\n"
"///    Bits [3:2] are used to assign values to bits [63:32] of the result. \\n\n"
"///    Bits [5:4] are used to assign values to bits [95:64] of the result. \\n\n"
"///    Bits [7:6] are used to assign values to bits [127:96] of the result. \\n\n"
"///    Bit value assignments: \\n\n"
"///    00: assign values from bits [31:0] of \\a a. \\n\n"
"///    01: assign values from bits [63:32] of \\a a. \\n\n"
"///    10: assign values from bits [95:64] of \\a a. \\n\n"
"///    11: assign values from bits [127:96] of \\a a.\n"
"/// \\returns A 128-bit integer vector containing the shuffled values.\n"
"#define _mm_shuffle_epi32(a, imm) \\\n"
"  (__m128i)__builtin_ia32_pshufd((__v4si)(__m128i)(a), (int)(imm))\n"
"\n"
"/// Constructs a 128-bit integer vector by shuffling four lower 16-bit\n"
"///    elements of a 128-bit integer vector of [8 x i16], using the immediate\n"
"///    value parameter as a specifier.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_shufflelo_epi16(__m128i a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSHUFLW / PSHUFLW </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit integer vector of [8 x i16]. Bits [127:64] are copied to bits\n"
"///    [127:64] of the result.\n"
"/// \\param imm\n"
"///    An 8-bit immediate value specifying which elements to copy from \\a a. \\n\n"
"///    Bits[1:0] are used to assign values to bits [15:0] of the result. \\n\n"
"///    Bits[3:2] are used to assign values to bits [31:16] of the result. \\n\n"
"///    Bits[5:4] are used to assign values to bits [47:32] of the result. \\n\n"
"///    Bits[7:6] are used to assign values to bits [63:48] of the result. \\n\n"
"///    Bit value assignments: \\n\n"
"///    00: assign values from bits [15:0] of \\a a. \\n\n"
"///    01: assign values from bits [31:16] of \\a a. \\n\n"
"///    10: assign values from bits [47:32] of \\a a. \\n\n"
"///    11: assign values from bits [63:48] of \\a a. \\n\n"
"/// \\returns A 128-bit integer vector containing the shuffled values.\n"
"#define _mm_shufflelo_epi16(a, imm) \\\n"
"  (__m128i)__builtin_ia32_pshuflw((__v8hi)(__m128i)(a), (int)(imm))\n"
"\n"
"/// Constructs a 128-bit integer vector by shuffling four upper 16-bit\n"
"///    elements of a 128-bit integer vector of [8 x i16], using the immediate\n"
"///    value parameter as a specifier.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_shufflehi_epi16(__m128i a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPSHUFHW / PSHUFHW </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit integer vector of [8 x i16]. Bits [63:0] are copied to bits\n"
"///    [63:0] of the result.\n"
"/// \\param imm\n"
"///    An 8-bit immediate value specifying which elements to copy from \\a a. \\n\n"
"///    Bits[1:0] are used to assign values to bits [79:64] of the result. \\n\n"
"///    Bits[3:2] are used to assign values to bits [95:80] of the result. \\n\n"
"///    Bits[5:4] are used to assign values to bits [111:96] of the result. \\n\n"
"///    Bits[7:6] are used to assign values to bits [127:112] of the result. \\n\n"
"///    Bit value assignments: \\n\n"
"///    00: assign values from bits [79:64] of \\a a. \\n\n"
"///    01: assign values from bits [95:80] of \\a a. \\n\n"
"///    10: assign values from bits [111:96] of \\a a. \\n\n"
"///    11: assign values from bits [127:112] of \\a a. \\n\n"
"/// \\returns A 128-bit integer vector containing the shuffled values.\n"
"#define _mm_shufflehi_epi16(a, imm) \\\n"
"  (__m128i)__builtin_ia32_pshufhw((__v8hi)(__m128i)(a), (int)(imm))\n"
"\n"
"/// Unpacks the high-order (index 8-15) values from two 128-bit vectors\n"
"///    of [16 x i8] and interleaves them into a 128-bit vector of [16 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKHBW / PUNPCKHBW </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [16 x i8].\n"
"///    Bits [71:64] are written to bits [7:0] of the result. \\n\n"
"///    Bits [79:72] are written to bits [23:16] of the result. \\n\n"
"///    Bits [87:80] are written to bits [39:32] of the result. \\n\n"
"///    Bits [95:88] are written to bits [55:48] of the result. \\n\n"
"///    Bits [103:96] are written to bits [71:64] of the result. \\n\n"
"///    Bits [111:104] are written to bits [87:80] of the result. \\n\n"
"///    Bits [119:112] are written to bits [103:96] of the result. \\n\n"
"///    Bits [127:120] are written to bits [119:112] of the result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [16 x i8]. \\n\n"
"///    Bits [71:64] are written to bits [15:8] of the result. \\n\n"
"///    Bits [79:72] are written to bits [31:24] of the result. \\n\n"
"///    Bits [87:80] are written to bits [47:40] of the result. \\n\n"
"///    Bits [95:88] are written to bits [63:56] of the result. \\n\n"
"///    Bits [103:96] are written to bits [79:72] of the result. \\n\n"
"///    Bits [111:104] are written to bits [95:88] of the result. \\n\n"
"///    Bits [119:112] are written to bits [111:104] of the result. \\n\n"
"///    Bits [127:120] are written to bits [127:120] of the result.\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);\n"
"}\n"
"\n"
"/// Unpacks the high-order (index 4-7) values from two 128-bit vectors of\n"
"///    [8 x i16] and interleaves them into a 128-bit vector of [8 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKHWD / PUNPCKHWD </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16].\n"
"///    Bits [79:64] are written to bits [15:0] of the result. \\n\n"
"///    Bits [95:80] are written to bits [47:32] of the result. \\n\n"
"///    Bits [111:96] are written to bits [79:64] of the result. \\n\n"
"///    Bits [127:112] are written to bits [111:96] of the result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16].\n"
"///    Bits [79:64] are written to bits [31:16] of the result. \\n\n"
"///    Bits [95:80] are written to bits [63:48] of the result. \\n\n"
"///    Bits [111:96] are written to bits [95:80] of the result. \\n\n"
"///    Bits [127:112] are written to bits [127:112] of the result.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 4, 8+4, 5, 8+5, 6, 8+6, 7, 8+7);\n"
"}\n"
"\n"
"/// Unpacks the high-order (index 2,3) values from two 128-bit vectors of\n"
"///    [4 x i32] and interleaves them into a 128-bit vector of [4 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKHDQ / PUNPCKHDQ </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x i32]. \\n\n"
"///    Bits [95:64] are written to bits [31:0] of the destination. \\n\n"
"///    Bits [127:96] are written to bits [95:64] of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x i32]. \\n\n"
"///    Bits [95:64] are written to bits [64:32] of the destination. \\n\n"
"///    Bits [127:96] are written to bits [127:96] of the destination.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 2, 4+2, 3, 4+3);\n"
"}\n"
"\n"
"/// Unpacks the high-order 64-bit elements from two 128-bit vectors of\n"
"///    [2 x i64] and interleaves them into a 128-bit vector of [2 x i64].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKHQDQ / PUNPCKHQDQ </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x i64]. \\n\n"
"///    Bits [127:64] are written to bits [63:0] of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x i64]. \\n\n"
"///    Bits [127:64] are written to bits [127:64] of the destination.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_epi64(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 1, 2+1);\n"
"}\n"
"\n"
"/// Unpacks the low-order (index 0-7) values from two 128-bit vectors of\n"
"///    [16 x i8] and interleaves them into a 128-bit vector of [16 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKLBW / PUNPCKLBW </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [16 x i8]. \\n\n"
"///    Bits [7:0] are written to bits [7:0] of the result. \\n\n"
"///    Bits [15:8] are written to bits [23:16] of the result. \\n\n"
"///    Bits [23:16] are written to bits [39:32] of the result. \\n\n"
"///    Bits [31:24] are written to bits [55:48] of the result. \\n\n"
"///    Bits [39:32] are written to bits [71:64] of the result. \\n\n"
"///    Bits [47:40] are written to bits [87:80] of the result. \\n\n"
"///    Bits [55:48] are written to bits [103:96] of the result. \\n\n"
"///    Bits [63:56] are written to bits [119:112] of the result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [16 x i8].\n"
"///    Bits [7:0] are written to bits [15:8] of the result. \\n\n"
"///    Bits [15:8] are written to bits [31:24] of the result. \\n\n"
"///    Bits [23:16] are written to bits [47:40] of the result. \\n\n"
"///    Bits [31:24] are written to bits [63:56] of the result. \\n\n"
"///    Bits [39:32] are written to bits [79:72] of the result. \\n\n"
"///    Bits [47:40] are written to bits [95:88] of the result. \\n\n"
"///    Bits [55:48] are written to bits [111:104] of the result. \\n\n"
"///    Bits [63:56] are written to bits [127:120] of the result.\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7);\n"
"}\n"
"\n"
"/// Unpacks the low-order (index 0-3) values from each of the two 128-bit\n"
"///    vectors of [8 x i16] and interleaves them into a 128-bit vector of\n"
"///    [8 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKLWD / PUNPCKLWD </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16].\n"
"///    Bits [15:0] are written to bits [15:0] of the result. \\n\n"
"///    Bits [31:16] are written to bits [47:32] of the result. \\n\n"
"///    Bits [47:32] are written to bits [79:64] of the result. \\n\n"
"///    Bits [63:48] are written to bits [111:96] of the result.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16].\n"
"///    Bits [15:0] are written to bits [31:16] of the result. \\n\n"
"///    Bits [31:16] are written to bits [63:48] of the result. \\n\n"
"///    Bits [47:32] are written to bits [95:80] of the result. \\n\n"
"///    Bits [63:48] are written to bits [127:112] of the result.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 0, 8+0, 1, 8+1, 2, 8+2, 3, 8+3);\n"
"}\n"
"\n"
"/// Unpacks the low-order (index 0,1) values from two 128-bit vectors of\n"
"///    [4 x i32] and interleaves them into a 128-bit vector of [4 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKLDQ / PUNPCKLDQ </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x i32]. \\n\n"
"///    Bits [31:0] are written to bits [31:0] of the destination. \\n\n"
"///    Bits [63:32] are written to bits [95:64] of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x i32]. \\n\n"
"///    Bits [31:0] are written to bits [64:32] of the destination. \\n\n"
"///    Bits [63:32] are written to bits [127:96] of the destination.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 0, 4+0, 1, 4+1);\n"
"}\n"
"\n"
"/// Unpacks the low-order 64-bit elements from two 128-bit vectors of\n"
"///    [2 x i64] and interleaves them into a 128-bit vector of [2 x i64].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPUNPCKLQDQ / PUNPCKLQDQ </c>\n"
"///   instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x i64]. \\n\n"
"///    Bits [63:0] are written to bits [63:0] of the destination. \\n\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x i64]. \\n\n"
"///    Bits [63:0] are written to bits [127:64] of the destination. \\n\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the interleaved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_epi64(__m128i __a, __m128i __b)\n"
"{\n"
"  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 0, 2+0);\n"
"}\n"
"\n"
"/// Returns the lower 64 bits of a 128-bit integer vector as a 64-bit\n"
"///    integer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVDQ2Q </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector operand. The lower 64 bits are moved to the\n"
"///    destination.\n"
"/// \\returns A 64-bit integer containing the lower 64 bits of the parameter.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_movepi64_pi64(__m128i __a)\n"
"{\n"
"  return (__m64)__a[0];\n"
"}\n"
"\n"
"/// Moves the 64-bit operand to a 128-bit integer vector, zeroing the\n"
"///    upper bits.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVD+VMOVQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit value.\n"
"/// \\returns A 128-bit integer vector. The lower 64 bits contain the value from\n"
"///    the operand. The upper 64 bits are assigned zeros.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_movpi64_epi64(__m64 __a)\n"
"{\n"
"  return __extension__ (__m128i)(__v2di){ (long long)__a, 0 };\n"
"}\n"
"\n"
"/// Moves the lower 64 bits of a 128-bit integer vector to a 128-bit\n"
"///    integer vector, zeroing the upper bits.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector operand. The lower 64 bits are moved to the\n"
"///    destination.\n"
"/// \\returns A 128-bit integer vector. The lower 64 bits contain the value from\n"
"///    the operand. The upper 64 bits are assigned zeros.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_move_epi64(__m128i __a)\n"
"{\n"
"  return __builtin_shufflevector((__v2di)__a, _mm_setzero_si128(), 0, 2);\n"
"}\n"
"\n"
"/// Unpacks the high-order 64-bit elements from two 128-bit vectors of\n"
"///    [2 x double] and interleaves them into a 128-bit vector of [2 x\n"
"///    double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKHPD / UNPCKHPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. \\n\n"
"///    Bits [127:64] are written to bits [63:0] of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. \\n\n"
"///    Bits [127:64] are written to bits [127:64] of the destination.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the interleaved values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 1, 2+1);\n"
"}\n"
"\n"
"/// Unpacks the low-order 64-bit elements from two 128-bit vectors\n"
"///    of [2 x double] and interleaves them into a 128-bit vector of [2 x\n"
"///    double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. \\n\n"
"///    Bits [63:0] are written to bits [63:0] of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double]. \\n\n"
"///    Bits [63:0] are written to bits [127:64] of the destination.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the interleaved values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 0, 2+0);\n"
"}\n"
"\n"
"/// Extracts the sign bits of the double-precision values in the 128-bit\n"
"///    vector of [2 x double], zero-extends the value, and writes it to the\n"
"///    low-order bits of the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVMSKPD / MOVMSKPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the values with sign bits to\n"
"///    be extracted.\n"
"/// \\returns The sign bits from each of the double-precision elements in \\a __a,\n"
"///    written to bits [1:0]. The remaining bits are assigned values of zero.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_movemask_pd(__m128d __a)\n"
"{\n"
"  return __builtin_ia32_movmskpd((__v2df)__a);\n"
"}\n"
"\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [2 x double] from two\n"
"///    128-bit vector parameters of [2 x double], using the immediate-value\n"
"///     parameter as a specifier.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_shuffle_pd(__m128d a, __m128d b, const int i);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSHUFPD / SHUFPD </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param b\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param i\n"
"///    An 8-bit immediate value. The least significant two bits specify which\n"
"///    elements to copy from \\a a and \\a b: \\n\n"
"///    Bit[0] = 0: lower element of \\a a copied to lower element of result. \\n\n"
"///    Bit[0] = 1: upper element of \\a a copied to lower element of result. \\n\n"
"///    Bit[1] = 0: lower element of \\a b copied to upper element of result. \\n\n"
"///    Bit[1] = 1: upper element of \\a b copied to upper element of result. \\n\n"
"/// \\returns A 128-bit vector of [2 x double] containing the shuffled values.\n"
"#define _mm_shuffle_pd(a, b, i) \\\n"
"  (__m128d)__builtin_ia32_shufpd((__v2df)(__m128d)(a), (__v2df)(__m128d)(b), \\\n"
"                                 (int)(i))\n"
"\n"
"/// Casts a 128-bit floating-point vector of [2 x double] into a 128-bit\n"
"///    floating-point vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [2 x double].\n"
"/// \\returns A 128-bit floating-point vector of [4 x float] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_castpd_ps(__m128d __a)\n"
"{\n"
"  return (__m128)__a;\n"
"}\n"
"\n"
"/// Casts a 128-bit floating-point vector of [2 x double] into a 128-bit\n"
"///    integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [2 x double].\n"
"/// \\returns A 128-bit integer vector containing the same bitwise pattern as the\n"
"///    parameter.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_castpd_si128(__m128d __a)\n"
"{\n"
"  return (__m128i)__a;\n"
"}\n"
"\n"
"/// Casts a 128-bit floating-point vector of [4 x float] into a 128-bit\n"
"///    floating-point vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [4 x float].\n"
"/// \\returns A 128-bit floating-point vector of [2 x double] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_castps_pd(__m128 __a)\n"
"{\n"
"  return (__m128d)__a;\n"
"}\n"
"\n"
"/// Casts a 128-bit floating-point vector of [4 x float] into a 128-bit\n"
"///    integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [4 x float].\n"
"/// \\returns A 128-bit integer vector containing the same bitwise pattern as the\n"
"///    parameter.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_castps_si128(__m128 __a)\n"
"{\n"
"  return (__m128i)__a;\n"
"}\n"
"\n"
"/// Casts a 128-bit integer vector into a 128-bit floating-point vector\n"
"///    of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit floating-point vector of [4 x float] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_castsi128_ps(__m128i __a)\n"
"{\n"
"  return (__m128)__a;\n"
"}\n"
"\n"
"/// Casts a 128-bit integer vector into a 128-bit floating-point vector\n"
"///    of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit floating-point vector of [2 x double] containing the same\n"
"///    bitwise pattern as the parameter.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_castsi128_pd(__m128i __a)\n"
"{\n"
"  return (__m128d)__a;\n"
"}\n"
"\n"
"#if defined(__cplusplus)\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"/// Indicates that a spin loop is being executed for the purposes of\n"
"///    optimizing power consumption during the loop.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PAUSE </c> instruction.\n"
"///\n"
"void _mm_pause(void);\n"
"\n"
"#if defined(__cplusplus)\n"
"} // extern \"C\"\n"
"#endif\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __DEFAULT_FN_ATTRS_MMX\n"
"\n"
"#define _MM_SHUFFLE2(x, y) (((x) << 1) | (y))\n"
"\n"
"#define _MM_DENORMALS_ZERO_ON   (0x0040)\n"
"#define _MM_DENORMALS_ZERO_OFF  (0x0000)\n"
"\n"
"#define _MM_DENORMALS_ZERO_MASK (0x0040)\n"
"\n"
"#define _MM_GET_DENORMALS_ZERO_MODE() (_mm_getcsr() & _MM_DENORMALS_ZERO_MASK)\n"
"#define _MM_SET_DENORMALS_ZERO_MODE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_DENORMALS_ZERO_MASK) | (x)))\n"
"\n"
"#endif /* __EMMINTRIN_H */\n"
"" } , 
 { "/builtins/f16cintrin.h" , "/*===---- f16cintrin.h - F16C intrinsics -----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __IMMINTRIN_H\n"
"#error \"Never use <f16cintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __F16CINTRIN_H\n"
"#define __F16CINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS128 \\\n"
"  __attribute__((__always_inline__, __nodebug__, __target__(\"f16c\"), __min_vector_width__(128)))\n"
"#define __DEFAULT_FN_ATTRS256 \\\n"
"  __attribute__((__always_inline__, __nodebug__, __target__(\"f16c\"), __min_vector_width__(256)))\n"
"\n"
"/* NOTE: Intel documents the 128-bit versions of these as being in emmintrin.h,\n"
" * but that's because icc can emulate these without f16c using a library call.\n"
" * Since we don't do that let's leave these in f16cintrin.h.\n"
" */\n"
"\n"
"/// Converts a 16-bit half-precision float value into a 32-bit float\n"
"///    value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPH2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 16-bit half-precision float value.\n"
"/// \\returns The converted 32-bit float value.\n"
"static __inline float __DEFAULT_FN_ATTRS128\n"
"_cvtsh_ss(unsigned short __a)\n"
"{\n"
"  __v8hi v = {(short)__a, 0, 0, 0, 0, 0, 0, 0};\n"
"  __v4sf r = __builtin_ia32_vcvtph2ps(v);\n"
"  return r[0];\n"
"}\n"
"\n"
"/// Converts a 32-bit single-precision float value to a 16-bit\n"
"///    half-precision float value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// unsigned short _cvtss_sh(float a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPS2PH </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 32-bit single-precision float value to be converted to a 16-bit\n"
"///    half-precision float value.\n"
"/// \\param imm\n"
"///    An immediate value controlling rounding using bits [2:0]: \\n\n"
"///    000: Nearest \\n\n"
"///    001: Down \\n\n"
"///    010: Up \\n\n"
"///    011: Truncate \\n\n"
"///    1XX: Use MXCSR.RC for rounding\n"
"/// \\returns The converted 16-bit half-precision float value.\n"
"#define _cvtss_sh(a, imm) \\\n"
"  (unsigned short)(((__v8hi)__builtin_ia32_vcvtps2ph((__v4sf){a, 0, 0, 0}, \\\n"
"                                                     (imm)))[0])\n"
"\n"
"/// Converts a 128-bit vector containing 32-bit float values into a\n"
"///    128-bit vector containing 16-bit half-precision float values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_cvtps_ph(__m128 a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPS2PH </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector containing 32-bit float values.\n"
"/// \\param imm\n"
"///    An immediate value controlling rounding using bits [2:0]: \\n\n"
"///    000: Nearest \\n\n"
"///    001: Down \\n\n"
"///    010: Up \\n\n"
"///    011: Truncate \\n\n"
"///    1XX: Use MXCSR.RC for rounding\n"
"/// \\returns A 128-bit vector containing converted 16-bit half-precision float\n"
"///    values. The lower 64 bits are used to store the converted 16-bit\n"
"///    half-precision floating-point values.\n"
"#define _mm_cvtps_ph(a, imm) \\\n"
"  (__m128i)__builtin_ia32_vcvtps2ph((__v4sf)(__m128)(a), (imm))\n"
"\n"
"/// Converts a 128-bit vector containing 16-bit half-precision float\n"
"///    values into a 128-bit vector containing 32-bit float values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPH2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector containing 16-bit half-precision float values. The lower\n"
"///    64 bits are used in the conversion.\n"
"/// \\returns A 128-bit vector of [4 x float] containing converted float values.\n"
"static __inline __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_cvtph_ps(__m128i __a)\n"
"{\n"
"  return (__m128)__builtin_ia32_vcvtph2ps((__v8hi)__a);\n"
"}\n"
"\n"
"/// Converts a 256-bit vector of [8 x float] into a 128-bit vector\n"
"///    containing 16-bit half-precision float values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm256_cvtps_ph(__m256 a, const int imm);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPS2PH </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 256-bit vector containing 32-bit single-precision float values to be\n"
"///    converted to 16-bit half-precision float values.\n"
"/// \\param imm\n"
"///    An immediate value controlling rounding using bits [2:0]: \\n\n"
"///    000: Nearest \\n\n"
"///    001: Down \\n\n"
"///    010: Up \\n\n"
"///    011: Truncate \\n\n"
"///    1XX: Use MXCSR.RC for rounding\n"
"/// \\returns A 128-bit vector containing the converted 16-bit half-precision\n"
"///    float values.\n"
"#define _mm256_cvtps_ph(a, imm) \\\n"
" (__m128i)__builtin_ia32_vcvtps2ph256((__v8sf)(__m256)(a), (imm))\n"
"\n"
"/// Converts a 128-bit vector containing 16-bit half-precision float\n"
"///    values into a 256-bit vector of [8 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTPH2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector containing 16-bit half-precision float values to be\n"
"///    converted to 32-bit single-precision float values.\n"
"/// \\returns A vector of [8 x float] containing the converted 32-bit\n"
"///    single-precision float values.\n"
"static __inline __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_cvtph_ps(__m128i __a)\n"
"{\n"
"  return (__m256)__builtin_ia32_vcvtph2ps256((__v8hi)__a);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS128\n"
"#undef __DEFAULT_FN_ATTRS256\n"
"\n"
"#endif /* __F16CINTRIN_H */\n"
"" } , 
 { "/builtins/float.h" , "/*===---- float.h - Characteristics of floating point types ----------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __CLANG_FLOAT_H\n"
"#define __CLANG_FLOAT_H\n"
"\n"
"/* If we're on MinGW, fall back to the system's float.h, which might have\n"
" * additional definitions provided for Windows.\n"
" * For more details see http://msdn.microsoft.com/en-us/library/y0ybw9fy.aspx\n"
" *\n"
" * Also fall back on Darwin to allow additional definitions and\n"
" * implementation-defined values.\n"
" */\n"
"#if (defined(__APPLE__) || (defined(__MINGW32__) || defined(_MSC_VER))) && \\\n"
"    __STDC_HOSTED__ && __has_include_next(<float.h>)\n"
"\n"
"/* Prior to Apple's 10.7 SDK, float.h SDK header used to apply an extra level\n"
" * of #include_next<float.h> to keep Metrowerks compilers happy. Avoid this\n"
" * extra indirection.\n"
" */\n"
"#ifdef __APPLE__\n"
"#define _FLOAT_H_\n"
"#endif\n"
"\n"
"#  include_next <float.h>\n"
"\n"
"/* Undefine anything that we'll be redefining below. */\n"
"#  undef FLT_EVAL_METHOD\n"
"#  undef FLT_ROUNDS\n"
"#  undef FLT_RADIX\n"
"#  undef FLT_MANT_DIG\n"
"#  undef DBL_MANT_DIG\n"
"#  undef LDBL_MANT_DIG\n"
"#  if __STDC_VERSION__ >= 199901L || !defined(__STRICT_ANSI__)\n"
"#    undef DECIMAL_DIG\n"
"#  endif\n"
"#  undef FLT_DIG\n"
"#  undef DBL_DIG\n"
"#  undef LDBL_DIG\n"
"#  undef FLT_MIN_EXP\n"
"#  undef DBL_MIN_EXP\n"
"#  undef LDBL_MIN_EXP\n"
"#  undef FLT_MIN_10_EXP\n"
"#  undef DBL_MIN_10_EXP\n"
"#  undef LDBL_MIN_10_EXP\n"
"#  undef FLT_MAX_EXP\n"
"#  undef DBL_MAX_EXP\n"
"#  undef LDBL_MAX_EXP\n"
"#  undef FLT_MAX_10_EXP\n"
"#  undef DBL_MAX_10_EXP\n"
"#  undef LDBL_MAX_10_EXP\n"
"#  undef FLT_MAX\n"
"#  undef DBL_MAX\n"
"#  undef LDBL_MAX\n"
"#  undef FLT_EPSILON\n"
"#  undef DBL_EPSILON\n"
"#  undef LDBL_EPSILON\n"
"#  undef FLT_MIN\n"
"#  undef DBL_MIN\n"
"#  undef LDBL_MIN\n"
"#  if __STDC_VERSION__ >= 201112L || !defined(__STRICT_ANSI__)\n"
"#    undef FLT_TRUE_MIN\n"
"#    undef DBL_TRUE_MIN\n"
"#    undef LDBL_TRUE_MIN\n"
"#    undef FLT_DECIMAL_DIG\n"
"#    undef DBL_DECIMAL_DIG\n"
"#    undef LDBL_DECIMAL_DIG\n"
"#    undef FLT_HAS_SUBNORM\n"
"#    undef DBL_HAS_SUBNORM\n"
"#    undef LDBL_HAS_SUBNORM\n"
"#  endif\n"
"#endif\n"
"\n"
"/* Characteristics of floating point types, C99 5.2.4.2.2 */\n"
"\n"
"#define FLT_EVAL_METHOD __FLT_EVAL_METHOD__\n"
"#define FLT_ROUNDS (__builtin_flt_rounds())\n"
"#define FLT_RADIX __FLT_RADIX__\n"
"\n"
"#define FLT_MANT_DIG __FLT_MANT_DIG__\n"
"#define DBL_MANT_DIG __DBL_MANT_DIG__\n"
"#define LDBL_MANT_DIG __LDBL_MANT_DIG__\n"
"\n"
"#if __STDC_VERSION__ >= 199901L || !defined(__STRICT_ANSI__)\n"
"#  define DECIMAL_DIG __DECIMAL_DIG__\n"
"#endif\n"
"\n"
"#define FLT_DIG __FLT_DIG__\n"
"#define DBL_DIG __DBL_DIG__\n"
"#define LDBL_DIG __LDBL_DIG__\n"
"\n"
"#define FLT_MIN_EXP __FLT_MIN_EXP__\n"
"#define DBL_MIN_EXP __DBL_MIN_EXP__\n"
"#define LDBL_MIN_EXP __LDBL_MIN_EXP__\n"
"\n"
"#define FLT_MIN_10_EXP __FLT_MIN_10_EXP__\n"
"#define DBL_MIN_10_EXP __DBL_MIN_10_EXP__\n"
"#define LDBL_MIN_10_EXP __LDBL_MIN_10_EXP__\n"
"\n"
"#define FLT_MAX_EXP __FLT_MAX_EXP__\n"
"#define DBL_MAX_EXP __DBL_MAX_EXP__\n"
"#define LDBL_MAX_EXP __LDBL_MAX_EXP__\n"
"\n"
"#define FLT_MAX_10_EXP __FLT_MAX_10_EXP__\n"
"#define DBL_MAX_10_EXP __DBL_MAX_10_EXP__\n"
"#define LDBL_MAX_10_EXP __LDBL_MAX_10_EXP__\n"
"\n"
"#define FLT_MAX __FLT_MAX__\n"
"#define DBL_MAX __DBL_MAX__\n"
"#define LDBL_MAX __LDBL_MAX__\n"
"\n"
"#define FLT_EPSILON __FLT_EPSILON__\n"
"#define DBL_EPSILON __DBL_EPSILON__\n"
"#define LDBL_EPSILON __LDBL_EPSILON__\n"
"\n"
"#define FLT_MIN __FLT_MIN__\n"
"#define DBL_MIN __DBL_MIN__\n"
"#define LDBL_MIN __LDBL_MIN__\n"
"\n"
"#if __STDC_VERSION__ >= 201112L || !defined(__STRICT_ANSI__)\n"
"#  define FLT_TRUE_MIN __FLT_DENORM_MIN__\n"
"#  define DBL_TRUE_MIN __DBL_DENORM_MIN__\n"
"#  define LDBL_TRUE_MIN __LDBL_DENORM_MIN__\n"
"#  define FLT_DECIMAL_DIG __FLT_DECIMAL_DIG__\n"
"#  define DBL_DECIMAL_DIG __DBL_DECIMAL_DIG__\n"
"#  define LDBL_DECIMAL_DIG __LDBL_DECIMAL_DIG__\n"
"#  define FLT_HAS_SUBNORM __FLT_HAS_DENORM__\n"
"#  define DBL_HAS_SUBNORM __DBL_HAS_DENORM__\n"
"#  define LDBL_HAS_SUBNORM __LDBL_HAS_DENORM__\n"
"#endif\n"
"\n"
"#ifdef __STDC_WANT_IEC_60559_TYPES_EXT__\n"
"#  define FLT16_MANT_DIG    __FLT16_MANT_DIG__\n"
"#  define FLT16_DECIMAL_DIG __FLT16_DECIMAL_DIG__\n"
"#  define FLT16_DIG         __FLT16_DIG__\n"
"#  define FLT16_MIN_EXP     __FLT16_MIN_EXP__\n"
"#  define FLT16_MIN_10_EXP  __FLT16_MIN_10_EXP__\n"
"#  define FLT16_MAX_EXP     __FLT16_MAX_EXP__\n"
"#  define FLT16_MAX_10_EXP  __FLT16_MAX_10_EXP__\n"
"#  define FLT16_MAX         __FLT16_MAX__\n"
"#  define FLT16_EPSILON     __FLT16_EPSILON__\n"
"#  define FLT16_MIN         __FLT16_MIN__\n"
"#  define FLT16_TRUE_MIN    __FLT16_TRUE_MIN__\n"
"#endif /* __STDC_WANT_IEC_60559_TYPES_EXT__ */\n"
"\n"
"#endif /* __CLANG_FLOAT_H */\n"
"" } , 
 { "/builtins/fma4intrin.h" , "/*===---- fma4intrin.h - FMA4 intrinsics -----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __X86INTRIN_H\n"
"#error \"Never use <fma4intrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __FMA4INTRIN_H\n"
"#define __FMA4INTRIN_H\n"
"\n"
"#include <pmmintrin.h>\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS128 __attribute__((__always_inline__, __nodebug__, __target__(\"fma4\"), __min_vector_width__(128)))\n"
"#define __DEFAULT_FN_ATTRS256 __attribute__((__always_inline__, __nodebug__, __target__(\"fma4\"), __min_vector_width__(256)))\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_macc_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_macc_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_macc_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_macc_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd((__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_msub_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_msub_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_msub_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_msub_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd((__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_nmacc_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_nmacc_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_nmacc_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_nmacc_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_nmsub_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_nmsub_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_nmsub_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_nmsub_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_maddsub_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_maddsub_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_msubadd_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_msubadd_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_macc_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_macc_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_msub_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_msub_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_nmacc_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, (__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_nmacc_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, (__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_nmsub_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_nmsub_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, -(__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_maddsub_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_maddsub_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_msubadd_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_msubadd_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS128\n"
"#undef __DEFAULT_FN_ATTRS256\n"
"\n"
"#endif /* __FMA4INTRIN_H */\n"
"" } , 
 { "/builtins/fmaintrin.h" , "/*===---- fmaintrin.h - FMA intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <fmaintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __FMAINTRIN_H\n"
"#define __FMAINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS128 __attribute__((__always_inline__, __nodebug__, __target__(\"fma\"), __min_vector_width__(128)))\n"
"#define __DEFAULT_FN_ATTRS256 __attribute__((__always_inline__, __nodebug__, __target__(\"fma\"), __min_vector_width__(256)))\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fmadd_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fmadd_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fmadd_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fmadd_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fmsub_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fmsub_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fmsub_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fmsub_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fnmadd_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fnmadd_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fnmsub_ss(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fnmsub_sd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, (__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS128\n"
"_mm_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS128\n"
"_mm_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_fmadd_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_fmadd_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_fmsub_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_fmsub_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, (__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, (__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, -(__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS128\n"
"#undef __DEFAULT_FN_ATTRS256\n"
"\n"
"#endif /* __FMAINTRIN_H */\n"
"" } , 
 { "/builtins/fxsrintrin.h" , "/*===---- fxsrintrin.h - FXSR intrinsic ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <fxsrintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __FXSRINTRIN_H\n"
"#define __FXSRINTRIN_H\n"
"\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"fxsr\")))\n"
"\n"
"/// Saves the XMM, MMX, MXCSR and x87 FPU registers into a 512-byte\n"
"///    memory region pointed to by the input parameter \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> FXSAVE </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 512-byte memory region. The beginning of this memory\n"
"///    region should be aligned on a 16-byte boundary.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_fxsave(void *__p)\n"
"{\n"
"  __builtin_ia32_fxsave(__p);\n"
"}\n"
"\n"
"/// Restores the XMM, MMX, MXCSR and x87 FPU registers from the 512-byte\n"
"///    memory region pointed to by the input parameter \\a __p. The contents of\n"
"///    this memory region should have been written to by a previous \\c _fxsave\n"
"///    or \\c _fxsave64 intrinsic.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> FXRSTOR </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 512-byte memory region. The beginning of this memory\n"
"///    region should be aligned on a 16-byte boundary.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_fxrstor(void *__p)\n"
"{\n"
"  __builtin_ia32_fxrstor(__p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Saves the XMM, MMX, MXCSR and x87 FPU registers into a 512-byte\n"
"///    memory region pointed to by the input parameter \\a __p.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> FXSAVE64 </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 512-byte memory region. The beginning of this memory\n"
"///    region should be aligned on a 16-byte boundary.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_fxsave64(void *__p)\n"
"{\n"
"  __builtin_ia32_fxsave64(__p);\n"
"}\n"
"\n"
"/// Restores the XMM, MMX, MXCSR and x87 FPU registers from the 512-byte\n"
"///    memory region pointed to by the input parameter \\a __p. The contents of\n"
"///    this memory region should have been written to by a previous \\c _fxsave\n"
"///    or \\c _fxsave64 intrinsic.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> FXRSTOR64 </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 512-byte memory region. The beginning of this memory\n"
"///    region should be aligned on a 16-byte boundary.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_fxrstor64(void *__p)\n"
"{\n"
"  __builtin_ia32_fxrstor64(__p);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/gfniintrin.h" , "/*===----------------- gfniintrin.h - GFNI intrinsics ----------------------===\n"
" *\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <gfniintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __GFNIINTRIN_H\n"
"#define __GFNIINTRIN_H\n"
"\n"
"\n"
"#define _mm_gf2p8affineinv_epi64_epi8(A, B, I) \\\n"
"  (__m128i)__builtin_ia32_vgf2p8affineinvqb_v16qi((__v16qi)(__m128i)(A),          \\\n"
"                                                  (__v16qi)(__m128i)(B),          \\\n"
"                                                  (char)(I))\n"
"\n"
"#define _mm_mask_gf2p8affineinv_epi64_epi8(S, U, A, B, I) \\\n"
"  (__m128i)__builtin_ia32_selectb_128((__mmask16)(U),                             \\\n"
"        (__v16qi)_mm_gf2p8affineinv_epi64_epi8(A, B, I),                          \\\n"
"        (__v16qi)(__m128i)(S))\n"
"\n"
"\n"
"#define _mm_maskz_gf2p8affineinv_epi64_epi8(U, A, B, I) \\\n"
"  (__m128i)_mm_mask_gf2p8affineinv_epi64_epi8((__m128i)_mm_setzero_si128(),       \\\n"
"        U, A, B, I)\n"
"\n"
"\n"
"#define _mm256_gf2p8affineinv_epi64_epi8(A, B, I) \\\n"
"  (__m256i)__builtin_ia32_vgf2p8affineinvqb_v32qi((__v32qi)(__m256i)(A),          \\\n"
"                                                  (__v32qi)(__m256i)(B),          \\\n"
"                                                  (char)(I))\n"
"\n"
"#define _mm256_mask_gf2p8affineinv_epi64_epi8(S, U, A, B, I) \\\n"
"   (__m256i)__builtin_ia32_selectb_256((__mmask32)(U),                            \\\n"
"        (__v32qi)_mm256_gf2p8affineinv_epi64_epi8(A, B, I),                       \\\n"
"        (__v32qi)(__m256i)(S))\n"
"\n"
"#define _mm256_maskz_gf2p8affineinv_epi64_epi8(U, A, B, I) \\\n"
"  (__m256i)_mm256_mask_gf2p8affineinv_epi64_epi8((__m256i)_mm256_setzero_si256(), \\\n"
"        U, A, B, I)\n"
"\n"
"\n"
"#define _mm512_gf2p8affineinv_epi64_epi8(A, B, I) \\\n"
"  (__m512i)__builtin_ia32_vgf2p8affineinvqb_v64qi((__v64qi)(__m512i)(A),          \\\n"
"                                                  (__v64qi)(__m512i)(B),          \\\n"
"                                                  (char)(I))\n"
"\n"
"#define _mm512_mask_gf2p8affineinv_epi64_epi8(S, U, A, B, I) \\\n"
"   (__m512i)__builtin_ia32_selectb_512((__mmask64)(U),                            \\\n"
"        (__v64qi)_mm512_gf2p8affineinv_epi64_epi8(A, B, I),                       \\\n"
"        (__v64qi)(__m512i)(S))\n"
"\n"
"#define _mm512_maskz_gf2p8affineinv_epi64_epi8(U, A, B, I) \\\n"
"  (__m512i)_mm512_mask_gf2p8affineinv_epi64_epi8((__m512i)_mm512_setzero_si512(),    \\\n"
"        U, A, B, I)\n"
"\n"
"#define _mm_gf2p8affine_epi64_epi8(A, B, I) \\\n"
"  (__m128i)__builtin_ia32_vgf2p8affineqb_v16qi((__v16qi)(__m128i)(A),             \\\n"
"                                                  (__v16qi)(__m128i)(B),          \\\n"
"                                                  (char)(I))\n"
"\n"
"#define _mm_mask_gf2p8affine_epi64_epi8(S, U, A, B, I) \\\n"
"  (__m128i)__builtin_ia32_selectb_128((__mmask16)(U),                             \\\n"
"        (__v16qi)_mm_gf2p8affine_epi64_epi8(A, B, I),                             \\\n"
"        (__v16qi)(__m128i)(S))\n"
"\n"
"\n"
"#define _mm_maskz_gf2p8affine_epi64_epi8(U, A, B, I) \\\n"
"  (__m128i)_mm_mask_gf2p8affine_epi64_epi8((__m128i)_mm_setzero_si128(),          \\\n"
"        U, A, B, I)\n"
"\n"
"\n"
"#define _mm256_gf2p8affine_epi64_epi8(A, B, I) \\\n"
"  (__m256i)__builtin_ia32_vgf2p8affineqb_v32qi((__v32qi)(__m256i)(A),             \\\n"
"                                                  (__v32qi)(__m256i)(B),          \\\n"
"                                                  (char)(I))\n"
"\n"
"#define _mm256_mask_gf2p8affine_epi64_epi8(S, U, A, B, I) \\\n"
"   (__m256i)__builtin_ia32_selectb_256((__mmask32)(U),                            \\\n"
"        (__v32qi)_mm256_gf2p8affine_epi64_epi8(A, B, I),                          \\\n"
"        (__v32qi)(__m256i)(S))\n"
"\n"
"#define _mm256_maskz_gf2p8affine_epi64_epi8(U, A, B, I) \\\n"
"  (__m256i)_mm256_mask_gf2p8affine_epi64_epi8((__m256i)_mm256_setzero_si256(),    \\\n"
"        U, A, B, I)\n"
"\n"
"\n"
"#define _mm512_gf2p8affine_epi64_epi8(A, B, I) \\\n"
"  (__m512i)__builtin_ia32_vgf2p8affineqb_v64qi((__v64qi)(__m512i)(A),             \\\n"
"                                                  (__v64qi)(__m512i)(B),          \\\n"
"                                                  (char)(I))\n"
"\n"
"#define _mm512_mask_gf2p8affine_epi64_epi8(S, U, A, B, I) \\\n"
"   (__m512i)__builtin_ia32_selectb_512((__mmask64)(U),                            \\\n"
"        (__v64qi)_mm512_gf2p8affine_epi64_epi8(A, B, I),                          \\\n"
"        (__v64qi)(__m512i)(S))\n"
"\n"
"#define _mm512_maskz_gf2p8affine_epi64_epi8(U, A, B, I) \\\n"
"  (__m512i)_mm512_mask_gf2p8affine_epi64_epi8((__m512i)_mm512_setzero_si512(),       \\\n"
"        U, A, B, I)\n"
"\n"
"/* Default attributes for simple form (no masking). */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"gfni\"), __min_vector_width__(128)))\n"
"\n"
"/* Default attributes for YMM unmasked form. */\n"
"#define __DEFAULT_FN_ATTRS_Y __attribute__((__always_inline__, __nodebug__, __target__(\"avx,gfni\"), __min_vector_width__(256)))\n"
"\n"
"/* Default attributes for ZMM forms. */\n"
"#define __DEFAULT_FN_ATTRS_Z __attribute__((__always_inline__, __nodebug__, __target__(\"avx512bw,gfni\"), __min_vector_width__(512)))\n"
"\n"
"/* Default attributes for VLX forms. */\n"
"#define __DEFAULT_FN_ATTRS_VL128 __attribute__((__always_inline__, __nodebug__, __target__(\"avx512bw,avx512vl,gfni\"), __min_vector_width__(128)))\n"
"#define __DEFAULT_FN_ATTRS_VL256 __attribute__((__always_inline__, __nodebug__, __target__(\"avx512bw,avx512vl,gfni\"), __min_vector_width__(256)))\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_gf2p8mul_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i) __builtin_ia32_vgf2p8mulb_v16qi((__v16qi) __A,\n"
"              (__v16qi) __B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS_VL128\n"
"_mm_mask_gf2p8mul_epi8(__m128i __S, __mmask16 __U, __m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i) __builtin_ia32_selectb_128(__U,\n"
"              (__v16qi) _mm_gf2p8mul_epi8(__A, __B),\n"
"              (__v16qi) __S);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS_VL128\n"
"_mm_maskz_gf2p8mul_epi8(__mmask16 __U, __m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_mask_gf2p8mul_epi8((__m128i)_mm_setzero_si128(),\n"
"              __U, __A, __B);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS_Y\n"
"_mm256_gf2p8mul_epi8(__m256i __A, __m256i __B)\n"
"{\n"
"  return (__m256i) __builtin_ia32_vgf2p8mulb_v32qi((__v32qi) __A,\n"
"              (__v32qi) __B);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS_VL256\n"
"_mm256_mask_gf2p8mul_epi8(__m256i __S, __mmask32 __U, __m256i __A, __m256i __B)\n"
"{\n"
"  return (__m256i) __builtin_ia32_selectb_256(__U,\n"
"              (__v32qi) _mm256_gf2p8mul_epi8(__A, __B),\n"
"              (__v32qi) __S);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS_VL256\n"
"_mm256_maskz_gf2p8mul_epi8(__mmask32 __U, __m256i __A, __m256i __B)\n"
"{\n"
"  return _mm256_mask_gf2p8mul_epi8((__m256i)_mm256_setzero_si256(),\n"
"              __U, __A, __B);\n"
"}\n"
"\n"
"static __inline__ __m512i __DEFAULT_FN_ATTRS_Z\n"
"_mm512_gf2p8mul_epi8(__m512i __A, __m512i __B)\n"
"{\n"
"  return (__m512i) __builtin_ia32_vgf2p8mulb_v64qi((__v64qi) __A,\n"
"              (__v64qi) __B);\n"
"}\n"
"\n"
"static __inline__ __m512i __DEFAULT_FN_ATTRS_Z\n"
"_mm512_mask_gf2p8mul_epi8(__m512i __S, __mmask64 __U, __m512i __A, __m512i __B)\n"
"{\n"
"  return (__m512i) __builtin_ia32_selectb_512(__U,\n"
"              (__v64qi) _mm512_gf2p8mul_epi8(__A, __B),\n"
"              (__v64qi) __S);\n"
"}\n"
"\n"
"static __inline__ __m512i __DEFAULT_FN_ATTRS_Z\n"
"_mm512_maskz_gf2p8mul_epi8(__mmask64 __U, __m512i __A, __m512i __B)\n"
"{\n"
"  return _mm512_mask_gf2p8mul_epi8((__m512i)_mm512_setzero_si512(),\n"
"              __U, __A, __B);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __DEFAULT_FN_ATTRS_Y\n"
"#undef __DEFAULT_FN_ATTRS_Z\n"
"#undef __DEFAULT_FN_ATTRS_VL128\n"
"#undef __DEFAULT_FN_ATTRS_VL256\n"
"\n"
"#endif /* __GFNIINTRIN_H */\n"
"\n"
"" } , 
 { "/builtins/htmintrin.h" , "/*===---- htmintrin.h - Standard header for PowerPC HTM ---------------===*\\\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
"\\*===----------------------------------------------------------------------===*/\n"
"\n"
"#ifndef __HTMINTRIN_H\n"
"#define __HTMINTRIN_H\n"
"\n"
"#ifndef __HTM__\n"
"#error \"HTM instruction set not enabled\"\n"
"#endif\n"
"\n"
"#ifdef __powerpc__\n"
"\n"
"#include <stdint.h>\n"
"\n"
"typedef uint64_t texasr_t;\n"
"typedef uint32_t texasru_t;\n"
"typedef uint32_t texasrl_t;\n"
"typedef uintptr_t tfiar_t;\n"
"typedef uintptr_t tfhar_t;\n"
"\n"
"#define _HTM_STATE(CR0) ((CR0 >> 1) & 0x3)\n"
"#define _HTM_NONTRANSACTIONAL 0x0\n"
"#define _HTM_SUSPENDED        0x1\n"
"#define _HTM_TRANSACTIONAL    0x2\n"
"\n"
"#define _TEXASR_EXTRACT_BITS(TEXASR,BITNUM,SIZE) \\\n"
"  (((TEXASR) >> (63-(BITNUM))) & ((1<<(SIZE))-1))\n"
"#define _TEXASRU_EXTRACT_BITS(TEXASR,BITNUM,SIZE) \\\n"
"  (((TEXASR) >> (31-(BITNUM))) & ((1<<(SIZE))-1))\n"
"\n"
"#define _TEXASR_FAILURE_CODE(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 7, 8)\n"
"#define _TEXASRU_FAILURE_CODE(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 7, 8)\n"
"\n"
"#define _TEXASR_FAILURE_PERSISTENT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 7, 1)\n"
"#define _TEXASRU_FAILURE_PERSISTENT(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 7, 1)\n"
"\n"
"#define _TEXASR_DISALLOWED(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 8, 1)\n"
"#define _TEXASRU_DISALLOWED(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 8, 1)\n"
"\n"
"#define _TEXASR_NESTING_OVERFLOW(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 9, 1)\n"
"#define _TEXASRU_NESTING_OVERFLOW(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 9, 1)\n"
"\n"
"#define _TEXASR_FOOTPRINT_OVERFLOW(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 10, 1)\n"
"#define _TEXASRU_FOOTPRINT_OVERFLOW(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 10, 1)\n"
"\n"
"#define _TEXASR_SELF_INDUCED_CONFLICT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 11, 1)\n"
"#define _TEXASRU_SELF_INDUCED_CONFLICT(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 11, 1)\n"
"\n"
"#define _TEXASR_NON_TRANSACTIONAL_CONFLICT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 12, 1)\n"
"#define _TEXASRU_NON_TRANSACTIONAL_CONFLICT(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 12, 1)\n"
"\n"
"#define _TEXASR_TRANSACTION_CONFLICT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 13, 1)\n"
"#define _TEXASRU_TRANSACTION_CONFLICT(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 13, 1)\n"
"\n"
"#define _TEXASR_TRANSLATION_INVALIDATION_CONFLICT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 14, 1)\n"
"#define _TEXASRU_TRANSLATION_INVALIDATION_CONFLICT(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 14, 1)\n"
"\n"
"#define _TEXASR_IMPLEMENTAION_SPECIFIC(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 15, 1)\n"
"#define _TEXASRU_IMPLEMENTAION_SPECIFIC(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 15, 1)\n"
"\n"
"#define _TEXASR_INSTRUCTION_FETCH_CONFLICT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 16, 1)\n"
"#define _TEXASRU_INSTRUCTION_FETCH_CONFLICT(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 16, 1)\n"
"\n"
"#define _TEXASR_ABORT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 31, 1)\n"
"#define _TEXASRU_ABORT(TEXASRU) \\\n"
"  _TEXASRU_EXTRACT_BITS(TEXASRU, 31, 1)\n"
"\n"
"\n"
"#define _TEXASR_SUSPENDED(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 32, 1)\n"
"\n"
"#define _TEXASR_PRIVILEGE(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 35, 2)\n"
"\n"
"#define _TEXASR_FAILURE_SUMMARY(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 36, 1)\n"
"\n"
"#define _TEXASR_TFIAR_EXACT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 37, 1)\n"
"\n"
"#define _TEXASR_ROT(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 38, 1)\n"
"\n"
"#define _TEXASR_TRANSACTION_LEVEL(TEXASR) \\\n"
"  _TEXASR_EXTRACT_BITS(TEXASR, 63, 12)\n"
"\n"
"#endif /* __powerpc */\n"
"\n"
"#ifdef __s390__\n"
"\n"
"/* Condition codes generated by tbegin  */\n"
"#define _HTM_TBEGIN_STARTED       0\n"
"#define _HTM_TBEGIN_INDETERMINATE 1\n"
"#define _HTM_TBEGIN_TRANSIENT     2\n"
"#define _HTM_TBEGIN_PERSISTENT    3\n"
"\n"
"/* The abort codes below this threshold are reserved for machine use.  */\n"
"#define _HTM_FIRST_USER_ABORT_CODE 256\n"
"\n"
"/* The transaction diagnostic block is it is defined in the Principles\n"
"   of Operation chapter 5-91.  */\n"
"\n"
"struct __htm_tdb {\n"
"  unsigned char format;                /*   0 */\n"
"  unsigned char flags;\n"
"  unsigned char reserved1[4];\n"
"  unsigned short nesting_depth;\n"
"  unsigned long long abort_code;       /*   8 */\n"
"  unsigned long long conflict_token;   /*  16 */\n"
"  unsigned long long atia;             /*  24 */\n"
"  unsigned char eaid;                  /*  32 */\n"
"  unsigned char dxc;\n"
"  unsigned char reserved2[2];\n"
"  unsigned int program_int_id;\n"
"  unsigned long long exception_id;     /*  40 */\n"
"  unsigned long long bea;              /*  48 */\n"
"  unsigned char reserved3[72];         /*  56 */\n"
"  unsigned long long gprs[16];         /* 128 */\n"
"} __attribute__((__packed__, __aligned__ (8)));\n"
"\n"
"\n"
"/* Helper intrinsics to retry tbegin in case of transient failure.  */\n"
"\n"
"static __inline int __attribute__((__always_inline__, __nodebug__))\n"
"__builtin_tbegin_retry_null (int __retry)\n"
"{\n"
"  int cc, i = 0;\n"
"\n"
"  while ((cc = __builtin_tbegin(0)) == _HTM_TBEGIN_TRANSIENT\n"
"         && i++ < __retry)\n"
"    __builtin_tx_assist(i);\n"
"\n"
"  return cc;\n"
"}\n"
"\n"
"static __inline int __attribute__((__always_inline__, __nodebug__))\n"
"__builtin_tbegin_retry_tdb (void *__tdb, int __retry)\n"
"{\n"
"  int cc, i = 0;\n"
"\n"
"  while ((cc = __builtin_tbegin(__tdb)) == _HTM_TBEGIN_TRANSIENT\n"
"         && i++ < __retry)\n"
"    __builtin_tx_assist(i);\n"
"\n"
"  return cc;\n"
"}\n"
"\n"
"#define __builtin_tbegin_retry(tdb, retry) \\\n"
"  (__builtin_constant_p(tdb == 0) && tdb == 0 ? \\\n"
"   __builtin_tbegin_retry_null(retry) : \\\n"
"   __builtin_tbegin_retry_tdb(tdb, retry))\n"
"\n"
"static __inline int __attribute__((__always_inline__, __nodebug__))\n"
"__builtin_tbegin_retry_nofloat_null (int __retry)\n"
"{\n"
"  int cc, i = 0;\n"
"\n"
"  while ((cc = __builtin_tbegin_nofloat(0)) == _HTM_TBEGIN_TRANSIENT\n"
"         && i++ < __retry)\n"
"    __builtin_tx_assist(i);\n"
"\n"
"  return cc;\n"
"}\n"
"\n"
"static __inline int __attribute__((__always_inline__, __nodebug__))\n"
"__builtin_tbegin_retry_nofloat_tdb (void *__tdb, int __retry)\n"
"{\n"
"  int cc, i = 0;\n"
"\n"
"  while ((cc = __builtin_tbegin_nofloat(__tdb)) == _HTM_TBEGIN_TRANSIENT\n"
"         && i++ < __retry)\n"
"    __builtin_tx_assist(i);\n"
"\n"
"  return cc;\n"
"}\n"
"\n"
"#define __builtin_tbegin_retry_nofloat(tdb, retry) \\\n"
"  (__builtin_constant_p(tdb == 0) && tdb == 0 ? \\\n"
"   __builtin_tbegin_retry_nofloat_null(retry) : \\\n"
"   __builtin_tbegin_retry_nofloat_tdb(tdb, retry))\n"
"\n"
"#endif /* __s390__ */\n"
"\n"
"#endif /* __HTMINTRIN_H */\n"
"" } , 
 { "/builtins/htmxlintrin.h" , "/*===---- htmxlintrin.h - XL compiler HTM execution intrinsics-------------===*\\\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
"\\*===----------------------------------------------------------------------===*/\n"
"\n"
"#ifndef __HTMXLINTRIN_H\n"
"#define __HTMXLINTRIN_H\n"
"\n"
"#ifndef __HTM__\n"
"#error \"HTM instruction set not enabled\"\n"
"#endif\n"
"\n"
"#include <htmintrin.h>\n"
"\n"
"#ifdef __powerpc__\n"
"\n"
"#ifdef __cplusplus\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"#define _TEXASR_PTR(TM_BUF) ((texasr_t *)((char *)(TM_BUF) + 0))\n"
"#define _TEXASRU_PTR(TM_BUF) ((texasru_t *)((char *)(TM_BUF) + 0))\n"
"#define _TEXASRL_PTR(TM_BUF) ((texasrl_t *)((char *)(TM_BUF) + 4))\n"
"#define _TFIAR_PTR(TM_BUF) ((tfiar_t *)((char *)(TM_BUF) + 8))\n"
"\n"
"typedef char TM_buff_type[16];\n"
"\n"
"/* This macro can be used to determine whether a transaction was successfully\n"
"   started from the __TM_begin() and __TM_simple_begin() intrinsic functions\n"
"   below.  */\n"
"#define _HTM_TBEGIN_STARTED     1\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_simple_begin (void)\n"
"{\n"
"  if (__builtin_expect (__builtin_tbegin (0), 1))\n"
"    return _HTM_TBEGIN_STARTED;\n"
"  return 0;\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_begin (void* const __TM_buff)\n"
"{\n"
"  *_TEXASRL_PTR (__TM_buff) = 0;\n"
"  if (__builtin_expect (__builtin_tbegin (0), 1))\n"
"    return _HTM_TBEGIN_STARTED;\n"
"#ifdef __powerpc64__\n"
"  *_TEXASR_PTR (__TM_buff) = __builtin_get_texasr ();\n"
"#else\n"
"  *_TEXASRU_PTR (__TM_buff) = __builtin_get_texasru ();\n"
"  *_TEXASRL_PTR (__TM_buff) = __builtin_get_texasr ();\n"
"#endif\n"
"  *_TFIAR_PTR (__TM_buff) = __builtin_get_tfiar ();\n"
"  return 0;\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_end (void)\n"
"{\n"
"  if (__builtin_expect (__builtin_tend (0), 1))\n"
"    return 1;\n"
"  return 0;\n"
"}\n"
"\n"
"extern __inline void\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_abort (void)\n"
"{\n"
"  __builtin_tabort (0);\n"
"}\n"
"\n"
"extern __inline void\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_named_abort (unsigned char const __code)\n"
"{\n"
"  __builtin_tabort (__code);\n"
"}\n"
"\n"
"extern __inline void\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_resume (void)\n"
"{\n"
"  __builtin_tresume ();\n"
"}\n"
"\n"
"extern __inline void\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_suspend (void)\n"
"{\n"
"  __builtin_tsuspend ();\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_is_user_abort (void* const __TM_buff)\n"
"{\n"
"  texasru_t texasru = *_TEXASRU_PTR (__TM_buff);\n"
"  return _TEXASRU_ABORT (texasru);\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_is_named_user_abort (void* const __TM_buff, unsigned char *__code)\n"
"{\n"
"  texasru_t texasru = *_TEXASRU_PTR (__TM_buff);\n"
"\n"
"  *__code = _TEXASRU_FAILURE_CODE (texasru);\n"
"  return _TEXASRU_ABORT (texasru);\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_is_illegal (void* const __TM_buff)\n"
"{\n"
"  texasru_t texasru = *_TEXASRU_PTR (__TM_buff);\n"
"  return _TEXASRU_DISALLOWED (texasru);\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_is_footprint_exceeded (void* const __TM_buff)\n"
"{\n"
"  texasru_t texasru = *_TEXASRU_PTR (__TM_buff);\n"
"  return _TEXASRU_FOOTPRINT_OVERFLOW (texasru);\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_nesting_depth (void* const __TM_buff)\n"
"{\n"
"  texasrl_t texasrl;\n"
"\n"
"  if (_HTM_STATE (__builtin_ttest ()) == _HTM_NONTRANSACTIONAL)\n"
"    {\n"
"      texasrl = *_TEXASRL_PTR (__TM_buff);\n"
"      if (!_TEXASR_FAILURE_SUMMARY (texasrl))\n"
"        texasrl = 0;\n"
"    }\n"
"  else\n"
"    texasrl = (texasrl_t) __builtin_get_texasr ();\n"
"\n"
"  return _TEXASR_TRANSACTION_LEVEL (texasrl);\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_is_nested_too_deep(void* const __TM_buff)\n"
"{\n"
"  texasru_t texasru = *_TEXASRU_PTR (__TM_buff);\n"
"  return _TEXASRU_NESTING_OVERFLOW (texasru);\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_is_conflict(void* const __TM_buff)\n"
"{\n"
"  texasru_t texasru = *_TEXASRU_PTR (__TM_buff);\n"
"  /* Return TEXASR bits 11 (Self-Induced Conflict) through\n"
"     14 (Translation Invalidation Conflict).  */\n"
"  return (_TEXASRU_EXTRACT_BITS (texasru, 14, 4)) ? 1 : 0;\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_is_failure_persistent(void* const __TM_buff)\n"
"{\n"
"  texasru_t texasru = *_TEXASRU_PTR (__TM_buff);\n"
"  return _TEXASRU_FAILURE_PERSISTENT (texasru);\n"
"}\n"
"\n"
"extern __inline long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_failure_address(void* const __TM_buff)\n"
"{\n"
"  return *_TFIAR_PTR (__TM_buff);\n"
"}\n"
"\n"
"extern __inline long long\n"
"__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n"
"__TM_failure_code(void* const __TM_buff)\n"
"{\n"
"  return *_TEXASR_PTR (__TM_buff);\n"
"}\n"
"\n"
"#ifdef __cplusplus\n"
"}\n"
"#endif\n"
"\n"
"#endif /* __powerpc__ */\n"
"\n"
"#ifdef __s390__\n"
"\n"
"#include <stdint.h>\n"
"\n"
"/* These intrinsics are being made available for compatibility with\n"
"   the IBM XL compiler.  For documentation please see the \"z/OS XL\n"
"   C/C++ Programming Guide\" publicly available on the web.  */\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_simple_begin ()\n"
"{\n"
"  return __builtin_tbegin_nofloat (0);\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_begin (void* const __tdb)\n"
"{\n"
"  return __builtin_tbegin_nofloat (__tdb);\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_end ()\n"
"{\n"
"  return __builtin_tend ();\n"
"}\n"
"\n"
"static __inline void __attribute__((__always_inline__))\n"
"__TM_abort ()\n"
"{\n"
"  return __builtin_tabort (_HTM_FIRST_USER_ABORT_CODE);\n"
"}\n"
"\n"
"static __inline void __attribute__((__always_inline__, __nodebug__))\n"
"__TM_named_abort (unsigned char const __code)\n"
"{\n"
"  return __builtin_tabort ((int)_HTM_FIRST_USER_ABORT_CODE + __code);\n"
"}\n"
"\n"
"static __inline void __attribute__((__always_inline__, __nodebug__))\n"
"__TM_non_transactional_store (void* const __addr, long long const __value)\n"
"{\n"
"  __builtin_non_tx_store ((uint64_t*)__addr, (uint64_t)__value);\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_nesting_depth (void* const __tdb_ptr)\n"
"{\n"
"  int depth = __builtin_tx_nesting_depth ();\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  if (depth != 0)\n"
"    return depth;\n"
"\n"
"  if (tdb->format != 1)\n"
"    return 0;\n"
"  return tdb->nesting_depth;\n"
"}\n"
"\n"
"/* Transaction failure diagnostics */\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_is_user_abort (void* const __tdb_ptr)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  if (tdb->format != 1)\n"
"    return 0;\n"
"\n"
"  return !!(tdb->abort_code >= _HTM_FIRST_USER_ABORT_CODE);\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_is_named_user_abort (void* const __tdb_ptr, unsigned char* __code)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  if (tdb->format != 1)\n"
"    return 0;\n"
"\n"
"  if (tdb->abort_code >= _HTM_FIRST_USER_ABORT_CODE)\n"
"    {\n"
"      *__code = tdb->abort_code - _HTM_FIRST_USER_ABORT_CODE;\n"
"      return 1;\n"
"    }\n"
"  return 0;\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_is_illegal (void* const __tdb_ptr)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  return (tdb->format == 1\n"
"	  && (tdb->abort_code == 4 /* unfiltered program interruption */\n"
"	      || tdb->abort_code == 11 /* restricted instruction */));\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_is_footprint_exceeded (void* const __tdb_ptr)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  return (tdb->format == 1\n"
"	  && (tdb->abort_code == 7 /* fetch overflow */\n"
"	      || tdb->abort_code == 8 /* store overflow */));\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_is_nested_too_deep (void* const __tdb_ptr)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  return tdb->format == 1 && tdb->abort_code == 13; /* depth exceeded */\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_is_conflict (void* const __tdb_ptr)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  return (tdb->format == 1\n"
"	  && (tdb->abort_code == 9 /* fetch conflict */\n"
"	      || tdb->abort_code == 10 /* store conflict */));\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_is_failure_persistent (long const __result)\n"
"{\n"
"  return __result == _HTM_TBEGIN_PERSISTENT;\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_failure_address (void* const __tdb_ptr)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"  return tdb->atia;\n"
"}\n"
"\n"
"static __inline long __attribute__((__always_inline__, __nodebug__))\n"
"__TM_failure_code (void* const __tdb_ptr)\n"
"{\n"
"  struct __htm_tdb *tdb = (struct __htm_tdb*)__tdb_ptr;\n"
"\n"
"  return tdb->abort_code;\n"
"}\n"
"\n"
"#endif /* __s390__ */\n"
"\n"
"#endif /* __HTMXLINTRIN_H  */\n"
"" } , 
 { "/builtins/ia32intrin.h" , "/* ===-------- ia32intrin.h ---------------------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __X86INTRIN_H\n"
"#error \"Never use <ia32intrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __IA32INTRIN_H\n"
"#define __IA32INTRIN_H\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))\n"
"__readeflags(void)\n"
"{\n"
"  return __builtin_ia32_readeflags_u64();\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__))\n"
"__writeeflags(unsigned long long __f)\n"
"{\n"
"  __builtin_ia32_writeeflags_u64(__f);\n"
"}\n"
"\n"
"#else /* !__x86_64__ */\n"
"static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))\n"
"__readeflags(void)\n"
"{\n"
"  return __builtin_ia32_readeflags_u32();\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__))\n"
"__writeeflags(unsigned int __f)\n"
"{\n"
"  __builtin_ia32_writeeflags_u32(__f);\n"
"}\n"
"#endif /* !__x86_64__ */\n"
"\n"
"static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))\n"
"__rdpmc(int __A) {\n"
"  return __builtin_ia32_rdpmc(__A);\n"
"}\n"
"\n"
"/* __rdtscp */\n"
"static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))\n"
"__rdtscp(unsigned int *__A) {\n"
"  return __builtin_ia32_rdtscp(__A);\n"
"}\n"
"\n"
"#define _rdtsc() __rdtsc()\n"
"\n"
"#define _rdpmc(A) __rdpmc(A)\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__))\n"
"_wbinvd(void) {\n"
"  __builtin_ia32_wbinvd();\n"
"}\n"
"\n"
"#endif /* __IA32INTRIN_H */\n"
"" } , 
 { "/builtins/immintrin.h" , "/*===---- immintrin.h - Intel intrinsics -----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#define __IMMINTRIN_H\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__MMX__)\n"
"#include <mmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SSE__)\n"
"#include <xmmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SSE2__)\n"
"#include <emmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SSE3__)\n"
"#include <pmmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SSSE3__)\n"
"#include <tmmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__SSE4_2__) || defined(__SSE4_1__))\n"
"#include <smmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AES__) || defined(__PCLMUL__))\n"
"#include <wmmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__CLFLUSHOPT__)\n"
"#include <clflushoptintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__CLWB__)\n"
"#include <clwbintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX__)\n"
"#include <avxintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX2__)\n"
"#include <avx2intrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__F16C__)\n"
"#include <f16cintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__VPCLMULQDQ__)\n"
"#include <vpclmulqdqintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__BMI__)\n"
"#include <bmiintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__BMI2__)\n"
"#include <bmi2intrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__LZCNT__)\n"
"#include <lzcntintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__POPCNT__)\n"
"#include <popcntintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__FMA__)\n"
"#include <fmaintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512F__)\n"
"#include <avx512fintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512VL__)\n"
"#include <avx512vlintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512BW__)\n"
"#include <avx512bwintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512BITALG__)\n"
"#include <avx512bitalgintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512CD__)\n"
"#include <avx512cdintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512VPOPCNTDQ__)\n"
"#include <avx512vpopcntdqintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VL__) && defined(__AVX512VPOPCNTDQ__))\n"
"#include <avx512vpopcntdqvlintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512VNNI__)\n"
"#include <avx512vnniintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VL__) && defined(__AVX512VNNI__))\n"
"#include <avx512vlvnniintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512DQ__)\n"
"#include <avx512dqintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VL__) && defined(__AVX512BITALG__))\n"
"#include <avx512vlbitalgintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VL__) && defined(__AVX512BW__))\n"
"#include <avx512vlbwintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VL__) && defined(__AVX512CD__))\n"
"#include <avx512vlcdintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VL__) && defined(__AVX512DQ__))\n"
"#include <avx512vldqintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512ER__)\n"
"#include <avx512erintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512IFMA__)\n"
"#include <avx512ifmaintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512IFMA__) && defined(__AVX512VL__))\n"
"#include <avx512ifmavlintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512VBMI__)\n"
"#include <avx512vbmiintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VBMI__) && defined(__AVX512VL__))\n"
"#include <avx512vbmivlintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512VBMI2__)\n"
"#include <avx512vbmi2intrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"    (defined(__AVX512VBMI2__) && defined(__AVX512VL__))\n"
"#include <avx512vlvbmi2intrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__AVX512PF__)\n"
"#include <avx512pfintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__PKU__)\n"
"#include <pkuintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__VAES__)\n"
"#include <vaesintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__GFNI__)\n"
"#include <gfniintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__RDPID__)\n"
"/// Returns the value of the IA32_TSC_AUX MSR (0xc0000103).\n"
"///\n"
"/// \\headerfile <immintrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> RDPID </c> instruction.\n"
"static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__(\"rdpid\")))\n"
"_rdpid_u32(void) {\n"
"  return __builtin_ia32_rdpid();\n"
"}\n"
"#endif // __RDPID__\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__RDRND__)\n"
"static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__(\"rdrnd\")))\n"
"_rdrand16_step(unsigned short *__p)\n"
"{\n"
"  return __builtin_ia32_rdrand16_step(__p);\n"
"}\n"
"\n"
"static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__(\"rdrnd\")))\n"
"_rdrand32_step(unsigned int *__p)\n"
"{\n"
"  return __builtin_ia32_rdrand32_step(__p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__(\"rdrnd\")))\n"
"_rdrand64_step(unsigned long long *__p)\n"
"{\n"
"  return __builtin_ia32_rdrand64_step(__p);\n"
"}\n"
"#endif\n"
"#endif /* __RDRND__ */\n"
"\n"
"/* __bit_scan_forward */\n"
"static __inline__ int __attribute__((__always_inline__, __nodebug__))\n"
"_bit_scan_forward(int __A) {\n"
"  return __builtin_ctz(__A);\n"
"}\n"
"\n"
"/* __bit_scan_reverse */\n"
"static __inline__ int __attribute__((__always_inline__, __nodebug__))\n"
"_bit_scan_reverse(int __A) {\n"
"  return 31 - __builtin_clz(__A);\n"
"}\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__FSGSBASE__)\n"
"#ifdef __x86_64__\n"
"static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_readfsbase_u32(void)\n"
"{\n"
"  return __builtin_ia32_rdfsbase32();\n"
"}\n"
"\n"
"static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_readfsbase_u64(void)\n"
"{\n"
"  return __builtin_ia32_rdfsbase64();\n"
"}\n"
"\n"
"static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_readgsbase_u32(void)\n"
"{\n"
"  return __builtin_ia32_rdgsbase32();\n"
"}\n"
"\n"
"static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_readgsbase_u64(void)\n"
"{\n"
"  return __builtin_ia32_rdgsbase64();\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_writefsbase_u32(unsigned int __V)\n"
"{\n"
"  __builtin_ia32_wrfsbase32(__V);\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_writefsbase_u64(unsigned long long __V)\n"
"{\n"
"  __builtin_ia32_wrfsbase64(__V);\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_writegsbase_u32(unsigned int __V)\n"
"{\n"
"  __builtin_ia32_wrgsbase32(__V);\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"fsgsbase\")))\n"
"_writegsbase_u64(unsigned long long __V)\n"
"{\n"
"  __builtin_ia32_wrgsbase64(__V);\n"
"}\n"
"\n"
"#endif\n"
"#endif /* __FSGSBASE__ */\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__MOVBE__)\n"
"\n"
"/* The structs used below are to force the load/store to be unaligned. This\n"
" * is accomplished with the __packed__ attribute. The __may_alias__ prevents\n"
" * tbaa metadata from being generated based on the struct and the type of the\n"
" * field inside of it.\n"
" */\n"
"\n"
"static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__(\"movbe\")))\n"
"_loadbe_i16(void const * __P) {\n"
"  struct __loadu_i16 {\n"
"    short __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return __builtin_bswap16(((struct __loadu_i16*)__P)->__v);\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"movbe\")))\n"
"_storebe_i16(void * __P, short __D) {\n"
"  struct __storeu_i16 {\n"
"    short __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_i16*)__P)->__v = __builtin_bswap16(__D);\n"
"}\n"
"\n"
"static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__(\"movbe\")))\n"
"_loadbe_i32(void const * __P) {\n"
"  struct __loadu_i32 {\n"
"    int __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return __builtin_bswap32(((struct __loadu_i32*)__P)->__v);\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"movbe\")))\n"
"_storebe_i32(void * __P, int __D) {\n"
"  struct __storeu_i32 {\n"
"    int __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_i32*)__P)->__v = __builtin_bswap32(__D);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__(\"movbe\")))\n"
"_loadbe_i64(void const * __P) {\n"
"  struct __loadu_i64 {\n"
"    long long __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return __builtin_bswap64(((struct __loadu_i64*)__P)->__v);\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"movbe\")))\n"
"_storebe_i64(void * __P, long long __D) {\n"
"  struct __storeu_i64 {\n"
"    long long __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_i64*)__P)->__v = __builtin_bswap64(__D);\n"
"}\n"
"#endif\n"
"#endif /* __MOVBE */\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__RTM__)\n"
"#include <rtmintrin.h>\n"
"#include <xtestintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SHA__)\n"
"#include <shaintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__FXSR__)\n"
"#include <fxsrintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__XSAVE__)\n"
"#include <xsaveintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__XSAVEOPT__)\n"
"#include <xsaveoptintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__XSAVEC__)\n"
"#include <xsavecintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__XSAVES__)\n"
"#include <xsavesintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SHSTK__)\n"
"#include <cetintrin.h>\n"
"#endif\n"
"\n"
"/* Some intrinsics inside adxintrin.h are available only on processors with ADX,\n"
" * whereas others are also available at all times. */\n"
"#include <adxintrin.h>\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__RDSEED__)\n"
"#include <rdseedintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__WBNOINVD__)\n"
"#include <wbnoinvdintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__CLDEMOTE__)\n"
"#include <cldemoteintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__WAITPKG__)\n"
"#include <waitpkgintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || \\\n"
"  defined(__MOVDIRI__) || defined(__MOVDIR64B__)\n"
"#include <movdirintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__PCONFIG__)\n"
"#include <pconfigintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SGX__)\n"
"#include <sgxintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__PTWRITE__)\n"
"#include <ptwriteintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__INVPCID__)\n"
"#include <invpcidintrin.h>\n"
"#endif\n"
"\n"
"#ifdef _MSC_VER\n"
"/* Define the default attributes for these intrinsics */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__))\n"
"#ifdef __cplusplus\n"
"extern \"C\" {\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Exchange HLE\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__i386__) || defined(__x86_64__)\n"
"static __inline__ long __DEFAULT_FN_ATTRS\n"
"_InterlockedExchange_HLEAcquire(long volatile *_Target, long _Value) {\n"
"  __asm__ __volatile__(\".byte 0xf2 ; lock ; xchg %0, %1\"\n"
"                       : \"+r\" (_Value), \"+m\" (*_Target) :: \"memory\");\n"
"  return _Value;\n"
"}\n"
"static __inline__ long __DEFAULT_FN_ATTRS\n"
"_InterlockedExchange_HLERelease(long volatile *_Target, long _Value) {\n"
"  __asm__ __volatile__(\".byte 0xf3 ; lock ; xchg %0, %1\"\n"
"                       : \"+r\" (_Value), \"+m\" (*_Target) :: \"memory\");\n"
"  return _Value;\n"
"}\n"
"#endif\n"
"#if defined(__x86_64__)\n"
"static __inline__ __int64 __DEFAULT_FN_ATTRS\n"
"_InterlockedExchange64_HLEAcquire(__int64 volatile *_Target, __int64 _Value) {\n"
"  __asm__ __volatile__(\".byte 0xf2 ; lock ; xchg %0, %1\"\n"
"                       : \"+r\" (_Value), \"+m\" (*_Target) :: \"memory\");\n"
"  return _Value;\n"
"}\n"
"static __inline__ __int64 __DEFAULT_FN_ATTRS\n"
"_InterlockedExchange64_HLERelease(__int64 volatile *_Target, __int64 _Value) {\n"
"  __asm__ __volatile__(\".byte 0xf3 ; lock ; xchg %0, %1\"\n"
"                       : \"+r\" (_Value), \"+m\" (*_Target) :: \"memory\");\n"
"  return _Value;\n"
"}\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Compare Exchange HLE\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__i386__) || defined(__x86_64__)\n"
"static __inline__ long __DEFAULT_FN_ATTRS\n"
"_InterlockedCompareExchange_HLEAcquire(long volatile *_Destination,\n"
"                              long _Exchange, long _Comparand) {\n"
"  __asm__ __volatile__(\".byte 0xf2 ; lock ; cmpxchg %2, %1\"\n"
"                       : \"+a\" (_Comparand), \"+m\" (*_Destination)\n"
"                       : \"r\" (_Exchange) : \"memory\");\n"
"  return _Comparand;\n"
"}\n"
"static __inline__ long __DEFAULT_FN_ATTRS\n"
"_InterlockedCompareExchange_HLERelease(long volatile *_Destination,\n"
"                              long _Exchange, long _Comparand) {\n"
"  __asm__ __volatile__(\".byte 0xf3 ; lock ; cmpxchg %2, %1\"\n"
"                       : \"+a\" (_Comparand), \"+m\" (*_Destination)\n"
"                       : \"r\" (_Exchange) : \"memory\");\n"
"  return _Comparand;\n"
"}\n"
"#endif\n"
"#if defined(__x86_64__)\n"
"static __inline__ __int64 __DEFAULT_FN_ATTRS\n"
"_InterlockedCompareExchange64_HLEAcquire(__int64 volatile *_Destination,\n"
"                              __int64 _Exchange, __int64 _Comparand) {\n"
"  __asm__ __volatile__(\".byte 0xf2 ; lock ; cmpxchg %2, %1\"\n"
"                       : \"+a\" (_Comparand), \"+m\" (*_Destination)\n"
"                       : \"r\" (_Exchange) : \"memory\");\n"
"  return _Comparand;\n"
"}\n"
"static __inline__ __int64 __DEFAULT_FN_ATTRS\n"
"_InterlockedCompareExchange64_HLERelease(__int64 volatile *_Destination,\n"
"                              __int64 _Exchange, __int64 _Comparand) {\n"
"  __asm__ __volatile__(\".byte 0xf3 ; lock ; cmpxchg %2, %1\"\n"
"                       : \"+a\" (_Comparand), \"+m\" (*_Destination)\n"
"                       : \"r\" (_Exchange) : \"memory\");\n"
"  return _Comparand;\n"
"}\n"
"#endif\n"
"#ifdef __cplusplus\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* _MSC_VER */\n"
"\n"
"#endif /* __IMMINTRIN_H */\n"
"" } , 
 { "/builtins/intrin.h" , "/* ===-------- intrin.h ---------------------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"/* Only include this if we're compiling for the windows platform. */\n"
"#ifndef _MSC_VER\n"
"#include_next <intrin.h>\n"
"#else\n"
"\n"
"#ifndef __INTRIN_H\n"
"#define __INTRIN_H\n"
"\n"
"/* First include the standard intrinsics. */\n"
"#if defined(__i386__) || defined(__x86_64__)\n"
"#include <x86intrin.h>\n"
"#endif\n"
"\n"
"#if defined(__arm__)\n"
"#include <armintr.h>\n"
"#endif\n"
"\n"
"#if defined(__aarch64__)\n"
"#include <arm64intr.h>\n"
"#endif\n"
"\n"
"/* For the definition of jmp_buf. */\n"
"#if __STDC_HOSTED__\n"
"#include <setjmp.h>\n"
"#endif\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__))\n"
"\n"
"#ifdef __cplusplus\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"#if defined(__MMX__)\n"
"/* And the random ones that aren't in those files. */\n"
"__m64 _m_from_float(float);\n"
"float _m_to_float(__m64);\n"
"#endif\n"
"\n"
"/* Other assorted instruction intrinsics. */\n"
"void __addfsbyte(unsigned long, unsigned char);\n"
"void __addfsdword(unsigned long, unsigned long);\n"
"void __addfsword(unsigned long, unsigned short);\n"
"void __code_seg(const char *);\n"
"static __inline__\n"
"void __cpuid(int[4], int);\n"
"static __inline__\n"
"void __cpuidex(int[4], int, int);\n"
"static __inline__\n"
"__int64 __emul(int, int);\n"
"static __inline__\n"
"unsigned __int64 __emulu(unsigned int, unsigned int);\n"
"unsigned int __getcallerseflags(void);\n"
"static __inline__\n"
"void __halt(void);\n"
"unsigned char __inbyte(unsigned short);\n"
"void __inbytestring(unsigned short, unsigned char *, unsigned long);\n"
"void __incfsbyte(unsigned long);\n"
"void __incfsdword(unsigned long);\n"
"void __incfsword(unsigned long);\n"
"unsigned long __indword(unsigned short);\n"
"void __indwordstring(unsigned short, unsigned long *, unsigned long);\n"
"void __int2c(void);\n"
"void __invlpg(void *);\n"
"unsigned short __inword(unsigned short);\n"
"void __inwordstring(unsigned short, unsigned short *, unsigned long);\n"
"void __lidt(void *);\n"
"unsigned __int64 __ll_lshift(unsigned __int64, int);\n"
"__int64 __ll_rshift(__int64, int);\n"
"static __inline__\n"
"void __movsb(unsigned char *, unsigned char const *, size_t);\n"
"static __inline__\n"
"void __movsd(unsigned long *, unsigned long const *, size_t);\n"
"static __inline__\n"
"void __movsw(unsigned short *, unsigned short const *, size_t);\n"
"static __inline__\n"
"void __nop(void);\n"
"void __nvreg_restore_fence(void);\n"
"void __nvreg_save_fence(void);\n"
"void __outbyte(unsigned short, unsigned char);\n"
"void __outbytestring(unsigned short, unsigned char *, unsigned long);\n"
"void __outdword(unsigned short, unsigned long);\n"
"void __outdwordstring(unsigned short, unsigned long *, unsigned long);\n"
"void __outword(unsigned short, unsigned short);\n"
"void __outwordstring(unsigned short, unsigned short *, unsigned long);\n"
"unsigned long __readcr0(void);\n"
"unsigned long __readcr2(void);\n"
"static __inline__\n"
"unsigned long __readcr3(void);\n"
"unsigned long __readcr4(void);\n"
"unsigned long __readcr8(void);\n"
"unsigned int __readdr(unsigned int);\n"
"#ifdef __i386__\n"
"static __inline__\n"
"unsigned char __readfsbyte(unsigned long);\n"
"static __inline__\n"
"unsigned __int64 __readfsqword(unsigned long);\n"
"static __inline__\n"
"unsigned short __readfsword(unsigned long);\n"
"#endif\n"
"static __inline__\n"
"unsigned __int64 __readmsr(unsigned long);\n"
"unsigned __int64 __readpmc(unsigned long);\n"
"unsigned long __segmentlimit(unsigned long);\n"
"void __sidt(void *);\n"
"static __inline__\n"
"void __stosb(unsigned char *, unsigned char, size_t);\n"
"static __inline__\n"
"void __stosd(unsigned long *, unsigned long, size_t);\n"
"static __inline__\n"
"void __stosw(unsigned short *, unsigned short, size_t);\n"
"void __svm_clgi(void);\n"
"void __svm_invlpga(void *, int);\n"
"void __svm_skinit(int);\n"
"void __svm_stgi(void);\n"
"void __svm_vmload(size_t);\n"
"void __svm_vmrun(size_t);\n"
"void __svm_vmsave(size_t);\n"
"void __ud2(void);\n"
"unsigned __int64 __ull_rshift(unsigned __int64, int);\n"
"void __vmx_off(void);\n"
"void __vmx_vmptrst(unsigned __int64 *);\n"
"void __wbinvd(void);\n"
"void __writecr0(unsigned int);\n"
"static __inline__\n"
"void __writecr3(unsigned int);\n"
"void __writecr4(unsigned int);\n"
"void __writecr8(unsigned int);\n"
"void __writedr(unsigned int, unsigned int);\n"
"void __writefsbyte(unsigned long, unsigned char);\n"
"void __writefsdword(unsigned long, unsigned long);\n"
"void __writefsqword(unsigned long, unsigned __int64);\n"
"void __writefsword(unsigned long, unsigned short);\n"
"void __writemsr(unsigned long, unsigned __int64);\n"
"static __inline__\n"
"void *_AddressOfReturnAddress(void);\n"
"static __inline__\n"
"unsigned char _BitScanForward(unsigned long *_Index, unsigned long _Mask);\n"
"static __inline__\n"
"unsigned char _BitScanReverse(unsigned long *_Index, unsigned long _Mask);\n"
"unsigned char _bittest(long const *, long);\n"
"unsigned char _bittestandcomplement(long *, long);\n"
"unsigned char _bittestandreset(long *, long);\n"
"unsigned char _bittestandset(long *, long);\n"
"void __cdecl _disable(void);\n"
"void __cdecl _enable(void);\n"
"long _InterlockedAddLargeStatistic(__int64 volatile *_Addend, long _Value);\n"
"unsigned char _interlockedbittestandreset(long volatile *, long);\n"
"unsigned char _interlockedbittestandset(long volatile *, long);\n"
"void *_InterlockedCompareExchangePointer_HLEAcquire(void *volatile *, void *,\n"
"                                                    void *);\n"
"void *_InterlockedCompareExchangePointer_HLERelease(void *volatile *, void *,\n"
"                                                    void *);\n"
"long _InterlockedExchangeAdd_HLEAcquire(long volatile *, long);\n"
"long _InterlockedExchangeAdd_HLERelease(long volatile *, long);\n"
"__int64 _InterlockedExchangeAdd64_HLEAcquire(__int64 volatile *, __int64);\n"
"__int64 _InterlockedExchangeAdd64_HLERelease(__int64 volatile *, __int64);\n"
"void __cdecl _invpcid(unsigned int, void *);\n"
"static __inline__ void\n"
"__attribute__((__deprecated__(\"use other intrinsics or C++11 atomics instead\")))\n"
"_ReadBarrier(void);\n"
"static __inline__ void\n"
"__attribute__((__deprecated__(\"use other intrinsics or C++11 atomics instead\")))\n"
"_ReadWriteBarrier(void);\n"
"unsigned int _rorx_u32(unsigned int, const unsigned int);\n"
"int _sarx_i32(int, unsigned int);\n"
"#if __STDC_HOSTED__\n"
"int __cdecl _setjmp(jmp_buf);\n"
"#endif\n"
"unsigned int _shlx_u32(unsigned int, unsigned int);\n"
"unsigned int _shrx_u32(unsigned int, unsigned int);\n"
"void _Store_HLERelease(long volatile *, long);\n"
"void _Store64_HLERelease(__int64 volatile *, __int64);\n"
"void _StorePointer_HLERelease(void *volatile *, void *);\n"
"static __inline__ void\n"
"__attribute__((__deprecated__(\"use other intrinsics or C++11 atomics instead\")))\n"
"_WriteBarrier(void);\n"
"unsigned __int32 xbegin(void);\n"
"void _xend(void);\n"
"static __inline__\n"
"#define _XCR_XFEATURE_ENABLED_MASK 0\n"
"unsigned __int64 __cdecl _xgetbv(unsigned int);\n"
"void __cdecl _xsetbv(unsigned int, unsigned __int64);\n"
"\n"
"/* These additional intrinsics are turned on in x64/amd64/x86_64 mode. */\n"
"#ifdef __x86_64__\n"
"void __addgsbyte(unsigned long, unsigned char);\n"
"void __addgsdword(unsigned long, unsigned long);\n"
"void __addgsqword(unsigned long, unsigned __int64);\n"
"void __addgsword(unsigned long, unsigned short);\n"
"static __inline__\n"
"void __faststorefence(void);\n"
"void __incgsbyte(unsigned long);\n"
"void __incgsdword(unsigned long);\n"
"void __incgsqword(unsigned long);\n"
"void __incgsword(unsigned long);\n"
"static __inline__\n"
"void __movsq(unsigned long long *, unsigned long long const *, size_t);\n"
"static __inline__\n"
"unsigned char __readgsbyte(unsigned long);\n"
"static __inline__\n"
"unsigned long __readgsdword(unsigned long);\n"
"static __inline__\n"
"unsigned __int64 __readgsqword(unsigned long);\n"
"unsigned short __readgsword(unsigned long);\n"
"unsigned __int64 __shiftleft128(unsigned __int64 _LowPart,\n"
"                                unsigned __int64 _HighPart,\n"
"                                unsigned char _Shift);\n"
"unsigned __int64 __shiftright128(unsigned __int64 _LowPart,\n"
"                                 unsigned __int64 _HighPart,\n"
"                                 unsigned char _Shift);\n"
"static __inline__\n"
"void __stosq(unsigned __int64 *, unsigned __int64, size_t);\n"
"unsigned char __vmx_on(unsigned __int64 *);\n"
"unsigned char __vmx_vmclear(unsigned __int64 *);\n"
"unsigned char __vmx_vmlaunch(void);\n"
"unsigned char __vmx_vmptrld(unsigned __int64 *);\n"
"unsigned char __vmx_vmread(size_t, size_t *);\n"
"unsigned char __vmx_vmresume(void);\n"
"unsigned char __vmx_vmwrite(size_t, size_t);\n"
"void __writegsbyte(unsigned long, unsigned char);\n"
"void __writegsdword(unsigned long, unsigned long);\n"
"void __writegsqword(unsigned long, unsigned __int64);\n"
"void __writegsword(unsigned long, unsigned short);\n"
"unsigned char _bittest64(__int64 const *, __int64);\n"
"unsigned char _bittestandcomplement64(__int64 *, __int64);\n"
"unsigned char _bittestandreset64(__int64 *, __int64);\n"
"unsigned char _bittestandset64(__int64 *, __int64);\n"
"long _InterlockedAnd_np(long volatile *_Value, long _Mask);\n"
"short _InterlockedAnd16_np(short volatile *_Value, short _Mask);\n"
"__int64 _InterlockedAnd64_np(__int64 volatile *_Value, __int64 _Mask);\n"
"char _InterlockedAnd8_np(char volatile *_Value, char _Mask);\n"
"unsigned char _interlockedbittestandreset64(__int64 volatile *, __int64);\n"
"unsigned char _interlockedbittestandset64(__int64 volatile *, __int64);\n"
"long _InterlockedCompareExchange_np(long volatile *_Destination, long _Exchange,\n"
"                                    long _Comparand);\n"
"unsigned char _InterlockedCompareExchange128(__int64 volatile *_Destination,\n"
"                                             __int64 _ExchangeHigh,\n"
"                                             __int64 _ExchangeLow,\n"
"                                             __int64 *_CompareandResult);\n"
"unsigned char _InterlockedCompareExchange128_np(__int64 volatile *_Destination,\n"
"                                                __int64 _ExchangeHigh,\n"
"                                                __int64 _ExchangeLow,\n"
"                                                __int64 *_ComparandResult);\n"
"short _InterlockedCompareExchange16_np(short volatile *_Destination,\n"
"                                       short _Exchange, short _Comparand);\n"
"__int64 _InterlockedCompareExchange64_np(__int64 volatile *_Destination,\n"
"                                         __int64 _Exchange, __int64 _Comparand);\n"
"void *_InterlockedCompareExchangePointer_np(void *volatile *_Destination,\n"
"                                            void *_Exchange, void *_Comparand);\n"
"long _InterlockedOr_np(long volatile *_Value, long _Mask);\n"
"short _InterlockedOr16_np(short volatile *_Value, short _Mask);\n"
"__int64 _InterlockedOr64_np(__int64 volatile *_Value, __int64 _Mask);\n"
"char _InterlockedOr8_np(char volatile *_Value, char _Mask);\n"
"long _InterlockedXor_np(long volatile *_Value, long _Mask);\n"
"short _InterlockedXor16_np(short volatile *_Value, short _Mask);\n"
"__int64 _InterlockedXor64_np(__int64 volatile *_Value, __int64 _Mask);\n"
"char _InterlockedXor8_np(char volatile *_Value, char _Mask);\n"
"unsigned __int64 _rorx_u64(unsigned __int64, const unsigned int);\n"
"__int64 _sarx_i64(__int64, unsigned int);\n"
"unsigned __int64 _shlx_u64(unsigned __int64, unsigned int);\n"
"unsigned __int64 _shrx_u64(unsigned __int64, unsigned int);\n"
"static __inline__\n"
"__int64 __mulh(__int64, __int64);\n"
"static __inline__\n"
"unsigned __int64 __umulh(unsigned __int64, unsigned __int64);\n"
"static __inline__\n"
"__int64 _mul128(__int64, __int64, __int64*);\n"
"static __inline__\n"
"unsigned __int64 _umul128(unsigned __int64,\n"
"                          unsigned __int64,\n"
"                          unsigned __int64*);\n"
"\n"
"#endif /* __x86_64__ */\n"
"\n"
"#if defined(__x86_64__) || defined(__arm__) || defined(__aarch64__)\n"
"\n"
"static __inline__\n"
"unsigned char _BitScanForward64(unsigned long *_Index, unsigned __int64 _Mask);\n"
"static __inline__\n"
"unsigned char _BitScanReverse64(unsigned long *_Index, unsigned __int64 _Mask);\n"
"\n"
"static __inline__\n"
"__int64 _InterlockedDecrement64(__int64 volatile *_Addend);\n"
"static __inline__\n"
"__int64 _InterlockedExchange64(__int64 volatile *_Target, __int64 _Value);\n"
"static __inline__\n"
"__int64 _InterlockedExchangeAdd64(__int64 volatile *_Addend, __int64 _Value);\n"
"static __inline__\n"
"__int64 _InterlockedExchangeSub64(__int64 volatile *_Subend, __int64 _Value);\n"
"static __inline__\n"
"__int64 _InterlockedIncrement64(__int64 volatile *_Addend);\n"
"static __inline__\n"
"__int64 _InterlockedOr64(__int64 volatile *_Value, __int64 _Mask);\n"
"static __inline__\n"
"__int64 _InterlockedXor64(__int64 volatile *_Value, __int64 _Mask);\n"
"static __inline__\n"
"__int64 _InterlockedAnd64(__int64 volatile *_Value, __int64 _Mask);\n"
"\n"
"#endif\n"
"\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Exchange Add\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"char _InterlockedExchangeAdd8_acq(char volatile *_Addend, char _Value);\n"
"char _InterlockedExchangeAdd8_nf(char volatile *_Addend, char _Value);\n"
"char _InterlockedExchangeAdd8_rel(char volatile *_Addend, char _Value);\n"
"short _InterlockedExchangeAdd16_acq(short volatile *_Addend, short _Value);\n"
"short _InterlockedExchangeAdd16_nf(short volatile *_Addend, short _Value);\n"
"short _InterlockedExchangeAdd16_rel(short volatile *_Addend, short _Value);\n"
"long _InterlockedExchangeAdd_acq(long volatile *_Addend, long _Value);\n"
"long _InterlockedExchangeAdd_nf(long volatile *_Addend, long _Value);\n"
"long _InterlockedExchangeAdd_rel(long volatile *_Addend, long _Value);\n"
"__int64 _InterlockedExchangeAdd64_acq(__int64 volatile *_Addend, __int64 _Value);\n"
"__int64 _InterlockedExchangeAdd64_nf(__int64 volatile *_Addend, __int64 _Value);\n"
"__int64 _InterlockedExchangeAdd64_rel(__int64 volatile *_Addend, __int64 _Value);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Increment\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"short _InterlockedIncrement16_acq(short volatile *_Value);\n"
"short _InterlockedIncrement16_nf(short volatile *_Value);\n"
"short _InterlockedIncrement16_rel(short volatile *_Value);\n"
"long _InterlockedIncrement_acq(long volatile *_Value);\n"
"long _InterlockedIncrement_nf(long volatile *_Value);\n"
"long _InterlockedIncrement_rel(long volatile *_Value);\n"
"__int64 _InterlockedIncrement64_acq(__int64 volatile *_Value);\n"
"__int64 _InterlockedIncrement64_nf(__int64 volatile *_Value);\n"
"__int64 _InterlockedIncrement64_rel(__int64 volatile *_Value);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Decrement\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"short _InterlockedDecrement16_acq(short volatile *_Value);\n"
"short _InterlockedDecrement16_nf(short volatile *_Value);\n"
"short _InterlockedDecrement16_rel(short volatile *_Value);\n"
"long _InterlockedDecrement_acq(long volatile *_Value);\n"
"long _InterlockedDecrement_nf(long volatile *_Value);\n"
"long _InterlockedDecrement_rel(long volatile *_Value);\n"
"__int64 _InterlockedDecrement64_acq(__int64 volatile *_Value);\n"
"__int64 _InterlockedDecrement64_nf(__int64 volatile *_Value);\n"
"__int64 _InterlockedDecrement64_rel(__int64 volatile *_Value);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked And\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"char _InterlockedAnd8_acq(char volatile *_Value, char _Mask);\n"
"char _InterlockedAnd8_nf(char volatile *_Value, char _Mask);\n"
"char _InterlockedAnd8_rel(char volatile *_Value, char _Mask);\n"
"short _InterlockedAnd16_acq(short volatile *_Value, short _Mask);\n"
"short _InterlockedAnd16_nf(short volatile *_Value, short _Mask);\n"
"short _InterlockedAnd16_rel(short volatile *_Value, short _Mask);\n"
"long _InterlockedAnd_acq(long volatile *_Value, long _Mask);\n"
"long _InterlockedAnd_nf(long volatile *_Value, long _Mask);\n"
"long _InterlockedAnd_rel(long volatile *_Value, long _Mask);\n"
"__int64 _InterlockedAnd64_acq(__int64 volatile *_Value, __int64 _Mask);\n"
"__int64 _InterlockedAnd64_nf(__int64 volatile *_Value, __int64 _Mask);\n"
"__int64 _InterlockedAnd64_rel(__int64 volatile *_Value, __int64 _Mask);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Bit Counting and Testing\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"unsigned char _interlockedbittestandset_acq(long volatile *_BitBase,\n"
"                                            long _BitPos);\n"
"unsigned char _interlockedbittestandset_nf(long volatile *_BitBase,\n"
"                                           long _BitPos);\n"
"unsigned char _interlockedbittestandset_rel(long volatile *_BitBase,\n"
"                                            long _BitPos);\n"
"unsigned char _interlockedbittestandreset_acq(long volatile *_BitBase,\n"
"                                              long _BitPos);\n"
"unsigned char _interlockedbittestandreset_nf(long volatile *_BitBase,\n"
"                                             long _BitPos);\n"
"unsigned char _interlockedbittestandreset_rel(long volatile *_BitBase,\n"
"                                              long _BitPos);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Or\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"char _InterlockedOr8_acq(char volatile *_Value, char _Mask);\n"
"char _InterlockedOr8_nf(char volatile *_Value, char _Mask);\n"
"char _InterlockedOr8_rel(char volatile *_Value, char _Mask);\n"
"short _InterlockedOr16_acq(short volatile *_Value, short _Mask);\n"
"short _InterlockedOr16_nf(short volatile *_Value, short _Mask);\n"
"short _InterlockedOr16_rel(short volatile *_Value, short _Mask);\n"
"long _InterlockedOr_acq(long volatile *_Value, long _Mask);\n"
"long _InterlockedOr_nf(long volatile *_Value, long _Mask);\n"
"long _InterlockedOr_rel(long volatile *_Value, long _Mask);\n"
"__int64 _InterlockedOr64_acq(__int64 volatile *_Value, __int64 _Mask);\n"
"__int64 _InterlockedOr64_nf(__int64 volatile *_Value, __int64 _Mask);\n"
"__int64 _InterlockedOr64_rel(__int64 volatile *_Value, __int64 _Mask);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Xor\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"char _InterlockedXor8_acq(char volatile *_Value, char _Mask);\n"
"char _InterlockedXor8_nf(char volatile *_Value, char _Mask);\n"
"char _InterlockedXor8_rel(char volatile *_Value, char _Mask);\n"
"short _InterlockedXor16_acq(short volatile *_Value, short _Mask);\n"
"short _InterlockedXor16_nf(short volatile *_Value, short _Mask);\n"
"short _InterlockedXor16_rel(short volatile *_Value, short _Mask);\n"
"long _InterlockedXor_acq(long volatile *_Value, long _Mask);\n"
"long _InterlockedXor_nf(long volatile *_Value, long _Mask);\n"
"long _InterlockedXor_rel(long volatile *_Value, long _Mask);\n"
"__int64 _InterlockedXor64_acq(__int64 volatile *_Value, __int64 _Mask);\n"
"__int64 _InterlockedXor64_nf(__int64 volatile *_Value, __int64 _Mask);\n"
"__int64 _InterlockedXor64_rel(__int64 volatile *_Value, __int64 _Mask);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Exchange\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"char _InterlockedExchange8_acq(char volatile *_Target, char _Value);\n"
"char _InterlockedExchange8_nf(char volatile *_Target, char _Value);\n"
"char _InterlockedExchange8_rel(char volatile *_Target, char _Value);\n"
"short _InterlockedExchange16_acq(short volatile *_Target, short _Value);\n"
"short _InterlockedExchange16_nf(short volatile *_Target, short _Value);\n"
"short _InterlockedExchange16_rel(short volatile *_Target, short _Value);\n"
"long _InterlockedExchange_acq(long volatile *_Target, long _Value);\n"
"long _InterlockedExchange_nf(long volatile *_Target, long _Value);\n"
"long _InterlockedExchange_rel(long volatile *_Target, long _Value);\n"
"__int64 _InterlockedExchange64_acq(__int64 volatile *_Target, __int64 _Value);\n"
"__int64 _InterlockedExchange64_nf(__int64 volatile *_Target, __int64 _Value);\n"
"__int64 _InterlockedExchange64_rel(__int64 volatile *_Target, __int64 _Value);\n"
"#endif\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Interlocked Compare Exchange\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__arm__) || defined(__aarch64__)\n"
"char _InterlockedCompareExchange8_acq(char volatile *_Destination,\n"
"                             char _Exchange, char _Comparand);\n"
"char _InterlockedCompareExchange8_nf(char volatile *_Destination,\n"
"                             char _Exchange, char _Comparand);\n"
"char _InterlockedCompareExchange8_rel(char volatile *_Destination,\n"
"                             char _Exchange, char _Comparand);\n"
"short _InterlockedCompareExchange16_acq(short volatile *_Destination,\n"
"                              short _Exchange, short _Comparand);\n"
"short _InterlockedCompareExchange16_nf(short volatile *_Destination,\n"
"                              short _Exchange, short _Comparand);\n"
"short _InterlockedCompareExchange16_rel(short volatile *_Destination,\n"
"                              short _Exchange, short _Comparand);\n"
"long _InterlockedCompareExchange_acq(long volatile *_Destination,\n"
"                              long _Exchange, long _Comparand);\n"
"long _InterlockedCompareExchange_nf(long volatile *_Destination,\n"
"                              long _Exchange, long _Comparand);\n"
"long _InterlockedCompareExchange_rel(long volatile *_Destination,\n"
"                              long _Exchange, long _Comparand);\n"
"__int64 _InterlockedCompareExchange64_acq(__int64 volatile *_Destination,\n"
"                              __int64 _Exchange, __int64 _Comparand);\n"
"__int64 _InterlockedCompareExchange64_nf(__int64 volatile *_Destination,\n"
"                              __int64 _Exchange, __int64 _Comparand);\n"
"__int64 _InterlockedCompareExchange64_rel(__int64 volatile *_Destination,\n"
"                              __int64 _Exchange, __int64 _Comparand);\n"
"#endif\n"
"\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* movs, stos\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__i386__) || defined(__x86_64__)\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__movsb(unsigned char *__dst, unsigned char const *__src, size_t __n) {\n"
"  __asm__ __volatile__(\"rep movsb\" : \"+D\"(__dst), \"+S\"(__src), \"+c\"(__n)\n"
"                       : : \"memory\");\n"
"}\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__movsd(unsigned long *__dst, unsigned long const *__src, size_t __n) {\n"
"  __asm__ __volatile__(\"rep movsl\" : \"+D\"(__dst), \"+S\"(__src), \"+c\"(__n)\n"
"                       : : \"memory\");\n"
"}\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__movsw(unsigned short *__dst, unsigned short const *__src, size_t __n) {\n"
"  __asm__ __volatile__(\"rep movsw\" : \"+D\"(__dst), \"+S\"(__src), \"+c\"(__n)\n"
"                       : : \"memory\");\n"
"}\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__stosd(unsigned long *__dst, unsigned long __x, size_t __n) {\n"
"  __asm__ __volatile__(\"rep stosl\" : \"+D\"(__dst), \"+c\"(__n) : \"a\"(__x)\n"
"                       : \"memory\");\n"
"}\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__stosw(unsigned short *__dst, unsigned short __x, size_t __n) {\n"
"  __asm__ __volatile__(\"rep stosw\" : \"+D\"(__dst), \"+c\"(__n) : \"a\"(__x)\n"
"                       : \"memory\");\n"
"}\n"
"#endif\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__movsq(unsigned long long *__dst, unsigned long long const *__src, size_t __n) {\n"
"  __asm__ __volatile__(\"rep movsq\" : \"+D\"(__dst), \"+S\"(__src), \"+c\"(__n)\n"
"                       : : \"memory\");\n"
"}\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__stosq(unsigned __int64 *__dst, unsigned __int64 __x, size_t __n) {\n"
"  __asm__ __volatile__(\"rep stosq\" : \"+D\"(__dst), \"+c\"(__n) : \"a\"(__x)\n"
"                       : \"memory\");\n"
"}\n"
"#endif\n"
"\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Misc\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__i386__) || defined(__x86_64__)\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__cpuid(int __info[4], int __level) {\n"
"  __asm__ (\"cpuid\" : \"=a\"(__info[0]), \"=b\" (__info[1]), \"=c\"(__info[2]), \"=d\"(__info[3])\n"
"                   : \"a\"(__level), \"c\"(0));\n"
"}\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__cpuidex(int __info[4], int __level, int __ecx) {\n"
"  __asm__ (\"cpuid\" : \"=a\"(__info[0]), \"=b\" (__info[1]), \"=c\"(__info[2]), \"=d\"(__info[3])\n"
"                   : \"a\"(__level), \"c\"(__ecx));\n"
"}\n"
"static __inline__ unsigned __int64 __cdecl __DEFAULT_FN_ATTRS\n"
"_xgetbv(unsigned int __xcr_no) {\n"
"  unsigned int __eax, __edx;\n"
"  __asm__ (\"xgetbv\" : \"=a\" (__eax), \"=d\" (__edx) : \"c\" (__xcr_no));\n"
"  return ((unsigned __int64)__edx << 32) | __eax;\n"
"}\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__halt(void) {\n"
"  __asm__ volatile (\"hlt\");\n"
"}\n"
"#endif\n"
"\n"
"#if defined(__i386__) || defined(__x86_64__) || defined(__aarch64__)\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__nop(void) {\n"
"  __asm__ volatile (\"nop\");\n"
"}\n"
"#endif\n"
"\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* MS AArch64 specific\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__aarch64__)\n"
"unsigned __int64 __getReg(int);\n"
"long _InterlockedAdd(long volatile *Addend, long Value);\n"
"__int64 _ReadStatusReg(int);\n"
"void _WriteStatusReg(int, __int64);\n"
"\n"
"static inline unsigned short _byteswap_ushort (unsigned short val) {\n"
"  return __builtin_bswap16(val);\n"
"}\n"
"static inline unsigned long _byteswap_ulong (unsigned long val) {\n"
"  return __builtin_bswap32(val);\n"
"}\n"
"static inline unsigned __int64 _byteswap_uint64 (unsigned __int64 val) {\n"
"  return __builtin_bswap64(val);\n"
"}\n"
"#endif\n"
"\n"
"/*----------------------------------------------------------------------------*\\\n"
"|* Privileged intrinsics\n"
"\\*----------------------------------------------------------------------------*/\n"
"#if defined(__i386__) || defined(__x86_64__)\n"
"static __inline__ unsigned __int64 __DEFAULT_FN_ATTRS\n"
"__readmsr(unsigned long __register) {\n"
"  // Loads the contents of a 64-bit model specific register (MSR) specified in\n"
"  // the ECX register into registers EDX:EAX. The EDX register is loaded with\n"
"  // the high-order 32 bits of the MSR and the EAX register is loaded with the\n"
"  // low-order 32 bits. If less than 64 bits are implemented in the MSR being\n"
"  // read, the values returned to EDX:EAX in unimplemented bit locations are\n"
"  // undefined.\n"
"  unsigned long __edx;\n"
"  unsigned long __eax;\n"
"  __asm__ (\"rdmsr\" : \"=d\"(__edx), \"=a\"(__eax) : \"c\"(__register));\n"
"  return (((unsigned __int64)__edx) << 32) | (unsigned __int64)__eax;\n"
"}\n"
"\n"
"static __inline__ unsigned long __DEFAULT_FN_ATTRS\n"
"__readcr3(void) {\n"
"  unsigned long __cr3_val;\n"
"  __asm__ __volatile__ (\"mov %%cr3, %0\" : \"=q\"(__cr3_val) : : \"memory\");\n"
"  return __cr3_val;\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__writecr3(unsigned int __cr3_val) {\n"
"  __asm__ (\"mov %0, %%cr3\" : : \"q\"(__cr3_val) : \"memory\");\n"
"}\n"
"#endif\n"
"\n"
"#ifdef __cplusplus\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __INTRIN_H */\n"
"#endif /* _MSC_VER */\n"
"" } , 
 { "/builtins/inttypes.h" , "/*===---- inttypes.h - Standard header for integer printf macros ----------===*\\\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
"\\*===----------------------------------------------------------------------===*/\n"
"\n"
"#if !defined(__CLANG_INTTYPES_H) || !defined(_INTTYPES_H)\n"
"#define __CLANG_INTTYPES_H\n"
"\n"
"#if defined(_MSC_VER) && _MSC_VER < 1800\n"
"#error MSVC does not have inttypes.h prior to Visual Studio 2013\n"
"#endif\n"
"\n"
"#include_next <inttypes.h>\n"
"\n"
"#if defined(_MSC_VER) && _MSC_VER < 1900\n"
"/* MSVC headers define int32_t as int, but PRIx32 as \"lx\" instead of \"x\".\n"
" * This triggers format warnings, so fix it up here. */\n"
"#undef PRId32\n"
"#undef PRIdLEAST32\n"
"#undef PRIdFAST32\n"
"#undef PRIi32\n"
"#undef PRIiLEAST32\n"
"#undef PRIiFAST32\n"
"#undef PRIo32\n"
"#undef PRIoLEAST32\n"
"#undef PRIoFAST32\n"
"#undef PRIu32\n"
"#undef PRIuLEAST32\n"
"#undef PRIuFAST32\n"
"#undef PRIx32\n"
"#undef PRIxLEAST32\n"
"#undef PRIxFAST32\n"
"#undef PRIX32\n"
"#undef PRIXLEAST32\n"
"#undef PRIXFAST32\n"
"\n"
"#undef SCNd32\n"
"#undef SCNdLEAST32\n"
"#undef SCNdFAST32\n"
"#undef SCNi32\n"
"#undef SCNiLEAST32\n"
"#undef SCNiFAST32\n"
"#undef SCNo32\n"
"#undef SCNoLEAST32\n"
"#undef SCNoFAST32\n"
"#undef SCNu32\n"
"#undef SCNuLEAST32\n"
"#undef SCNuFAST32\n"
"#undef SCNx32\n"
"#undef SCNxLEAST32\n"
"#undef SCNxFAST32\n"
"\n"
"#define PRId32 \"d\"\n"
"#define PRIdLEAST32 \"d\"\n"
"#define PRIdFAST32 \"d\"\n"
"#define PRIi32 \"i\"\n"
"#define PRIiLEAST32 \"i\"\n"
"#define PRIiFAST32 \"i\"\n"
"#define PRIo32 \"o\"\n"
"#define PRIoLEAST32 \"o\"\n"
"#define PRIoFAST32 \"o\"\n"
"#define PRIu32 \"u\"\n"
"#define PRIuLEAST32 \"u\"\n"
"#define PRIuFAST32 \"u\"\n"
"#define PRIx32 \"x\"\n"
"#define PRIxLEAST32 \"x\"\n"
"#define PRIxFAST32 \"x\"\n"
"#define PRIX32 \"X\"\n"
"#define PRIXLEAST32 \"X\"\n"
"#define PRIXFAST32 \"X\"\n"
"\n"
"#define SCNd32 \"d\"\n"
"#define SCNdLEAST32 \"d\"\n"
"#define SCNdFAST32 \"d\"\n"
"#define SCNi32 \"i\"\n"
"#define SCNiLEAST32 \"i\"\n"
"#define SCNiFAST32 \"i\"\n"
"#define SCNo32 \"o\"\n"
"#define SCNoLEAST32 \"o\"\n"
"#define SCNoFAST32 \"o\"\n"
"#define SCNu32 \"u\"\n"
"#define SCNuLEAST32 \"u\"\n"
"#define SCNuFAST32 \"u\"\n"
"#define SCNx32 \"x\"\n"
"#define SCNxLEAST32 \"x\"\n"
"#define SCNxFAST32 \"x\"\n"
"#endif\n"
"\n"
"#endif /* __CLANG_INTTYPES_H */\n"
"" } , 
 { "/builtins/invpcidintrin.h" , "/*===------------- invpcidintrin.h - INVPCID intrinsic ---------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <invpcidintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __INVPCIDINTRIN_H\n"
"#define __INVPCIDINTRIN_H\n"
"\n"
"static __inline__ void\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"invpcid\")))\n"
"_invpcid(unsigned int __type, void *__descriptor) {\n"
"  __builtin_ia32_invpcid(__type, __descriptor);\n"
"}\n"
"\n"
"#endif /* __INVPCIDINTRIN_H */\n"
"" } , 
 { "/builtins/iso646.h" , "/*===---- iso646.h - Standard header for alternate spellings of operators---===\n"
" *\n"
" * Copyright (c) 2008 Eli Friedman\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __ISO646_H\n"
"#define __ISO646_H\n"
"\n"
"#ifndef __cplusplus\n"
"#define and    &&\n"
"#define and_eq &=\n"
"#define bitand &\n"
"#define bitor  |\n"
"#define compl  ~\n"
"#define not    !\n"
"#define not_eq !=\n"
"#define or     ||\n"
"#define or_eq  |=\n"
"#define xor    ^\n"
"#define xor_eq ^=\n"
"#endif\n"
"\n"
"#endif /* __ISO646_H */\n"
"" } , 
 { "/builtins/limits.h" , "/*===---- limits.h - Standard header for integer sizes --------------------===*\\\n"
" *\n"
" * Copyright (c) 2009 Chris Lattner\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
"\\*===----------------------------------------------------------------------===*/\n"
"\n"
"#if !defined(__CLANG_LIMITS_H) || !defined(_LIBC_LIMITS_H_)\n"
"#define __CLANG_LIMITS_H\n"
"\n"
"/* The system's limits.h may, in turn, try to #include_next GCC's limits.h.\n"
"   Avert this #include_next madness. */\n"
"#if defined __GNUC__ && !defined _GCC_LIMITS_H_\n"
"#define _GCC_LIMITS_H_\n"
"#endif\n"
"\n"
"/* System headers include a number of constants from POSIX in <limits.h>.\n"
"   Include it if we're hosted. */\n"
"#if __STDC_HOSTED__ && __has_include_next(<limits.h>)\n"
"#include_next <limits.h>\n"
"#endif\n"
"\n"
"/* Many system headers try to \"help us out\" by defining these.  No really, we\n"
"   know how big each datatype is. */\n"
"#undef  SCHAR_MIN\n"
"#undef  SCHAR_MAX\n"
"#undef  UCHAR_MAX\n"
"#undef  SHRT_MIN\n"
"#undef  SHRT_MAX\n"
"#undef  USHRT_MAX\n"
"#undef  INT_MIN\n"
"#undef  INT_MAX\n"
"#undef  UINT_MAX\n"
"#undef  LONG_MIN\n"
"#undef  LONG_MAX\n"
"#undef  ULONG_MAX\n"
"\n"
"#undef  CHAR_BIT\n"
"#undef  CHAR_MIN\n"
"#undef  CHAR_MAX\n"
"\n"
"/* C90/99 5.2.4.2.1 */\n"
"#define SCHAR_MAX __SCHAR_MAX__\n"
"#define SHRT_MAX  __SHRT_MAX__\n"
"#define INT_MAX   __INT_MAX__\n"
"#define LONG_MAX  __LONG_MAX__\n"
"\n"
"#define SCHAR_MIN (-__SCHAR_MAX__-1)\n"
"#define SHRT_MIN  (-__SHRT_MAX__ -1)\n"
"#define INT_MIN   (-__INT_MAX__  -1)\n"
"#define LONG_MIN  (-__LONG_MAX__ -1L)\n"
"\n"
"#define UCHAR_MAX (__SCHAR_MAX__*2  +1)\n"
"#define USHRT_MAX (__SHRT_MAX__ *2  +1)\n"
"#define UINT_MAX  (__INT_MAX__  *2U +1U)\n"
"#define ULONG_MAX (__LONG_MAX__ *2UL+1UL)\n"
"\n"
"#ifndef MB_LEN_MAX\n"
"#define MB_LEN_MAX 1\n"
"#endif\n"
"\n"
"#define CHAR_BIT  __CHAR_BIT__\n"
"\n"
"#ifdef __CHAR_UNSIGNED__  /* -funsigned-char */\n"
"#define CHAR_MIN 0\n"
"#define CHAR_MAX UCHAR_MAX\n"
"#else\n"
"#define CHAR_MIN SCHAR_MIN\n"
"#define CHAR_MAX __SCHAR_MAX__\n"
"#endif\n"
"\n"
"/* C99 5.2.4.2.1: Added long long.\n"
"   C++11 18.3.3.2: same contents as the Standard C Library header <limits.h>.\n"
" */\n"
"#if __STDC_VERSION__ >= 199901L || __cplusplus >= 201103L\n"
"\n"
"#undef  LLONG_MIN\n"
"#undef  LLONG_MAX\n"
"#undef  ULLONG_MAX\n"
"\n"
"#define LLONG_MAX  __LONG_LONG_MAX__\n"
"#define LLONG_MIN  (-__LONG_LONG_MAX__-1LL)\n"
"#define ULLONG_MAX (__LONG_LONG_MAX__*2ULL+1ULL)\n"
"#endif\n"
"\n"
"/* LONG_LONG_MIN/LONG_LONG_MAX/ULONG_LONG_MAX are a GNU extension.  It's too bad\n"
"   that we don't have something like #pragma poison that could be used to\n"
"   deprecate a macro - the code should just use LLONG_MAX and friends.\n"
" */\n"
"#if defined(__GNU_LIBRARY__) ? defined(__USE_GNU) : !defined(__STRICT_ANSI__)\n"
"\n"
"#undef   LONG_LONG_MIN\n"
"#undef   LONG_LONG_MAX\n"
"#undef   ULONG_LONG_MAX\n"
"\n"
"#define LONG_LONG_MAX  __LONG_LONG_MAX__\n"
"#define LONG_LONG_MIN  (-__LONG_LONG_MAX__-1LL)\n"
"#define ULONG_LONG_MAX (__LONG_LONG_MAX__*2ULL+1ULL)\n"
"#endif\n"
"\n"
"#endif /* __CLANG_LIMITS_H */\n"
"" } , 
 { "/builtins/lwpintrin.h" , "/*===---- lwpintrin.h - LWP intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __X86INTRIN_H\n"
"#error \"Never use <lwpintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __LWPINTRIN_H\n"
"#define __LWPINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"lwp\")))\n"
"\n"
"/// Parses the LWPCB at the specified address and enables\n"
"///        profiling if valid.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> LLWPCB </c> instruction.\n"
"///\n"
"/// \\param __addr\n"
"///    Address to the new Lightweight Profiling Control Block (LWPCB). If the\n"
"///    LWPCB is valid, writes the address into the LWP_CBADDR MSR and enables\n"
"///    Lightweight Profiling.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"__llwpcb (void *__addr)\n"
"{\n"
"  __builtin_ia32_llwpcb(__addr);\n"
"}\n"
"\n"
"/// Flushes the LWP state to memory and returns the address of the LWPCB.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> SLWPCB </c> instruction.\n"
"///\n"
"/// \\return\n"
"///    Address to the current Lightweight Profiling Control Block (LWPCB).\n"
"///    If LWP is not currently enabled, returns NULL.\n"
"static __inline__ void* __DEFAULT_FN_ATTRS\n"
"__slwpcb (void)\n"
"{\n"
"  return __builtin_ia32_slwpcb();\n"
"}\n"
"\n"
"/// Inserts programmed event record into the LWP event ring buffer\n"
"///        and advances the ring buffer pointer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> LWPINS </c> instruction.\n"
"///\n"
"/// \\param DATA2\n"
"///    A 32-bit value is zero-extended and inserted into the 64-bit Data2 field.\n"
"/// \\param DATA1\n"
"///    A 32-bit value is inserted into the 32-bit Data1 field.\n"
"/// \\param FLAGS\n"
"///    A 32-bit immediate value is inserted into the 32-bit Flags field.\n"
"/// \\returns If the ring buffer is full and LWP is running in Synchronized Mode,\n"
"///    the event record overwrites the last record in the buffer, the MissedEvents\n"
"///    counter in the LWPCB is incremented, the head pointer is not advanced, and\n"
"///    1 is returned. Otherwise 0 is returned.\n"
"#define __lwpins32(DATA2, DATA1, FLAGS) \\\n"
"  (__builtin_ia32_lwpins32((unsigned int) (DATA2), (unsigned int) (DATA1), \\\n"
"                           (unsigned int) (FLAGS)))\n"
"\n"
"/// Decrements the LWP programmed value sample event counter. If the result is\n"
"///        negative, inserts an event record into the LWP event ring buffer in memory\n"
"///        and advances the ring buffer pointer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> LWPVAL </c> instruction.\n"
"///\n"
"/// \\param DATA2\n"
"///    A 32-bit value is zero-extended and inserted into the 64-bit Data2 field.\n"
"/// \\param DATA1\n"
"///    A 32-bit value is inserted into the 32-bit Data1 field.\n"
"/// \\param FLAGS\n"
"///    A 32-bit immediate value is inserted into the 32-bit Flags field.\n"
"#define __lwpval32(DATA2, DATA1, FLAGS) \\\n"
"  (__builtin_ia32_lwpval32((unsigned int) (DATA2), (unsigned int) (DATA1), \\\n"
"                           (unsigned int) (FLAGS)))\n"
"\n"
"#ifdef __x86_64__\n"
"\n"
"/// Inserts programmed event record into the LWP event ring buffer\n"
"///        and advances the ring buffer pointer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> LWPINS </c> instruction.\n"
"///\n"
"/// \\param DATA2\n"
"///    A 64-bit value is inserted into the 64-bit Data2 field.\n"
"/// \\param DATA1\n"
"///    A 32-bit value is inserted into the 32-bit Data1 field.\n"
"/// \\param FLAGS\n"
"///    A 32-bit immediate value is inserted into the 32-bit Flags field.\n"
"/// \\returns If the ring buffer is full and LWP is running in Synchronized Mode,\n"
"///    the event record overwrites the last record in the buffer, the MissedEvents\n"
"///    counter in the LWPCB is incremented, the head pointer is not advanced, and\n"
"///    1 is returned. Otherwise 0 is returned.\n"
"#define __lwpins64(DATA2, DATA1, FLAGS) \\\n"
"  (__builtin_ia32_lwpins64((unsigned long long) (DATA2), (unsigned int) (DATA1), \\\n"
"                           (unsigned int) (FLAGS)))\n"
"\n"
"/// Decrements the LWP programmed value sample event counter. If the result is\n"
"///        negative, inserts an event record into the LWP event ring buffer in memory\n"
"///        and advances the ring buffer pointer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> LWPVAL </c> instruction.\n"
"///\n"
"/// \\param DATA2\n"
"///    A 64-bit value is and inserted into the 64-bit Data2 field.\n"
"/// \\param DATA1\n"
"///    A 32-bit value is inserted into the 32-bit Data1 field.\n"
"/// \\param FLAGS\n"
"///    A 32-bit immediate value is inserted into the 32-bit Flags field.\n"
"#define __lwpval64(DATA2, DATA1, FLAGS) \\\n"
"  (__builtin_ia32_lwpval64((unsigned long long) (DATA2), (unsigned int) (DATA1), \\\n"
"                           (unsigned int) (FLAGS)))\n"
"\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __LWPINTRIN_H */\n"
"" } , 
 { "/builtins/lzcntintrin.h" , "/*===---- lzcntintrin.h - LZCNT intrinsics ---------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <lzcntintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __LZCNTINTRIN_H\n"
"#define __LZCNTINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"lzcnt\")))\n"
"\n"
"#ifndef _MSC_VER\n"
"/// Counts the number of leading zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c LZCNT instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 16-bit integer whose leading zeros are to be counted.\n"
"/// \\returns An unsigned 16-bit integer containing the number of leading zero\n"
"///    bits in the operand.\n"
"#define __lzcnt16(X) __builtin_ia32_lzcnt_u16((unsigned short)(X))\n"
"#endif // _MSC_VER\n"
"\n"
"/// Counts the number of leading zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c LZCNT instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 32-bit integer whose leading zeros are to be counted.\n"
"/// \\returns An unsigned 32-bit integer containing the number of leading zero\n"
"///    bits in the operand.\n"
"/// \\see _lzcnt_u32\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__lzcnt32(unsigned int __X)\n"
"{\n"
"  return __builtin_ia32_lzcnt_u32(__X);\n"
"}\n"
"\n"
"/// Counts the number of leading zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c LZCNT instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 32-bit integer whose leading zeros are to be counted.\n"
"/// \\returns An unsigned 32-bit integer containing the number of leading zero\n"
"///    bits in the operand.\n"
"/// \\see __lzcnt32\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_lzcnt_u32(unsigned int __X)\n"
"{\n"
"  return __builtin_ia32_lzcnt_u32(__X);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"#ifndef _MSC_VER\n"
"/// Counts the number of leading zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c LZCNT instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer whose leading zeros are to be counted.\n"
"/// \\returns An unsigned 64-bit integer containing the number of leading zero\n"
"///    bits in the operand.\n"
"/// \\see _lzcnt_u64\n"
"#define __lzcnt64(X) __builtin_ia32_lzcnt_u64((unsigned long long)(X))\n"
"#endif // _MSC_VER\n"
"\n"
"/// Counts the number of leading zero bits in the operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c LZCNT instruction.\n"
"///\n"
"/// \\param __X\n"
"///    An unsigned 64-bit integer whose leading zeros are to be counted.\n"
"/// \\returns An unsigned 64-bit integer containing the number of leading zero\n"
"///    bits in the operand.\n"
"/// \\see __lzcnt64\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"_lzcnt_u64(unsigned long long __X)\n"
"{\n"
"  return __builtin_ia32_lzcnt_u64(__X);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __LZCNTINTRIN_H */\n"
"" } , 
 { "/builtins/mm3dnow.h" , "/*===---- mm3dnow.h - 3DNow! intrinsics ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef _MM3DNOW_H_INCLUDED\n"
"#define _MM3DNOW_H_INCLUDED\n"
"\n"
"#include <mmintrin.h>\n"
"#include <prfchwintrin.h>\n"
"\n"
"typedef float __v2sf __attribute__((__vector_size__(8)));\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"3dnow\"), __min_vector_width__(64)))\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__(\"3dnow\")))\n"
"_m_femms(void) {\n"
"  __builtin_ia32_femms();\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pavgusb(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pavgusb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pf2id(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pf2id((__v2sf)__m);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfacc(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfacc((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfadd(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfadd((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfcmpeq(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfcmpeq((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfcmpge(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfcmpge((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfcmpgt(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfcmpgt((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfmax(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfmax((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfmin(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfmin((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfmul(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfmul((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfrcp(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pfrcp((__v2sf)__m);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfrcpit1(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfrcpit1((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfrcpit2(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfrcpit2((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfrsqrt(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pfrsqrt((__v2sf)__m);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfrsqrtit1(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfrsqit1((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfsub(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfsub((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfsubr(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfsubr((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pi2fd(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pi2fd((__v2si)__m);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pmulhrw(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pmulhrw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/* Handle the 3dnowa instructions here. */\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"3dnowa\"), __min_vector_width__(64)))\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pf2iw(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pf2iw((__v2sf)__m);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfnacc(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfnacc((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pfpnacc(__m64 __m1, __m64 __m2) {\n"
"  return (__m64)__builtin_ia32_pfpnacc((__v2sf)__m1, (__v2sf)__m2);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pi2fw(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pi2fw((__v2si)__m);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pswapdsf(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pswapdsf((__v2sf)__m);\n"
"}\n"
"\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_m_pswapdsi(__m64 __m) {\n"
"  return (__m64)__builtin_ia32_pswapdsi((__v2si)__m);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/mm_malloc.h" , "/*===---- mm_malloc.h - Allocating and Freeing Aligned Memory Blocks -------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __MM_MALLOC_H\n"
"#define __MM_MALLOC_H\n"
"\n"
"#include <stdlib.h>\n"
"\n"
"#ifdef _WIN32\n"
"#include <malloc.h>\n"
"#else\n"
"#ifndef __cplusplus\n"
"extern int posix_memalign(void **__memptr, size_t __alignment, size_t __size);\n"
"#else\n"
"// Some systems (e.g. those with GNU libc) declare posix_memalign with an\n"
"// exception specifier. Via an \"egregious workaround\" in\n"
"// Sema::CheckEquivalentExceptionSpec, Clang accepts the following as a valid\n"
"// redeclaration of glibc's declaration.\n"
"extern \"C\" int posix_memalign(void **__memptr, size_t __alignment, size_t __size);\n"
"#endif\n"
"#endif\n"
"\n"
"#if !(defined(_WIN32) && defined(_mm_malloc))\n"
"static __inline__ void *__attribute__((__always_inline__, __nodebug__,\n"
"                                       __malloc__))\n"
"_mm_malloc(size_t __size, size_t __align)\n"
"{\n"
"  if (__align == 1) {\n"
"    return malloc(__size);\n"
"  }\n"
"\n"
"  if (!(__align & (__align - 1)) && __align < sizeof(void *))\n"
"    __align = sizeof(void *);\n"
"\n"
"  void *__mallocedMemory;\n"
"#if defined(__MINGW32__)\n"
"  __mallocedMemory = __mingw_aligned_malloc(__size, __align);\n"
"#elif defined(_WIN32)\n"
"  __mallocedMemory = _aligned_malloc(__size, __align);\n"
"#else\n"
"  if (posix_memalign(&__mallocedMemory, __align, __size))\n"
"    return 0;\n"
"#endif\n"
"\n"
"  return __mallocedMemory;\n"
"}\n"
"\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__))\n"
"_mm_free(void *__p)\n"
"{\n"
"  free(__p);\n"
"}\n"
"#endif\n"
"\n"
"#endif /* __MM_MALLOC_H */\n"
"" } , 
 { "/builtins/mmintrin.h" , "/*===---- mmintrin.h - MMX intrinsics --------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __MMINTRIN_H\n"
"#define __MMINTRIN_H\n"
"\n"
"typedef long long __m64 __attribute__((__vector_size__(8)));\n"
"\n"
"typedef long long __v1di __attribute__((__vector_size__(8)));\n"
"typedef int __v2si __attribute__((__vector_size__(8)));\n"
"typedef short __v4hi __attribute__((__vector_size__(8)));\n"
"typedef char __v8qi __attribute__((__vector_size__(8)));\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"mmx\"), __min_vector_width__(64)))\n"
"\n"
"/// Clears the MMX state by setting the state of the x87 stack registers\n"
"///    to empty.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> EMMS </c> instruction.\n"
"///\n"
"static __inline__ void  __attribute__((__always_inline__, __nodebug__, __target__(\"mmx\")))\n"
"_mm_empty(void)\n"
"{\n"
"    __builtin_ia32_emms();\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector, setting the lower 32 bits to the\n"
"///    value of the 32-bit integer parameter and setting the upper 32 bits to 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVD </c> instruction.\n"
"///\n"
"/// \\param __i\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector. The lower 32 bits contain the value of the\n"
"///    parameter. The upper 32 bits are set to 0.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi32_si64(int __i)\n"
"{\n"
"    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);\n"
"}\n"
"\n"
"/// Returns the lower 32 bits of a 64-bit integer vector as a 32-bit\n"
"///    signed integer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVD </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector.\n"
"/// \\returns A 32-bit signed integer value containing the lower 32 bits of the\n"
"///    parameter.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi64_si32(__m64 __m)\n"
"{\n"
"    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);\n"
"}\n"
"\n"
"/// Casts a 64-bit signed integer value into a 64-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVQ </c> instruction.\n"
"///\n"
"/// \\param __i\n"
"///    A 64-bit signed integer.\n"
"/// \\returns A 64-bit integer vector containing the same bitwise pattern as the\n"
"///    parameter.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi64_m64(long long __i)\n"
"{\n"
"    return (__m64)__i;\n"
"}\n"
"\n"
"/// Casts a 64-bit integer vector into a 64-bit signed integer value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVQ </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector.\n"
"/// \\returns A 64-bit signed integer containing the same bitwise pattern as the\n"
"///    parameter.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_mm_cvtm64_si64(__m64 __m)\n"
"{\n"
"    return (long long)__m;\n"
"}\n"
"\n"
"/// Converts 16-bit signed integers from both 64-bit integer vector\n"
"///    parameters of [4 x i16] into 8-bit signed integer values, and constructs\n"
"///    a 64-bit integer vector of [8 x i8] as the result. Positive values\n"
"///    greater than 0x7F are saturated to 0x7F. Negative values less than 0x80\n"
"///    are saturated to 0x80.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PACKSSWB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a\n"
"///    16-bit signed integer and is converted to an 8-bit signed integer with\n"
"///    saturation. Positive values greater than 0x7F are saturated to 0x7F.\n"
"///    Negative values less than 0x80 are saturated to 0x80. The converted\n"
"///    [4 x i8] values are written to the lower 32 bits of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a\n"
"///    16-bit signed integer and is converted to an 8-bit signed integer with\n"
"///    saturation. Positive values greater than 0x7F are saturated to 0x7F.\n"
"///    Negative values less than 0x80 are saturated to 0x80. The converted\n"
"///    [4 x i8] values are written to the upper 32 bits of the result.\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the converted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_packs_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Converts 32-bit signed integers from both 64-bit integer vector\n"
"///    parameters of [2 x i32] into 16-bit signed integer values, and constructs\n"
"///    a 64-bit integer vector of [4 x i16] as the result. Positive values\n"
"///    greater than 0x7FFF are saturated to 0x7FFF. Negative values less than\n"
"///    0x8000 are saturated to 0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PACKSSDW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [2 x i32]. Each 32-bit element is treated as a\n"
"///    32-bit signed integer and is converted to a 16-bit signed integer with\n"
"///    saturation. Positive values greater than 0x7FFF are saturated to 0x7FFF.\n"
"///    Negative values less than 0x8000 are saturated to 0x8000. The converted\n"
"///    [2 x i16] values are written to the lower 32 bits of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [2 x i32]. Each 32-bit element is treated as a\n"
"///    32-bit signed integer and is converted to a 16-bit signed integer with\n"
"///    saturation. Positive values greater than 0x7FFF are saturated to 0x7FFF.\n"
"///    Negative values less than 0x8000 are saturated to 0x8000. The converted\n"
"///    [2 x i16] values are written to the upper 32 bits of the result.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the converted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_packs_pi32(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);\n"
"}\n"
"\n"
"/// Converts 16-bit signed integers from both 64-bit integer vector\n"
"///    parameters of [4 x i16] into 8-bit unsigned integer values, and\n"
"///    constructs a 64-bit integer vector of [8 x i8] as the result. Values\n"
"///    greater than 0xFF are saturated to 0xFF. Values less than 0 are saturated\n"
"///    to 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PACKUSWB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a\n"
"///    16-bit signed integer and is converted to an 8-bit unsigned integer with\n"
"///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less\n"
"///    than 0 are saturated to 0. The converted [4 x i8] values are written to\n"
"///    the lower 32 bits of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a\n"
"///    16-bit signed integer and is converted to an 8-bit unsigned integer with\n"
"///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less\n"
"///    than 0 are saturated to 0. The converted [4 x i8] values are written to\n"
"///    the upper 32 bits of the result.\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the converted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_packs_pu16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Unpacks the upper 32 bits from two 64-bit integer vectors of [8 x i8]\n"
"///    and interleaves them into a 64-bit integer vector of [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PUNPCKHBW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8]. \\n\n"
"///    Bits [39:32] are written to bits [7:0] of the result. \\n\n"
"///    Bits [47:40] are written to bits [23:16] of the result. \\n\n"
"///    Bits [55:48] are written to bits [39:32] of the result. \\n\n"
"///    Bits [63:56] are written to bits [55:48] of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"///    Bits [39:32] are written to bits [15:8] of the result. \\n\n"
"///    Bits [47:40] are written to bits [31:24] of the result. \\n\n"
"///    Bits [55:48] are written to bits [47:40] of the result. \\n\n"
"///    Bits [63:56] are written to bits [63:56] of the result.\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the interleaved\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Unpacks the upper 32 bits from two 64-bit integer vectors of\n"
"///    [4 x i16] and interleaves them into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PUNPCKHWD </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"///    Bits [47:32] are written to bits [15:0] of the result. \\n\n"
"///    Bits [63:48] are written to bits [47:32] of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"///    Bits [47:32] are written to bits [31:16] of the result. \\n\n"
"///    Bits [63:48] are written to bits [63:48] of the result.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the interleaved\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Unpacks the upper 32 bits from two 64-bit integer vectors of\n"
"///    [2 x i32] and interleaves them into a 64-bit integer vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PUNPCKHDQ </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [2 x i32]. The upper 32 bits are written to\n"
"///    the lower 32 bits of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [2 x i32]. The upper 32 bits are written to\n"
"///    the upper 32 bits of the result.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the interleaved\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_pi32(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);\n"
"}\n"
"\n"
"/// Unpacks the lower 32 bits from two 64-bit integer vectors of [8 x i8]\n"
"///    and interleaves them into a 64-bit integer vector of [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PUNPCKLBW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"///    Bits [7:0] are written to bits [7:0] of the result. \\n\n"
"///    Bits [15:8] are written to bits [23:16] of the result. \\n\n"
"///    Bits [23:16] are written to bits [39:32] of the result. \\n\n"
"///    Bits [31:24] are written to bits [55:48] of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"///    Bits [7:0] are written to bits [15:8] of the result. \\n\n"
"///    Bits [15:8] are written to bits [31:24] of the result. \\n\n"
"///    Bits [23:16] are written to bits [47:40] of the result. \\n\n"
"///    Bits [31:24] are written to bits [63:56] of the result.\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the interleaved\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Unpacks the lower 32 bits from two 64-bit integer vectors of\n"
"///    [4 x i16] and interleaves them into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PUNPCKLWD </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"///    Bits [15:0] are written to bits [15:0] of the result. \\n\n"
"///    Bits [31:16] are written to bits [47:32] of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"///    Bits [15:0] are written to bits [31:16] of the result. \\n\n"
"///    Bits [31:16] are written to bits [63:48] of the result.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the interleaved\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Unpacks the lower 32 bits from two 64-bit integer vectors of\n"
"///    [2 x i32] and interleaves them into a 64-bit integer vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PUNPCKLDQ </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [2 x i32]. The lower 32 bits are written to\n"
"///    the lower 32 bits of the result.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [2 x i32]. The lower 32 bits are written to\n"
"///    the upper 32 bits of the result.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the interleaved\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_pi32(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);\n"
"}\n"
"\n"
"/// Adds each 8-bit integer element of the first 64-bit integer vector\n"
"///    of [8 x i8] to the corresponding 8-bit integer element of the second\n"
"///    64-bit integer vector of [8 x i8]. The lower 8 bits of the results are\n"
"///    packed into a 64-bit integer vector of [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the sums of both\n"
"///    parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_add_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Adds each 16-bit integer element of the first 64-bit integer vector\n"
"///    of [4 x i16] to the corresponding 16-bit integer element of the second\n"
"///    64-bit integer vector of [4 x i16]. The lower 16 bits of the results are\n"
"///    packed into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the sums of both\n"
"///    parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_add_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Adds each 32-bit integer element of the first 64-bit integer vector\n"
"///    of [2 x i32] to the corresponding 32-bit integer element of the second\n"
"///    64-bit integer vector of [2 x i32]. The lower 32 bits of the results are\n"
"///    packed into a 64-bit integer vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDD </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the sums of both\n"
"///    parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_add_pi32(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);\n"
"}\n"
"\n"
"/// Adds each 8-bit signed integer element of the first 64-bit integer\n"
"///    vector of [8 x i8] to the corresponding 8-bit signed integer element of\n"
"///    the second 64-bit integer vector of [8 x i8]. Positive sums greater than\n"
"///    0x7F are saturated to 0x7F. Negative sums less than 0x80 are saturated to\n"
"///    0x80. The results are packed into a 64-bit integer vector of [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDSB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the saturated sums\n"
"///    of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_adds_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_paddsb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Adds each 16-bit signed integer element of the first 64-bit integer\n"
"///    vector of [4 x i16] to the corresponding 16-bit signed integer element of\n"
"///    the second 64-bit integer vector of [4 x i16]. Positive sums greater than\n"
"///    0x7FFF are saturated to 0x7FFF. Negative sums less than 0x8000 are\n"
"///    saturated to 0x8000. The results are packed into a 64-bit integer vector\n"
"///    of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDSW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the saturated sums\n"
"///    of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_adds_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_paddsw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Adds each 8-bit unsigned integer element of the first 64-bit integer\n"
"///    vector of [8 x i8] to the corresponding 8-bit unsigned integer element of\n"
"///    the second 64-bit integer vector of [8 x i8]. Sums greater than 0xFF are\n"
"///    saturated to 0xFF. The results are packed into a 64-bit integer vector of\n"
"///    [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDUSB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the saturated\n"
"///    unsigned sums of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_adds_pu8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_paddusb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Adds each 16-bit unsigned integer element of the first 64-bit integer\n"
"///    vector of [4 x i16] to the corresponding 16-bit unsigned integer element\n"
"///    of the second 64-bit integer vector of [4 x i16]. Sums greater than\n"
"///    0xFFFF are saturated to 0xFFFF. The results are packed into a 64-bit\n"
"///    integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PADDUSW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the saturated\n"
"///    unsigned sums of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_adds_pu16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_paddusw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Subtracts each 8-bit integer element of the second 64-bit integer\n"
"///    vector of [8 x i8] from the corresponding 8-bit integer element of the\n"
"///    first 64-bit integer vector of [8 x i8]. The lower 8 bits of the results\n"
"///    are packed into a 64-bit integer vector of [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8] containing the minuends.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8] containing the subtrahends.\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the differences of\n"
"///    both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sub_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_psubb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Subtracts each 16-bit integer element of the second 64-bit integer\n"
"///    vector of [4 x i16] from the corresponding 16-bit integer element of the\n"
"///    first 64-bit integer vector of [4 x i16]. The lower 16 bits of the\n"
"///    results are packed into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16] containing the minuends.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16] containing the subtrahends.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the differences of\n"
"///    both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sub_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_psubw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Subtracts each 32-bit integer element of the second 64-bit integer\n"
"///    vector of [2 x i32] from the corresponding 32-bit integer element of the\n"
"///    first 64-bit integer vector of [2 x i32]. The lower 32 bits of the\n"
"///    results are packed into a 64-bit integer vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBD </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [2 x i32] containing the minuends.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [2 x i32] containing the subtrahends.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the differences of\n"
"///    both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sub_pi32(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_psubd((__v2si)__m1, (__v2si)__m2);\n"
"}\n"
"\n"
"/// Subtracts each 8-bit signed integer element of the second 64-bit\n"
"///    integer vector of [8 x i8] from the corresponding 8-bit signed integer\n"
"///    element of the first 64-bit integer vector of [8 x i8]. Positive results\n"
"///    greater than 0x7F are saturated to 0x7F. Negative results less than 0x80\n"
"///    are saturated to 0x80. The results are packed into a 64-bit integer\n"
"///    vector of [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBSB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8] containing the minuends.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8] containing the subtrahends.\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the saturated\n"
"///    differences of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_subs_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_psubsb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Subtracts each 16-bit signed integer element of the second 64-bit\n"
"///    integer vector of [4 x i16] from the corresponding 16-bit signed integer\n"
"///    element of the first 64-bit integer vector of [4 x i16]. Positive results\n"
"///    greater than 0x7FFF are saturated to 0x7FFF. Negative results less than\n"
"///    0x8000 are saturated to 0x8000. The results are packed into a 64-bit\n"
"///    integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBSW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16] containing the minuends.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16] containing the subtrahends.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the saturated\n"
"///    differences of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_subs_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_psubsw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Subtracts each 8-bit unsigned integer element of the second 64-bit\n"
"///    integer vector of [8 x i8] from the corresponding 8-bit unsigned integer\n"
"///    element of the first 64-bit integer vector of [8 x i8].\n"
"///\n"
"///    If an element of the first vector is less than the corresponding element\n"
"///    of the second vector, the result is saturated to 0. The results are\n"
"///    packed into a 64-bit integer vector of [8 x i8].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBUSB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8] containing the minuends.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8] containing the subtrahends.\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the saturated\n"
"///    differences of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_subs_pu8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_psubusb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Subtracts each 16-bit unsigned integer element of the second 64-bit\n"
"///    integer vector of [4 x i16] from the corresponding 16-bit unsigned\n"
"///    integer element of the first 64-bit integer vector of [4 x i16].\n"
"///\n"
"///    If an element of the first vector is less than the corresponding element\n"
"///    of the second vector, the result is saturated to 0. The results are\n"
"///    packed into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSUBUSW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16] containing the minuends.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16] containing the subtrahends.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the saturated\n"
"///    differences of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_subs_pu16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_psubusw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Multiplies each 16-bit signed integer element of the first 64-bit\n"
"///    integer vector of [4 x i16] by the corresponding 16-bit signed integer\n"
"///    element of the second 64-bit integer vector of [4 x i16] and get four\n"
"///    32-bit products. Adds adjacent pairs of products to get two 32-bit sums.\n"
"///    The lower 32 bits of these two sums are packed into a 64-bit integer\n"
"///    vector of [2 x i32].\n"
"///\n"
"///    For example, bits [15:0] of both parameters are multiplied, bits [31:16]\n"
"///    of both parameters are multiplied, and the sum of both results is written\n"
"///    to bits [31:0] of the result.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMADDWD </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the sums of\n"
"///    products of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_madd_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pmaddwd((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Multiplies each 16-bit signed integer element of the first 64-bit\n"
"///    integer vector of [4 x i16] by the corresponding 16-bit signed integer\n"
"///    element of the second 64-bit integer vector of [4 x i16]. Packs the upper\n"
"///    16 bits of the 32-bit products into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMULHW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the upper 16 bits\n"
"///    of the products of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_mulhi_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pmulhw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Multiplies each 16-bit signed integer element of the first 64-bit\n"
"///    integer vector of [4 x i16] by the corresponding 16-bit signed integer\n"
"///    element of the second 64-bit integer vector of [4 x i16]. Packs the lower\n"
"///    16 bits of the 32-bit products into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMULLW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the lower 16 bits\n"
"///    of the products of both parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_mullo_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pmullw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Left-shifts each 16-bit signed integer element of the first\n"
"///    parameter, which is a 64-bit integer vector of [4 x i16], by the number\n"
"///    of bits specified by the second parameter, which is a 64-bit integer. The\n"
"///    lower 16 bits of the results are packed into a 64-bit integer vector of\n"
"///    [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSLLW </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the left-shifted\n"
"///    values. If \\a __count is greater or equal to 16, the result is set to all\n"
"///    0.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sll_pi16(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psllw((__v4hi)__m, __count);\n"
"}\n"
"\n"
"/// Left-shifts each 16-bit signed integer element of a 64-bit integer\n"
"///    vector of [4 x i16] by the number of bits specified by a 32-bit integer.\n"
"///    The lower 16 bits of the results are packed into a 64-bit integer vector\n"
"///    of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSLLW </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the left-shifted\n"
"///    values. If \\a __count is greater or equal to 16, the result is set to all\n"
"///    0.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_slli_pi16(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psllwi((__v4hi)__m, __count);\n"
"}\n"
"\n"
"/// Left-shifts each 32-bit signed integer element of the first\n"
"///    parameter, which is a 64-bit integer vector of [2 x i32], by the number\n"
"///    of bits specified by the second parameter, which is a 64-bit integer. The\n"
"///    lower 32 bits of the results are packed into a 64-bit integer vector of\n"
"///    [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSLLD </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the left-shifted\n"
"///    values. If \\a __count is greater or equal to 32, the result is set to all\n"
"///    0.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sll_pi32(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_pslld((__v2si)__m, __count);\n"
"}\n"
"\n"
"/// Left-shifts each 32-bit signed integer element of a 64-bit integer\n"
"///    vector of [2 x i32] by the number of bits specified by a 32-bit integer.\n"
"///    The lower 32 bits of the results are packed into a 64-bit integer vector\n"
"///    of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSLLD </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the left-shifted\n"
"///    values. If \\a __count is greater or equal to 32, the result is set to all\n"
"///    0.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_slli_pi32(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_pslldi((__v2si)__m, __count);\n"
"}\n"
"\n"
"/// Left-shifts the first 64-bit integer parameter by the number of bits\n"
"///    specified by the second 64-bit integer parameter. The lower 64 bits of\n"
"///    result are returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSLLQ </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector containing the left-shifted value. If\n"
"///     \\a __count is greater or equal to 64, the result is set to 0.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sll_si64(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psllq((__v1di)__m, __count);\n"
"}\n"
"\n"
"/// Left-shifts the first parameter, which is a 64-bit integer, by the\n"
"///    number of bits specified by the second parameter, which is a 32-bit\n"
"///    integer. The lower 64 bits of result are returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSLLQ </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector containing the left-shifted value. If\n"
"///     \\a __count is greater or equal to 64, the result is set to 0.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_slli_si64(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psllqi((__v1di)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 16-bit integer element of the first parameter,\n"
"///    which is a 64-bit integer vector of [4 x i16], by the number of bits\n"
"///    specified by the second parameter, which is a 64-bit integer.\n"
"///\n"
"///    High-order bits are filled with the sign bit of the initial value of each\n"
"///    16-bit element. The 16-bit results are packed into a 64-bit integer\n"
"///    vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRAW </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sra_pi16(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psraw((__v4hi)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 16-bit integer element of a 64-bit integer vector\n"
"///    of [4 x i16] by the number of bits specified by a 32-bit integer.\n"
"///\n"
"///    High-order bits are filled with the sign bit of the initial value of each\n"
"///    16-bit element. The 16-bit results are packed into a 64-bit integer\n"
"///    vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRAW </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srai_pi16(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrawi((__v4hi)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 32-bit integer element of the first parameter,\n"
"///    which is a 64-bit integer vector of [2 x i32], by the number of bits\n"
"///    specified by the second parameter, which is a 64-bit integer.\n"
"///\n"
"///    High-order bits are filled with the sign bit of the initial value of each\n"
"///    32-bit element. The 32-bit results are packed into a 64-bit integer\n"
"///    vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRAD </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_sra_pi32(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrad((__v2si)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 32-bit integer element of a 64-bit integer vector\n"
"///    of [2 x i32] by the number of bits specified by a 32-bit integer.\n"
"///\n"
"///    High-order bits are filled with the sign bit of the initial value of each\n"
"///    32-bit element. The 32-bit results are packed into a 64-bit integer\n"
"///    vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRAD </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srai_pi32(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psradi((__v2si)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 16-bit integer element of the first parameter,\n"
"///    which is a 64-bit integer vector of [4 x i16], by the number of bits\n"
"///    specified by the second parameter, which is a 64-bit integer.\n"
"///\n"
"///    High-order bits are cleared. The 16-bit results are packed into a 64-bit\n"
"///    integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRLW </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srl_pi16(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrlw((__v4hi)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 16-bit integer element of a 64-bit integer vector\n"
"///    of [4 x i16] by the number of bits specified by a 32-bit integer.\n"
"///\n"
"///    High-order bits are cleared. The 16-bit results are packed into a 64-bit\n"
"///    integer vector of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRLW </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srli_pi16(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrlwi((__v4hi)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 32-bit integer element of the first parameter,\n"
"///    which is a 64-bit integer vector of [2 x i32], by the number of bits\n"
"///    specified by the second parameter, which is a 64-bit integer.\n"
"///\n"
"///    High-order bits are cleared. The 32-bit results are packed into a 64-bit\n"
"///    integer vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRLD </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srl_pi32(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrld((__v2si)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts each 32-bit integer element of a 64-bit integer vector\n"
"///    of [2 x i32] by the number of bits specified by a 32-bit integer.\n"
"///\n"
"///    High-order bits are cleared. The 32-bit results are packed into a 64-bit\n"
"///    integer vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRLD </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the right-shifted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srli_pi32(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrldi((__v2si)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts the first 64-bit integer parameter by the number of bits\n"
"///    specified by the second 64-bit integer parameter.\n"
"///\n"
"///    High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRLQ </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\param __count\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\returns A 64-bit integer vector containing the right-shifted value.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srl_si64(__m64 __m, __m64 __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrlq((__v1di)__m, __count);\n"
"}\n"
"\n"
"/// Right-shifts the first parameter, which is a 64-bit integer, by the\n"
"///    number of bits specified by the second parameter, which is a 32-bit\n"
"///    integer.\n"
"///\n"
"///    High-order bits are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSRLQ </c> instruction.\n"
"///\n"
"/// \\param __m\n"
"///    A 64-bit integer vector interpreted as a single 64-bit integer.\n"
"/// \\param __count\n"
"///    A 32-bit integer value.\n"
"/// \\returns A 64-bit integer vector containing the right-shifted value.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_srli_si64(__m64 __m, int __count)\n"
"{\n"
"    return (__m64)__builtin_ia32_psrlqi((__v1di)__m, __count);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 64-bit integer vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PAND </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector.\n"
"/// \\returns A 64-bit integer vector containing the bitwise AND of both\n"
"///    parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_and_si64(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return __builtin_ia32_pand((__v1di)__m1, (__v1di)__m2);\n"
"}\n"
"\n"
"/// Performs a bitwise NOT of the first 64-bit integer vector, and then\n"
"///    performs a bitwise AND of the intermediate result and the second 64-bit\n"
"///    integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PANDN </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector. The one's complement of this parameter is used\n"
"///    in the bitwise AND.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector.\n"
"/// \\returns A 64-bit integer vector containing the bitwise AND of the second\n"
"///    parameter and the one's complement of the first parameter.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_andnot_si64(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return __builtin_ia32_pandn((__v1di)__m1, (__v1di)__m2);\n"
"}\n"
"\n"
"/// Performs a bitwise OR of two 64-bit integer vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> POR </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector.\n"
"/// \\returns A 64-bit integer vector containing the bitwise OR of both\n"
"///    parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_or_si64(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return __builtin_ia32_por((__v1di)__m1, (__v1di)__m2);\n"
"}\n"
"\n"
"/// Performs a bitwise exclusive OR of two 64-bit integer vectors.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PXOR </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector.\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector.\n"
"/// \\returns A 64-bit integer vector containing the bitwise exclusive OR of both\n"
"///    parameters.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_xor_si64(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return __builtin_ia32_pxor((__v1di)__m1, (__v1di)__m2);\n"
"}\n"
"\n"
"/// Compares the 8-bit integer elements of two 64-bit integer vectors of\n"
"///    [8 x i8] to determine if the element of the first vector is equal to the\n"
"///    corresponding element of the second vector.\n"
"///\n"
"///    The comparison yields 0 for false, 0xFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PCMPEQB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the comparison\n"
"///    results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pcmpeqb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Compares the 16-bit integer elements of two 64-bit integer vectors of\n"
"///    [4 x i16] to determine if the element of the first vector is equal to the\n"
"///    corresponding element of the second vector.\n"
"///\n"
"///    The comparison yields 0 for false, 0xFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PCMPEQW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the comparison\n"
"///    results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pcmpeqw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Compares the 32-bit integer elements of two 64-bit integer vectors of\n"
"///    [2 x i32] to determine if the element of the first vector is equal to the\n"
"///    corresponding element of the second vector.\n"
"///\n"
"///    The comparison yields 0 for false, 0xFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PCMPEQD </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the comparison\n"
"///    results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_pi32(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pcmpeqd((__v2si)__m1, (__v2si)__m2);\n"
"}\n"
"\n"
"/// Compares the 8-bit integer elements of two 64-bit integer vectors of\n"
"///    [8 x i8] to determine if the element of the first vector is greater than\n"
"///    the corresponding element of the second vector.\n"
"///\n"
"///    The comparison yields 0 for false, 0xFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PCMPGTB </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [8 x i8].\n"
"/// \\returns A 64-bit integer vector of [8 x i8] containing the comparison\n"
"///    results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_pi8(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pcmpgtb((__v8qi)__m1, (__v8qi)__m2);\n"
"}\n"
"\n"
"/// Compares the 16-bit integer elements of two 64-bit integer vectors of\n"
"///    [4 x i16] to determine if the element of the first vector is greater than\n"
"///    the corresponding element of the second vector.\n"
"///\n"
"///    The comparison yields 0 for false, 0xFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PCMPGTW </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the comparison\n"
"///    results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_pi16(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pcmpgtw((__v4hi)__m1, (__v4hi)__m2);\n"
"}\n"
"\n"
"/// Compares the 32-bit integer elements of two 64-bit integer vectors of\n"
"///    [2 x i32] to determine if the element of the first vector is greater than\n"
"///    the corresponding element of the second vector.\n"
"///\n"
"///    The comparison yields 0 for false, 0xFFFFFFFF for true.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PCMPGTD </c> instruction.\n"
"///\n"
"/// \\param __m1\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\param __m2\n"
"///    A 64-bit integer vector of [2 x i32].\n"
"/// \\returns A 64-bit integer vector of [2 x i32] containing the comparison\n"
"///    results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_pi32(__m64 __m1, __m64 __m2)\n"
"{\n"
"    return (__m64)__builtin_ia32_pcmpgtd((__v2si)__m1, (__v2si)__m2);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector initialized to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PXOR </c> instruction.\n"
"///\n"
"/// \\returns An initialized 64-bit integer vector with all elements set to zero.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_setzero_si64(void)\n"
"{\n"
"    return __extension__ (__m64){ 0LL };\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector initialized with the specified\n"
"///    32-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __i1\n"
"///    A 32-bit integer value used to initialize the upper 32 bits of the\n"
"///    result.\n"
"/// \\param __i0\n"
"///    A 32-bit integer value used to initialize the lower 32 bits of the\n"
"///    result.\n"
"/// \\returns An initialized 64-bit integer vector.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_set_pi32(int __i1, int __i0)\n"
"{\n"
"    return (__m64)__builtin_ia32_vec_init_v2si(__i0, __i1);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector initialized with the specified\n"
"///    16-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __s3\n"
"///    A 16-bit integer value used to initialize bits [63:48] of the result.\n"
"/// \\param __s2\n"
"///    A 16-bit integer value used to initialize bits [47:32] of the result.\n"
"/// \\param __s1\n"
"///    A 16-bit integer value used to initialize bits [31:16] of the result.\n"
"/// \\param __s0\n"
"///    A 16-bit integer value used to initialize bits [15:0] of the result.\n"
"/// \\returns An initialized 64-bit integer vector.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_set_pi16(short __s3, short __s2, short __s1, short __s0)\n"
"{\n"
"    return (__m64)__builtin_ia32_vec_init_v4hi(__s0, __s1, __s2, __s3);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector initialized with the specified\n"
"///    8-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __b7\n"
"///    An 8-bit integer value used to initialize bits [63:56] of the result.\n"
"/// \\param __b6\n"
"///    An 8-bit integer value used to initialize bits [55:48] of the result.\n"
"/// \\param __b5\n"
"///    An 8-bit integer value used to initialize bits [47:40] of the result.\n"
"/// \\param __b4\n"
"///    An 8-bit integer value used to initialize bits [39:32] of the result.\n"
"/// \\param __b3\n"
"///    An 8-bit integer value used to initialize bits [31:24] of the result.\n"
"/// \\param __b2\n"
"///    An 8-bit integer value used to initialize bits [23:16] of the result.\n"
"/// \\param __b1\n"
"///    An 8-bit integer value used to initialize bits [15:8] of the result.\n"
"/// \\param __b0\n"
"///    An 8-bit integer value used to initialize bits [7:0] of the result.\n"
"/// \\returns An initialized 64-bit integer vector.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_set_pi8(char __b7, char __b6, char __b5, char __b4, char __b3, char __b2,\n"
"            char __b1, char __b0)\n"
"{\n"
"    return (__m64)__builtin_ia32_vec_init_v8qi(__b0, __b1, __b2, __b3,\n"
"                                               __b4, __b5, __b6, __b7);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector of [2 x i32], with each of the\n"
"///    32-bit integer vector elements set to the specified 32-bit integer\n"
"///    value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __i\n"
"///    A 32-bit integer value used to initialize each vector element of the\n"
"///    result.\n"
"/// \\returns An initialized 64-bit integer vector of [2 x i32].\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_set1_pi32(int __i)\n"
"{\n"
"    return _mm_set_pi32(__i, __i);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector of [4 x i16], with each of the\n"
"///    16-bit integer vector elements set to the specified 16-bit integer\n"
"///    value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A 16-bit integer value used to initialize each vector element of the\n"
"///    result.\n"
"/// \\returns An initialized 64-bit integer vector of [4 x i16].\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_set1_pi16(short __w)\n"
"{\n"
"    return _mm_set_pi16(__w, __w, __w, __w);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector of [8 x i8], with each of the\n"
"///    8-bit integer vector elements set to the specified 8-bit integer value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __b\n"
"///    An 8-bit integer value used to initialize each vector element of the\n"
"///    result.\n"
"/// \\returns An initialized 64-bit integer vector of [8 x i8].\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_set1_pi8(char __b)\n"
"{\n"
"    return _mm_set_pi8(__b, __b, __b, __b, __b, __b, __b, __b);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector, initialized in reverse order with\n"
"///    the specified 32-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __i0\n"
"///    A 32-bit integer value used to initialize the lower 32 bits of the\n"
"///    result.\n"
"/// \\param __i1\n"
"///    A 32-bit integer value used to initialize the upper 32 bits of the\n"
"///    result.\n"
"/// \\returns An initialized 64-bit integer vector.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_setr_pi32(int __i0, int __i1)\n"
"{\n"
"    return _mm_set_pi32(__i1, __i0);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector, initialized in reverse order with\n"
"///    the specified 16-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __w0\n"
"///    A 16-bit integer value used to initialize bits [15:0] of the result.\n"
"/// \\param __w1\n"
"///    A 16-bit integer value used to initialize bits [31:16] of the result.\n"
"/// \\param __w2\n"
"///    A 16-bit integer value used to initialize bits [47:32] of the result.\n"
"/// \\param __w3\n"
"///    A 16-bit integer value used to initialize bits [63:48] of the result.\n"
"/// \\returns An initialized 64-bit integer vector.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_setr_pi16(short __w0, short __w1, short __w2, short __w3)\n"
"{\n"
"    return _mm_set_pi16(__w3, __w2, __w1, __w0);\n"
"}\n"
"\n"
"/// Constructs a 64-bit integer vector, initialized in reverse order with\n"
"///    the specified 8-bit integer values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __b0\n"
"///    An 8-bit integer value used to initialize bits [7:0] of the result.\n"
"/// \\param __b1\n"
"///    An 8-bit integer value used to initialize bits [15:8] of the result.\n"
"/// \\param __b2\n"
"///    An 8-bit integer value used to initialize bits [23:16] of the result.\n"
"/// \\param __b3\n"
"///    An 8-bit integer value used to initialize bits [31:24] of the result.\n"
"/// \\param __b4\n"
"///    An 8-bit integer value used to initialize bits [39:32] of the result.\n"
"/// \\param __b5\n"
"///    An 8-bit integer value used to initialize bits [47:40] of the result.\n"
"/// \\param __b6\n"
"///    An 8-bit integer value used to initialize bits [55:48] of the result.\n"
"/// \\param __b7\n"
"///    An 8-bit integer value used to initialize bits [63:56] of the result.\n"
"/// \\returns An initialized 64-bit integer vector.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS\n"
"_mm_setr_pi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,\n"
"             char __b6, char __b7)\n"
"{\n"
"    return _mm_set_pi8(__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"/* Aliases for compatibility. */\n"
"#define _m_empty _mm_empty\n"
"#define _m_from_int _mm_cvtsi32_si64\n"
"#define _m_from_int64 _mm_cvtsi64_m64\n"
"#define _m_to_int _mm_cvtsi64_si32\n"
"#define _m_to_int64 _mm_cvtm64_si64\n"
"#define _m_packsswb _mm_packs_pi16\n"
"#define _m_packssdw _mm_packs_pi32\n"
"#define _m_packuswb _mm_packs_pu16\n"
"#define _m_punpckhbw _mm_unpackhi_pi8\n"
"#define _m_punpckhwd _mm_unpackhi_pi16\n"
"#define _m_punpckhdq _mm_unpackhi_pi32\n"
"#define _m_punpcklbw _mm_unpacklo_pi8\n"
"#define _m_punpcklwd _mm_unpacklo_pi16\n"
"#define _m_punpckldq _mm_unpacklo_pi32\n"
"#define _m_paddb _mm_add_pi8\n"
"#define _m_paddw _mm_add_pi16\n"
"#define _m_paddd _mm_add_pi32\n"
"#define _m_paddsb _mm_adds_pi8\n"
"#define _m_paddsw _mm_adds_pi16\n"
"#define _m_paddusb _mm_adds_pu8\n"
"#define _m_paddusw _mm_adds_pu16\n"
"#define _m_psubb _mm_sub_pi8\n"
"#define _m_psubw _mm_sub_pi16\n"
"#define _m_psubd _mm_sub_pi32\n"
"#define _m_psubsb _mm_subs_pi8\n"
"#define _m_psubsw _mm_subs_pi16\n"
"#define _m_psubusb _mm_subs_pu8\n"
"#define _m_psubusw _mm_subs_pu16\n"
"#define _m_pmaddwd _mm_madd_pi16\n"
"#define _m_pmulhw _mm_mulhi_pi16\n"
"#define _m_pmullw _mm_mullo_pi16\n"
"#define _m_psllw _mm_sll_pi16\n"
"#define _m_psllwi _mm_slli_pi16\n"
"#define _m_pslld _mm_sll_pi32\n"
"#define _m_pslldi _mm_slli_pi32\n"
"#define _m_psllq _mm_sll_si64\n"
"#define _m_psllqi _mm_slli_si64\n"
"#define _m_psraw _mm_sra_pi16\n"
"#define _m_psrawi _mm_srai_pi16\n"
"#define _m_psrad _mm_sra_pi32\n"
"#define _m_psradi _mm_srai_pi32\n"
"#define _m_psrlw _mm_srl_pi16\n"
"#define _m_psrlwi _mm_srli_pi16\n"
"#define _m_psrld _mm_srl_pi32\n"
"#define _m_psrldi _mm_srli_pi32\n"
"#define _m_psrlq _mm_srl_si64\n"
"#define _m_psrlqi _mm_srli_si64\n"
"#define _m_pand _mm_and_si64\n"
"#define _m_pandn _mm_andnot_si64\n"
"#define _m_por _mm_or_si64\n"
"#define _m_pxor _mm_xor_si64\n"
"#define _m_pcmpeqb _mm_cmpeq_pi8\n"
"#define _m_pcmpeqw _mm_cmpeq_pi16\n"
"#define _m_pcmpeqd _mm_cmpeq_pi32\n"
"#define _m_pcmpgtb _mm_cmpgt_pi8\n"
"#define _m_pcmpgtw _mm_cmpgt_pi16\n"
"#define _m_pcmpgtd _mm_cmpgt_pi32\n"
"\n"
"#endif /* __MMINTRIN_H */\n"
"\n"
"" } , 
 { "/builtins/movdirintrin.h" , "/*===------------------------- movdirintrin.h ------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <movdirintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef _MOVDIRINTRIN_H\n"
"#define _MOVDIRINTRIN_H\n"
"\n"
"/* Move doubleword as direct store */\n"
"static __inline__ void\n"
"__attribute__((__always_inline__, __nodebug__,  __target__(\"movdiri\")))\n"
"_directstoreu_u32 (void *__dst, unsigned int  __value)\n"
"{\n"
"  __builtin_ia32_directstore_u32((unsigned int *)__dst, (unsigned int)__value);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"\n"
"/* Move quadword as direct store */\n"
"static __inline__ void\n"
"__attribute__((__always_inline__, __nodebug__,  __target__(\"movdiri\")))\n"
"_directstoreu_u64 (void *__dst, unsigned long __value)\n"
"{\n"
"  __builtin_ia32_directstore_u64((unsigned long *)__dst, __value);\n"
"}\n"
"\n"
"#endif /* __x86_64__ */\n"
"\n"
"/*\n"
" * movdir64b - Move 64 bytes as direct store.\n"
" * The destination must be 64 byte aligned, and the store is atomic.\n"
" * The source address has no alignment requirement, and the load from\n"
" * the source address is not atomic.\n"
" */\n"
"static __inline__ void\n"
"__attribute__((__always_inline__, __nodebug__,  __target__(\"movdir64b\")))\n"
"_movdir64b (void *__dst __attribute__((align_value(64))), const void *__src)\n"
"{\n"
"  __builtin_ia32_movdir64b(__dst, __src);\n"
"}\n"
"\n"
"#endif /* _MOVDIRINTRIN_H */\n"
"" } , 
 { "/builtins/msa.h" , "/*===---- msa.h - MIPS MSA intrinsics --------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef _MSA_H\n"
"#define _MSA_H 1\n"
"\n"
"#if defined(__mips_msa)\n"
"typedef signed char v16i8 __attribute__((vector_size(16), aligned(16)));\n"
"typedef signed char v16i8_b __attribute__((vector_size(16), aligned(1)));\n"
"typedef unsigned char v16u8 __attribute__((vector_size(16), aligned(16)));\n"
"typedef unsigned char v16u8_b __attribute__((vector_size(16), aligned(1)));\n"
"typedef short v8i16 __attribute__((vector_size(16), aligned(16)));\n"
"typedef short v8i16_h __attribute__((vector_size(16), aligned(2)));\n"
"typedef unsigned short v8u16 __attribute__((vector_size(16), aligned(16)));\n"
"typedef unsigned short v8u16_h __attribute__((vector_size(16), aligned(2)));\n"
"typedef int v4i32 __attribute__((vector_size(16), aligned(16)));\n"
"typedef int v4i32_w __attribute__((vector_size(16), aligned(4)));\n"
"typedef unsigned int v4u32 __attribute__((vector_size(16), aligned(16)));\n"
"typedef unsigned int v4u32_w __attribute__((vector_size(16), aligned(4)));\n"
"typedef long long v2i64 __attribute__((vector_size(16), aligned(16)));\n"
"typedef long long v2i64_d __attribute__((vector_size(16), aligned(8)));\n"
"typedef unsigned long long v2u64 __attribute__((vector_size(16), aligned(16)));\n"
"typedef unsigned long long v2u64_d __attribute__((vector_size(16), aligned(8)));\n"
"typedef float v4f32 __attribute__((vector_size(16), aligned(16)));\n"
"typedef float v4f32_w __attribute__((vector_size(16), aligned(4)));\n"
"typedef double v2f64 __attribute__ ((vector_size(16), aligned(16)));\n"
"typedef double v2f64_d __attribute__ ((vector_size(16), aligned(8)));\n"
"\n"
"#define __msa_sll_b __builtin_msa_sll_b\n"
"#define __msa_sll_h __builtin_msa_sll_h\n"
"#define __msa_sll_w __builtin_msa_sll_w\n"
"#define __msa_sll_d __builtin_msa_sll_d\n"
"#define __msa_slli_b __builtin_msa_slli_b\n"
"#define __msa_slli_h __builtin_msa_slli_h\n"
"#define __msa_slli_w __builtin_msa_slli_w\n"
"#define __msa_slli_d __builtin_msa_slli_d\n"
"#define __msa_sra_b __builtin_msa_sra_b\n"
"#define __msa_sra_h __builtin_msa_sra_h\n"
"#define __msa_sra_w __builtin_msa_sra_w\n"
"#define __msa_sra_d __builtin_msa_sra_d\n"
"#define __msa_srai_b __builtin_msa_srai_b\n"
"#define __msa_srai_h __builtin_msa_srai_h\n"
"#define __msa_srai_w __builtin_msa_srai_w\n"
"#define __msa_srai_d __builtin_msa_srai_d\n"
"#define __msa_srar_b __builtin_msa_srar_b\n"
"#define __msa_srar_h __builtin_msa_srar_h\n"
"#define __msa_srar_w __builtin_msa_srar_w\n"
"#define __msa_srar_d __builtin_msa_srar_d\n"
"#define __msa_srari_b __builtin_msa_srari_b\n"
"#define __msa_srari_h __builtin_msa_srari_h\n"
"#define __msa_srari_w __builtin_msa_srari_w\n"
"#define __msa_srari_d __builtin_msa_srari_d\n"
"#define __msa_srl_b __builtin_msa_srl_b\n"
"#define __msa_srl_h __builtin_msa_srl_h\n"
"#define __msa_srl_w __builtin_msa_srl_w\n"
"#define __msa_srl_d __builtin_msa_srl_d\n"
"#define __msa_srli_b __builtin_msa_srli_b\n"
"#define __msa_srli_h __builtin_msa_srli_h\n"
"#define __msa_srli_w __builtin_msa_srli_w\n"
"#define __msa_srli_d __builtin_msa_srli_d\n"
"#define __msa_srlr_b __builtin_msa_srlr_b\n"
"#define __msa_srlr_h __builtin_msa_srlr_h\n"
"#define __msa_srlr_w __builtin_msa_srlr_w\n"
"#define __msa_srlr_d __builtin_msa_srlr_d\n"
"#define __msa_srlri_b __builtin_msa_srlri_b\n"
"#define __msa_srlri_h __builtin_msa_srlri_h\n"
"#define __msa_srlri_w __builtin_msa_srlri_w\n"
"#define __msa_srlri_d __builtin_msa_srlri_d\n"
"#define __msa_bclr_b __builtin_msa_bclr_b\n"
"#define __msa_bclr_h __builtin_msa_bclr_h\n"
"#define __msa_bclr_w __builtin_msa_bclr_w\n"
"#define __msa_bclr_d __builtin_msa_bclr_d\n"
"#define __msa_bclri_b __builtin_msa_bclri_b\n"
"#define __msa_bclri_h __builtin_msa_bclri_h\n"
"#define __msa_bclri_w __builtin_msa_bclri_w\n"
"#define __msa_bclri_d __builtin_msa_bclri_d\n"
"#define __msa_bset_b __builtin_msa_bset_b\n"
"#define __msa_bset_h __builtin_msa_bset_h\n"
"#define __msa_bset_w __builtin_msa_bset_w\n"
"#define __msa_bset_d __builtin_msa_bset_d\n"
"#define __msa_bseti_b __builtin_msa_bseti_b\n"
"#define __msa_bseti_h __builtin_msa_bseti_h\n"
"#define __msa_bseti_w __builtin_msa_bseti_w\n"
"#define __msa_bseti_d __builtin_msa_bseti_d\n"
"#define __msa_bneg_b __builtin_msa_bneg_b\n"
"#define __msa_bneg_h __builtin_msa_bneg_h\n"
"#define __msa_bneg_w __builtin_msa_bneg_w\n"
"#define __msa_bneg_d __builtin_msa_bneg_d\n"
"#define __msa_bnegi_b __builtin_msa_bnegi_b\n"
"#define __msa_bnegi_h __builtin_msa_bnegi_h\n"
"#define __msa_bnegi_w __builtin_msa_bnegi_w\n"
"#define __msa_bnegi_d __builtin_msa_bnegi_d\n"
"#define __msa_binsl_b __builtin_msa_binsl_b\n"
"#define __msa_binsl_h __builtin_msa_binsl_h\n"
"#define __msa_binsl_w __builtin_msa_binsl_w\n"
"#define __msa_binsl_d __builtin_msa_binsl_d\n"
"#define __msa_binsli_b __builtin_msa_binsli_b\n"
"#define __msa_binsli_h __builtin_msa_binsli_h\n"
"#define __msa_binsli_w __builtin_msa_binsli_w\n"
"#define __msa_binsli_d __builtin_msa_binsli_d\n"
"#define __msa_binsr_b __builtin_msa_binsr_b\n"
"#define __msa_binsr_h __builtin_msa_binsr_h\n"
"#define __msa_binsr_w __builtin_msa_binsr_w\n"
"#define __msa_binsr_d __builtin_msa_binsr_d\n"
"#define __msa_binsri_b __builtin_msa_binsri_b\n"
"#define __msa_binsri_h __builtin_msa_binsri_h\n"
"#define __msa_binsri_w __builtin_msa_binsri_w\n"
"#define __msa_binsri_d __builtin_msa_binsri_d\n"
"#define __msa_addv_b __builtin_msa_addv_b\n"
"#define __msa_addv_h __builtin_msa_addv_h\n"
"#define __msa_addv_w __builtin_msa_addv_w\n"
"#define __msa_addv_d __builtin_msa_addv_d\n"
"#define __msa_addvi_b __builtin_msa_addvi_b\n"
"#define __msa_addvi_h __builtin_msa_addvi_h\n"
"#define __msa_addvi_w __builtin_msa_addvi_w\n"
"#define __msa_addvi_d __builtin_msa_addvi_d\n"
"#define __msa_subv_b __builtin_msa_subv_b\n"
"#define __msa_subv_h __builtin_msa_subv_h\n"
"#define __msa_subv_w __builtin_msa_subv_w\n"
"#define __msa_subv_d __builtin_msa_subv_d\n"
"#define __msa_subvi_b __builtin_msa_subvi_b\n"
"#define __msa_subvi_h __builtin_msa_subvi_h\n"
"#define __msa_subvi_w __builtin_msa_subvi_w\n"
"#define __msa_subvi_d __builtin_msa_subvi_d\n"
"#define __msa_max_s_b __builtin_msa_max_s_b\n"
"#define __msa_max_s_h __builtin_msa_max_s_h\n"
"#define __msa_max_s_w __builtin_msa_max_s_w\n"
"#define __msa_max_s_d __builtin_msa_max_s_d\n"
"#define __msa_maxi_s_b __builtin_msa_maxi_s_b\n"
"#define __msa_maxi_s_h __builtin_msa_maxi_s_h\n"
"#define __msa_maxi_s_w __builtin_msa_maxi_s_w\n"
"#define __msa_maxi_s_d __builtin_msa_maxi_s_d\n"
"#define __msa_max_u_b __builtin_msa_max_u_b\n"
"#define __msa_max_u_h __builtin_msa_max_u_h\n"
"#define __msa_max_u_w __builtin_msa_max_u_w\n"
"#define __msa_max_u_d __builtin_msa_max_u_d\n"
"#define __msa_maxi_u_b __builtin_msa_maxi_u_b\n"
"#define __msa_maxi_u_h __builtin_msa_maxi_u_h\n"
"#define __msa_maxi_u_w __builtin_msa_maxi_u_w\n"
"#define __msa_maxi_u_d __builtin_msa_maxi_u_d\n"
"#define __msa_min_s_b __builtin_msa_min_s_b\n"
"#define __msa_min_s_h __builtin_msa_min_s_h\n"
"#define __msa_min_s_w __builtin_msa_min_s_w\n"
"#define __msa_min_s_d __builtin_msa_min_s_d\n"
"#define __msa_mini_s_b __builtin_msa_mini_s_b\n"
"#define __msa_mini_s_h __builtin_msa_mini_s_h\n"
"#define __msa_mini_s_w __builtin_msa_mini_s_w\n"
"#define __msa_mini_s_d __builtin_msa_mini_s_d\n"
"#define __msa_min_u_b __builtin_msa_min_u_b\n"
"#define __msa_min_u_h __builtin_msa_min_u_h\n"
"#define __msa_min_u_w __builtin_msa_min_u_w\n"
"#define __msa_min_u_d __builtin_msa_min_u_d\n"
"#define __msa_mini_u_b __builtin_msa_mini_u_b\n"
"#define __msa_mini_u_h __builtin_msa_mini_u_h\n"
"#define __msa_mini_u_w __builtin_msa_mini_u_w\n"
"#define __msa_mini_u_d __builtin_msa_mini_u_d\n"
"#define __msa_max_a_b __builtin_msa_max_a_b\n"
"#define __msa_max_a_h __builtin_msa_max_a_h\n"
"#define __msa_max_a_w __builtin_msa_max_a_w\n"
"#define __msa_max_a_d __builtin_msa_max_a_d\n"
"#define __msa_min_a_b __builtin_msa_min_a_b\n"
"#define __msa_min_a_h __builtin_msa_min_a_h\n"
"#define __msa_min_a_w __builtin_msa_min_a_w\n"
"#define __msa_min_a_d __builtin_msa_min_a_d\n"
"#define __msa_ceq_b __builtin_msa_ceq_b\n"
"#define __msa_ceq_h __builtin_msa_ceq_h\n"
"#define __msa_ceq_w __builtin_msa_ceq_w\n"
"#define __msa_ceq_d __builtin_msa_ceq_d\n"
"#define __msa_ceqi_b __builtin_msa_ceqi_b\n"
"#define __msa_ceqi_h __builtin_msa_ceqi_h\n"
"#define __msa_ceqi_w __builtin_msa_ceqi_w\n"
"#define __msa_ceqi_d __builtin_msa_ceqi_d\n"
"#define __msa_clt_s_b __builtin_msa_clt_s_b\n"
"#define __msa_clt_s_h __builtin_msa_clt_s_h\n"
"#define __msa_clt_s_w __builtin_msa_clt_s_w\n"
"#define __msa_clt_s_d __builtin_msa_clt_s_d\n"
"#define __msa_clti_s_b __builtin_msa_clti_s_b\n"
"#define __msa_clti_s_h __builtin_msa_clti_s_h\n"
"#define __msa_clti_s_w __builtin_msa_clti_s_w\n"
"#define __msa_clti_s_d __builtin_msa_clti_s_d\n"
"#define __msa_clt_u_b __builtin_msa_clt_u_b\n"
"#define __msa_clt_u_h __builtin_msa_clt_u_h\n"
"#define __msa_clt_u_w __builtin_msa_clt_u_w\n"
"#define __msa_clt_u_d __builtin_msa_clt_u_d\n"
"#define __msa_clti_u_b __builtin_msa_clti_u_b\n"
"#define __msa_clti_u_h __builtin_msa_clti_u_h\n"
"#define __msa_clti_u_w __builtin_msa_clti_u_w\n"
"#define __msa_clti_u_d __builtin_msa_clti_u_d\n"
"#define __msa_cle_s_b __builtin_msa_cle_s_b\n"
"#define __msa_cle_s_h __builtin_msa_cle_s_h\n"
"#define __msa_cle_s_w __builtin_msa_cle_s_w\n"
"#define __msa_cle_s_d __builtin_msa_cle_s_d\n"
"#define __msa_clei_s_b __builtin_msa_clei_s_b\n"
"#define __msa_clei_s_h __builtin_msa_clei_s_h\n"
"#define __msa_clei_s_w __builtin_msa_clei_s_w\n"
"#define __msa_clei_s_d __builtin_msa_clei_s_d\n"
"#define __msa_cle_u_b __builtin_msa_cle_u_b\n"
"#define __msa_cle_u_h __builtin_msa_cle_u_h\n"
"#define __msa_cle_u_w __builtin_msa_cle_u_w\n"
"#define __msa_cle_u_d __builtin_msa_cle_u_d\n"
"#define __msa_clei_u_b __builtin_msa_clei_u_b\n"
"#define __msa_clei_u_h __builtin_msa_clei_u_h\n"
"#define __msa_clei_u_w __builtin_msa_clei_u_w\n"
"#define __msa_clei_u_d __builtin_msa_clei_u_d\n"
"#define __msa_ld_b __builtin_msa_ld_b\n"
"#define __msa_ld_h __builtin_msa_ld_h\n"
"#define __msa_ld_w __builtin_msa_ld_w\n"
"#define __msa_ld_d __builtin_msa_ld_d\n"
"#define __msa_st_b __builtin_msa_st_b\n"
"#define __msa_st_h __builtin_msa_st_h\n"
"#define __msa_st_w __builtin_msa_st_w\n"
"#define __msa_st_d __builtin_msa_st_d\n"
"#define __msa_sat_s_b __builtin_msa_sat_s_b\n"
"#define __msa_sat_s_h __builtin_msa_sat_s_h\n"
"#define __msa_sat_s_w __builtin_msa_sat_s_w\n"
"#define __msa_sat_s_d __builtin_msa_sat_s_d\n"
"#define __msa_sat_u_b __builtin_msa_sat_u_b\n"
"#define __msa_sat_u_h __builtin_msa_sat_u_h\n"
"#define __msa_sat_u_w __builtin_msa_sat_u_w\n"
"#define __msa_sat_u_d __builtin_msa_sat_u_d\n"
"#define __msa_add_a_b __builtin_msa_add_a_b\n"
"#define __msa_add_a_h __builtin_msa_add_a_h\n"
"#define __msa_add_a_w __builtin_msa_add_a_w\n"
"#define __msa_add_a_d __builtin_msa_add_a_d\n"
"#define __msa_adds_a_b __builtin_msa_adds_a_b\n"
"#define __msa_adds_a_h __builtin_msa_adds_a_h\n"
"#define __msa_adds_a_w __builtin_msa_adds_a_w\n"
"#define __msa_adds_a_d __builtin_msa_adds_a_d\n"
"#define __msa_adds_s_b __builtin_msa_adds_s_b\n"
"#define __msa_adds_s_h __builtin_msa_adds_s_h\n"
"#define __msa_adds_s_w __builtin_msa_adds_s_w\n"
"#define __msa_adds_s_d __builtin_msa_adds_s_d\n"
"#define __msa_adds_u_b __builtin_msa_adds_u_b\n"
"#define __msa_adds_u_h __builtin_msa_adds_u_h\n"
"#define __msa_adds_u_w __builtin_msa_adds_u_w\n"
"#define __msa_adds_u_d __builtin_msa_adds_u_d\n"
"#define __msa_ave_s_b __builtin_msa_ave_s_b\n"
"#define __msa_ave_s_h __builtin_msa_ave_s_h\n"
"#define __msa_ave_s_w __builtin_msa_ave_s_w\n"
"#define __msa_ave_s_d __builtin_msa_ave_s_d\n"
"#define __msa_ave_u_b __builtin_msa_ave_u_b\n"
"#define __msa_ave_u_h __builtin_msa_ave_u_h\n"
"#define __msa_ave_u_w __builtin_msa_ave_u_w\n"
"#define __msa_ave_u_d __builtin_msa_ave_u_d\n"
"#define __msa_aver_s_b __builtin_msa_aver_s_b\n"
"#define __msa_aver_s_h __builtin_msa_aver_s_h\n"
"#define __msa_aver_s_w __builtin_msa_aver_s_w\n"
"#define __msa_aver_s_d __builtin_msa_aver_s_d\n"
"#define __msa_aver_u_b __builtin_msa_aver_u_b\n"
"#define __msa_aver_u_h __builtin_msa_aver_u_h\n"
"#define __msa_aver_u_w __builtin_msa_aver_u_w\n"
"#define __msa_aver_u_d __builtin_msa_aver_u_d\n"
"#define __msa_subs_s_b __builtin_msa_subs_s_b\n"
"#define __msa_subs_s_h __builtin_msa_subs_s_h\n"
"#define __msa_subs_s_w __builtin_msa_subs_s_w\n"
"#define __msa_subs_s_d __builtin_msa_subs_s_d\n"
"#define __msa_subs_u_b __builtin_msa_subs_u_b\n"
"#define __msa_subs_u_h __builtin_msa_subs_u_h\n"
"#define __msa_subs_u_w __builtin_msa_subs_u_w\n"
"#define __msa_subs_u_d __builtin_msa_subs_u_d\n"
"#define __msa_subsuu_s_b __builtin_msa_subsuu_s_b\n"
"#define __msa_subsuu_s_h __builtin_msa_subsuu_s_h\n"
"#define __msa_subsuu_s_w __builtin_msa_subsuu_s_w\n"
"#define __msa_subsuu_s_d __builtin_msa_subsuu_s_d\n"
"#define __msa_subsus_u_b __builtin_msa_subsus_u_b\n"
"#define __msa_subsus_u_h __builtin_msa_subsus_u_h\n"
"#define __msa_subsus_u_w __builtin_msa_subsus_u_w\n"
"#define __msa_subsus_u_d __builtin_msa_subsus_u_d\n"
"#define __msa_asub_s_b __builtin_msa_asub_s_b\n"
"#define __msa_asub_s_h __builtin_msa_asub_s_h\n"
"#define __msa_asub_s_w __builtin_msa_asub_s_w\n"
"#define __msa_asub_s_d __builtin_msa_asub_s_d\n"
"#define __msa_asub_u_b __builtin_msa_asub_u_b\n"
"#define __msa_asub_u_h __builtin_msa_asub_u_h\n"
"#define __msa_asub_u_w __builtin_msa_asub_u_w\n"
"#define __msa_asub_u_d __builtin_msa_asub_u_d\n"
"#define __msa_mulv_b __builtin_msa_mulv_b\n"
"#define __msa_mulv_h __builtin_msa_mulv_h\n"
"#define __msa_mulv_w __builtin_msa_mulv_w\n"
"#define __msa_mulv_d __builtin_msa_mulv_d\n"
"#define __msa_maddv_b __builtin_msa_maddv_b\n"
"#define __msa_maddv_h __builtin_msa_maddv_h\n"
"#define __msa_maddv_w __builtin_msa_maddv_w\n"
"#define __msa_maddv_d __builtin_msa_maddv_d\n"
"#define __msa_msubv_b __builtin_msa_msubv_b\n"
"#define __msa_msubv_h __builtin_msa_msubv_h\n"
"#define __msa_msubv_w __builtin_msa_msubv_w\n"
"#define __msa_msubv_d __builtin_msa_msubv_d\n"
"#define __msa_div_s_b __builtin_msa_div_s_b\n"
"#define __msa_div_s_h __builtin_msa_div_s_h\n"
"#define __msa_div_s_w __builtin_msa_div_s_w\n"
"#define __msa_div_s_d __builtin_msa_div_s_d\n"
"#define __msa_div_u_b __builtin_msa_div_u_b\n"
"#define __msa_div_u_h __builtin_msa_div_u_h\n"
"#define __msa_div_u_w __builtin_msa_div_u_w\n"
"#define __msa_div_u_d __builtin_msa_div_u_d\n"
"#define __msa_hadd_s_h __builtin_msa_hadd_s_h\n"
"#define __msa_hadd_s_w __builtin_msa_hadd_s_w\n"
"#define __msa_hadd_s_d __builtin_msa_hadd_s_d\n"
"#define __msa_hadd_u_h __builtin_msa_hadd_u_h\n"
"#define __msa_hadd_u_w __builtin_msa_hadd_u_w\n"
"#define __msa_hadd_u_d __builtin_msa_hadd_u_d\n"
"#define __msa_hsub_s_h __builtin_msa_hsub_s_h\n"
"#define __msa_hsub_s_w __builtin_msa_hsub_s_w\n"
"#define __msa_hsub_s_d __builtin_msa_hsub_s_d\n"
"#define __msa_hsub_u_h __builtin_msa_hsub_u_h\n"
"#define __msa_hsub_u_w __builtin_msa_hsub_u_w\n"
"#define __msa_hsub_u_d __builtin_msa_hsub_u_d\n"
"#define __msa_mod_s_b __builtin_msa_mod_s_b\n"
"#define __msa_mod_s_h __builtin_msa_mod_s_h\n"
"#define __msa_mod_s_w __builtin_msa_mod_s_w\n"
"#define __msa_mod_s_d __builtin_msa_mod_s_d\n"
"#define __msa_mod_u_b __builtin_msa_mod_u_b\n"
"#define __msa_mod_u_h __builtin_msa_mod_u_h\n"
"#define __msa_mod_u_w __builtin_msa_mod_u_w\n"
"#define __msa_mod_u_d __builtin_msa_mod_u_d\n"
"#define __msa_dotp_s_h __builtin_msa_dotp_s_h\n"
"#define __msa_dotp_s_w __builtin_msa_dotp_s_w\n"
"#define __msa_dotp_s_d __builtin_msa_dotp_s_d\n"
"#define __msa_dotp_u_h __builtin_msa_dotp_u_h\n"
"#define __msa_dotp_u_w __builtin_msa_dotp_u_w\n"
"#define __msa_dotp_u_d __builtin_msa_dotp_u_d\n"
"#define __msa_dpadd_s_h __builtin_msa_dpadd_s_h\n"
"#define __msa_dpadd_s_w __builtin_msa_dpadd_s_w\n"
"#define __msa_dpadd_s_d __builtin_msa_dpadd_s_d\n"
"#define __msa_dpadd_u_h __builtin_msa_dpadd_u_h\n"
"#define __msa_dpadd_u_w __builtin_msa_dpadd_u_w\n"
"#define __msa_dpadd_u_d __builtin_msa_dpadd_u_d\n"
"#define __msa_dpsub_s_h __builtin_msa_dpsub_s_h\n"
"#define __msa_dpsub_s_w __builtin_msa_dpsub_s_w\n"
"#define __msa_dpsub_s_d __builtin_msa_dpsub_s_d\n"
"#define __msa_dpsub_u_h __builtin_msa_dpsub_u_h\n"
"#define __msa_dpsub_u_w __builtin_msa_dpsub_u_w\n"
"#define __msa_dpsub_u_d __builtin_msa_dpsub_u_d\n"
"#define __msa_sld_b __builtin_msa_sld_b\n"
"#define __msa_sld_h __builtin_msa_sld_h\n"
"#define __msa_sld_w __builtin_msa_sld_w\n"
"#define __msa_sld_d __builtin_msa_sld_d\n"
"#define __msa_sldi_b __builtin_msa_sldi_b\n"
"#define __msa_sldi_h __builtin_msa_sldi_h\n"
"#define __msa_sldi_w __builtin_msa_sldi_w\n"
"#define __msa_sldi_d __builtin_msa_sldi_d\n"
"#define __msa_splat_b __builtin_msa_splat_b\n"
"#define __msa_splat_h __builtin_msa_splat_h\n"
"#define __msa_splat_w __builtin_msa_splat_w\n"
"#define __msa_splat_d __builtin_msa_splat_d\n"
"#define __msa_splati_b __builtin_msa_splati_b\n"
"#define __msa_splati_h __builtin_msa_splati_h\n"
"#define __msa_splati_w __builtin_msa_splati_w\n"
"#define __msa_splati_d __builtin_msa_splati_d\n"
"#define __msa_pckev_b __builtin_msa_pckev_b\n"
"#define __msa_pckev_h __builtin_msa_pckev_h\n"
"#define __msa_pckev_w __builtin_msa_pckev_w\n"
"#define __msa_pckev_d __builtin_msa_pckev_d\n"
"#define __msa_pckod_b __builtin_msa_pckod_b\n"
"#define __msa_pckod_h __builtin_msa_pckod_h\n"
"#define __msa_pckod_w __builtin_msa_pckod_w\n"
"#define __msa_pckod_d __builtin_msa_pckod_d\n"
"#define __msa_ilvl_b __builtin_msa_ilvl_b\n"
"#define __msa_ilvl_h __builtin_msa_ilvl_h\n"
"#define __msa_ilvl_w __builtin_msa_ilvl_w\n"
"#define __msa_ilvl_d __builtin_msa_ilvl_d\n"
"#define __msa_ilvr_b __builtin_msa_ilvr_b\n"
"#define __msa_ilvr_h __builtin_msa_ilvr_h\n"
"#define __msa_ilvr_w __builtin_msa_ilvr_w\n"
"#define __msa_ilvr_d __builtin_msa_ilvr_d\n"
"#define __msa_ilvev_b __builtin_msa_ilvev_b\n"
"#define __msa_ilvev_h __builtin_msa_ilvev_h\n"
"#define __msa_ilvev_w __builtin_msa_ilvev_w\n"
"#define __msa_ilvev_d __builtin_msa_ilvev_d\n"
"#define __msa_ilvod_b __builtin_msa_ilvod_b\n"
"#define __msa_ilvod_h __builtin_msa_ilvod_h\n"
"#define __msa_ilvod_w __builtin_msa_ilvod_w\n"
"#define __msa_ilvod_d __builtin_msa_ilvod_d\n"
"#define __msa_vshf_b __builtin_msa_vshf_b\n"
"#define __msa_vshf_h __builtin_msa_vshf_h\n"
"#define __msa_vshf_w __builtin_msa_vshf_w\n"
"#define __msa_vshf_d __builtin_msa_vshf_d\n"
"#define __msa_and_v __builtin_msa_and_v\n"
"#define __msa_andi_b __builtin_msa_andi_b\n"
"#define __msa_or_v __builtin_msa_or_v\n"
"#define __msa_ori_b __builtin_msa_ori_b\n"
"#define __msa_nor_v __builtin_msa_nor_v\n"
"#define __msa_nori_b __builtin_msa_nori_b\n"
"#define __msa_xor_v __builtin_msa_xor_v\n"
"#define __msa_xori_b __builtin_msa_xori_b\n"
"#define __msa_bmnz_v __builtin_msa_bmnz_v\n"
"#define __msa_bmnzi_b __builtin_msa_bmnzi_b\n"
"#define __msa_bmz_v __builtin_msa_bmz_v\n"
"#define __msa_bmzi_b __builtin_msa_bmzi_b\n"
"#define __msa_bsel_v __builtin_msa_bsel_v\n"
"#define __msa_bseli_b __builtin_msa_bseli_b\n"
"#define __msa_shf_b __builtin_msa_shf_b\n"
"#define __msa_shf_h __builtin_msa_shf_h\n"
"#define __msa_shf_w __builtin_msa_shf_w\n"
"#define __msa_test_bnz_v __builtin_msa_bnz_v\n"
"#define __msa_test_bz_v __builtin_msa_bz_v\n"
"#define __msa_fill_b __builtin_msa_fill_b\n"
"#define __msa_fill_h __builtin_msa_fill_h\n"
"#define __msa_fill_w __builtin_msa_fill_w\n"
"#define __msa_fill_d __builtin_msa_fill_d\n"
"#define __msa_pcnt_b __builtin_msa_pcnt_b\n"
"#define __msa_pcnt_h __builtin_msa_pcnt_h\n"
"#define __msa_pcnt_w __builtin_msa_pcnt_w\n"
"#define __msa_pcnt_d __builtin_msa_pcnt_d\n"
"#define __msa_nloc_b __builtin_msa_nloc_b\n"
"#define __msa_nloc_h __builtin_msa_nloc_h\n"
"#define __msa_nloc_w __builtin_msa_nloc_w\n"
"#define __msa_nloc_d __builtin_msa_nloc_d\n"
"#define __msa_nlzc_b __builtin_msa_nlzc_b\n"
"#define __msa_nlzc_h __builtin_msa_nlzc_h\n"
"#define __msa_nlzc_w __builtin_msa_nlzc_w\n"
"#define __msa_nlzc_d __builtin_msa_nlzc_d\n"
"#define __msa_copy_s_b __builtin_msa_copy_s_b\n"
"#define __msa_copy_s_h __builtin_msa_copy_s_h\n"
"#define __msa_copy_s_w __builtin_msa_copy_s_w\n"
"#define __msa_copy_s_d __builtin_msa_copy_s_d\n"
"#define __msa_copy_u_b __builtin_msa_copy_u_b\n"
"#define __msa_copy_u_h __builtin_msa_copy_u_h\n"
"#define __msa_copy_u_w __builtin_msa_copy_u_w\n"
"#define __msa_copy_u_d __builtin_msa_copy_u_d\n"
"#define __msa_insert_b __builtin_msa_insert_b\n"
"#define __msa_insert_h __builtin_msa_insert_h\n"
"#define __msa_insert_w __builtin_msa_insert_w\n"
"#define __msa_insert_d __builtin_msa_insert_d\n"
"#define __msa_insve_b __builtin_msa_insve_b\n"
"#define __msa_insve_h __builtin_msa_insve_h\n"
"#define __msa_insve_w __builtin_msa_insve_w\n"
"#define __msa_insve_d __builtin_msa_insve_d\n"
"#define __msa_test_bnz_b __builtin_msa_bnz_b\n"
"#define __msa_test_bnz_h __builtin_msa_bnz_h\n"
"#define __msa_test_bnz_w __builtin_msa_bnz_w\n"
"#define __msa_test_bnz_d __builtin_msa_bnz_d\n"
"#define __msa_test_bz_b __builtin_msa_bz_b\n"
"#define __msa_test_bz_h __builtin_msa_bz_h\n"
"#define __msa_test_bz_w __builtin_msa_bz_w\n"
"#define __msa_test_bz_d __builtin_msa_bz_d\n"
"#define __msa_ldi_b __builtin_msa_ldi_b\n"
"#define __msa_ldi_h __builtin_msa_ldi_h\n"
"#define __msa_ldi_w __builtin_msa_ldi_w\n"
"#define __msa_ldi_d __builtin_msa_ldi_d\n"
"#define __msa_fcaf_w __builtin_msa_fcaf_w\n"
"#define __msa_fcaf_d __builtin_msa_fcaf_d\n"
"#define __msa_fcor_w __builtin_msa_fcor_w\n"
"#define __msa_fcor_d __builtin_msa_fcor_d\n"
"#define __msa_fcun_w __builtin_msa_fcun_w\n"
"#define __msa_fcun_d __builtin_msa_fcun_d\n"
"#define __msa_fcune_w __builtin_msa_fcune_w\n"
"#define __msa_fcune_d __builtin_msa_fcune_d\n"
"#define __msa_fcueq_w __builtin_msa_fcueq_w\n"
"#define __msa_fcueq_d __builtin_msa_fcueq_d\n"
"#define __msa_fceq_w __builtin_msa_fceq_w\n"
"#define __msa_fceq_d __builtin_msa_fceq_d\n"
"#define __msa_fcne_w __builtin_msa_fcne_w\n"
"#define __msa_fcne_d __builtin_msa_fcne_d\n"
"#define __msa_fclt_w __builtin_msa_fclt_w\n"
"#define __msa_fclt_d __builtin_msa_fclt_d\n"
"#define __msa_fcult_w __builtin_msa_fcult_w\n"
"#define __msa_fcult_d __builtin_msa_fcult_d\n"
"#define __msa_fcle_w __builtin_msa_fcle_w\n"
"#define __msa_fcle_d __builtin_msa_fcle_d\n"
"#define __msa_fcule_w __builtin_msa_fcule_w\n"
"#define __msa_fcule_d __builtin_msa_fcule_d\n"
"#define __msa_fsaf_w __builtin_msa_fsaf_w\n"
"#define __msa_fsaf_d __builtin_msa_fsaf_d\n"
"#define __msa_fsor_w __builtin_msa_fsor_w\n"
"#define __msa_fsor_d __builtin_msa_fsor_d\n"
"#define __msa_fsun_w __builtin_msa_fsun_w\n"
"#define __msa_fsun_d __builtin_msa_fsun_d\n"
"#define __msa_fsune_w __builtin_msa_fsune_w\n"
"#define __msa_fsune_d __builtin_msa_fsune_d\n"
"#define __msa_fsueq_w __builtin_msa_fsueq_w\n"
"#define __msa_fsueq_d __builtin_msa_fsueq_d\n"
"#define __msa_fseq_w __builtin_msa_fseq_w\n"
"#define __msa_fseq_d __builtin_msa_fseq_d\n"
"#define __msa_fsne_w __builtin_msa_fsne_w\n"
"#define __msa_fsne_d __builtin_msa_fsne_d\n"
"#define __msa_fslt_w __builtin_msa_fslt_w\n"
"#define __msa_fslt_d __builtin_msa_fslt_d\n"
"#define __msa_fsult_w __builtin_msa_fsult_w\n"
"#define __msa_fsult_d __builtin_msa_fsult_d\n"
"#define __msa_fsle_w __builtin_msa_fsle_w\n"
"#define __msa_fsle_d __builtin_msa_fsle_d\n"
"#define __msa_fsule_w __builtin_msa_fsule_w\n"
"#define __msa_fsule_d __builtin_msa_fsule_d\n"
"#define __msa_fadd_w __builtin_msa_fadd_w\n"
"#define __msa_fadd_d __builtin_msa_fadd_d\n"
"#define __msa_fsub_w __builtin_msa_fsub_w\n"
"#define __msa_fsub_d __builtin_msa_fsub_d\n"
"#define __msa_fmul_w __builtin_msa_fmul_w\n"
"#define __msa_fmul_d __builtin_msa_fmul_d\n"
"#define __msa_fdiv_w __builtin_msa_fdiv_w\n"
"#define __msa_fdiv_d __builtin_msa_fdiv_d\n"
"#define __msa_fmadd_w __builtin_msa_fmadd_w\n"
"#define __msa_fmadd_d __builtin_msa_fmadd_d\n"
"#define __msa_fmsub_w __builtin_msa_fmsub_w\n"
"#define __msa_fmsub_d __builtin_msa_fmsub_d\n"
"#define __msa_fexp2_w __builtin_msa_fexp2_w\n"
"#define __msa_fexp2_d __builtin_msa_fexp2_d\n"
"#define __msa_fexdo_h __builtin_msa_fexdo_h\n"
"#define __msa_fexdo_w __builtin_msa_fexdo_w\n"
"#define __msa_ftq_h __builtin_msa_ftq_h\n"
"#define __msa_ftq_w __builtin_msa_ftq_w\n"
"#define __msa_fmin_w __builtin_msa_fmin_w\n"
"#define __msa_fmin_d __builtin_msa_fmin_d\n"
"#define __msa_fmin_a_w __builtin_msa_fmin_a_w\n"
"#define __msa_fmin_a_d __builtin_msa_fmin_a_d\n"
"#define __msa_fmax_w __builtin_msa_fmax_w\n"
"#define __msa_fmax_d __builtin_msa_fmax_d\n"
"#define __msa_fmax_a_w __builtin_msa_fmax_a_w\n"
"#define __msa_fmax_a_d __builtin_msa_fmax_a_d\n"
"#define __msa_mul_q_h __builtin_msa_mul_q_h\n"
"#define __msa_mul_q_w __builtin_msa_mul_q_w\n"
"#define __msa_mulr_q_h __builtin_msa_mulr_q_h\n"
"#define __msa_mulr_q_w __builtin_msa_mulr_q_w\n"
"#define __msa_madd_q_h __builtin_msa_madd_q_h\n"
"#define __msa_madd_q_w __builtin_msa_madd_q_w\n"
"#define __msa_maddr_q_h __builtin_msa_maddr_q_h\n"
"#define __msa_maddr_q_w __builtin_msa_maddr_q_w\n"
"#define __msa_msub_q_h __builtin_msa_msub_q_h\n"
"#define __msa_msub_q_w __builtin_msa_msub_q_w\n"
"#define __msa_msubr_q_h __builtin_msa_msubr_q_h\n"
"#define __msa_msubr_q_w __builtin_msa_msubr_q_w\n"
"#define __msa_fclass_w __builtin_msa_fclass_w\n"
"#define __msa_fclass_d __builtin_msa_fclass_d\n"
"#define __msa_fsqrt_w __builtin_msa_fsqrt_w\n"
"#define __msa_fsqrt_d __builtin_msa_fsqrt_d\n"
"#define __msa_frcp_w __builtin_msa_frcp_w\n"
"#define __msa_frcp_d __builtin_msa_frcp_d\n"
"#define __msa_frint_w __builtin_msa_frint_w\n"
"#define __msa_frint_d __builtin_msa_frint_d\n"
"#define __msa_frsqrt_w __builtin_msa_frsqrt_w\n"
"#define __msa_frsqrt_d __builtin_msa_frsqrt_d\n"
"#define __msa_flog2_w __builtin_msa_flog2_w\n"
"#define __msa_flog2_d __builtin_msa_flog2_d\n"
"#define __msa_fexupl_w __builtin_msa_fexupl_w\n"
"#define __msa_fexupl_d __builtin_msa_fexupl_d\n"
"#define __msa_fexupr_w __builtin_msa_fexupr_w\n"
"#define __msa_fexupr_d __builtin_msa_fexupr_d\n"
"#define __msa_ffql_w __builtin_msa_ffql_w\n"
"#define __msa_ffql_d __builtin_msa_ffql_d\n"
"#define __msa_ffqr_w __builtin_msa_ffqr_w\n"
"#define __msa_ffqr_d __builtin_msa_ffqr_d\n"
"#define __msa_ftint_s_w __builtin_msa_ftint_s_w\n"
"#define __msa_ftint_s_d __builtin_msa_ftint_s_d\n"
"#define __msa_ftint_u_w __builtin_msa_ftint_u_w\n"
"#define __msa_ftint_u_d __builtin_msa_ftint_u_d\n"
"#define __msa_ftrunc_s_w __builtin_msa_ftrunc_s_w\n"
"#define __msa_ftrunc_s_d __builtin_msa_ftrunc_s_d\n"
"#define __msa_ftrunc_u_w __builtin_msa_ftrunc_u_w\n"
"#define __msa_ftrunc_u_d __builtin_msa_ftrunc_u_d\n"
"#define __msa_ffint_s_w __builtin_msa_ffint_s_w\n"
"#define __msa_ffint_s_d __builtin_msa_ffint_s_d\n"
"#define __msa_ffint_u_w __builtin_msa_ffint_u_w\n"
"#define __msa_ffint_u_d __builtin_msa_ffint_u_d\n"
"#define __msa_cfcmsa __builtin_msa_cfcmsa\n"
"#define __msa_move_v __builtin_msa_move_v\n"
"#define __msa_cast_to_vector_float __builtin_msa_cast_to_vector_float\n"
"#define __msa_cast_to_vector_double __builtin_msa_cast_to_vector_double\n"
"#define __msa_cast_to_scalar_float __builtin_msa_cast_to_scalar_float\n"
"#define __msa_cast_to_scalar_double __builtin_msa_cast_to_scalar_double\n"
"#endif /* defined(__mips_msa) */\n"
"#endif /* _MSA_H */\n"
"" } , 
 { "/builtins/mwaitxintrin.h" , "/*===---- mwaitxintrin.h - MONITORX/MWAITX intrinsics ----------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __X86INTRIN_H\n"
"#error \"Never use <mwaitxintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __MWAITXINTRIN_H\n"
"#define __MWAITXINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"mwaitx\")))\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_monitorx(void const * __p, unsigned __extensions, unsigned __hints)\n"
"{\n"
"  __builtin_ia32_monitorx((void *)__p, __extensions, __hints);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_mwaitx(unsigned __extensions, unsigned __hints, unsigned __clock)\n"
"{\n"
"  __builtin_ia32_mwaitx(__extensions, __hints, __clock);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __MWAITXINTRIN_H */\n"
"" } , 
 { "/builtins/nmmintrin.h" , "/*===---- nmmintrin.h - SSE4 intrinsics ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __NMMINTRIN_H\n"
"#define __NMMINTRIN_H\n"
"\n"
"/* To match expectations of gcc we put the sse4.2 definitions into smmintrin.h,\n"
"   just include it now then.  */\n"
"#include <smmintrin.h>\n"
"#endif /* __NMMINTRIN_H */\n"
"" } , 
 { "/builtins/omp-tools.h" , "/*\n"
" * include/50/omp-tools.h.var\n"
" */\n"
"\n"
"//===----------------------------------------------------------------------===//\n"
"//\n"
"//                     The LLVM Compiler Infrastructure\n"
"//\n"
"// This file is dual licensed under the MIT and the University of Illinois Open\n"
"// Source Licenses. See LICENSE.txt for details.\n"
"//\n"
"//===----------------------------------------------------------------------===//\n"
"\n"
"#ifndef __OMPT__\n"
"#define __OMPT__\n"
"\n"
"/*****************************************************************************\n"
" * system include files\n"
" *****************************************************************************/\n"
"\n"
"#include <stdint.h>\n"
"#include <stddef.h>\n"
"\n"
"/*****************************************************************************\n"
" * iteration macros\n"
" *****************************************************************************/\n"
"\n"
"#define FOREACH_OMPT_INQUIRY_FN(macro)      \\\n"
"    macro (ompt_enumerate_states)           \\\n"
"    macro (ompt_enumerate_mutex_impls)      \\\n"
"                                            \\\n"
"    macro (ompt_set_callback)               \\\n"
"    macro (ompt_get_callback)               \\\n"
"                                            \\\n"
"    macro (ompt_get_state)                  \\\n"
"                                            \\\n"
"    macro (ompt_get_parallel_info)          \\\n"
"    macro (ompt_get_task_info)              \\\n"
"    macro (ompt_get_task_memory)            \\\n"
"    macro (ompt_get_thread_data)            \\\n"
"    macro (ompt_get_unique_id)              \\\n"
"    macro (ompt_finalize_tool)              \\\n"
"                                            \\\n"
"    macro(ompt_get_num_procs)               \\\n"
"    macro(ompt_get_num_places)              \\\n"
"    macro(ompt_get_place_proc_ids)          \\\n"
"    macro(ompt_get_place_num)               \\\n"
"    macro(ompt_get_partition_place_nums)    \\\n"
"    macro(ompt_get_proc_id)                 \\\n"
"                                            \\\n"
"    macro(ompt_get_target_info)             \\\n"
"    macro(ompt_get_num_devices)\n"
"\n"
"#define FOREACH_OMPT_STATE(macro)                                                                \\\n"
"                                                                                                \\\n"
"    /* first available state */                                                                 \\\n"
"    macro (ompt_state_undefined, 0x102)      /* undefined thread state */                        \\\n"
"                                                                                                \\\n"
"    /* work states (0..15) */                                                                   \\\n"
"    macro (ompt_state_work_serial, 0x000)    /* working outside parallel */                      \\\n"
"    macro (ompt_state_work_parallel, 0x001)  /* working within parallel */                       \\\n"
"    macro (ompt_state_work_reduction, 0x002) /* performing a reduction */                        \\\n"
"                                                                                                \\\n"
"    /* barrier wait states (16..31) */                                                          \\\n"
"    macro (ompt_state_wait_barrier, 0x010)   /* waiting at a barrier */                          \\\n"
"    macro (ompt_state_wait_barrier_implicit_parallel, 0x011)                                     \\\n"
"                                            /* implicit barrier at the end of parallel region */\\\n"
"    macro (ompt_state_wait_barrier_implicit_workshare, 0x012)                                    \\\n"
"                                            /* implicit barrier at the end of worksharing */    \\\n"
"    macro (ompt_state_wait_barrier_implicit, 0x013)  /* implicit barrier */                      \\\n"
"    macro (ompt_state_wait_barrier_explicit, 0x014)  /* explicit barrier */                      \\\n"
"                                                                                                \\\n"
"    /* task wait states (32..63) */                                                             \\\n"
"    macro (ompt_state_wait_taskwait, 0x020)  /* waiting at a taskwait */                         \\\n"
"    macro (ompt_state_wait_taskgroup, 0x021) /* waiting at a taskgroup */                        \\\n"
"                                                                                                \\\n"
"    /* mutex wait states (64..127) */                                                           \\\n"
"    macro (ompt_state_wait_mutex, 0x040)                                                         \\\n"
"    macro (ompt_state_wait_lock, 0x041)      /* waiting for lock */                              \\\n"
"    macro (ompt_state_wait_critical, 0x042)  /* waiting for critical */                          \\\n"
"    macro (ompt_state_wait_atomic, 0x043)    /* waiting for atomic */                            \\\n"
"    macro (ompt_state_wait_ordered, 0x044)   /* waiting for ordered */                           \\\n"
"                                                                                                \\\n"
"    /* target wait states (128..255) */                                                         \\\n"
"    macro (ompt_state_wait_target, 0x080)        /* waiting for target region */                 \\\n"
"    macro (ompt_state_wait_target_map, 0x081)    /* waiting for target data mapping operation */ \\\n"
"    macro (ompt_state_wait_target_update, 0x082) /* waiting for target update operation */       \\\n"
"                                                                                                \\\n"
"    /* misc (256..511) */                                                                       \\\n"
"    macro (ompt_state_idle, 0x100)           /* waiting for work */                              \\\n"
"    macro (ompt_state_overhead, 0x101)       /* overhead excluding wait states */                \\\n"
"                                                                                                \\\n"
"    /* implementation-specific states (512..) */\n"
"\n"
"\n"
"#define FOREACH_KMP_MUTEX_IMPL(macro)                                                \\\n"
"    macro (kmp_mutex_impl_none, 0)         /* unknown implementation */              \\\n"
"    macro (kmp_mutex_impl_spin, 1)         /* based on spin */                       \\\n"
"    macro (kmp_mutex_impl_queuing, 2)      /* based on some fair policy */           \\\n"
"    macro (kmp_mutex_impl_speculative, 3)  /* based on HW-supported speculation */\n"
"\n"
"#define FOREACH_OMPT_EVENT(macro)                                                                                        \\\n"
"                                                                                                                         \\\n"
"    /*--- Mandatory Events ---*/                                                                                         \\\n"
"    macro (ompt_callback_thread_begin,      ompt_callback_thread_begin_t,       1) /* thread begin                    */ \\\n"
"    macro (ompt_callback_thread_end,        ompt_callback_thread_end_t,         2) /* thread end                      */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_parallel_begin,    ompt_callback_parallel_begin_t,     3) /* parallel begin                  */ \\\n"
"    macro (ompt_callback_parallel_end,      ompt_callback_parallel_end_t,       4) /* parallel end                    */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_task_create,       ompt_callback_task_create_t,        5) /* task begin                      */ \\\n"
"    macro (ompt_callback_task_schedule,     ompt_callback_task_schedule_t,      6) /* task schedule                   */ \\\n"
"    macro (ompt_callback_implicit_task,     ompt_callback_implicit_task_t,      7) /* implicit task                   */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_target,            ompt_callback_target_t,             8) /* target                          */ \\\n"
"    macro (ompt_callback_target_data_op,    ompt_callback_target_data_op_t,     9) /* target data op                  */ \\\n"
"    macro (ompt_callback_target_submit,     ompt_callback_target_submit_t,     10) /* target  submit                  */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_control_tool,      ompt_callback_control_tool_t,      11) /* control tool                    */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_device_initialize, ompt_callback_device_initialize_t, 12) /* device initialize               */ \\\n"
"    macro (ompt_callback_device_finalize,   ompt_callback_device_finalize_t,   13) /* device finalize                 */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_device_load,       ompt_callback_device_load_t,       14) /* device load                     */ \\\n"
"    macro (ompt_callback_device_unload,     ompt_callback_device_unload_t,     15) /* device unload                   */ \\\n"
"                                                                                                                         \\\n"
"    /* Optional Events */                                                                                                \\\n"
"    macro (ompt_callback_sync_region_wait,  ompt_callback_sync_region_t,       16) /* sync region wait begin or end   */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_mutex_released,    ompt_callback_mutex_t,             17) /* mutex released                  */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_dependences,       ompt_callback_dependences_t,       18) /* report task dependences         */ \\\n"
"    macro (ompt_callback_task_dependence,   ompt_callback_task_dependence_t,   19) /* report task dependence          */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_work,              ompt_callback_work_t,              20) /* task at work begin or end       */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_master,            ompt_callback_master_t,            21) /* task at master begin or end     */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_target_map,        ompt_callback_target_map_t,        22) /* target map                      */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_sync_region,       ompt_callback_sync_region_t,       23) /* sync region begin or end        */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_lock_init,         ompt_callback_mutex_acquire_t,     24) /* lock init                       */ \\\n"
"    macro (ompt_callback_lock_destroy,      ompt_callback_mutex_t,             25) /* lock destroy                    */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_mutex_acquire,     ompt_callback_mutex_acquire_t,     26) /* mutex acquire                   */ \\\n"
"    macro (ompt_callback_mutex_acquired,    ompt_callback_mutex_t,             27) /* mutex acquired                  */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_nest_lock,         ompt_callback_nest_lock_t,         28) /* nest lock                       */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_flush,             ompt_callback_flush_t,             29) /* after executing flush           */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_cancel,            ompt_callback_cancel_t,            30) /* cancel innermost binding region */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_reduction,         ompt_callback_sync_region_t,       31) /* reduction                       */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_dispatch,          ompt_callback_dispatch_t,          32) /* dispatch of work                */\n"
"\n"
"/*****************************************************************************\n"
" * implementation specific types\n"
" *****************************************************************************/\n"
"\n"
"typedef enum kmp_mutex_impl_t {\n"
"#define kmp_mutex_impl_macro(impl, code) impl = code,\n"
"    FOREACH_KMP_MUTEX_IMPL(kmp_mutex_impl_macro)\n"
"#undef kmp_mutex_impl_macro\n"
"} kmp_mutex_impl_t;\n"
"\n"
"/*****************************************************************************\n"
" * definitions generated from spec\n"
" *****************************************************************************/\n"
"\n"
"typedef enum ompt_callbacks_t {\n"
"  ompt_callback_thread_begin             = 1,\n"
"  ompt_callback_thread_end               = 2,\n"
"  ompt_callback_parallel_begin           = 3,\n"
"  ompt_callback_parallel_end             = 4,\n"
"  ompt_callback_task_create              = 5,\n"
"  ompt_callback_task_schedule            = 6,\n"
"  ompt_callback_implicit_task            = 7,\n"
"  ompt_callback_target                   = 8,\n"
"  ompt_callback_target_data_op           = 9,\n"
"  ompt_callback_target_submit            = 10,\n"
"  ompt_callback_control_tool             = 11,\n"
"  ompt_callback_device_initialize        = 12,\n"
"  ompt_callback_device_finalize          = 13,\n"
"  ompt_callback_device_load              = 14,\n"
"  ompt_callback_device_unload            = 15,\n"
"  ompt_callback_sync_region_wait         = 16,\n"
"  ompt_callback_mutex_released           = 17,\n"
"  ompt_callback_dependences              = 18,\n"
"  ompt_callback_task_dependence          = 19,\n"
"  ompt_callback_work                     = 20,\n"
"  ompt_callback_master                   = 21,\n"
"  ompt_callback_target_map               = 22,\n"
"  ompt_callback_sync_region              = 23,\n"
"  ompt_callback_lock_init                = 24,\n"
"  ompt_callback_lock_destroy             = 25,\n"
"  ompt_callback_mutex_acquire            = 26,\n"
"  ompt_callback_mutex_acquired           = 27,\n"
"  ompt_callback_nest_lock                = 28,\n"
"  ompt_callback_flush                    = 29,\n"
"  ompt_callback_cancel                   = 30,\n"
"  ompt_callback_reduction                = 31,\n"
"  ompt_callback_dispatch                 = 32\n"
"} ompt_callbacks_t;\n"
"\n"
"typedef enum ompt_record_t {\n"
"  ompt_record_ompt               = 1,\n"
"  ompt_record_native             = 2,\n"
"  ompt_record_invalid            = 3\n"
"} ompt_record_t;\n"
"\n"
"typedef enum ompt_record_native_t {\n"
"  ompt_record_native_info  = 1,\n"
"  ompt_record_native_event = 2\n"
"} ompt_record_native_t;\n"
"\n"
"typedef enum ompt_set_result_t {\n"
"  ompt_set_error            = 0,\n"
"  ompt_set_never            = 1,\n"
"  ompt_set_impossible       = 2,\n"
"  ompt_set_sometimes        = 3,\n"
"  ompt_set_sometimes_paired = 4,\n"
"  ompt_set_always           = 5\n"
"} ompt_set_result_t;\n"
"\n"
"typedef uint64_t ompt_id_t;\n"
"\n"
"typedef uint64_t ompt_device_time_t;\n"
"\n"
"typedef uint64_t ompt_buffer_cursor_t;\n"
"\n"
"typedef enum ompt_thread_t {\n"
"  ompt_thread_initial                 = 1,\n"
"  ompt_thread_worker                  = 2,\n"
"  ompt_thread_other                   = 3,\n"
"  ompt_thread_unknown                 = 4\n"
"} ompt_thread_t;\n"
"\n"
"typedef enum ompt_scope_endpoint_t {\n"
"  ompt_scope_begin                    = 1,\n"
"  ompt_scope_end                      = 2\n"
"} ompt_scope_endpoint_t;\n"
"\n"
"typedef enum ompt_dispatch_t {\n"
"  ompt_dispatch_iteration             = 1,\n"
"  ompt_dispatch_section               = 2\n"
"} ompt_dispatch_t;\n"
"\n"
"typedef enum ompt_sync_region_t {\n"
"  ompt_sync_region_barrier                = 1,\n"
"  ompt_sync_region_barrier_implicit       = 2,\n"
"  ompt_sync_region_barrier_explicit       = 3,\n"
"  ompt_sync_region_barrier_implementation = 4,\n"
"  ompt_sync_region_taskwait               = 5,\n"
"  ompt_sync_region_taskgroup              = 6,\n"
"  ompt_sync_region_reduction              = 7\n"
"} ompt_sync_region_t;\n"
"\n"
"typedef enum ompt_target_data_op_t {\n"
"  ompt_target_data_alloc                = 1,\n"
"  ompt_target_data_transfer_to_device   = 2,\n"
"  ompt_target_data_transfer_from_device = 3,\n"
"  ompt_target_data_delete               = 4,\n"
"  ompt_target_data_associate            = 5,\n"
"  ompt_target_data_disassociate         = 6\n"
"} ompt_target_data_op_t;\n"
"\n"
"typedef enum ompt_work_t {\n"
"  ompt_work_loop               = 1,\n"
"  ompt_work_sections           = 2,\n"
"  ompt_work_single_executor    = 3,\n"
"  ompt_work_single_other       = 4,\n"
"  ompt_work_workshare          = 5,\n"
"  ompt_work_distribute         = 6,\n"
"  ompt_work_taskloop           = 7\n"
"} ompt_work_t;\n"
"\n"
"typedef enum ompt_mutex_t {\n"
"  ompt_mutex_lock                     = 1,\n"
"  ompt_mutex_test_lock                = 2,\n"
"  ompt_mutex_nest_lock                = 3,\n"
"  ompt_mutex_test_nest_lock           = 4,\n"
"  ompt_mutex_critical                 = 5,\n"
"  ompt_mutex_atomic                   = 6,\n"
"  ompt_mutex_ordered                  = 7\n"
"} ompt_mutex_t;\n"
"\n"
"typedef enum ompt_native_mon_flag_t {\n"
"  ompt_native_data_motion_explicit    = 0x01,\n"
"  ompt_native_data_motion_implicit    = 0x02,\n"
"  ompt_native_kernel_invocation       = 0x04,\n"
"  ompt_native_kernel_execution        = 0x08,\n"
"  ompt_native_driver                  = 0x10,\n"
"  ompt_native_runtime                 = 0x20,\n"
"  ompt_native_overhead                = 0x40,\n"
"  ompt_native_idleness                = 0x80\n"
"} ompt_native_mon_flag_t;\n"
"\n"
"typedef enum ompt_task_flag_t {\n"
"  ompt_task_initial                   = 0x00000001,\n"
"  ompt_task_implicit                  = 0x00000002,\n"
"  ompt_task_explicit                  = 0x00000004,\n"
"  ompt_task_target                    = 0x00000008,\n"
"  ompt_task_undeferred                = 0x08000000,\n"
"  ompt_task_untied                    = 0x10000000,\n"
"  ompt_task_final                     = 0x20000000,\n"
"  ompt_task_mergeable                 = 0x40000000,\n"
"  ompt_task_merged                    = 0x80000000\n"
"} ompt_task_flag_t;\n"
"\n"
"typedef enum ompt_task_status_t {\n"
"  ompt_task_complete      = 1,\n"
"  ompt_task_yield         = 2,\n"
"  ompt_task_cancel        = 3,\n"
"  ompt_task_detach        = 4,\n"
"  ompt_task_early_fulfill = 5,\n"
"  ompt_task_late_fulfill  = 6,\n"
"  ompt_task_switch        = 7\n"
"} ompt_task_status_t;\n"
"\n"
"typedef enum ompt_target_t {\n"
"  ompt_target                         = 1,\n"
"  ompt_target_enter_data              = 2,\n"
"  ompt_target_exit_data               = 3,\n"
"  ompt_target_update                  = 4\n"
"} ompt_target_t;\n"
"\n"
"typedef enum ompt_parallel_flag_t {\n"
"  ompt_parallel_invoker_program = 0x00000001,\n"
"  ompt_parallel_invoker_runtime = 0x00000002,\n"
"  ompt_parallel_league          = 0x40000000,\n"
"  ompt_parallel_team            = 0x80000000\n"
"} ompt_parallel_flag_t;\n"
"\n"
"typedef enum ompt_target_map_flag_t {\n"
"  ompt_target_map_flag_to             = 0x01,\n"
"  ompt_target_map_flag_from           = 0x02,\n"
"  ompt_target_map_flag_alloc          = 0x04,\n"
"  ompt_target_map_flag_release        = 0x08,\n"
"  ompt_target_map_flag_delete         = 0x10,\n"
"  ompt_target_map_flag_implicit       = 0x20\n"
"} ompt_target_map_flag_t;\n"
"\n"
"typedef enum ompt_dependence_type_t {\n"
"  ompt_dependence_type_in              = 1,\n"
"  ompt_dependence_type_out             = 2,\n"
"  ompt_dependence_type_inout           = 3,\n"
"  ompt_dependence_type_mutexinoutset   = 4,\n"
"  ompt_dependence_type_source          = 5,\n"
"  ompt_dependence_type_sink            = 6\n"
"} ompt_dependence_type_t;\n"
"\n"
"typedef enum ompt_cancel_flag_t {\n"
"  ompt_cancel_parallel       = 0x01,\n"
"  ompt_cancel_sections       = 0x02,\n"
"  ompt_cancel_loop           = 0x04,\n"
"  ompt_cancel_taskgroup      = 0x08,\n"
"  ompt_cancel_activated      = 0x10,\n"
"  ompt_cancel_detected       = 0x20,\n"
"  ompt_cancel_discarded_task = 0x40\n"
"} ompt_cancel_flag_t;\n"
"\n"
"typedef uint64_t ompt_hwid_t;\n"
"\n"
"typedef uint64_t ompt_wait_id_t;\n"
"\n"
"typedef enum ompt_frame_flag_t {\n"
"  ompt_frame_runtime        = 0x00,\n"
"  ompt_frame_application    = 0x01,\n"
"  ompt_frame_cfa            = 0x10,\n"
"  ompt_frame_framepointer   = 0x20,\n"
"  ompt_frame_stackaddress   = 0x30\n"
"} ompt_frame_flag_t; \n"
"\n"
"typedef enum ompt_state_t {\n"
"  ompt_state_work_serial                      = 0x000,\n"
"  ompt_state_work_parallel                    = 0x001,\n"
"  ompt_state_work_reduction                   = 0x002,\n"
"\n"
"  ompt_state_wait_barrier                     = 0x010,\n"
"  ompt_state_wait_barrier_implicit_parallel   = 0x011,\n"
"  ompt_state_wait_barrier_implicit_workshare  = 0x012,\n"
"  ompt_state_wait_barrier_implicit            = 0x013,\n"
"  ompt_state_wait_barrier_explicit            = 0x014,\n"
"\n"
"  ompt_state_wait_taskwait                    = 0x020,\n"
"  ompt_state_wait_taskgroup                   = 0x021,\n"
"\n"
"  ompt_state_wait_mutex                       = 0x040,\n"
"  ompt_state_wait_lock                        = 0x041,\n"
"  ompt_state_wait_critical                    = 0x042,\n"
"  ompt_state_wait_atomic                      = 0x043,\n"
"  ompt_state_wait_ordered                     = 0x044,\n"
"\n"
"  ompt_state_wait_target                      = 0x080,\n"
"  ompt_state_wait_target_map                  = 0x081,\n"
"  ompt_state_wait_target_update               = 0x082,\n"
"\n"
"  ompt_state_idle                             = 0x100,\n"
"  ompt_state_overhead                         = 0x101,\n"
"  ompt_state_undefined                        = 0x102\n"
"} ompt_state_t;\n"
"\n"
"typedef uint64_t (*ompt_get_unique_id_t) (void);\n"
"\n"
"typedef uint64_t ompd_size_t;\n"
"\n"
"typedef uint64_t ompd_wait_id_t;\n"
"\n"
"typedef uint64_t ompd_addr_t;\n"
"typedef int64_t  ompd_word_t;\n"
"typedef uint64_t ompd_seg_t;\n"
"\n"
"typedef uint64_t ompd_device_t;\n"
"\n"
"typedef uint64_t ompd_thread_id_t;\n"
"\n"
"typedef enum ompd_scope_t {\n"
"  ompd_scope_global = 1,\n"
"  ompd_scope_address_space = 2,\n"
"  ompd_scope_thread = 3,\n"
"  ompd_scope_parallel = 4,\n"
"  ompd_scope_implicit_task = 5,\n"
"  ompd_scope_task = 6\n"
"} ompd_scope_t;\n"
"\n"
"typedef uint64_t ompd_icv_id_t;\n"
"\n"
"typedef enum ompd_rc_t {\n"
"  ompd_rc_ok = 0,\n"
"  ompd_rc_unavailable = 1,\n"
"  ompd_rc_stale_handle = 2,\n"
"  ompd_rc_bad_input = 3,\n"
"  ompd_rc_error = 4,\n"
"  ompd_rc_unsupported = 5,\n"
"  ompd_rc_needs_state_tracking = 6,\n"
"  ompd_rc_incompatible = 7,\n"
"  ompd_rc_device_read_error = 8,\n"
"  ompd_rc_device_write_error = 9,\n"
"  ompd_rc_nomem = 10,\n"
"} ompd_rc_t;\n"
"\n"
"typedef void (*ompt_interface_fn_t) (void);\n"
"\n"
"typedef ompt_interface_fn_t (*ompt_function_lookup_t) (\n"
"  const char *interface_function_name\n"
");\n"
"\n"
"typedef union ompt_data_t {\n"
"  uint64_t value;\n"
"  void *ptr;\n"
"} ompt_data_t;\n"
"\n"
"typedef struct ompt_frame_t {\n"
"  ompt_data_t exit_frame;\n"
"  ompt_data_t enter_frame;\n"
"  int exit_frame_flags;\n"
"  int enter_frame_flags;\n"
"} ompt_frame_t;\n"
"\n"
"typedef void (*ompt_callback_t) (void);\n"
"\n"
"typedef void ompt_device_t;\n"
"\n"
"typedef void ompt_buffer_t;\n"
"\n"
"typedef void (*ompt_callback_buffer_request_t) (\n"
"  int device_num,\n"
"  ompt_buffer_t **buffer,\n"
"  size_t *bytes\n"
");\n"
"\n"
"typedef void (*ompt_callback_buffer_complete_t) (\n"
"  int device_num,\n"
"  ompt_buffer_t *buffer,\n"
"  size_t bytes,\n"
"  ompt_buffer_cursor_t begin,\n"
"  int buffer_owned\n"
");\n"
"\n"
"typedef void (*ompt_finalize_t) (\n"
"  ompt_data_t *tool_data\n"
");\n"
"\n"
"typedef int (*ompt_initialize_t) (\n"
"  ompt_function_lookup_t lookup,\n"
"  int initial_device_num,\n"
"  ompt_data_t *tool_data\n"
");\n"
"\n"
"typedef struct ompt_start_tool_result_t {\n"
"  ompt_initialize_t initialize;\n"
"  ompt_finalize_t finalize;\n"
"  ompt_data_t tool_data;\n"
"} ompt_start_tool_result_t;\n"
"\n"
"typedef struct ompt_record_abstract_t {\n"
"  ompt_record_native_t rclass;\n"
"  const char *type;\n"
"  ompt_device_time_t start_time;\n"
"  ompt_device_time_t end_time;\n"
"  ompt_hwid_t hwid;\n"
"} ompt_record_abstract_t;\n"
"\n"
"typedef struct ompt_dependence_t {\n"
"  ompt_data_t variable;\n"
"  ompt_dependence_type_t dependence_type;\n"
"} ompt_dependence_t;\n"
"\n"
"typedef int (*ompt_enumerate_states_t) (\n"
"  int current_state,\n"
"  int *next_state,\n"
"  const char **next_state_name\n"
");\n"
"\n"
"typedef int (*ompt_enumerate_mutex_impls_t) (\n"
"  int current_impl,\n"
"  int *next_impl,\n"
"  const char **next_impl_name\n"
");\n"
"\n"
"typedef ompt_set_result_t (*ompt_set_callback_t) (\n"
"  ompt_callbacks_t event,\n"
"  ompt_callback_t callback\n"
");\n"
"\n"
"typedef int (*ompt_get_callback_t) (\n"
"  ompt_callbacks_t event,\n"
"  ompt_callback_t *callback\n"
");\n"
"\n"
"typedef ompt_data_t *(*ompt_get_thread_data_t) (void);\n"
"\n"
"typedef int (*ompt_get_num_procs_t) (void);\n"
"\n"
"typedef int (*ompt_get_num_places_t) (void);\n"
"\n"
"typedef int (*ompt_get_place_proc_ids_t) (\n"
"  int place_num,\n"
"  int ids_size,\n"
"  int *ids\n"
");\n"
"\n"
"typedef int (*ompt_get_place_num_t) (void);\n"
"\n"
"typedef int (*ompt_get_partition_place_nums_t) (\n"
"  int place_nums_size,\n"
"  int *place_nums\n"
");\n"
"\n"
"typedef int (*ompt_get_proc_id_t) (void);\n"
"\n"
"typedef int (*ompt_get_state_t) (\n"
"  ompt_wait_id_t *wait_id\n"
");\n"
"\n"
"typedef int (*ompt_get_parallel_info_t) (\n"
"  int ancestor_level,\n"
"  ompt_data_t **parallel_data,\n"
"  int *team_size\n"
");\n"
"\n"
"typedef int (*ompt_get_task_info_t) (\n"
"  int ancestor_level,\n"
"  int *flags,\n"
"  ompt_data_t **task_data,\n"
"  ompt_frame_t **task_frame,\n"
"  ompt_data_t **parallel_data,\n"
"  int *thread_num\n"
");\n"
"\n"
"typedef int (*ompt_get_task_memory_t)(\n"
"  void **addr,\n"
"  size_t *size,\n"
"  int block\n"
");\n"
"\n"
"typedef int (*ompt_get_target_info_t) (\n"
"  uint64_t *device_num,\n"
"  ompt_id_t *target_id,\n"
"  ompt_id_t *host_op_id\n"
");\n"
"\n"
"typedef int (*ompt_get_num_devices_t) (void);\n"
"\n"
"typedef void (*ompt_finalize_tool_t) (void);\n"
"\n"
"typedef int (*ompt_get_device_num_procs_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef ompt_device_time_t (*ompt_get_device_time_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef double (*ompt_translate_time_t) (\n"
"  ompt_device_t *device,\n"
"  ompt_device_time_t time\n"
");\n"
"\n"
"typedef ompt_set_result_t (*ompt_set_trace_ompt_t) (\n"
"  ompt_device_t *device,\n"
"  unsigned int enable,\n"
"  unsigned int etype\n"
");\n"
"\n"
"typedef ompt_set_result_t (*ompt_set_trace_native_t) (\n"
"  ompt_device_t *device,\n"
"  int enable,\n"
"  int flags\n"
");\n"
"\n"
"typedef int (*ompt_start_trace_t) (\n"
"  ompt_device_t *device,\n"
"  ompt_callback_buffer_request_t request,\n"
"  ompt_callback_buffer_complete_t complete\n"
");\n"
"\n"
"typedef int (*ompt_pause_trace_t) (\n"
"  ompt_device_t *device,\n"
"  int begin_pause\n"
");\n"
"\n"
"typedef int (*ompt_flush_trace_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef int (*ompt_stop_trace_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef int (*ompt_advance_buffer_cursor_t) (\n"
"  ompt_device_t *device,\n"
"  ompt_buffer_t *buffer,\n"
"  size_t size,\n"
"  ompt_buffer_cursor_t current,\n"
"  ompt_buffer_cursor_t *next\n"
");\n"
"\n"
"typedef ompt_record_t (*ompt_get_record_type_t) (\n"
"  ompt_buffer_t *buffer,\n"
"  ompt_buffer_cursor_t current\n"
");\n"
"\n"
"typedef void *(*ompt_get_record_native_t) (\n"
"  ompt_buffer_t *buffer,\n"
"  ompt_buffer_cursor_t current,\n"
"  ompt_id_t *host_op_id\n"
");\n"
"\n"
"typedef ompt_record_abstract_t *\n"
"(*ompt_get_record_abstract_t) (\n"
"  void *native_record\n"
");\n"
"\n"
"typedef void (*ompt_callback_thread_begin_t) (\n"
"  ompt_thread_t thread_type,\n"
"  ompt_data_t *thread_data\n"
");\n"
"\n"
"typedef struct ompt_record_thread_begin_t {\n"
"  ompt_thread_t thread_type;\n"
"} ompt_record_thread_begin_t;\n"
"\n"
"typedef void (*ompt_callback_thread_end_t) (\n"
"  ompt_data_t *thread_data\n"
");\n"
"\n"
"typedef void (*ompt_callback_parallel_begin_t) (\n"
"  ompt_data_t *encountering_task_data,\n"
"  const ompt_frame_t *encountering_task_frame,\n"
"  ompt_data_t *parallel_data,\n"
"  unsigned int requested_parallelism,\n"
"  int flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_parallel_begin_t {\n"
"  ompt_id_t encountering_task_id;\n"
"  ompt_id_t parallel_id;\n"
"  unsigned int requested_parallelism;\n"
"  int flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_parallel_begin_t;\n"
"\n"
"typedef void (*ompt_callback_parallel_end_t) (\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *encountering_task_data,\n"
"  int flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_parallel_end_t {\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t encountering_task_id;\n"
"  int flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_parallel_end_t;\n"
"\n"
"typedef void (*ompt_callback_work_t) (\n"
"  ompt_work_t wstype,\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  uint64_t count,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_work_t {\n"
"  ompt_work_t wstype;\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  uint64_t count;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_work_t;\n"
"\n"
"typedef void (*ompt_callback_dispatch_t) (\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  ompt_dispatch_t kind,\n"
"  ompt_data_t instance \n"
");\n"
"\n"
"typedef struct ompt_record_dispatch_t {\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  ompt_dispatch_t kind;\n"
"  ompt_data_t instance; \n"
"} ompt_record_dispatch_t;\n"
"\n"
"typedef void (*ompt_callback_task_create_t) (\n"
"  ompt_data_t *encountering_task_data,\n"
"  const ompt_frame_t *encountering_task_frame,\n"
"  ompt_data_t *new_task_data,\n"
"  int flags,\n"
"  int has_dependences,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_task_create_t {\n"
"  ompt_id_t encountering_task_id;\n"
"  ompt_id_t new_task_id;\n"
"  int flags;\n"
"  int has_dependences;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_task_create_t;\n"
"\n"
"typedef void (*ompt_callback_dependences_t) (\n"
"  ompt_data_t *task_data,\n"
"  const ompt_dependence_t *deps,\n"
"  int ndeps\n"
");\n"
"\n"
"typedef struct ompt_record_dependences_t {\n"
"  ompt_id_t task_id;\n"
"  ompt_dependence_t dep;\n"
"  int ndeps;\n"
"} ompt_record_dependences_t;\n"
"\n"
"typedef void (*ompt_callback_task_dependence_t) (\n"
"  ompt_data_t *src_task_data,\n"
"  ompt_data_t *sink_task_data\n"
");\n"
"\n"
"typedef struct ompt_record_task_dependence_t {\n"
"  ompt_id_t src_task_id;\n"
"  ompt_id_t sink_task_id;\n"
"} ompt_record_task_dependence_t;\n"
"\n"
"typedef void (*ompt_callback_task_schedule_t) (\n"
"  ompt_data_t *prior_task_data,\n"
"  ompt_task_status_t prior_task_status,\n"
"  ompt_data_t *next_task_data\n"
");\n"
"\n"
"typedef struct ompt_record_task_schedule_t {\n"
"  ompt_id_t prior_task_id;\n"
"  ompt_task_status_t prior_task_status;\n"
"  ompt_id_t next_task_id;\n"
"} ompt_record_task_schedule_t;\n"
"\n"
"typedef void (*ompt_callback_implicit_task_t) (\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  unsigned int actual_parallelism,\n"
"  unsigned int index,\n"
"  int flags\n"
");\n"
"\n"
"typedef struct ompt_record_implicit_task_t {\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  unsigned int actual_parallelism;\n"
"  unsigned int index;\n"
"  int flags;\n"
"} ompt_record_implicit_task_t;\n"
"\n"
"typedef void (*ompt_callback_master_t) (\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_master_t {\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_master_t;\n"
"\n"
"typedef void (*ompt_callback_sync_region_t) (\n"
"  ompt_sync_region_t kind,\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_sync_region_t {\n"
"  ompt_sync_region_t kind;\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_sync_region_t;\n"
"\n"
"typedef void (*ompt_callback_mutex_acquire_t) (\n"
"  ompt_mutex_t kind,\n"
"  unsigned int hint,\n"
"  unsigned int impl,\n"
"  ompt_wait_id_t wait_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_mutex_acquire_t {\n"
"  ompt_mutex_t kind;\n"
"  unsigned int hint;\n"
"  unsigned int impl;\n"
"  ompt_wait_id_t wait_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_mutex_acquire_t;\n"
"\n"
"typedef void (*ompt_callback_mutex_t) (\n"
"  ompt_mutex_t kind,\n"
"  ompt_wait_id_t wait_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_mutex_t {\n"
"  ompt_mutex_t kind;\n"
"  ompt_wait_id_t wait_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_mutex_t;\n"
"\n"
"typedef void (*ompt_callback_nest_lock_t) (\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_wait_id_t wait_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_nest_lock_t {\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_wait_id_t wait_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_nest_lock_t;\n"
"\n"
"typedef void (*ompt_callback_flush_t) (\n"
"  ompt_data_t *thread_data,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_flush_t {\n"
"  const void *codeptr_ra;\n"
"} ompt_record_flush_t;\n"
"\n"
"typedef void (*ompt_callback_cancel_t) (\n"
"  ompt_data_t *task_data,\n"
"  int flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_cancel_t {\n"
"  ompt_id_t task_id;\n"
"  int flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_cancel_t;\n"
"\n"
"typedef void (*ompt_callback_device_initialize_t) (\n"
"  int device_num,\n"
"  const char *type,\n"
"  ompt_device_t *device,\n"
"  ompt_function_lookup_t lookup,\n"
"  const char *documentation\n"
");\n"
"\n"
"typedef void (*ompt_callback_device_finalize_t) (\n"
"  int device_num\n"
");\n"
"\n"
"typedef void (*ompt_callback_device_load_t) (\n"
"  int device_num,\n"
"  const char *filename,\n"
"  int64_t offset_in_file,\n"
"  void *vma_in_file,\n"
"  size_t bytes,\n"
"  void *host_addr,\n"
"  void *device_addr,\n"
"  uint64_t module_id\n"
");\n"
"\n"
"typedef void (*ompt_callback_device_unload_t) (\n"
"  int device_num,\n"
"  uint64_t module_id\n"
");\n"
"\n"
"typedef void (*ompt_callback_target_data_op_t) (\n"
"  ompt_id_t target_id,\n"
"  ompt_id_t host_op_id,\n"
"  ompt_target_data_op_t optype,\n"
"  void *src_addr,\n"
"  int src_device_num,\n"
"  void *dest_addr,\n"
"  int dest_device_num,\n"
"  size_t bytes,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_target_data_op_t {\n"
"  ompt_id_t host_op_id;\n"
"  ompt_target_data_op_t optype;\n"
"  void *src_addr;\n"
"  int src_device_num;\n"
"  void *dest_addr;\n"
"  int dest_device_num;\n"
"  size_t bytes;\n"
"  ompt_device_time_t end_time;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_target_data_op_t;\n"
"\n"
"typedef void (*ompt_callback_target_t) (\n"
"  ompt_target_t kind,\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  int device_num,\n"
"  ompt_data_t *task_data,\n"
"  ompt_id_t target_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_target_t {\n"
"  ompt_target_t kind;\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  int device_num;\n"
"  ompt_id_t task_id;\n"
"  ompt_id_t target_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_target_t;\n"
"\n"
"typedef void (*ompt_callback_target_map_t) (\n"
"  ompt_id_t target_id,\n"
"  unsigned int nitems,\n"
"  void **host_addr,\n"
"  void **device_addr,\n"
"  size_t *bytes,\n"
"  unsigned int *mapping_flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_target_map_t {\n"
"  ompt_id_t target_id;\n"
"  unsigned int nitems;\n"
"  void **host_addr;\n"
"  void **device_addr;\n"
"  size_t *bytes;\n"
"  unsigned int *mapping_flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_target_map_t;\n"
"\n"
"typedef void (*ompt_callback_target_submit_t) (\n"
"  ompt_id_t target_id,\n"
"  ompt_id_t host_op_id,\n"
"  unsigned int requested_num_teams\n"
");\n"
"\n"
"typedef struct ompt_record_target_kernel_t {\n"
"  ompt_id_t host_op_id;\n"
"  unsigned int requested_num_teams;\n"
"  unsigned int granted_num_teams;\n"
"  ompt_device_time_t end_time;\n"
"} ompt_record_target_kernel_t;\n"
"\n"
"typedef int (*ompt_callback_control_tool_t) (\n"
"  uint64_t command,\n"
"  uint64_t modifier,\n"
"  void *arg,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_control_tool_t {\n"
"  uint64_t command;\n"
"  uint64_t modifier;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_control_tool_t;\n"
"\n"
"typedef struct ompd_address_t {\n"
"  ompd_seg_t segment;\n"
"  ompd_addr_t address;\n"
"} ompd_address_t;\n"
"\n"
"typedef struct ompd_frame_info_t {\n"
"  ompd_address_t frame_address;\n"
"  ompd_word_t frame_flag;\n"
"} ompd_frame_info_t;\n"
"\n"
"typedef struct _ompd_aspace_handle ompd_address_space_handle_t;\n"
"typedef struct _ompd_thread_handle ompd_thread_handle_t;\n"
"typedef struct _ompd_parallel_handle ompd_parallel_handle_t;\n"
"typedef struct _ompd_task_handle ompd_task_handle_t;\n"
"\n"
"typedef struct _ompd_aspace_cont ompd_address_space_context_t;\n"
"typedef struct _ompd_thread_cont ompd_thread_context_t;\n"
"\n"
"typedef struct ompd_device_type_sizes_t {\n"
"  uint8_t sizeof_char;\n"
"  uint8_t sizeof_short;\n"
"  uint8_t sizeof_int;\n"
"  uint8_t sizeof_long;\n"
"  uint8_t sizeof_long_long;\n"
"  uint8_t sizeof_pointer;\n"
"} ompd_device_type_sizes_t;\n"
"\n"
"typedef struct ompt_record_ompt_t {\n"
"  ompt_callbacks_t type;\n"
"  ompt_device_time_t time;\n"
"  ompt_id_t thread_id;\n"
"  ompt_id_t target_id;\n"
"  union {\n"
"    ompt_record_thread_begin_t thread_begin;\n"
"    ompt_record_parallel_begin_t parallel_begin;\n"
"    ompt_record_parallel_end_t parallel_end;\n"
"    ompt_record_work_t work;\n"
"    ompt_record_dispatch_t dispatch;\n"
"    ompt_record_task_create_t task_create;\n"
"    ompt_record_dependences_t dependences;\n"
"    ompt_record_task_dependence_t task_dependence;\n"
"    ompt_record_task_schedule_t task_schedule;\n"
"    ompt_record_implicit_task_t implicit_task;\n"
"    ompt_record_master_t master;\n"
"    ompt_record_sync_region_t sync_region;\n"
"    ompt_record_mutex_acquire_t mutex_acquire;\n"
"    ompt_record_mutex_t mutex;\n"
"    ompt_record_nest_lock_t nest_lock;\n"
"    ompt_record_flush_t flush;\n"
"    ompt_record_cancel_t cancel;\n"
"    ompt_record_target_t target;\n"
"    ompt_record_target_data_op_t target_data_op;\n"
"    ompt_record_target_map_t target_map;\n"
"    ompt_record_target_kernel_t target_kernel;\n"
"    ompt_record_control_tool_t control_tool;\n"
"  } record;\n"
"} ompt_record_ompt_t;\n"
"\n"
"typedef ompt_record_ompt_t *(*ompt_get_record_ompt_t) (\n"
"  ompt_buffer_t *buffer,\n"
"  ompt_buffer_cursor_t current\n"
");\n"
"\n"
"#define ompt_id_none 0\n"
"#define ompt_data_none {0}\n"
"#define ompt_time_none 0\n"
"#define ompt_hwid_none 0\n"
"#define ompt_addr_none ~0\n"
"#define ompt_mutex_impl_none 0\n"
"#define ompt_wait_id_none 0\n"
"\n"
"#define ompd_segment_none 0\n"
"\n"
"#endif /* __OMPT__ */\n"
"" } , 
 { "/builtins/omp.h" , "/*\n"
" * include/50/omp.h.var\n"
" */\n"
"\n"
"\n"
"//===----------------------------------------------------------------------===//\n"
"//\n"
"//                     The LLVM Compiler Infrastructure\n"
"//\n"
"// This file is dual licensed under the MIT and the University of Illinois Open\n"
"// Source Licenses. See LICENSE.txt for details.\n"
"//\n"
"//===----------------------------------------------------------------------===//\n"
"\n"
"\n"
"#ifndef __OMP_H\n"
"#   define __OMP_H\n"
"\n"
"#   define KMP_VERSION_MAJOR    5\n"
"#   define KMP_VERSION_MINOR    0\n"
"#   define KMP_VERSION_BUILD    20140926\n"
"#   define KMP_BUILD_DATE       \"No_Timestamp\"\n"
"\n"
"#   ifdef __cplusplus\n"
"    extern \"C\" {\n"
"#   endif\n"
"\n"
"#   define omp_set_affinity_format   ompc_set_affinity_format\n"
"#   define omp_get_affinity_format   ompc_get_affinity_format\n"
"#   define omp_display_affinity      ompc_display_affinity\n"
"#   define omp_capture_affinity      ompc_capture_affinity\n"
"\n"
"#   if defined(_WIN32)\n"
"#       define __KAI_KMPC_CONVENTION __cdecl\n"
"#       ifndef __KMP_IMP\n"
"#           define __KMP_IMP __declspec(dllimport)\n"
"#       endif\n"
"#   else\n"
"#       define __KAI_KMPC_CONVENTION\n"
"#       ifndef __KMP_IMP\n"
"#           define __KMP_IMP\n"
"#       endif\n"
"#   endif\n"
"\n"
"    /* schedule kind constants */\n"
"    typedef enum omp_sched_t {\n"
"	omp_sched_static  = 1,\n"
"	omp_sched_dynamic = 2,\n"
"	omp_sched_guided  = 3,\n"
"	omp_sched_auto    = 4\n"
"    } omp_sched_t;\n"
"\n"
"    /* set API functions */\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_set_num_threads (int);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_set_dynamic     (int);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_set_nested      (int);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_set_max_active_levels (int);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_set_schedule          (omp_sched_t, int);\n"
"\n"
"    /* query API functions */\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_num_threads  (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_dynamic      (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_nested       (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_max_threads  (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_thread_num   (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_num_procs    (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_in_parallel      (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_in_final         (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_active_level        (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_level               (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_ancestor_thread_num (int);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_team_size           (int);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_thread_limit        (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_max_active_levels   (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_get_schedule            (omp_sched_t *, int *);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_get_max_task_priority   (void);\n"
"\n"
"    /* lock API functions */\n"
"    typedef struct omp_lock_t {\n"
"        void * _lk;\n"
"    } omp_lock_t;\n"
"\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_init_lock    (omp_lock_t *);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_set_lock     (omp_lock_t *);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_unset_lock   (omp_lock_t *);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_destroy_lock (omp_lock_t *);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_test_lock    (omp_lock_t *);\n"
"\n"
"    /* nested lock API functions */\n"
"    typedef struct omp_nest_lock_t {\n"
"        void * _lk;\n"
"    } omp_nest_lock_t;\n"
"\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_init_nest_lock    (omp_nest_lock_t *);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_set_nest_lock     (omp_nest_lock_t *);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_unset_nest_lock   (omp_nest_lock_t *);\n"
"    extern void   __KAI_KMPC_CONVENTION  omp_destroy_nest_lock (omp_nest_lock_t *);\n"
"    extern int    __KAI_KMPC_CONVENTION  omp_test_nest_lock    (omp_nest_lock_t *);\n"
"\n"
"    /* OpenMP 5.0  Synchronization hints*/\n"
"    typedef enum omp_sync_hint_t {\n"
"        omp_sync_hint_none           = 0,\n"
"        omp_lock_hint_none           = omp_sync_hint_none,\n"
"        omp_sync_hint_uncontended    = 1,\n"
"        omp_lock_hint_uncontended    = omp_sync_hint_uncontended,\n"
"        omp_sync_hint_contended      = (1<<1),\n"
"        omp_lock_hint_contended      = omp_sync_hint_contended,\n"
"        omp_sync_hint_nonspeculative = (1<<2),\n"
"        omp_lock_hint_nonspeculative = omp_sync_hint_nonspeculative,\n"
"        omp_sync_hint_speculative    = (1<<3),\n"
"        omp_lock_hint_speculative    = omp_sync_hint_speculative,\n"
"        kmp_lock_hint_hle            = (1<<16),\n"
"        kmp_lock_hint_rtm            = (1<<17),\n"
"        kmp_lock_hint_adaptive       = (1<<18)\n"
"    } omp_sync_hint_t;\n"
"\n"
"    /* lock hint type for dynamic user lock */\n"
"    typedef omp_sync_hint_t omp_lock_hint_t;\n"
"\n"
"    /* hinted lock initializers */\n"
"    extern void __KAI_KMPC_CONVENTION omp_init_lock_with_hint(omp_lock_t *, omp_lock_hint_t);\n"
"    extern void __KAI_KMPC_CONVENTION omp_init_nest_lock_with_hint(omp_nest_lock_t *, omp_lock_hint_t);\n"
"\n"
"    /* time API functions */\n"
"    extern double __KAI_KMPC_CONVENTION  omp_get_wtime (void);\n"
"    extern double __KAI_KMPC_CONVENTION  omp_get_wtick (void);\n"
"\n"
"    /* OpenMP 4.0 */\n"
"    extern int  __KAI_KMPC_CONVENTION  omp_get_default_device (void);\n"
"    extern void __KAI_KMPC_CONVENTION  omp_set_default_device (int);\n"
"    extern int  __KAI_KMPC_CONVENTION  omp_is_initial_device (void);\n"
"    extern int  __KAI_KMPC_CONVENTION  omp_get_num_devices (void);\n"
"    extern int  __KAI_KMPC_CONVENTION  omp_get_num_teams (void);\n"
"    extern int  __KAI_KMPC_CONVENTION  omp_get_team_num (void);\n"
"    extern int  __KAI_KMPC_CONVENTION  omp_get_cancellation (void);\n"
"\n"
"#   include <stdlib.h>\n"
"    /* OpenMP 4.5 */\n"
"    extern int   __KAI_KMPC_CONVENTION  omp_get_initial_device (void);\n"
"    extern void* __KAI_KMPC_CONVENTION  omp_target_alloc(size_t, int);\n"
"    extern void  __KAI_KMPC_CONVENTION  omp_target_free(void *, int);\n"
"    extern int   __KAI_KMPC_CONVENTION  omp_target_is_present(void *, int);\n"
"    extern int   __KAI_KMPC_CONVENTION  omp_target_memcpy(void *, void *, size_t, size_t, size_t, int, int);\n"
"    extern int   __KAI_KMPC_CONVENTION  omp_target_memcpy_rect(void *, void *, size_t, int, const size_t *,\n"
"                                            const size_t *, const size_t *, const size_t *, const size_t *, int, int);\n"
"    extern int   __KAI_KMPC_CONVENTION  omp_target_associate_ptr(void *, void *, size_t, size_t, int);\n"
"    extern int   __KAI_KMPC_CONVENTION  omp_target_disassociate_ptr(void *, int);\n"
"\n"
"    /* OpenMP 5.0 */\n"
"    extern int   __KAI_KMPC_CONVENTION  omp_get_device_num (void);\n"
"\n"
"    /* kmp API functions */\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_get_stacksize          (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_stacksize          (int);\n"
"    extern size_t __KAI_KMPC_CONVENTION  kmp_get_stacksize_s        (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_stacksize_s        (size_t);\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_get_blocktime          (void);\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_get_library            (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_blocktime          (int);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_library            (int);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_library_serial     (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_library_turnaround (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_library_throughput (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_defaults           (char const *);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_disp_num_buffers   (int);\n"
"\n"
"    /* Intel affinity API */\n"
"    typedef void * kmp_affinity_mask_t;\n"
"\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_set_affinity             (kmp_affinity_mask_t *);\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_get_affinity             (kmp_affinity_mask_t *);\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_get_affinity_max_proc    (void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_create_affinity_mask     (kmp_affinity_mask_t *);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_destroy_affinity_mask    (kmp_affinity_mask_t *);\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_set_affinity_mask_proc   (int, kmp_affinity_mask_t *);\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_unset_affinity_mask_proc (int, kmp_affinity_mask_t *);\n"
"    extern int    __KAI_KMPC_CONVENTION  kmp_get_affinity_mask_proc   (int, kmp_affinity_mask_t *);\n"
"\n"
"    /* OpenMP 4.0 affinity API */\n"
"    typedef enum omp_proc_bind_t {\n"
"        omp_proc_bind_false = 0,\n"
"        omp_proc_bind_true = 1,\n"
"        omp_proc_bind_master = 2,\n"
"        omp_proc_bind_close = 3,\n"
"        omp_proc_bind_spread = 4\n"
"    } omp_proc_bind_t;\n"
"\n"
"    extern omp_proc_bind_t __KAI_KMPC_CONVENTION omp_get_proc_bind (void);\n"
"\n"
"    /* OpenMP 4.5 affinity API */\n"
"    extern int  __KAI_KMPC_CONVENTION omp_get_num_places (void);\n"
"    extern int  __KAI_KMPC_CONVENTION omp_get_place_num_procs (int);\n"
"    extern void __KAI_KMPC_CONVENTION omp_get_place_proc_ids (int, int *);\n"
"    extern int  __KAI_KMPC_CONVENTION omp_get_place_num (void);\n"
"    extern int  __KAI_KMPC_CONVENTION omp_get_partition_num_places (void);\n"
"    extern void __KAI_KMPC_CONVENTION omp_get_partition_place_nums (int *);\n"
"\n"
"    extern void * __KAI_KMPC_CONVENTION  kmp_malloc  (size_t);\n"
"    extern void * __KAI_KMPC_CONVENTION  kmp_aligned_malloc  (size_t, size_t);\n"
"    extern void * __KAI_KMPC_CONVENTION  kmp_calloc  (size_t, size_t);\n"
"    extern void * __KAI_KMPC_CONVENTION  kmp_realloc (void *, size_t);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_free    (void *);\n"
"\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_warnings_on(void);\n"
"    extern void   __KAI_KMPC_CONVENTION  kmp_set_warnings_off(void);\n"
"\n"
"    /* OpenMP 5.0 Tool Control */\n"
"    typedef enum omp_control_tool_result_t {\n"
"        omp_control_tool_notool = -2,\n"
"        omp_control_tool_nocallback = -1,\n"
"        omp_control_tool_success = 0,\n"
"        omp_control_tool_ignored = 1\n"
"    } omp_control_tool_result_t;\n"
"\n"
"    typedef enum omp_control_tool_t {\n"
"        omp_control_tool_start = 1,\n"
"        omp_control_tool_pause = 2,\n"
"        omp_control_tool_flush = 3,\n"
"        omp_control_tool_end = 4\n"
"    } omp_control_tool_t;\n"
"    \n"
"    extern int __KAI_KMPC_CONVENTION omp_control_tool(int, int, void*);\n"
"\n"
"    /* OpenMP 5.0 Memory Management */\n"
"    typedef void *omp_allocator_t;\n"
"    extern __KMP_IMP const omp_allocator_t *OMP_NULL_ALLOCATOR;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_default_mem_alloc;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_large_cap_mem_alloc;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_const_mem_alloc;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_high_bw_mem_alloc;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_low_lat_mem_alloc;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_cgroup_mem_alloc;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_pteam_mem_alloc;\n"
"    extern __KMP_IMP const omp_allocator_t *omp_thread_mem_alloc;\n"
"\n"
"    extern void __KAI_KMPC_CONVENTION omp_set_default_allocator(const omp_allocator_t *);\n"
"    extern const omp_allocator_t * __KAI_KMPC_CONVENTION omp_get_default_allocator(void);\n"
"#ifdef __cplusplus\n"
"    extern void *__KAI_KMPC_CONVENTION omp_alloc(size_t size, const omp_allocator_t *allocator = OMP_NULL_ALLOCATOR);\n"
"    extern void __KAI_KMPC_CONVENTION omp_free(void * ptr, const omp_allocator_t *allocator = OMP_NULL_ALLOCATOR);\n"
"#else\n"
"    extern void *__KAI_KMPC_CONVENTION omp_alloc(size_t size, const omp_allocator_t *allocator);\n"
"    extern void __KAI_KMPC_CONVENTION omp_free(void *ptr, const omp_allocator_t *allocator);\n"
"#endif\n"
"\n"
"    /* OpenMP 5.0 Affinity Format */\n"
"    extern void __KAI_KMPC_CONVENTION omp_set_affinity_format(char const *);\n"
"    extern size_t __KAI_KMPC_CONVENTION omp_get_affinity_format(char *, size_t);\n"
"    extern void __KAI_KMPC_CONVENTION omp_display_affinity(char const *);\n"
"    extern size_t __KAI_KMPC_CONVENTION omp_capture_affinity(char *, size_t, char const *);\n"
"\n"
"#   undef __KAI_KMPC_CONVENTION\n"
"#   undef __KMP_IMP\n"
"\n"
"    /* Warning:\n"
"       The following typedefs are not standard, deprecated and will be removed in a future release.\n"
"    */\n"
"    typedef int     omp_int_t;\n"
"    typedef double  omp_wtime_t;\n"
"\n"
"#   ifdef __cplusplus\n"
"    }\n"
"#   endif\n"
"\n"
"#endif /* __OMP_H */\n"
"" } , 
 { "/builtins/ompt.h" , "/*\n"
" * include/50/omp-tools.h.var\n"
" */\n"
"\n"
"//===----------------------------------------------------------------------===//\n"
"//\n"
"//                     The LLVM Compiler Infrastructure\n"
"//\n"
"// This file is dual licensed under the MIT and the University of Illinois Open\n"
"// Source Licenses. See LICENSE.txt for details.\n"
"//\n"
"//===----------------------------------------------------------------------===//\n"
"\n"
"#ifndef __OMPT__\n"
"#define __OMPT__\n"
"\n"
"/*****************************************************************************\n"
" * system include files\n"
" *****************************************************************************/\n"
"\n"
"#include <stdint.h>\n"
"#include <stddef.h>\n"
"\n"
"/*****************************************************************************\n"
" * iteration macros\n"
" *****************************************************************************/\n"
"\n"
"#define FOREACH_OMPT_INQUIRY_FN(macro)      \\\n"
"    macro (ompt_enumerate_states)           \\\n"
"    macro (ompt_enumerate_mutex_impls)      \\\n"
"                                            \\\n"
"    macro (ompt_set_callback)               \\\n"
"    macro (ompt_get_callback)               \\\n"
"                                            \\\n"
"    macro (ompt_get_state)                  \\\n"
"                                            \\\n"
"    macro (ompt_get_parallel_info)          \\\n"
"    macro (ompt_get_task_info)              \\\n"
"    macro (ompt_get_task_memory)            \\\n"
"    macro (ompt_get_thread_data)            \\\n"
"    macro (ompt_get_unique_id)              \\\n"
"    macro (ompt_finalize_tool)              \\\n"
"                                            \\\n"
"    macro(ompt_get_num_procs)               \\\n"
"    macro(ompt_get_num_places)              \\\n"
"    macro(ompt_get_place_proc_ids)          \\\n"
"    macro(ompt_get_place_num)               \\\n"
"    macro(ompt_get_partition_place_nums)    \\\n"
"    macro(ompt_get_proc_id)                 \\\n"
"                                            \\\n"
"    macro(ompt_get_target_info)             \\\n"
"    macro(ompt_get_num_devices)\n"
"\n"
"#define FOREACH_OMPT_STATE(macro)                                                                \\\n"
"                                                                                                \\\n"
"    /* first available state */                                                                 \\\n"
"    macro (ompt_state_undefined, 0x102)      /* undefined thread state */                        \\\n"
"                                                                                                \\\n"
"    /* work states (0..15) */                                                                   \\\n"
"    macro (ompt_state_work_serial, 0x000)    /* working outside parallel */                      \\\n"
"    macro (ompt_state_work_parallel, 0x001)  /* working within parallel */                       \\\n"
"    macro (ompt_state_work_reduction, 0x002) /* performing a reduction */                        \\\n"
"                                                                                                \\\n"
"    /* barrier wait states (16..31) */                                                          \\\n"
"    macro (ompt_state_wait_barrier, 0x010)   /* waiting at a barrier */                          \\\n"
"    macro (ompt_state_wait_barrier_implicit_parallel, 0x011)                                     \\\n"
"                                            /* implicit barrier at the end of parallel region */\\\n"
"    macro (ompt_state_wait_barrier_implicit_workshare, 0x012)                                    \\\n"
"                                            /* implicit barrier at the end of worksharing */    \\\n"
"    macro (ompt_state_wait_barrier_implicit, 0x013)  /* implicit barrier */                      \\\n"
"    macro (ompt_state_wait_barrier_explicit, 0x014)  /* explicit barrier */                      \\\n"
"                                                                                                \\\n"
"    /* task wait states (32..63) */                                                             \\\n"
"    macro (ompt_state_wait_taskwait, 0x020)  /* waiting at a taskwait */                         \\\n"
"    macro (ompt_state_wait_taskgroup, 0x021) /* waiting at a taskgroup */                        \\\n"
"                                                                                                \\\n"
"    /* mutex wait states (64..127) */                                                           \\\n"
"    macro (ompt_state_wait_mutex, 0x040)                                                         \\\n"
"    macro (ompt_state_wait_lock, 0x041)      /* waiting for lock */                              \\\n"
"    macro (ompt_state_wait_critical, 0x042)  /* waiting for critical */                          \\\n"
"    macro (ompt_state_wait_atomic, 0x043)    /* waiting for atomic */                            \\\n"
"    macro (ompt_state_wait_ordered, 0x044)   /* waiting for ordered */                           \\\n"
"                                                                                                \\\n"
"    /* target wait states (128..255) */                                                         \\\n"
"    macro (ompt_state_wait_target, 0x080)        /* waiting for target region */                 \\\n"
"    macro (ompt_state_wait_target_map, 0x081)    /* waiting for target data mapping operation */ \\\n"
"    macro (ompt_state_wait_target_update, 0x082) /* waiting for target update operation */       \\\n"
"                                                                                                \\\n"
"    /* misc (256..511) */                                                                       \\\n"
"    macro (ompt_state_idle, 0x100)           /* waiting for work */                              \\\n"
"    macro (ompt_state_overhead, 0x101)       /* overhead excluding wait states */                \\\n"
"                                                                                                \\\n"
"    /* implementation-specific states (512..) */\n"
"\n"
"\n"
"#define FOREACH_KMP_MUTEX_IMPL(macro)                                                \\\n"
"    macro (kmp_mutex_impl_none, 0)         /* unknown implementation */              \\\n"
"    macro (kmp_mutex_impl_spin, 1)         /* based on spin */                       \\\n"
"    macro (kmp_mutex_impl_queuing, 2)      /* based on some fair policy */           \\\n"
"    macro (kmp_mutex_impl_speculative, 3)  /* based on HW-supported speculation */\n"
"\n"
"#define FOREACH_OMPT_EVENT(macro)                                                                                        \\\n"
"                                                                                                                         \\\n"
"    /*--- Mandatory Events ---*/                                                                                         \\\n"
"    macro (ompt_callback_thread_begin,      ompt_callback_thread_begin_t,       1) /* thread begin                    */ \\\n"
"    macro (ompt_callback_thread_end,        ompt_callback_thread_end_t,         2) /* thread end                      */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_parallel_begin,    ompt_callback_parallel_begin_t,     3) /* parallel begin                  */ \\\n"
"    macro (ompt_callback_parallel_end,      ompt_callback_parallel_end_t,       4) /* parallel end                    */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_task_create,       ompt_callback_task_create_t,        5) /* task begin                      */ \\\n"
"    macro (ompt_callback_task_schedule,     ompt_callback_task_schedule_t,      6) /* task schedule                   */ \\\n"
"    macro (ompt_callback_implicit_task,     ompt_callback_implicit_task_t,      7) /* implicit task                   */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_target,            ompt_callback_target_t,             8) /* target                          */ \\\n"
"    macro (ompt_callback_target_data_op,    ompt_callback_target_data_op_t,     9) /* target data op                  */ \\\n"
"    macro (ompt_callback_target_submit,     ompt_callback_target_submit_t,     10) /* target  submit                  */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_control_tool,      ompt_callback_control_tool_t,      11) /* control tool                    */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_device_initialize, ompt_callback_device_initialize_t, 12) /* device initialize               */ \\\n"
"    macro (ompt_callback_device_finalize,   ompt_callback_device_finalize_t,   13) /* device finalize                 */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_device_load,       ompt_callback_device_load_t,       14) /* device load                     */ \\\n"
"    macro (ompt_callback_device_unload,     ompt_callback_device_unload_t,     15) /* device unload                   */ \\\n"
"                                                                                                                         \\\n"
"    /* Optional Events */                                                                                                \\\n"
"    macro (ompt_callback_sync_region_wait,  ompt_callback_sync_region_t,       16) /* sync region wait begin or end   */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_mutex_released,    ompt_callback_mutex_t,             17) /* mutex released                  */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_dependences,       ompt_callback_dependences_t,       18) /* report task dependences         */ \\\n"
"    macro (ompt_callback_task_dependence,   ompt_callback_task_dependence_t,   19) /* report task dependence          */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_work,              ompt_callback_work_t,              20) /* task at work begin or end       */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_master,            ompt_callback_master_t,            21) /* task at master begin or end     */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_target_map,        ompt_callback_target_map_t,        22) /* target map                      */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_sync_region,       ompt_callback_sync_region_t,       23) /* sync region begin or end        */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_lock_init,         ompt_callback_mutex_acquire_t,     24) /* lock init                       */ \\\n"
"    macro (ompt_callback_lock_destroy,      ompt_callback_mutex_t,             25) /* lock destroy                    */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_mutex_acquire,     ompt_callback_mutex_acquire_t,     26) /* mutex acquire                   */ \\\n"
"    macro (ompt_callback_mutex_acquired,    ompt_callback_mutex_t,             27) /* mutex acquired                  */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_nest_lock,         ompt_callback_nest_lock_t,         28) /* nest lock                       */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_flush,             ompt_callback_flush_t,             29) /* after executing flush           */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_cancel,            ompt_callback_cancel_t,            30) /* cancel innermost binding region */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_reduction,         ompt_callback_sync_region_t,       31) /* reduction                       */ \\\n"
"                                                                                                                         \\\n"
"    macro (ompt_callback_dispatch,          ompt_callback_dispatch_t,          32) /* dispatch of work                */\n"
"\n"
"/*****************************************************************************\n"
" * implementation specific types\n"
" *****************************************************************************/\n"
"\n"
"typedef enum kmp_mutex_impl_t {\n"
"#define kmp_mutex_impl_macro(impl, code) impl = code,\n"
"    FOREACH_KMP_MUTEX_IMPL(kmp_mutex_impl_macro)\n"
"#undef kmp_mutex_impl_macro\n"
"} kmp_mutex_impl_t;\n"
"\n"
"/*****************************************************************************\n"
" * definitions generated from spec\n"
" *****************************************************************************/\n"
"\n"
"typedef enum ompt_callbacks_t {\n"
"  ompt_callback_thread_begin             = 1,\n"
"  ompt_callback_thread_end               = 2,\n"
"  ompt_callback_parallel_begin           = 3,\n"
"  ompt_callback_parallel_end             = 4,\n"
"  ompt_callback_task_create              = 5,\n"
"  ompt_callback_task_schedule            = 6,\n"
"  ompt_callback_implicit_task            = 7,\n"
"  ompt_callback_target                   = 8,\n"
"  ompt_callback_target_data_op           = 9,\n"
"  ompt_callback_target_submit            = 10,\n"
"  ompt_callback_control_tool             = 11,\n"
"  ompt_callback_device_initialize        = 12,\n"
"  ompt_callback_device_finalize          = 13,\n"
"  ompt_callback_device_load              = 14,\n"
"  ompt_callback_device_unload            = 15,\n"
"  ompt_callback_sync_region_wait         = 16,\n"
"  ompt_callback_mutex_released           = 17,\n"
"  ompt_callback_dependences              = 18,\n"
"  ompt_callback_task_dependence          = 19,\n"
"  ompt_callback_work                     = 20,\n"
"  ompt_callback_master                   = 21,\n"
"  ompt_callback_target_map               = 22,\n"
"  ompt_callback_sync_region              = 23,\n"
"  ompt_callback_lock_init                = 24,\n"
"  ompt_callback_lock_destroy             = 25,\n"
"  ompt_callback_mutex_acquire            = 26,\n"
"  ompt_callback_mutex_acquired           = 27,\n"
"  ompt_callback_nest_lock                = 28,\n"
"  ompt_callback_flush                    = 29,\n"
"  ompt_callback_cancel                   = 30,\n"
"  ompt_callback_reduction                = 31,\n"
"  ompt_callback_dispatch                 = 32\n"
"} ompt_callbacks_t;\n"
"\n"
"typedef enum ompt_record_t {\n"
"  ompt_record_ompt               = 1,\n"
"  ompt_record_native             = 2,\n"
"  ompt_record_invalid            = 3\n"
"} ompt_record_t;\n"
"\n"
"typedef enum ompt_record_native_t {\n"
"  ompt_record_native_info  = 1,\n"
"  ompt_record_native_event = 2\n"
"} ompt_record_native_t;\n"
"\n"
"typedef enum ompt_set_result_t {\n"
"  ompt_set_error            = 0,\n"
"  ompt_set_never            = 1,\n"
"  ompt_set_impossible       = 2,\n"
"  ompt_set_sometimes        = 3,\n"
"  ompt_set_sometimes_paired = 4,\n"
"  ompt_set_always           = 5\n"
"} ompt_set_result_t;\n"
"\n"
"typedef uint64_t ompt_id_t;\n"
"\n"
"typedef uint64_t ompt_device_time_t;\n"
"\n"
"typedef uint64_t ompt_buffer_cursor_t;\n"
"\n"
"typedef enum ompt_thread_t {\n"
"  ompt_thread_initial                 = 1,\n"
"  ompt_thread_worker                  = 2,\n"
"  ompt_thread_other                   = 3,\n"
"  ompt_thread_unknown                 = 4\n"
"} ompt_thread_t;\n"
"\n"
"typedef enum ompt_scope_endpoint_t {\n"
"  ompt_scope_begin                    = 1,\n"
"  ompt_scope_end                      = 2\n"
"} ompt_scope_endpoint_t;\n"
"\n"
"typedef enum ompt_dispatch_t {\n"
"  ompt_dispatch_iteration             = 1,\n"
"  ompt_dispatch_section               = 2\n"
"} ompt_dispatch_t;\n"
"\n"
"typedef enum ompt_sync_region_t {\n"
"  ompt_sync_region_barrier                = 1,\n"
"  ompt_sync_region_barrier_implicit       = 2,\n"
"  ompt_sync_region_barrier_explicit       = 3,\n"
"  ompt_sync_region_barrier_implementation = 4,\n"
"  ompt_sync_region_taskwait               = 5,\n"
"  ompt_sync_region_taskgroup              = 6,\n"
"  ompt_sync_region_reduction              = 7\n"
"} ompt_sync_region_t;\n"
"\n"
"typedef enum ompt_target_data_op_t {\n"
"  ompt_target_data_alloc                = 1,\n"
"  ompt_target_data_transfer_to_device   = 2,\n"
"  ompt_target_data_transfer_from_device = 3,\n"
"  ompt_target_data_delete               = 4,\n"
"  ompt_target_data_associate            = 5,\n"
"  ompt_target_data_disassociate         = 6\n"
"} ompt_target_data_op_t;\n"
"\n"
"typedef enum ompt_work_t {\n"
"  ompt_work_loop               = 1,\n"
"  ompt_work_sections           = 2,\n"
"  ompt_work_single_executor    = 3,\n"
"  ompt_work_single_other       = 4,\n"
"  ompt_work_workshare          = 5,\n"
"  ompt_work_distribute         = 6,\n"
"  ompt_work_taskloop           = 7\n"
"} ompt_work_t;\n"
"\n"
"typedef enum ompt_mutex_t {\n"
"  ompt_mutex_lock                     = 1,\n"
"  ompt_mutex_test_lock                = 2,\n"
"  ompt_mutex_nest_lock                = 3,\n"
"  ompt_mutex_test_nest_lock           = 4,\n"
"  ompt_mutex_critical                 = 5,\n"
"  ompt_mutex_atomic                   = 6,\n"
"  ompt_mutex_ordered                  = 7\n"
"} ompt_mutex_t;\n"
"\n"
"typedef enum ompt_native_mon_flag_t {\n"
"  ompt_native_data_motion_explicit    = 0x01,\n"
"  ompt_native_data_motion_implicit    = 0x02,\n"
"  ompt_native_kernel_invocation       = 0x04,\n"
"  ompt_native_kernel_execution        = 0x08,\n"
"  ompt_native_driver                  = 0x10,\n"
"  ompt_native_runtime                 = 0x20,\n"
"  ompt_native_overhead                = 0x40,\n"
"  ompt_native_idleness                = 0x80\n"
"} ompt_native_mon_flag_t;\n"
"\n"
"typedef enum ompt_task_flag_t {\n"
"  ompt_task_initial                   = 0x00000001,\n"
"  ompt_task_implicit                  = 0x00000002,\n"
"  ompt_task_explicit                  = 0x00000004,\n"
"  ompt_task_target                    = 0x00000008,\n"
"  ompt_task_undeferred                = 0x08000000,\n"
"  ompt_task_untied                    = 0x10000000,\n"
"  ompt_task_final                     = 0x20000000,\n"
"  ompt_task_mergeable                 = 0x40000000,\n"
"  ompt_task_merged                    = 0x80000000\n"
"} ompt_task_flag_t;\n"
"\n"
"typedef enum ompt_task_status_t {\n"
"  ompt_task_complete      = 1,\n"
"  ompt_task_yield         = 2,\n"
"  ompt_task_cancel        = 3,\n"
"  ompt_task_detach        = 4,\n"
"  ompt_task_early_fulfill = 5,\n"
"  ompt_task_late_fulfill  = 6,\n"
"  ompt_task_switch        = 7\n"
"} ompt_task_status_t;\n"
"\n"
"typedef enum ompt_target_t {\n"
"  ompt_target                         = 1,\n"
"  ompt_target_enter_data              = 2,\n"
"  ompt_target_exit_data               = 3,\n"
"  ompt_target_update                  = 4\n"
"} ompt_target_t;\n"
"\n"
"typedef enum ompt_parallel_flag_t {\n"
"  ompt_parallel_invoker_program = 0x00000001,\n"
"  ompt_parallel_invoker_runtime = 0x00000002,\n"
"  ompt_parallel_league          = 0x40000000,\n"
"  ompt_parallel_team            = 0x80000000\n"
"} ompt_parallel_flag_t;\n"
"\n"
"typedef enum ompt_target_map_flag_t {\n"
"  ompt_target_map_flag_to             = 0x01,\n"
"  ompt_target_map_flag_from           = 0x02,\n"
"  ompt_target_map_flag_alloc          = 0x04,\n"
"  ompt_target_map_flag_release        = 0x08,\n"
"  ompt_target_map_flag_delete         = 0x10,\n"
"  ompt_target_map_flag_implicit       = 0x20\n"
"} ompt_target_map_flag_t;\n"
"\n"
"typedef enum ompt_dependence_type_t {\n"
"  ompt_dependence_type_in              = 1,\n"
"  ompt_dependence_type_out             = 2,\n"
"  ompt_dependence_type_inout           = 3,\n"
"  ompt_dependence_type_mutexinoutset   = 4,\n"
"  ompt_dependence_type_source          = 5,\n"
"  ompt_dependence_type_sink            = 6\n"
"} ompt_dependence_type_t;\n"
"\n"
"typedef enum ompt_cancel_flag_t {\n"
"  ompt_cancel_parallel       = 0x01,\n"
"  ompt_cancel_sections       = 0x02,\n"
"  ompt_cancel_loop           = 0x04,\n"
"  ompt_cancel_taskgroup      = 0x08,\n"
"  ompt_cancel_activated      = 0x10,\n"
"  ompt_cancel_detected       = 0x20,\n"
"  ompt_cancel_discarded_task = 0x40\n"
"} ompt_cancel_flag_t;\n"
"\n"
"typedef uint64_t ompt_hwid_t;\n"
"\n"
"typedef uint64_t ompt_wait_id_t;\n"
"\n"
"typedef enum ompt_frame_flag_t {\n"
"  ompt_frame_runtime        = 0x00,\n"
"  ompt_frame_application    = 0x01,\n"
"  ompt_frame_cfa            = 0x10,\n"
"  ompt_frame_framepointer   = 0x20,\n"
"  ompt_frame_stackaddress   = 0x30\n"
"} ompt_frame_flag_t; \n"
"\n"
"typedef enum ompt_state_t {\n"
"  ompt_state_work_serial                      = 0x000,\n"
"  ompt_state_work_parallel                    = 0x001,\n"
"  ompt_state_work_reduction                   = 0x002,\n"
"\n"
"  ompt_state_wait_barrier                     = 0x010,\n"
"  ompt_state_wait_barrier_implicit_parallel   = 0x011,\n"
"  ompt_state_wait_barrier_implicit_workshare  = 0x012,\n"
"  ompt_state_wait_barrier_implicit            = 0x013,\n"
"  ompt_state_wait_barrier_explicit            = 0x014,\n"
"\n"
"  ompt_state_wait_taskwait                    = 0x020,\n"
"  ompt_state_wait_taskgroup                   = 0x021,\n"
"\n"
"  ompt_state_wait_mutex                       = 0x040,\n"
"  ompt_state_wait_lock                        = 0x041,\n"
"  ompt_state_wait_critical                    = 0x042,\n"
"  ompt_state_wait_atomic                      = 0x043,\n"
"  ompt_state_wait_ordered                     = 0x044,\n"
"\n"
"  ompt_state_wait_target                      = 0x080,\n"
"  ompt_state_wait_target_map                  = 0x081,\n"
"  ompt_state_wait_target_update               = 0x082,\n"
"\n"
"  ompt_state_idle                             = 0x100,\n"
"  ompt_state_overhead                         = 0x101,\n"
"  ompt_state_undefined                        = 0x102\n"
"} ompt_state_t;\n"
"\n"
"typedef uint64_t (*ompt_get_unique_id_t) (void);\n"
"\n"
"typedef uint64_t ompd_size_t;\n"
"\n"
"typedef uint64_t ompd_wait_id_t;\n"
"\n"
"typedef uint64_t ompd_addr_t;\n"
"typedef int64_t  ompd_word_t;\n"
"typedef uint64_t ompd_seg_t;\n"
"\n"
"typedef uint64_t ompd_device_t;\n"
"\n"
"typedef uint64_t ompd_thread_id_t;\n"
"\n"
"typedef enum ompd_scope_t {\n"
"  ompd_scope_global = 1,\n"
"  ompd_scope_address_space = 2,\n"
"  ompd_scope_thread = 3,\n"
"  ompd_scope_parallel = 4,\n"
"  ompd_scope_implicit_task = 5,\n"
"  ompd_scope_task = 6\n"
"} ompd_scope_t;\n"
"\n"
"typedef uint64_t ompd_icv_id_t;\n"
"\n"
"typedef enum ompd_rc_t {\n"
"  ompd_rc_ok = 0,\n"
"  ompd_rc_unavailable = 1,\n"
"  ompd_rc_stale_handle = 2,\n"
"  ompd_rc_bad_input = 3,\n"
"  ompd_rc_error = 4,\n"
"  ompd_rc_unsupported = 5,\n"
"  ompd_rc_needs_state_tracking = 6,\n"
"  ompd_rc_incompatible = 7,\n"
"  ompd_rc_device_read_error = 8,\n"
"  ompd_rc_device_write_error = 9,\n"
"  ompd_rc_nomem = 10,\n"
"} ompd_rc_t;\n"
"\n"
"typedef void (*ompt_interface_fn_t) (void);\n"
"\n"
"typedef ompt_interface_fn_t (*ompt_function_lookup_t) (\n"
"  const char *interface_function_name\n"
");\n"
"\n"
"typedef union ompt_data_t {\n"
"  uint64_t value;\n"
"  void *ptr;\n"
"} ompt_data_t;\n"
"\n"
"typedef struct ompt_frame_t {\n"
"  ompt_data_t exit_frame;\n"
"  ompt_data_t enter_frame;\n"
"  int exit_frame_flags;\n"
"  int enter_frame_flags;\n"
"} ompt_frame_t;\n"
"\n"
"typedef void (*ompt_callback_t) (void);\n"
"\n"
"typedef void ompt_device_t;\n"
"\n"
"typedef void ompt_buffer_t;\n"
"\n"
"typedef void (*ompt_callback_buffer_request_t) (\n"
"  int device_num,\n"
"  ompt_buffer_t **buffer,\n"
"  size_t *bytes\n"
");\n"
"\n"
"typedef void (*ompt_callback_buffer_complete_t) (\n"
"  int device_num,\n"
"  ompt_buffer_t *buffer,\n"
"  size_t bytes,\n"
"  ompt_buffer_cursor_t begin,\n"
"  int buffer_owned\n"
");\n"
"\n"
"typedef void (*ompt_finalize_t) (\n"
"  ompt_data_t *tool_data\n"
");\n"
"\n"
"typedef int (*ompt_initialize_t) (\n"
"  ompt_function_lookup_t lookup,\n"
"  int initial_device_num,\n"
"  ompt_data_t *tool_data\n"
");\n"
"\n"
"typedef struct ompt_start_tool_result_t {\n"
"  ompt_initialize_t initialize;\n"
"  ompt_finalize_t finalize;\n"
"  ompt_data_t tool_data;\n"
"} ompt_start_tool_result_t;\n"
"\n"
"typedef struct ompt_record_abstract_t {\n"
"  ompt_record_native_t rclass;\n"
"  const char *type;\n"
"  ompt_device_time_t start_time;\n"
"  ompt_device_time_t end_time;\n"
"  ompt_hwid_t hwid;\n"
"} ompt_record_abstract_t;\n"
"\n"
"typedef struct ompt_dependence_t {\n"
"  ompt_data_t variable;\n"
"  ompt_dependence_type_t dependence_type;\n"
"} ompt_dependence_t;\n"
"\n"
"typedef int (*ompt_enumerate_states_t) (\n"
"  int current_state,\n"
"  int *next_state,\n"
"  const char **next_state_name\n"
");\n"
"\n"
"typedef int (*ompt_enumerate_mutex_impls_t) (\n"
"  int current_impl,\n"
"  int *next_impl,\n"
"  const char **next_impl_name\n"
");\n"
"\n"
"typedef ompt_set_result_t (*ompt_set_callback_t) (\n"
"  ompt_callbacks_t event,\n"
"  ompt_callback_t callback\n"
");\n"
"\n"
"typedef int (*ompt_get_callback_t) (\n"
"  ompt_callbacks_t event,\n"
"  ompt_callback_t *callback\n"
");\n"
"\n"
"typedef ompt_data_t *(*ompt_get_thread_data_t) (void);\n"
"\n"
"typedef int (*ompt_get_num_procs_t) (void);\n"
"\n"
"typedef int (*ompt_get_num_places_t) (void);\n"
"\n"
"typedef int (*ompt_get_place_proc_ids_t) (\n"
"  int place_num,\n"
"  int ids_size,\n"
"  int *ids\n"
");\n"
"\n"
"typedef int (*ompt_get_place_num_t) (void);\n"
"\n"
"typedef int (*ompt_get_partition_place_nums_t) (\n"
"  int place_nums_size,\n"
"  int *place_nums\n"
");\n"
"\n"
"typedef int (*ompt_get_proc_id_t) (void);\n"
"\n"
"typedef int (*ompt_get_state_t) (\n"
"  ompt_wait_id_t *wait_id\n"
");\n"
"\n"
"typedef int (*ompt_get_parallel_info_t) (\n"
"  int ancestor_level,\n"
"  ompt_data_t **parallel_data,\n"
"  int *team_size\n"
");\n"
"\n"
"typedef int (*ompt_get_task_info_t) (\n"
"  int ancestor_level,\n"
"  int *flags,\n"
"  ompt_data_t **task_data,\n"
"  ompt_frame_t **task_frame,\n"
"  ompt_data_t **parallel_data,\n"
"  int *thread_num\n"
");\n"
"\n"
"typedef int (*ompt_get_task_memory_t)(\n"
"  void **addr,\n"
"  size_t *size,\n"
"  int block\n"
");\n"
"\n"
"typedef int (*ompt_get_target_info_t) (\n"
"  uint64_t *device_num,\n"
"  ompt_id_t *target_id,\n"
"  ompt_id_t *host_op_id\n"
");\n"
"\n"
"typedef int (*ompt_get_num_devices_t) (void);\n"
"\n"
"typedef void (*ompt_finalize_tool_t) (void);\n"
"\n"
"typedef int (*ompt_get_device_num_procs_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef ompt_device_time_t (*ompt_get_device_time_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef double (*ompt_translate_time_t) (\n"
"  ompt_device_t *device,\n"
"  ompt_device_time_t time\n"
");\n"
"\n"
"typedef ompt_set_result_t (*ompt_set_trace_ompt_t) (\n"
"  ompt_device_t *device,\n"
"  unsigned int enable,\n"
"  unsigned int etype\n"
");\n"
"\n"
"typedef ompt_set_result_t (*ompt_set_trace_native_t) (\n"
"  ompt_device_t *device,\n"
"  int enable,\n"
"  int flags\n"
");\n"
"\n"
"typedef int (*ompt_start_trace_t) (\n"
"  ompt_device_t *device,\n"
"  ompt_callback_buffer_request_t request,\n"
"  ompt_callback_buffer_complete_t complete\n"
");\n"
"\n"
"typedef int (*ompt_pause_trace_t) (\n"
"  ompt_device_t *device,\n"
"  int begin_pause\n"
");\n"
"\n"
"typedef int (*ompt_flush_trace_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef int (*ompt_stop_trace_t) (\n"
"  ompt_device_t *device\n"
");\n"
"\n"
"typedef int (*ompt_advance_buffer_cursor_t) (\n"
"  ompt_device_t *device,\n"
"  ompt_buffer_t *buffer,\n"
"  size_t size,\n"
"  ompt_buffer_cursor_t current,\n"
"  ompt_buffer_cursor_t *next\n"
");\n"
"\n"
"typedef ompt_record_t (*ompt_get_record_type_t) (\n"
"  ompt_buffer_t *buffer,\n"
"  ompt_buffer_cursor_t current\n"
");\n"
"\n"
"typedef void *(*ompt_get_record_native_t) (\n"
"  ompt_buffer_t *buffer,\n"
"  ompt_buffer_cursor_t current,\n"
"  ompt_id_t *host_op_id\n"
");\n"
"\n"
"typedef ompt_record_abstract_t *\n"
"(*ompt_get_record_abstract_t) (\n"
"  void *native_record\n"
");\n"
"\n"
"typedef void (*ompt_callback_thread_begin_t) (\n"
"  ompt_thread_t thread_type,\n"
"  ompt_data_t *thread_data\n"
");\n"
"\n"
"typedef struct ompt_record_thread_begin_t {\n"
"  ompt_thread_t thread_type;\n"
"} ompt_record_thread_begin_t;\n"
"\n"
"typedef void (*ompt_callback_thread_end_t) (\n"
"  ompt_data_t *thread_data\n"
");\n"
"\n"
"typedef void (*ompt_callback_parallel_begin_t) (\n"
"  ompt_data_t *encountering_task_data,\n"
"  const ompt_frame_t *encountering_task_frame,\n"
"  ompt_data_t *parallel_data,\n"
"  unsigned int requested_parallelism,\n"
"  int flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_parallel_begin_t {\n"
"  ompt_id_t encountering_task_id;\n"
"  ompt_id_t parallel_id;\n"
"  unsigned int requested_parallelism;\n"
"  int flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_parallel_begin_t;\n"
"\n"
"typedef void (*ompt_callback_parallel_end_t) (\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *encountering_task_data,\n"
"  int flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_parallel_end_t {\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t encountering_task_id;\n"
"  int flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_parallel_end_t;\n"
"\n"
"typedef void (*ompt_callback_work_t) (\n"
"  ompt_work_t wstype,\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  uint64_t count,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_work_t {\n"
"  ompt_work_t wstype;\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  uint64_t count;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_work_t;\n"
"\n"
"typedef void (*ompt_callback_dispatch_t) (\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  ompt_dispatch_t kind,\n"
"  ompt_data_t instance \n"
");\n"
"\n"
"typedef struct ompt_record_dispatch_t {\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  ompt_dispatch_t kind;\n"
"  ompt_data_t instance; \n"
"} ompt_record_dispatch_t;\n"
"\n"
"typedef void (*ompt_callback_task_create_t) (\n"
"  ompt_data_t *encountering_task_data,\n"
"  const ompt_frame_t *encountering_task_frame,\n"
"  ompt_data_t *new_task_data,\n"
"  int flags,\n"
"  int has_dependences,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_task_create_t {\n"
"  ompt_id_t encountering_task_id;\n"
"  ompt_id_t new_task_id;\n"
"  int flags;\n"
"  int has_dependences;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_task_create_t;\n"
"\n"
"typedef void (*ompt_callback_dependences_t) (\n"
"  ompt_data_t *task_data,\n"
"  const ompt_dependence_t *deps,\n"
"  int ndeps\n"
");\n"
"\n"
"typedef struct ompt_record_dependences_t {\n"
"  ompt_id_t task_id;\n"
"  ompt_dependence_t dep;\n"
"  int ndeps;\n"
"} ompt_record_dependences_t;\n"
"\n"
"typedef void (*ompt_callback_task_dependence_t) (\n"
"  ompt_data_t *src_task_data,\n"
"  ompt_data_t *sink_task_data\n"
");\n"
"\n"
"typedef struct ompt_record_task_dependence_t {\n"
"  ompt_id_t src_task_id;\n"
"  ompt_id_t sink_task_id;\n"
"} ompt_record_task_dependence_t;\n"
"\n"
"typedef void (*ompt_callback_task_schedule_t) (\n"
"  ompt_data_t *prior_task_data,\n"
"  ompt_task_status_t prior_task_status,\n"
"  ompt_data_t *next_task_data\n"
");\n"
"\n"
"typedef struct ompt_record_task_schedule_t {\n"
"  ompt_id_t prior_task_id;\n"
"  ompt_task_status_t prior_task_status;\n"
"  ompt_id_t next_task_id;\n"
"} ompt_record_task_schedule_t;\n"
"\n"
"typedef void (*ompt_callback_implicit_task_t) (\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  unsigned int actual_parallelism,\n"
"  unsigned int index,\n"
"  int flags\n"
");\n"
"\n"
"typedef struct ompt_record_implicit_task_t {\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  unsigned int actual_parallelism;\n"
"  unsigned int index;\n"
"  int flags;\n"
"} ompt_record_implicit_task_t;\n"
"\n"
"typedef void (*ompt_callback_master_t) (\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_master_t {\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_master_t;\n"
"\n"
"typedef void (*ompt_callback_sync_region_t) (\n"
"  ompt_sync_region_t kind,\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_data_t *parallel_data,\n"
"  ompt_data_t *task_data,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_sync_region_t {\n"
"  ompt_sync_region_t kind;\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_id_t parallel_id;\n"
"  ompt_id_t task_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_sync_region_t;\n"
"\n"
"typedef void (*ompt_callback_mutex_acquire_t) (\n"
"  ompt_mutex_t kind,\n"
"  unsigned int hint,\n"
"  unsigned int impl,\n"
"  ompt_wait_id_t wait_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_mutex_acquire_t {\n"
"  ompt_mutex_t kind;\n"
"  unsigned int hint;\n"
"  unsigned int impl;\n"
"  ompt_wait_id_t wait_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_mutex_acquire_t;\n"
"\n"
"typedef void (*ompt_callback_mutex_t) (\n"
"  ompt_mutex_t kind,\n"
"  ompt_wait_id_t wait_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_mutex_t {\n"
"  ompt_mutex_t kind;\n"
"  ompt_wait_id_t wait_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_mutex_t;\n"
"\n"
"typedef void (*ompt_callback_nest_lock_t) (\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  ompt_wait_id_t wait_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_nest_lock_t {\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  ompt_wait_id_t wait_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_nest_lock_t;\n"
"\n"
"typedef void (*ompt_callback_flush_t) (\n"
"  ompt_data_t *thread_data,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_flush_t {\n"
"  const void *codeptr_ra;\n"
"} ompt_record_flush_t;\n"
"\n"
"typedef void (*ompt_callback_cancel_t) (\n"
"  ompt_data_t *task_data,\n"
"  int flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_cancel_t {\n"
"  ompt_id_t task_id;\n"
"  int flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_cancel_t;\n"
"\n"
"typedef void (*ompt_callback_device_initialize_t) (\n"
"  int device_num,\n"
"  const char *type,\n"
"  ompt_device_t *device,\n"
"  ompt_function_lookup_t lookup,\n"
"  const char *documentation\n"
");\n"
"\n"
"typedef void (*ompt_callback_device_finalize_t) (\n"
"  int device_num\n"
");\n"
"\n"
"typedef void (*ompt_callback_device_load_t) (\n"
"  int device_num,\n"
"  const char *filename,\n"
"  int64_t offset_in_file,\n"
"  void *vma_in_file,\n"
"  size_t bytes,\n"
"  void *host_addr,\n"
"  void *device_addr,\n"
"  uint64_t module_id\n"
");\n"
"\n"
"typedef void (*ompt_callback_device_unload_t) (\n"
"  int device_num,\n"
"  uint64_t module_id\n"
");\n"
"\n"
"typedef void (*ompt_callback_target_data_op_t) (\n"
"  ompt_id_t target_id,\n"
"  ompt_id_t host_op_id,\n"
"  ompt_target_data_op_t optype,\n"
"  void *src_addr,\n"
"  int src_device_num,\n"
"  void *dest_addr,\n"
"  int dest_device_num,\n"
"  size_t bytes,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_target_data_op_t {\n"
"  ompt_id_t host_op_id;\n"
"  ompt_target_data_op_t optype;\n"
"  void *src_addr;\n"
"  int src_device_num;\n"
"  void *dest_addr;\n"
"  int dest_device_num;\n"
"  size_t bytes;\n"
"  ompt_device_time_t end_time;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_target_data_op_t;\n"
"\n"
"typedef void (*ompt_callback_target_t) (\n"
"  ompt_target_t kind,\n"
"  ompt_scope_endpoint_t endpoint,\n"
"  int device_num,\n"
"  ompt_data_t *task_data,\n"
"  ompt_id_t target_id,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_target_t {\n"
"  ompt_target_t kind;\n"
"  ompt_scope_endpoint_t endpoint;\n"
"  int device_num;\n"
"  ompt_id_t task_id;\n"
"  ompt_id_t target_id;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_target_t;\n"
"\n"
"typedef void (*ompt_callback_target_map_t) (\n"
"  ompt_id_t target_id,\n"
"  unsigned int nitems,\n"
"  void **host_addr,\n"
"  void **device_addr,\n"
"  size_t *bytes,\n"
"  unsigned int *mapping_flags,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_target_map_t {\n"
"  ompt_id_t target_id;\n"
"  unsigned int nitems;\n"
"  void **host_addr;\n"
"  void **device_addr;\n"
"  size_t *bytes;\n"
"  unsigned int *mapping_flags;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_target_map_t;\n"
"\n"
"typedef void (*ompt_callback_target_submit_t) (\n"
"  ompt_id_t target_id,\n"
"  ompt_id_t host_op_id,\n"
"  unsigned int requested_num_teams\n"
");\n"
"\n"
"typedef struct ompt_record_target_kernel_t {\n"
"  ompt_id_t host_op_id;\n"
"  unsigned int requested_num_teams;\n"
"  unsigned int granted_num_teams;\n"
"  ompt_device_time_t end_time;\n"
"} ompt_record_target_kernel_t;\n"
"\n"
"typedef int (*ompt_callback_control_tool_t) (\n"
"  uint64_t command,\n"
"  uint64_t modifier,\n"
"  void *arg,\n"
"  const void *codeptr_ra\n"
");\n"
"\n"
"typedef struct ompt_record_control_tool_t {\n"
"  uint64_t command;\n"
"  uint64_t modifier;\n"
"  const void *codeptr_ra;\n"
"} ompt_record_control_tool_t;\n"
"\n"
"typedef struct ompd_address_t {\n"
"  ompd_seg_t segment;\n"
"  ompd_addr_t address;\n"
"} ompd_address_t;\n"
"\n"
"typedef struct ompd_frame_info_t {\n"
"  ompd_address_t frame_address;\n"
"  ompd_word_t frame_flag;\n"
"} ompd_frame_info_t;\n"
"\n"
"typedef struct _ompd_aspace_handle ompd_address_space_handle_t;\n"
"typedef struct _ompd_thread_handle ompd_thread_handle_t;\n"
"typedef struct _ompd_parallel_handle ompd_parallel_handle_t;\n"
"typedef struct _ompd_task_handle ompd_task_handle_t;\n"
"\n"
"typedef struct _ompd_aspace_cont ompd_address_space_context_t;\n"
"typedef struct _ompd_thread_cont ompd_thread_context_t;\n"
"\n"
"typedef struct ompd_device_type_sizes_t {\n"
"  uint8_t sizeof_char;\n"
"  uint8_t sizeof_short;\n"
"  uint8_t sizeof_int;\n"
"  uint8_t sizeof_long;\n"
"  uint8_t sizeof_long_long;\n"
"  uint8_t sizeof_pointer;\n"
"} ompd_device_type_sizes_t;\n"
"\n"
"typedef struct ompt_record_ompt_t {\n"
"  ompt_callbacks_t type;\n"
"  ompt_device_time_t time;\n"
"  ompt_id_t thread_id;\n"
"  ompt_id_t target_id;\n"
"  union {\n"
"    ompt_record_thread_begin_t thread_begin;\n"
"    ompt_record_parallel_begin_t parallel_begin;\n"
"    ompt_record_parallel_end_t parallel_end;\n"
"    ompt_record_work_t work;\n"
"    ompt_record_dispatch_t dispatch;\n"
"    ompt_record_task_create_t task_create;\n"
"    ompt_record_dependences_t dependences;\n"
"    ompt_record_task_dependence_t task_dependence;\n"
"    ompt_record_task_schedule_t task_schedule;\n"
"    ompt_record_implicit_task_t implicit_task;\n"
"    ompt_record_master_t master;\n"
"    ompt_record_sync_region_t sync_region;\n"
"    ompt_record_mutex_acquire_t mutex_acquire;\n"
"    ompt_record_mutex_t mutex;\n"
"    ompt_record_nest_lock_t nest_lock;\n"
"    ompt_record_flush_t flush;\n"
"    ompt_record_cancel_t cancel;\n"
"    ompt_record_target_t target;\n"
"    ompt_record_target_data_op_t target_data_op;\n"
"    ompt_record_target_map_t target_map;\n"
"    ompt_record_target_kernel_t target_kernel;\n"
"    ompt_record_control_tool_t control_tool;\n"
"  } record;\n"
"} ompt_record_ompt_t;\n"
"\n"
"typedef ompt_record_ompt_t *(*ompt_get_record_ompt_t) (\n"
"  ompt_buffer_t *buffer,\n"
"  ompt_buffer_cursor_t current\n"
");\n"
"\n"
"#define ompt_id_none 0\n"
"#define ompt_data_none {0}\n"
"#define ompt_time_none 0\n"
"#define ompt_hwid_none 0\n"
"#define ompt_addr_none ~0\n"
"#define ompt_mutex_impl_none 0\n"
"#define ompt_wait_id_none 0\n"
"\n"
"#define ompd_segment_none 0\n"
"\n"
"#endif /* __OMPT__ */\n"
"" } , 
 { "/builtins/opencl-c.h" , "//===--- opencl-c.h - OpenCL C language builtin function header -----------===//\n"
"//\n"
"//                     The LLVM Compiler Infrastructure\n"
"//\n"
"// This file is distributed under the University of Illinois Open Source\n"
"// License. See LICENSE.TXT for details.\n"
"//\n"
"//===----------------------------------------------------------------------===//\n"
"\n"
"#ifndef _OPENCL_H_\n"
"#define _OPENCL_H_\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#ifndef cl_khr_depth_images\n"
"#define cl_khr_depth_images\n"
"#endif //cl_khr_depth_images\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"#if __OPENCL_C_VERSION__ < CL_VERSION_2_0\n"
"#ifdef cl_khr_3d_image_writes\n"
"#pragma OPENCL EXTENSION cl_khr_3d_image_writes : enable\n"
"#endif //cl_khr_3d_image_writes\n"
"#endif //__OPENCL_C_VERSION__ < CL_VERSION_2_0\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"#ifndef cl_intel_planar_yuv\n"
"#define cl_intel_planar_yuv\n"
"#endif // cl_intel_planar_yuv\n"
"#pragma OPENCL EXTENSION cl_intel_planar_yuv : begin\n"
"#pragma OPENCL EXTENSION cl_intel_planar_yuv : end\n"
"#endif // __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"\n"
"#define __ovld __attribute__((overloadable))\n"
"#define __conv __attribute__((convergent))\n"
"\n"
"// Optimizations\n"
"#define __purefn __attribute__((pure))\n"
"#define __cnfn __attribute__((const))\n"
"\n"
"// built-in scalar data types:\n"
"\n"
"/**\n"
" * An unsigned 8-bit integer.\n"
" */\n"
"typedef unsigned char uchar;\n"
"\n"
"/**\n"
" * An unsigned 16-bit integer.\n"
" */\n"
"typedef unsigned short ushort;\n"
"\n"
"/**\n"
" * An unsigned 32-bit integer.\n"
" */\n"
"typedef unsigned int uint;\n"
"\n"
"/**\n"
" * An unsigned 64-bit integer.\n"
" */\n"
"typedef unsigned long ulong;\n"
"\n"
"/**\n"
" * The unsigned integer type of the result of the sizeof operator. This\n"
" * is a 32-bit unsigned integer if CL_DEVICE_ADDRESS_BITS\n"
" * defined in table 4.3 is 32-bits and is a 64-bit unsigned integer if\n"
" * CL_DEVICE_ADDRESS_BITS is 64-bits.\n"
" */\n"
"typedef __SIZE_TYPE__ size_t;\n"
"\n"
"/**\n"
" * A signed integer type that is the result of subtracting two pointers.\n"
" * This is a 32-bit signed integer if CL_DEVICE_ADDRESS_BITS\n"
" * defined in table 4.3 is 32-bits and is a 64-bit signed integer if\n"
" * CL_DEVICE_ADDRESS_BITS is 64-bits.\n"
" */\n"
"typedef __PTRDIFF_TYPE__ ptrdiff_t;\n"
"\n"
"/**\n"
"* A signed integer type with the property that any valid pointer to\n"
"* void can be converted to this type, then converted back to pointer\n"
"* to void, and the result will compare equal to the original pointer.\n"
"*/\n"
"typedef __INTPTR_TYPE__ intptr_t;\n"
"\n"
"/**\n"
"* An unsigned integer type with the property that any valid pointer to\n"
"* void can be converted to this type, then converted back to pointer\n"
"* to void, and the result will compare equal to the original pointer.\n"
"*/\n"
"typedef __UINTPTR_TYPE__ uintptr_t;\n"
"\n"
"// built-in vector data types:\n"
"typedef char char2 __attribute__((ext_vector_type(2)));\n"
"typedef char char3 __attribute__((ext_vector_type(3)));\n"
"typedef char char4 __attribute__((ext_vector_type(4)));\n"
"typedef char char8 __attribute__((ext_vector_type(8)));\n"
"typedef char char16 __attribute__((ext_vector_type(16)));\n"
"typedef uchar uchar2 __attribute__((ext_vector_type(2)));\n"
"typedef uchar uchar3 __attribute__((ext_vector_type(3)));\n"
"typedef uchar uchar4 __attribute__((ext_vector_type(4)));\n"
"typedef uchar uchar8 __attribute__((ext_vector_type(8)));\n"
"typedef uchar uchar16 __attribute__((ext_vector_type(16)));\n"
"typedef short short2 __attribute__((ext_vector_type(2)));\n"
"typedef short short3 __attribute__((ext_vector_type(3)));\n"
"typedef short short4 __attribute__((ext_vector_type(4)));\n"
"typedef short short8 __attribute__((ext_vector_type(8)));\n"
"typedef short short16 __attribute__((ext_vector_type(16)));\n"
"typedef ushort ushort2 __attribute__((ext_vector_type(2)));\n"
"typedef ushort ushort3 __attribute__((ext_vector_type(3)));\n"
"typedef ushort ushort4 __attribute__((ext_vector_type(4)));\n"
"typedef ushort ushort8 __attribute__((ext_vector_type(8)));\n"
"typedef ushort ushort16 __attribute__((ext_vector_type(16)));\n"
"typedef int int2 __attribute__((ext_vector_type(2)));\n"
"typedef int int3 __attribute__((ext_vector_type(3)));\n"
"typedef int int4 __attribute__((ext_vector_type(4)));\n"
"typedef int int8 __attribute__((ext_vector_type(8)));\n"
"typedef int int16 __attribute__((ext_vector_type(16)));\n"
"typedef uint uint2 __attribute__((ext_vector_type(2)));\n"
"typedef uint uint3 __attribute__((ext_vector_type(3)));\n"
"typedef uint uint4 __attribute__((ext_vector_type(4)));\n"
"typedef uint uint8 __attribute__((ext_vector_type(8)));\n"
"typedef uint uint16 __attribute__((ext_vector_type(16)));\n"
"typedef long long2 __attribute__((ext_vector_type(2)));\n"
"typedef long long3 __attribute__((ext_vector_type(3)));\n"
"typedef long long4 __attribute__((ext_vector_type(4)));\n"
"typedef long long8 __attribute__((ext_vector_type(8)));\n"
"typedef long long16 __attribute__((ext_vector_type(16)));\n"
"typedef ulong ulong2 __attribute__((ext_vector_type(2)));\n"
"typedef ulong ulong3 __attribute__((ext_vector_type(3)));\n"
"typedef ulong ulong4 __attribute__((ext_vector_type(4)));\n"
"typedef ulong ulong8 __attribute__((ext_vector_type(8)));\n"
"typedef ulong ulong16 __attribute__((ext_vector_type(16)));\n"
"typedef float float2 __attribute__((ext_vector_type(2)));\n"
"typedef float float3 __attribute__((ext_vector_type(3)));\n"
"typedef float float4 __attribute__((ext_vector_type(4)));\n"
"typedef float float8 __attribute__((ext_vector_type(8)));\n"
"typedef float float16 __attribute__((ext_vector_type(16)));\n"
"#ifdef cl_khr_fp16\n"
"#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n"
"typedef half half2 __attribute__((ext_vector_type(2)));\n"
"typedef half half3 __attribute__((ext_vector_type(3)));\n"
"typedef half half4 __attribute__((ext_vector_type(4)));\n"
"typedef half half8 __attribute__((ext_vector_type(8)));\n"
"typedef half half16 __attribute__((ext_vector_type(16)));\n"
"#endif\n"
"#ifdef cl_khr_fp64\n"
"#if __OPENCL_C_VERSION__ < CL_VERSION_1_2\n"
"#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n"
"#endif\n"
"typedef double double2 __attribute__((ext_vector_type(2)));\n"
"typedef double double3 __attribute__((ext_vector_type(3)));\n"
"typedef double double4 __attribute__((ext_vector_type(4)));\n"
"typedef double double8 __attribute__((ext_vector_type(8)));\n"
"typedef double double16 __attribute__((ext_vector_type(16)));\n"
"#endif\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#define NULL ((void*)0)\n"
"#endif\n"
"\n"
"/**\n"
" * Value of maximum non-infinite single-precision floating-point\n"
" * number.\n"
" */\n"
"#define MAXFLOAT 0x1.fffffep127f\n"
"\n"
"/**\n"
" * A positive float constant expression. HUGE_VALF evaluates\n"
" * to +infinity. Used as an error value returned by the built-in\n"
" * math functions.\n"
" */\n"
"#define HUGE_VALF (__builtin_huge_valf())\n"
"\n"
"/**\n"
" * A positive double constant expression. HUGE_VAL evaluates\n"
" * to +infinity. Used as an error value returned by the built-in\n"
" * math functions.\n"
" */\n"
"#define HUGE_VAL (__builtin_huge_val())\n"
"\n"
"/**\n"
" * A constant expression of type float representing positive or\n"
" * unsigned infinity.\n"
" */\n"
"#define INFINITY (__builtin_inff())\n"
"\n"
"/**\n"
" * A constant expression of type float representing a quiet NaN.\n"
" */\n"
"#define NAN as_float(INT_MAX)\n"
"\n"
"#define FP_ILOGB0    INT_MIN\n"
"#define FP_ILOGBNAN    INT_MAX\n"
"\n"
"#define FLT_DIG 6\n"
"#define FLT_MANT_DIG 24\n"
"#define FLT_MAX_10_EXP +38\n"
"#define FLT_MAX_EXP +128\n"
"#define FLT_MIN_10_EXP -37\n"
"#define FLT_MIN_EXP -125\n"
"#define FLT_RADIX 2\n"
"#define FLT_MAX 0x1.fffffep127f\n"
"#define FLT_MIN 0x1.0p-126f\n"
"#define FLT_EPSILON 0x1.0p-23f\n"
"\n"
"#define M_E_F         2.71828182845904523536028747135266250f\n"
"#define M_LOG2E_F     1.44269504088896340735992468100189214f\n"
"#define M_LOG10E_F    0.434294481903251827651128918916605082f\n"
"#define M_LN2_F       0.693147180559945309417232121458176568f\n"
"#define M_LN10_F      2.30258509299404568401799145468436421f\n"
"#define M_PI_F        3.14159265358979323846264338327950288f\n"
"#define M_PI_2_F      1.57079632679489661923132169163975144f\n"
"#define M_PI_4_F      0.785398163397448309615660845819875721f\n"
"#define M_1_PI_F      0.318309886183790671537767526745028724f\n"
"#define M_2_PI_F      0.636619772367581343075535053490057448f\n"
"#define M_2_SQRTPI_F  1.12837916709551257389615890312154517f\n"
"#define M_SQRT2_F     1.41421356237309504880168872420969808f\n"
"#define M_SQRT1_2_F   0.707106781186547524400844362104849039f\n"
"\n"
"#define DBL_DIG 15\n"
"#define DBL_MANT_DIG 53\n"
"#define DBL_MAX_10_EXP +308\n"
"#define DBL_MAX_EXP +1024\n"
"#define DBL_MIN_10_EXP -307\n"
"#define DBL_MIN_EXP -1021\n"
"#define DBL_RADIX 2\n"
"#define DBL_MAX 0x1.fffffffffffffp1023\n"
"#define DBL_MIN 0x1.0p-1022\n"
"#define DBL_EPSILON 0x1.0p-52\n"
"\n"
"#define M_E           0x1.5bf0a8b145769p+1\n"
"#define M_LOG2E       0x1.71547652b82fep+0\n"
"#define M_LOG10E      0x1.bcb7b1526e50ep-2\n"
"#define M_LN2         0x1.62e42fefa39efp-1\n"
"#define M_LN10        0x1.26bb1bbb55516p+1\n"
"#define M_PI          0x1.921fb54442d18p+1\n"
"#define M_PI_2        0x1.921fb54442d18p+0\n"
"#define M_PI_4        0x1.921fb54442d18p-1\n"
"#define M_1_PI        0x1.45f306dc9c883p-2\n"
"#define M_2_PI        0x1.45f306dc9c883p-1\n"
"#define M_2_SQRTPI    0x1.20dd750429b6dp+0\n"
"#define M_SQRT2       0x1.6a09e667f3bcdp+0\n"
"#define M_SQRT1_2     0x1.6a09e667f3bcdp-1\n"
"\n"
"#ifdef cl_khr_fp16\n"
"\n"
"#define HALF_DIG 3\n"
"#define HALF_MANT_DIG 11\n"
"#define HALF_MAX_10_EXP +4\n"
"#define HALF_MAX_EXP +16\n"
"#define HALF_MIN_10_EXP -4\n"
"#define HALF_MIN_EXP -13\n"
"#define HALF_RADIX 2\n"
"#define HALF_MAX ((0x1.ffcp15h))\n"
"#define HALF_MIN ((0x1.0p-14h))\n"
"#define HALF_EPSILON ((0x1.0p-10h))\n"
"\n"
"#define M_E_H         2.71828182845904523536028747135266250h\n"
"#define M_LOG2E_H     1.44269504088896340735992468100189214h\n"
"#define M_LOG10E_H    0.434294481903251827651128918916605082h\n"
"#define M_LN2_H       0.693147180559945309417232121458176568h\n"
"#define M_LN10_H      2.30258509299404568401799145468436421h\n"
"#define M_PI_H        3.14159265358979323846264338327950288h\n"
"#define M_PI_2_H      1.57079632679489661923132169163975144h\n"
"#define M_PI_4_H      0.785398163397448309615660845819875721h\n"
"#define M_1_PI_H      0.318309886183790671537767526745028724h\n"
"#define M_2_PI_H      0.636619772367581343075535053490057448h\n"
"#define M_2_SQRTPI_H  1.12837916709551257389615890312154517h\n"
"#define M_SQRT2_H     1.41421356237309504880168872420969808h\n"
"#define M_SQRT1_2_H   0.707106781186547524400844362104849039h\n"
"\n"
"#endif //cl_khr_fp16\n"
"\n"
"#define CHAR_BIT    8\n"
"#define SCHAR_MAX  127\n"
"#define SCHAR_MIN  (-128)\n"
"#define UCHAR_MAX  255\n"
"#define CHAR_MAX  SCHAR_MAX\n"
"#define CHAR_MIN  SCHAR_MIN\n"
"#define USHRT_MAX  65535\n"
"#define SHRT_MAX  32767\n"
"#define SHRT_MIN  (-32768)\n"
"#define UINT_MAX  0xffffffff\n"
"#define INT_MAX    2147483647\n"
"#define INT_MIN    (-2147483647-1)\n"
"#define ULONG_MAX  0xffffffffffffffffUL\n"
"#define LONG_MAX  0x7fffffffffffffffL\n"
"#define LONG_MIN  (-0x7fffffffffffffffL-1)\n"
"\n"
"// OpenCL v1.1/1.2/2.0 s6.2.3 - Explicit conversions\n"
"\n"
"char __ovld __cnfn convert_char_rte(char);\n"
"char __ovld __cnfn convert_char_sat_rte(char);\n"
"char __ovld __cnfn convert_char_rtz(char);\n"
"char __ovld __cnfn convert_char_sat_rtz(char);\n"
"char __ovld __cnfn convert_char_rtp(char);\n"
"char __ovld __cnfn convert_char_sat_rtp(char);\n"
"char __ovld __cnfn convert_char_rtn(char);\n"
"char __ovld __cnfn convert_char_sat_rtn(char);\n"
"char __ovld __cnfn convert_char(char);\n"
"char __ovld __cnfn convert_char_sat(char);\n"
"char __ovld __cnfn convert_char_rte(uchar);\n"
"char __ovld __cnfn convert_char_sat_rte(uchar);\n"
"char __ovld __cnfn convert_char_rtz(uchar);\n"
"char __ovld __cnfn convert_char_sat_rtz(uchar);\n"
"char __ovld __cnfn convert_char_rtp(uchar);\n"
"char __ovld __cnfn convert_char_sat_rtp(uchar);\n"
"char __ovld __cnfn convert_char_rtn(uchar);\n"
"char __ovld __cnfn convert_char_sat_rtn(uchar);\n"
"char __ovld __cnfn convert_char(uchar);\n"
"char __ovld __cnfn convert_char_sat(uchar);\n"
"char __ovld __cnfn convert_char_rte(short);\n"
"char __ovld __cnfn convert_char_sat_rte(short);\n"
"char __ovld __cnfn convert_char_rtz(short);\n"
"char __ovld __cnfn convert_char_sat_rtz(short);\n"
"char __ovld __cnfn convert_char_rtp(short);\n"
"char __ovld __cnfn convert_char_sat_rtp(short);\n"
"char __ovld __cnfn convert_char_rtn(short);\n"
"char __ovld __cnfn convert_char_sat_rtn(short);\n"
"char __ovld __cnfn convert_char(short);\n"
"char __ovld __cnfn convert_char_sat(short);\n"
"char __ovld __cnfn convert_char_rte(ushort);\n"
"char __ovld __cnfn convert_char_sat_rte(ushort);\n"
"char __ovld __cnfn convert_char_rtz(ushort);\n"
"char __ovld __cnfn convert_char_sat_rtz(ushort);\n"
"char __ovld __cnfn convert_char_rtp(ushort);\n"
"char __ovld __cnfn convert_char_sat_rtp(ushort);\n"
"char __ovld __cnfn convert_char_rtn(ushort);\n"
"char __ovld __cnfn convert_char_sat_rtn(ushort);\n"
"char __ovld __cnfn convert_char(ushort);\n"
"char __ovld __cnfn convert_char_sat(ushort);\n"
"char __ovld __cnfn convert_char_rte(int);\n"
"char __ovld __cnfn convert_char_sat_rte(int);\n"
"char __ovld __cnfn convert_char_rtz(int);\n"
"char __ovld __cnfn convert_char_sat_rtz(int);\n"
"char __ovld __cnfn convert_char_rtp(int);\n"
"char __ovld __cnfn convert_char_sat_rtp(int);\n"
"char __ovld __cnfn convert_char_rtn(int);\n"
"char __ovld __cnfn convert_char_sat_rtn(int);\n"
"char __ovld __cnfn convert_char(int);\n"
"char __ovld __cnfn convert_char_sat(int);\n"
"char __ovld __cnfn convert_char_rte(uint);\n"
"char __ovld __cnfn convert_char_sat_rte(uint);\n"
"char __ovld __cnfn convert_char_rtz(uint);\n"
"char __ovld __cnfn convert_char_sat_rtz(uint);\n"
"char __ovld __cnfn convert_char_rtp(uint);\n"
"char __ovld __cnfn convert_char_sat_rtp(uint);\n"
"char __ovld __cnfn convert_char_rtn(uint);\n"
"char __ovld __cnfn convert_char_sat_rtn(uint);\n"
"char __ovld __cnfn convert_char(uint);\n"
"char __ovld __cnfn convert_char_sat(uint);\n"
"char __ovld __cnfn convert_char_rte(long);\n"
"char __ovld __cnfn convert_char_sat_rte(long);\n"
"char __ovld __cnfn convert_char_rtz(long);\n"
"char __ovld __cnfn convert_char_sat_rtz(long);\n"
"char __ovld __cnfn convert_char_rtp(long);\n"
"char __ovld __cnfn convert_char_sat_rtp(long);\n"
"char __ovld __cnfn convert_char_rtn(long);\n"
"char __ovld __cnfn convert_char_sat_rtn(long);\n"
"char __ovld __cnfn convert_char(long);\n"
"char __ovld __cnfn convert_char_sat(long);\n"
"char __ovld __cnfn convert_char_rte(ulong);\n"
"char __ovld __cnfn convert_char_sat_rte(ulong);\n"
"char __ovld __cnfn convert_char_rtz(ulong);\n"
"char __ovld __cnfn convert_char_sat_rtz(ulong);\n"
"char __ovld __cnfn convert_char_rtp(ulong);\n"
"char __ovld __cnfn convert_char_sat_rtp(ulong);\n"
"char __ovld __cnfn convert_char_rtn(ulong);\n"
"char __ovld __cnfn convert_char_sat_rtn(ulong);\n"
"char __ovld __cnfn convert_char(ulong);\n"
"char __ovld __cnfn convert_char_sat(ulong);\n"
"char __ovld __cnfn convert_char_rte(float);\n"
"char __ovld __cnfn convert_char_sat_rte(float);\n"
"char __ovld __cnfn convert_char_rtz(float);\n"
"char __ovld __cnfn convert_char_sat_rtz(float);\n"
"char __ovld __cnfn convert_char_rtp(float);\n"
"char __ovld __cnfn convert_char_sat_rtp(float);\n"
"char __ovld __cnfn convert_char_rtn(float);\n"
"char __ovld __cnfn convert_char_sat_rtn(float);\n"
"char __ovld __cnfn convert_char(float);\n"
"char __ovld __cnfn convert_char_sat(float);\n"
"uchar __ovld __cnfn convert_uchar_rte(char);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(char);\n"
"uchar __ovld __cnfn convert_uchar_rtz(char);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(char);\n"
"uchar __ovld __cnfn convert_uchar_rtp(char);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(char);\n"
"uchar __ovld __cnfn convert_uchar_rtn(char);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(char);\n"
"uchar __ovld __cnfn convert_uchar(char);\n"
"uchar __ovld __cnfn convert_uchar_sat(char);\n"
"uchar __ovld __cnfn convert_uchar_rte(uchar);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(uchar);\n"
"uchar __ovld __cnfn convert_uchar_rtz(uchar);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(uchar);\n"
"uchar __ovld __cnfn convert_uchar_rtp(uchar);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(uchar);\n"
"uchar __ovld __cnfn convert_uchar_rtn(uchar);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(uchar);\n"
"uchar __ovld __cnfn convert_uchar(uchar);\n"
"uchar __ovld __cnfn convert_uchar_sat(uchar);\n"
"uchar __ovld __cnfn convert_uchar_rte(short);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(short);\n"
"uchar __ovld __cnfn convert_uchar_rtz(short);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(short);\n"
"uchar __ovld __cnfn convert_uchar_rtp(short);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(short);\n"
"uchar __ovld __cnfn convert_uchar_rtn(short);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(short);\n"
"uchar __ovld __cnfn convert_uchar(short);\n"
"uchar __ovld __cnfn convert_uchar_sat(short);\n"
"uchar __ovld __cnfn convert_uchar_rte(ushort);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(ushort);\n"
"uchar __ovld __cnfn convert_uchar_rtz(ushort);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(ushort);\n"
"uchar __ovld __cnfn convert_uchar_rtp(ushort);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(ushort);\n"
"uchar __ovld __cnfn convert_uchar_rtn(ushort);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(ushort);\n"
"uchar __ovld __cnfn convert_uchar(ushort);\n"
"uchar __ovld __cnfn convert_uchar_sat(ushort);\n"
"uchar __ovld __cnfn convert_uchar_rte(int);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(int);\n"
"uchar __ovld __cnfn convert_uchar_rtz(int);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(int);\n"
"uchar __ovld __cnfn convert_uchar_rtp(int);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(int);\n"
"uchar __ovld __cnfn convert_uchar_rtn(int);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(int);\n"
"uchar __ovld __cnfn convert_uchar(int);\n"
"uchar __ovld __cnfn convert_uchar_sat(int);\n"
"uchar __ovld __cnfn convert_uchar_rte(uint);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(uint);\n"
"uchar __ovld __cnfn convert_uchar_rtz(uint);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(uint);\n"
"uchar __ovld __cnfn convert_uchar_rtp(uint);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(uint);\n"
"uchar __ovld __cnfn convert_uchar_rtn(uint);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(uint);\n"
"uchar __ovld __cnfn convert_uchar(uint);\n"
"uchar __ovld __cnfn convert_uchar_sat(uint);\n"
"uchar __ovld __cnfn convert_uchar_rte(long);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(long);\n"
"uchar __ovld __cnfn convert_uchar_rtz(long);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(long);\n"
"uchar __ovld __cnfn convert_uchar_rtp(long);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(long);\n"
"uchar __ovld __cnfn convert_uchar_rtn(long);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(long);\n"
"uchar __ovld __cnfn convert_uchar(long);\n"
"uchar __ovld __cnfn convert_uchar_sat(long);\n"
"uchar __ovld __cnfn convert_uchar_rte(ulong);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(ulong);\n"
"uchar __ovld __cnfn convert_uchar_rtz(ulong);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(ulong);\n"
"uchar __ovld __cnfn convert_uchar_rtp(ulong);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(ulong);\n"
"uchar __ovld __cnfn convert_uchar_rtn(ulong);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(ulong);\n"
"uchar __ovld __cnfn convert_uchar(ulong);\n"
"uchar __ovld __cnfn convert_uchar_sat(ulong);\n"
"uchar __ovld __cnfn convert_uchar_rte(float);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(float);\n"
"uchar __ovld __cnfn convert_uchar_rtz(float);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(float);\n"
"uchar __ovld __cnfn convert_uchar_rtp(float);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(float);\n"
"uchar __ovld __cnfn convert_uchar_rtn(float);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(float);\n"
"uchar __ovld __cnfn convert_uchar(float);\n"
"uchar __ovld __cnfn convert_uchar_sat(float);\n"
"\n"
"short __ovld __cnfn convert_short_rte(char);\n"
"short __ovld __cnfn convert_short_sat_rte(char);\n"
"short __ovld __cnfn convert_short_rtz(char);\n"
"short __ovld __cnfn convert_short_sat_rtz(char);\n"
"short __ovld __cnfn convert_short_rtp(char);\n"
"short __ovld __cnfn convert_short_sat_rtp(char);\n"
"short __ovld __cnfn convert_short_rtn(char);\n"
"short __ovld __cnfn convert_short_sat_rtn(char);\n"
"short __ovld __cnfn convert_short(char);\n"
"short __ovld __cnfn convert_short_sat(char);\n"
"short __ovld __cnfn convert_short_rte(uchar);\n"
"short __ovld __cnfn convert_short_sat_rte(uchar);\n"
"short __ovld __cnfn convert_short_rtz(uchar);\n"
"short __ovld __cnfn convert_short_sat_rtz(uchar);\n"
"short __ovld __cnfn convert_short_rtp(uchar);\n"
"short __ovld __cnfn convert_short_sat_rtp(uchar);\n"
"short __ovld __cnfn convert_short_rtn(uchar);\n"
"short __ovld __cnfn convert_short_sat_rtn(uchar);\n"
"short __ovld __cnfn convert_short(uchar);\n"
"short __ovld __cnfn convert_short_sat(uchar);\n"
"short __ovld __cnfn convert_short_rte(short);\n"
"short __ovld __cnfn convert_short_sat_rte(short);\n"
"short __ovld __cnfn convert_short_rtz(short);\n"
"short __ovld __cnfn convert_short_sat_rtz(short);\n"
"short __ovld __cnfn convert_short_rtp(short);\n"
"short __ovld __cnfn convert_short_sat_rtp(short);\n"
"short __ovld __cnfn convert_short_rtn(short);\n"
"short __ovld __cnfn convert_short_sat_rtn(short);\n"
"short __ovld __cnfn convert_short(short);\n"
"short __ovld __cnfn convert_short_sat(short);\n"
"short __ovld __cnfn convert_short_rte(ushort);\n"
"short __ovld __cnfn convert_short_sat_rte(ushort);\n"
"short __ovld __cnfn convert_short_rtz(ushort);\n"
"short __ovld __cnfn convert_short_sat_rtz(ushort);\n"
"short __ovld __cnfn convert_short_rtp(ushort);\n"
"short __ovld __cnfn convert_short_sat_rtp(ushort);\n"
"short __ovld __cnfn convert_short_rtn(ushort);\n"
"short __ovld __cnfn convert_short_sat_rtn(ushort);\n"
"short __ovld __cnfn convert_short(ushort);\n"
"short __ovld __cnfn convert_short_sat(ushort);\n"
"short __ovld __cnfn convert_short_rte(int);\n"
"short __ovld __cnfn convert_short_sat_rte(int);\n"
"short __ovld __cnfn convert_short_rtz(int);\n"
"short __ovld __cnfn convert_short_sat_rtz(int);\n"
"short __ovld __cnfn convert_short_rtp(int);\n"
"short __ovld __cnfn convert_short_sat_rtp(int);\n"
"short __ovld __cnfn convert_short_rtn(int);\n"
"short __ovld __cnfn convert_short_sat_rtn(int);\n"
"short __ovld __cnfn convert_short(int);\n"
"short __ovld __cnfn convert_short_sat(int);\n"
"short __ovld __cnfn convert_short_rte(uint);\n"
"short __ovld __cnfn convert_short_sat_rte(uint);\n"
"short __ovld __cnfn convert_short_rtz(uint);\n"
"short __ovld __cnfn convert_short_sat_rtz(uint);\n"
"short __ovld __cnfn convert_short_rtp(uint);\n"
"short __ovld __cnfn convert_short_sat_rtp(uint);\n"
"short __ovld __cnfn convert_short_rtn(uint);\n"
"short __ovld __cnfn convert_short_sat_rtn(uint);\n"
"short __ovld __cnfn convert_short(uint);\n"
"short __ovld __cnfn convert_short_sat(uint);\n"
"short __ovld __cnfn convert_short_rte(long);\n"
"short __ovld __cnfn convert_short_sat_rte(long);\n"
"short __ovld __cnfn convert_short_rtz(long);\n"
"short __ovld __cnfn convert_short_sat_rtz(long);\n"
"short __ovld __cnfn convert_short_rtp(long);\n"
"short __ovld __cnfn convert_short_sat_rtp(long);\n"
"short __ovld __cnfn convert_short_rtn(long);\n"
"short __ovld __cnfn convert_short_sat_rtn(long);\n"
"short __ovld __cnfn convert_short(long);\n"
"short __ovld __cnfn convert_short_sat(long);\n"
"short __ovld __cnfn convert_short_rte(ulong);\n"
"short __ovld __cnfn convert_short_sat_rte(ulong);\n"
"short __ovld __cnfn convert_short_rtz(ulong);\n"
"short __ovld __cnfn convert_short_sat_rtz(ulong);\n"
"short __ovld __cnfn convert_short_rtp(ulong);\n"
"short __ovld __cnfn convert_short_sat_rtp(ulong);\n"
"short __ovld __cnfn convert_short_rtn(ulong);\n"
"short __ovld __cnfn convert_short_sat_rtn(ulong);\n"
"short __ovld __cnfn convert_short(ulong);\n"
"short __ovld __cnfn convert_short_sat(ulong);\n"
"short __ovld __cnfn convert_short_rte(float);\n"
"short __ovld __cnfn convert_short_sat_rte(float);\n"
"short __ovld __cnfn convert_short_rtz(float);\n"
"short __ovld __cnfn convert_short_sat_rtz(float);\n"
"short __ovld __cnfn convert_short_rtp(float);\n"
"short __ovld __cnfn convert_short_sat_rtp(float);\n"
"short __ovld __cnfn convert_short_rtn(float);\n"
"short __ovld __cnfn convert_short_sat_rtn(float);\n"
"short __ovld __cnfn convert_short(float);\n"
"short __ovld __cnfn convert_short_sat(float);\n"
"ushort __ovld __cnfn convert_ushort_rte(char);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(char);\n"
"ushort __ovld __cnfn convert_ushort_rtz(char);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(char);\n"
"ushort __ovld __cnfn convert_ushort_rtp(char);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(char);\n"
"ushort __ovld __cnfn convert_ushort_rtn(char);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(char);\n"
"ushort __ovld __cnfn convert_ushort(char);\n"
"ushort __ovld __cnfn convert_ushort_sat(char);\n"
"ushort __ovld __cnfn convert_ushort_rte(uchar);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(uchar);\n"
"ushort __ovld __cnfn convert_ushort_rtz(uchar);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(uchar);\n"
"ushort __ovld __cnfn convert_ushort_rtp(uchar);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(uchar);\n"
"ushort __ovld __cnfn convert_ushort_rtn(uchar);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(uchar);\n"
"ushort __ovld __cnfn convert_ushort(uchar);\n"
"ushort __ovld __cnfn convert_ushort_sat(uchar);\n"
"ushort __ovld __cnfn convert_ushort_rte(short);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(short);\n"
"ushort __ovld __cnfn convert_ushort_rtz(short);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(short);\n"
"ushort __ovld __cnfn convert_ushort_rtp(short);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(short);\n"
"ushort __ovld __cnfn convert_ushort_rtn(short);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(short);\n"
"ushort __ovld __cnfn convert_ushort(short);\n"
"ushort __ovld __cnfn convert_ushort_sat(short);\n"
"ushort __ovld __cnfn convert_ushort_rte(ushort);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(ushort);\n"
"ushort __ovld __cnfn convert_ushort_rtz(ushort);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(ushort);\n"
"ushort __ovld __cnfn convert_ushort_rtp(ushort);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(ushort);\n"
"ushort __ovld __cnfn convert_ushort_rtn(ushort);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(ushort);\n"
"ushort __ovld __cnfn convert_ushort(ushort);\n"
"ushort __ovld __cnfn convert_ushort_sat(ushort);\n"
"ushort __ovld __cnfn convert_ushort_rte(int);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(int);\n"
"ushort __ovld __cnfn convert_ushort_rtz(int);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(int);\n"
"ushort __ovld __cnfn convert_ushort_rtp(int);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(int);\n"
"ushort __ovld __cnfn convert_ushort_rtn(int);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(int);\n"
"ushort __ovld __cnfn convert_ushort(int);\n"
"ushort __ovld __cnfn convert_ushort_sat(int);\n"
"ushort __ovld __cnfn convert_ushort_rte(uint);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(uint);\n"
"ushort __ovld __cnfn convert_ushort_rtz(uint);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(uint);\n"
"ushort __ovld __cnfn convert_ushort_rtp(uint);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(uint);\n"
"ushort __ovld __cnfn convert_ushort_rtn(uint);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(uint);\n"
"ushort __ovld __cnfn convert_ushort(uint);\n"
"ushort __ovld __cnfn convert_ushort_sat(uint);\n"
"ushort __ovld __cnfn convert_ushort_rte(long);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(long);\n"
"ushort __ovld __cnfn convert_ushort_rtz(long);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(long);\n"
"ushort __ovld __cnfn convert_ushort_rtp(long);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(long);\n"
"ushort __ovld __cnfn convert_ushort_rtn(long);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(long);\n"
"ushort __ovld __cnfn convert_ushort(long);\n"
"ushort __ovld __cnfn convert_ushort_sat(long);\n"
"ushort __ovld __cnfn convert_ushort_rte(ulong);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(ulong);\n"
"ushort __ovld __cnfn convert_ushort_rtz(ulong);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(ulong);\n"
"ushort __ovld __cnfn convert_ushort_rtp(ulong);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(ulong);\n"
"ushort __ovld __cnfn convert_ushort_rtn(ulong);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(ulong);\n"
"ushort __ovld __cnfn convert_ushort(ulong);\n"
"ushort __ovld __cnfn convert_ushort_sat(ulong);\n"
"ushort __ovld __cnfn convert_ushort_rte(float);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(float);\n"
"ushort __ovld __cnfn convert_ushort_rtz(float);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(float);\n"
"ushort __ovld __cnfn convert_ushort_rtp(float);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(float);\n"
"ushort __ovld __cnfn convert_ushort_rtn(float);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(float);\n"
"ushort __ovld __cnfn convert_ushort(float);\n"
"ushort __ovld __cnfn convert_ushort_sat(float);\n"
"int __ovld __cnfn convert_int_rte(char);\n"
"int __ovld __cnfn convert_int_sat_rte(char);\n"
"int __ovld __cnfn convert_int_rtz(char);\n"
"int __ovld __cnfn convert_int_sat_rtz(char);\n"
"int __ovld __cnfn convert_int_rtp(char);\n"
"int __ovld __cnfn convert_int_sat_rtp(char);\n"
"int __ovld __cnfn convert_int_rtn(char);\n"
"int __ovld __cnfn convert_int_sat_rtn(char);\n"
"int __ovld __cnfn convert_int(char);\n"
"int __ovld __cnfn convert_int_sat(char);\n"
"int __ovld __cnfn convert_int_rte(uchar);\n"
"int __ovld __cnfn convert_int_sat_rte(uchar);\n"
"int __ovld __cnfn convert_int_rtz(uchar);\n"
"int __ovld __cnfn convert_int_sat_rtz(uchar);\n"
"int __ovld __cnfn convert_int_rtp(uchar);\n"
"int __ovld __cnfn convert_int_sat_rtp(uchar);\n"
"int __ovld __cnfn convert_int_rtn(uchar);\n"
"int __ovld __cnfn convert_int_sat_rtn(uchar);\n"
"int __ovld __cnfn convert_int(uchar);\n"
"int __ovld __cnfn convert_int_sat(uchar);\n"
"int __ovld __cnfn convert_int_rte(short);\n"
"int __ovld __cnfn convert_int_sat_rte(short);\n"
"int __ovld __cnfn convert_int_rtz(short);\n"
"int __ovld __cnfn convert_int_sat_rtz(short);\n"
"int __ovld __cnfn convert_int_rtp(short);\n"
"int __ovld __cnfn convert_int_sat_rtp(short);\n"
"int __ovld __cnfn convert_int_rtn(short);\n"
"int __ovld __cnfn convert_int_sat_rtn(short);\n"
"int __ovld __cnfn convert_int(short);\n"
"int __ovld __cnfn convert_int_sat(short);\n"
"int __ovld __cnfn convert_int_rte(ushort);\n"
"int __ovld __cnfn convert_int_sat_rte(ushort);\n"
"int __ovld __cnfn convert_int_rtz(ushort);\n"
"int __ovld __cnfn convert_int_sat_rtz(ushort);\n"
"int __ovld __cnfn convert_int_rtp(ushort);\n"
"int __ovld __cnfn convert_int_sat_rtp(ushort);\n"
"int __ovld __cnfn convert_int_rtn(ushort);\n"
"int __ovld __cnfn convert_int_sat_rtn(ushort);\n"
"int __ovld __cnfn convert_int(ushort);\n"
"int __ovld __cnfn convert_int_sat(ushort);\n"
"int __ovld __cnfn convert_int_rte(int);\n"
"int __ovld __cnfn convert_int_sat_rte(int);\n"
"int __ovld __cnfn convert_int_rtz(int);\n"
"int __ovld __cnfn convert_int_sat_rtz(int);\n"
"int __ovld __cnfn convert_int_rtp(int);\n"
"int __ovld __cnfn convert_int_sat_rtp(int);\n"
"int __ovld __cnfn convert_int_rtn(int);\n"
"int __ovld __cnfn convert_int_sat_rtn(int);\n"
"int __ovld __cnfn convert_int(int);\n"
"int __ovld __cnfn convert_int_sat(int);\n"
"int __ovld __cnfn convert_int_rte(uint);\n"
"int __ovld __cnfn convert_int_sat_rte(uint);\n"
"int __ovld __cnfn convert_int_rtz(uint);\n"
"int __ovld __cnfn convert_int_sat_rtz(uint);\n"
"int __ovld __cnfn convert_int_rtp(uint);\n"
"int __ovld __cnfn convert_int_sat_rtp(uint);\n"
"int __ovld __cnfn convert_int_rtn(uint);\n"
"int __ovld __cnfn convert_int_sat_rtn(uint);\n"
"int __ovld __cnfn convert_int(uint);\n"
"int __ovld __cnfn convert_int_sat(uint);\n"
"int __ovld __cnfn convert_int_rte(long);\n"
"int __ovld __cnfn convert_int_sat_rte(long);\n"
"int __ovld __cnfn convert_int_rtz(long);\n"
"int __ovld __cnfn convert_int_sat_rtz(long);\n"
"int __ovld __cnfn convert_int_rtp(long);\n"
"int __ovld __cnfn convert_int_sat_rtp(long);\n"
"int __ovld __cnfn convert_int_rtn(long);\n"
"int __ovld __cnfn convert_int_sat_rtn(long);\n"
"int __ovld __cnfn convert_int(long);\n"
"int __ovld __cnfn convert_int_sat(long);\n"
"int __ovld __cnfn convert_int_rte(ulong);\n"
"int __ovld __cnfn convert_int_sat_rte(ulong);\n"
"int __ovld __cnfn convert_int_rtz(ulong);\n"
"int __ovld __cnfn convert_int_sat_rtz(ulong);\n"
"int __ovld __cnfn convert_int_rtp(ulong);\n"
"int __ovld __cnfn convert_int_sat_rtp(ulong);\n"
"int __ovld __cnfn convert_int_rtn(ulong);\n"
"int __ovld __cnfn convert_int_sat_rtn(ulong);\n"
"int __ovld __cnfn convert_int(ulong);\n"
"int __ovld __cnfn convert_int_sat(ulong);\n"
"int __ovld __cnfn convert_int_rte(float);\n"
"int __ovld __cnfn convert_int_sat_rte(float);\n"
"int __ovld __cnfn convert_int_rtz(float);\n"
"int __ovld __cnfn convert_int_sat_rtz(float);\n"
"int __ovld __cnfn convert_int_rtp(float);\n"
"int __ovld __cnfn convert_int_sat_rtp(float);\n"
"int __ovld __cnfn convert_int_rtn(float);\n"
"int __ovld __cnfn convert_int_sat_rtn(float);\n"
"int __ovld __cnfn convert_int(float);\n"
"int __ovld __cnfn convert_int_sat(float);\n"
"uint __ovld __cnfn convert_uint_rte(char);\n"
"uint __ovld __cnfn convert_uint_sat_rte(char);\n"
"uint __ovld __cnfn convert_uint_rtz(char);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(char);\n"
"uint __ovld __cnfn convert_uint_rtp(char);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(char);\n"
"uint __ovld __cnfn convert_uint_rtn(char);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(char);\n"
"uint __ovld __cnfn convert_uint(char);\n"
"uint __ovld __cnfn convert_uint_sat(char);\n"
"uint __ovld __cnfn convert_uint_rte(uchar);\n"
"uint __ovld __cnfn convert_uint_sat_rte(uchar);\n"
"uint __ovld __cnfn convert_uint_rtz(uchar);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(uchar);\n"
"uint __ovld __cnfn convert_uint_rtp(uchar);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(uchar);\n"
"uint __ovld __cnfn convert_uint_rtn(uchar);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(uchar);\n"
"uint __ovld __cnfn convert_uint(uchar);\n"
"uint __ovld __cnfn convert_uint_sat(uchar);\n"
"uint __ovld __cnfn convert_uint_rte(short);\n"
"uint __ovld __cnfn convert_uint_sat_rte(short);\n"
"uint __ovld __cnfn convert_uint_rtz(short);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(short);\n"
"uint __ovld __cnfn convert_uint_rtp(short);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(short);\n"
"uint __ovld __cnfn convert_uint_rtn(short);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(short);\n"
"uint __ovld __cnfn convert_uint(short);\n"
"uint __ovld __cnfn convert_uint_sat(short);\n"
"uint __ovld __cnfn convert_uint_rte(ushort);\n"
"uint __ovld __cnfn convert_uint_sat_rte(ushort);\n"
"uint __ovld __cnfn convert_uint_rtz(ushort);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(ushort);\n"
"uint __ovld __cnfn convert_uint_rtp(ushort);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(ushort);\n"
"uint __ovld __cnfn convert_uint_rtn(ushort);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(ushort);\n"
"uint __ovld __cnfn convert_uint(ushort);\n"
"uint __ovld __cnfn convert_uint_sat(ushort);\n"
"uint __ovld __cnfn convert_uint_rte(int);\n"
"uint __ovld __cnfn convert_uint_sat_rte(int);\n"
"uint __ovld __cnfn convert_uint_rtz(int);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(int);\n"
"uint __ovld __cnfn convert_uint_rtp(int);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(int);\n"
"uint __ovld __cnfn convert_uint_rtn(int);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(int);\n"
"uint __ovld __cnfn convert_uint(int);\n"
"uint __ovld __cnfn convert_uint_sat(int);\n"
"uint __ovld __cnfn convert_uint_rte(uint);\n"
"uint __ovld __cnfn convert_uint_sat_rte(uint);\n"
"uint __ovld __cnfn convert_uint_rtz(uint);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(uint);\n"
"uint __ovld __cnfn convert_uint_rtp(uint);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(uint);\n"
"uint __ovld __cnfn convert_uint_rtn(uint);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(uint);\n"
"uint __ovld __cnfn convert_uint(uint);\n"
"uint __ovld __cnfn convert_uint_sat(uint);\n"
"uint __ovld __cnfn convert_uint_rte(long);\n"
"uint __ovld __cnfn convert_uint_sat_rte(long);\n"
"uint __ovld __cnfn convert_uint_rtz(long);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(long);\n"
"uint __ovld __cnfn convert_uint_rtp(long);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(long);\n"
"uint __ovld __cnfn convert_uint_rtn(long);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(long);\n"
"uint __ovld __cnfn convert_uint(long);\n"
"uint __ovld __cnfn convert_uint_sat(long);\n"
"uint __ovld __cnfn convert_uint_rte(ulong);\n"
"uint __ovld __cnfn convert_uint_sat_rte(ulong);\n"
"uint __ovld __cnfn convert_uint_rtz(ulong);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(ulong);\n"
"uint __ovld __cnfn convert_uint_rtp(ulong);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(ulong);\n"
"uint __ovld __cnfn convert_uint_rtn(ulong);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(ulong);\n"
"uint __ovld __cnfn convert_uint(ulong);\n"
"uint __ovld __cnfn convert_uint_sat(ulong);\n"
"uint __ovld __cnfn convert_uint_rte(float);\n"
"uint __ovld __cnfn convert_uint_sat_rte(float);\n"
"uint __ovld __cnfn convert_uint_rtz(float);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(float);\n"
"uint __ovld __cnfn convert_uint_rtp(float);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(float);\n"
"uint __ovld __cnfn convert_uint_rtn(float);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(float);\n"
"uint __ovld __cnfn convert_uint(float);\n"
"uint __ovld __cnfn convert_uint_sat(float);\n"
"long __ovld __cnfn convert_long_rte(char);\n"
"long __ovld __cnfn convert_long_sat_rte(char);\n"
"long __ovld __cnfn convert_long_rtz(char);\n"
"long __ovld __cnfn convert_long_sat_rtz(char);\n"
"long __ovld __cnfn convert_long_rtp(char);\n"
"long __ovld __cnfn convert_long_sat_rtp(char);\n"
"long __ovld __cnfn convert_long_rtn(char);\n"
"long __ovld __cnfn convert_long_sat_rtn(char);\n"
"long __ovld __cnfn convert_long(char);\n"
"long __ovld __cnfn convert_long_sat(char);\n"
"long __ovld __cnfn convert_long_rte(uchar);\n"
"long __ovld __cnfn convert_long_sat_rte(uchar);\n"
"long __ovld __cnfn convert_long_rtz(uchar);\n"
"long __ovld __cnfn convert_long_sat_rtz(uchar);\n"
"long __ovld __cnfn convert_long_rtp(uchar);\n"
"long __ovld __cnfn convert_long_sat_rtp(uchar);\n"
"long __ovld __cnfn convert_long_rtn(uchar);\n"
"long __ovld __cnfn convert_long_sat_rtn(uchar);\n"
"long __ovld __cnfn convert_long(uchar);\n"
"long __ovld __cnfn convert_long_sat(uchar);\n"
"long __ovld __cnfn convert_long_rte(short);\n"
"long __ovld __cnfn convert_long_sat_rte(short);\n"
"long __ovld __cnfn convert_long_rtz(short);\n"
"long __ovld __cnfn convert_long_sat_rtz(short);\n"
"long __ovld __cnfn convert_long_rtp(short);\n"
"long __ovld __cnfn convert_long_sat_rtp(short);\n"
"long __ovld __cnfn convert_long_rtn(short);\n"
"long __ovld __cnfn convert_long_sat_rtn(short);\n"
"long __ovld __cnfn convert_long(short);\n"
"long __ovld __cnfn convert_long_sat(short);\n"
"long __ovld __cnfn convert_long_rte(ushort);\n"
"long __ovld __cnfn convert_long_sat_rte(ushort);\n"
"long __ovld __cnfn convert_long_rtz(ushort);\n"
"long __ovld __cnfn convert_long_sat_rtz(ushort);\n"
"long __ovld __cnfn convert_long_rtp(ushort);\n"
"long __ovld __cnfn convert_long_sat_rtp(ushort);\n"
"long __ovld __cnfn convert_long_rtn(ushort);\n"
"long __ovld __cnfn convert_long_sat_rtn(ushort);\n"
"long __ovld __cnfn convert_long(ushort);\n"
"long __ovld __cnfn convert_long_sat(ushort);\n"
"long __ovld __cnfn convert_long_rte(int);\n"
"long __ovld __cnfn convert_long_sat_rte(int);\n"
"long __ovld __cnfn convert_long_rtz(int);\n"
"long __ovld __cnfn convert_long_sat_rtz(int);\n"
"long __ovld __cnfn convert_long_rtp(int);\n"
"long __ovld __cnfn convert_long_sat_rtp(int);\n"
"long __ovld __cnfn convert_long_rtn(int);\n"
"long __ovld __cnfn convert_long_sat_rtn(int);\n"
"long __ovld __cnfn convert_long(int);\n"
"long __ovld __cnfn convert_long_sat(int);\n"
"long __ovld __cnfn convert_long_rte(uint);\n"
"long __ovld __cnfn convert_long_sat_rte(uint);\n"
"long __ovld __cnfn convert_long_rtz(uint);\n"
"long __ovld __cnfn convert_long_sat_rtz(uint);\n"
"long __ovld __cnfn convert_long_rtp(uint);\n"
"long __ovld __cnfn convert_long_sat_rtp(uint);\n"
"long __ovld __cnfn convert_long_rtn(uint);\n"
"long __ovld __cnfn convert_long_sat_rtn(uint);\n"
"long __ovld __cnfn convert_long(uint);\n"
"long __ovld __cnfn convert_long_sat(uint);\n"
"long __ovld __cnfn convert_long_rte(long);\n"
"long __ovld __cnfn convert_long_sat_rte(long);\n"
"long __ovld __cnfn convert_long_rtz(long);\n"
"long __ovld __cnfn convert_long_sat_rtz(long);\n"
"long __ovld __cnfn convert_long_rtp(long);\n"
"long __ovld __cnfn convert_long_sat_rtp(long);\n"
"long __ovld __cnfn convert_long_rtn(long);\n"
"long __ovld __cnfn convert_long_sat_rtn(long);\n"
"long __ovld __cnfn convert_long(long);\n"
"long __ovld __cnfn convert_long_sat(long);\n"
"long __ovld __cnfn convert_long_rte(ulong);\n"
"long __ovld __cnfn convert_long_sat_rte(ulong);\n"
"long __ovld __cnfn convert_long_rtz(ulong);\n"
"long __ovld __cnfn convert_long_sat_rtz(ulong);\n"
"long __ovld __cnfn convert_long_rtp(ulong);\n"
"long __ovld __cnfn convert_long_sat_rtp(ulong);\n"
"long __ovld __cnfn convert_long_rtn(ulong);\n"
"long __ovld __cnfn convert_long_sat_rtn(ulong);\n"
"long __ovld __cnfn convert_long(ulong);\n"
"long __ovld __cnfn convert_long_sat(ulong);\n"
"long __ovld __cnfn convert_long_rte(float);\n"
"long __ovld __cnfn convert_long_sat_rte(float);\n"
"long __ovld __cnfn convert_long_rtz(float);\n"
"long __ovld __cnfn convert_long_sat_rtz(float);\n"
"long __ovld __cnfn convert_long_rtp(float);\n"
"long __ovld __cnfn convert_long_sat_rtp(float);\n"
"long __ovld __cnfn convert_long_rtn(float);\n"
"long __ovld __cnfn convert_long_sat_rtn(float);\n"
"long __ovld __cnfn convert_long(float);\n"
"long __ovld __cnfn convert_long_sat(float);\n"
"ulong __ovld __cnfn convert_ulong_rte(char);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(char);\n"
"ulong __ovld __cnfn convert_ulong_rtz(char);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(char);\n"
"ulong __ovld __cnfn convert_ulong_rtp(char);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(char);\n"
"ulong __ovld __cnfn convert_ulong_rtn(char);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(char);\n"
"ulong __ovld __cnfn convert_ulong(char);\n"
"ulong __ovld __cnfn convert_ulong_sat(char);\n"
"ulong __ovld __cnfn convert_ulong_rte(uchar);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(uchar);\n"
"ulong __ovld __cnfn convert_ulong_rtz(uchar);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(uchar);\n"
"ulong __ovld __cnfn convert_ulong_rtp(uchar);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(uchar);\n"
"ulong __ovld __cnfn convert_ulong_rtn(uchar);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(uchar);\n"
"ulong __ovld __cnfn convert_ulong(uchar);\n"
"ulong __ovld __cnfn convert_ulong_sat(uchar);\n"
"ulong __ovld __cnfn convert_ulong_rte(short);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(short);\n"
"ulong __ovld __cnfn convert_ulong_rtz(short);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(short);\n"
"ulong __ovld __cnfn convert_ulong_rtp(short);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(short);\n"
"ulong __ovld __cnfn convert_ulong_rtn(short);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(short);\n"
"ulong __ovld __cnfn convert_ulong(short);\n"
"ulong __ovld __cnfn convert_ulong_sat(short);\n"
"ulong __ovld __cnfn convert_ulong_rte(ushort);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(ushort);\n"
"ulong __ovld __cnfn convert_ulong_rtz(ushort);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(ushort);\n"
"ulong __ovld __cnfn convert_ulong_rtp(ushort);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(ushort);\n"
"ulong __ovld __cnfn convert_ulong_rtn(ushort);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(ushort);\n"
"ulong __ovld __cnfn convert_ulong(ushort);\n"
"ulong __ovld __cnfn convert_ulong_sat(ushort);\n"
"ulong __ovld __cnfn convert_ulong_rte(int);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(int);\n"
"ulong __ovld __cnfn convert_ulong_rtz(int);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(int);\n"
"ulong __ovld __cnfn convert_ulong_rtp(int);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(int);\n"
"ulong __ovld __cnfn convert_ulong_rtn(int);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(int);\n"
"ulong __ovld __cnfn convert_ulong(int);\n"
"ulong __ovld __cnfn convert_ulong_sat(int);\n"
"ulong __ovld __cnfn convert_ulong_rte(uint);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(uint);\n"
"ulong __ovld __cnfn convert_ulong_rtz(uint);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(uint);\n"
"ulong __ovld __cnfn convert_ulong_rtp(uint);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(uint);\n"
"ulong __ovld __cnfn convert_ulong_rtn(uint);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(uint);\n"
"ulong __ovld __cnfn convert_ulong(uint);\n"
"ulong __ovld __cnfn convert_ulong_sat(uint);\n"
"ulong __ovld __cnfn convert_ulong_rte(long);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(long);\n"
"ulong __ovld __cnfn convert_ulong_rtz(long);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(long);\n"
"ulong __ovld __cnfn convert_ulong_rtp(long);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(long);\n"
"ulong __ovld __cnfn convert_ulong_rtn(long);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(long);\n"
"ulong __ovld __cnfn convert_ulong(long);\n"
"ulong __ovld __cnfn convert_ulong_sat(long);\n"
"ulong __ovld __cnfn convert_ulong_rte(ulong);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(ulong);\n"
"ulong __ovld __cnfn convert_ulong_rtz(ulong);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(ulong);\n"
"ulong __ovld __cnfn convert_ulong_rtp(ulong);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(ulong);\n"
"ulong __ovld __cnfn convert_ulong_rtn(ulong);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(ulong);\n"
"ulong __ovld __cnfn convert_ulong(ulong);\n"
"ulong __ovld __cnfn convert_ulong_sat(ulong);\n"
"ulong __ovld __cnfn convert_ulong_rte(float);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(float);\n"
"ulong __ovld __cnfn convert_ulong_rtz(float);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(float);\n"
"ulong __ovld __cnfn convert_ulong_rtp(float);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(float);\n"
"ulong __ovld __cnfn convert_ulong_rtn(float);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(float);\n"
"ulong __ovld __cnfn convert_ulong(float);\n"
"ulong __ovld __cnfn convert_ulong_sat(float);\n"
"float __ovld __cnfn convert_float_rte(char);\n"
"float __ovld __cnfn convert_float_rtz(char);\n"
"float __ovld __cnfn convert_float_rtp(char);\n"
"float __ovld __cnfn convert_float_rtn(char);\n"
"float __ovld __cnfn convert_float(char);\n"
"float __ovld __cnfn convert_float_rte(uchar);\n"
"float __ovld __cnfn convert_float_rtz(uchar);\n"
"float __ovld __cnfn convert_float_rtp(uchar);\n"
"float __ovld __cnfn convert_float_rtn(uchar);\n"
"float __ovld __cnfn convert_float(uchar);\n"
"float __ovld __cnfn convert_float_rte(short);\n"
"float __ovld __cnfn convert_float_rtz(short);\n"
"float __ovld __cnfn convert_float_rtp(short);\n"
"float __ovld __cnfn convert_float_rtn(short);\n"
"float __ovld __cnfn convert_float(short);\n"
"float __ovld __cnfn convert_float_rte(ushort);\n"
"float __ovld __cnfn convert_float_rtz(ushort);\n"
"float __ovld __cnfn convert_float_rtp(ushort);\n"
"float __ovld __cnfn convert_float_rtn(ushort);\n"
"float __ovld __cnfn convert_float(ushort);\n"
"float __ovld __cnfn convert_float_rte(int);\n"
"float __ovld __cnfn convert_float_rtz(int);\n"
"float __ovld __cnfn convert_float_rtp(int);\n"
"float __ovld __cnfn convert_float_rtn(int);\n"
"float __ovld __cnfn convert_float(int);\n"
"float __ovld __cnfn convert_float_rte(uint);\n"
"float __ovld __cnfn convert_float_rtz(uint);\n"
"float __ovld __cnfn convert_float_rtp(uint);\n"
"float __ovld __cnfn convert_float_rtn(uint);\n"
"float __ovld __cnfn convert_float(uint);\n"
"float __ovld __cnfn convert_float_rte(long);\n"
"float __ovld __cnfn convert_float_rtz(long);\n"
"float __ovld __cnfn convert_float_rtp(long);\n"
"float __ovld __cnfn convert_float_rtn(long);\n"
"float __ovld __cnfn convert_float(long);\n"
"float __ovld __cnfn convert_float_rte(ulong);\n"
"float __ovld __cnfn convert_float_rtz(ulong);\n"
"float __ovld __cnfn convert_float_rtp(ulong);\n"
"float __ovld __cnfn convert_float_rtn(ulong);\n"
"float __ovld __cnfn convert_float(ulong);\n"
"float __ovld __cnfn convert_float_rte(float);\n"
"float __ovld __cnfn convert_float_rtz(float);\n"
"float __ovld __cnfn convert_float_rtp(float);\n"
"float __ovld __cnfn convert_float_rtn(float);\n"
"float __ovld __cnfn convert_float(float);\n"
"char2 __ovld __cnfn convert_char2_rte(char2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(char2);\n"
"char2 __ovld __cnfn convert_char2_rtz(char2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(char2);\n"
"char2 __ovld __cnfn convert_char2_rtp(char2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(char2);\n"
"char2 __ovld __cnfn convert_char2_rtn(char2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(char2);\n"
"char2 __ovld __cnfn convert_char2(char2);\n"
"char2 __ovld __cnfn convert_char2_sat(char2);\n"
"char2 __ovld __cnfn convert_char2_rte(uchar2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(uchar2);\n"
"char2 __ovld __cnfn convert_char2_rtz(uchar2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(uchar2);\n"
"char2 __ovld __cnfn convert_char2_rtp(uchar2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(uchar2);\n"
"char2 __ovld __cnfn convert_char2_rtn(uchar2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(uchar2);\n"
"char2 __ovld __cnfn convert_char2(uchar2);\n"
"char2 __ovld __cnfn convert_char2_sat(uchar2);\n"
"char2 __ovld __cnfn convert_char2_rte(short2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(short2);\n"
"char2 __ovld __cnfn convert_char2_rtz(short2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(short2);\n"
"char2 __ovld __cnfn convert_char2_rtp(short2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(short2);\n"
"char2 __ovld __cnfn convert_char2_rtn(short2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(short2);\n"
"char2 __ovld __cnfn convert_char2(short2);\n"
"char2 __ovld __cnfn convert_char2_sat(short2);\n"
"char2 __ovld __cnfn convert_char2_rte(ushort2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(ushort2);\n"
"char2 __ovld __cnfn convert_char2_rtz(ushort2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(ushort2);\n"
"char2 __ovld __cnfn convert_char2_rtp(ushort2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(ushort2);\n"
"char2 __ovld __cnfn convert_char2_rtn(ushort2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(ushort2);\n"
"char2 __ovld __cnfn convert_char2(ushort2);\n"
"char2 __ovld __cnfn convert_char2_sat(ushort2);\n"
"char2 __ovld __cnfn convert_char2_rte(int2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(int2);\n"
"char2 __ovld __cnfn convert_char2_rtz(int2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(int2);\n"
"char2 __ovld __cnfn convert_char2_rtp(int2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(int2);\n"
"char2 __ovld __cnfn convert_char2_rtn(int2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(int2);\n"
"char2 __ovld __cnfn convert_char2(int2);\n"
"char2 __ovld __cnfn convert_char2_sat(int2);\n"
"char2 __ovld __cnfn convert_char2_rte(uint2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(uint2);\n"
"char2 __ovld __cnfn convert_char2_rtz(uint2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(uint2);\n"
"char2 __ovld __cnfn convert_char2_rtp(uint2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(uint2);\n"
"char2 __ovld __cnfn convert_char2_rtn(uint2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(uint2);\n"
"char2 __ovld __cnfn convert_char2(uint2);\n"
"char2 __ovld __cnfn convert_char2_sat(uint2);\n"
"char2 __ovld __cnfn convert_char2_rte(long2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(long2);\n"
"char2 __ovld __cnfn convert_char2_rtz(long2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(long2);\n"
"char2 __ovld __cnfn convert_char2_rtp(long2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(long2);\n"
"char2 __ovld __cnfn convert_char2_rtn(long2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(long2);\n"
"char2 __ovld __cnfn convert_char2(long2);\n"
"char2 __ovld __cnfn convert_char2_sat(long2);\n"
"char2 __ovld __cnfn convert_char2_rte(ulong2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(ulong2);\n"
"char2 __ovld __cnfn convert_char2_rtz(ulong2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(ulong2);\n"
"char2 __ovld __cnfn convert_char2_rtp(ulong2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(ulong2);\n"
"char2 __ovld __cnfn convert_char2_rtn(ulong2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(ulong2);\n"
"char2 __ovld __cnfn convert_char2(ulong2);\n"
"char2 __ovld __cnfn convert_char2_sat(ulong2);\n"
"char2 __ovld __cnfn convert_char2_rte(float2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(float2);\n"
"char2 __ovld __cnfn convert_char2_rtz(float2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(float2);\n"
"char2 __ovld __cnfn convert_char2_rtp(float2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(float2);\n"
"char2 __ovld __cnfn convert_char2_rtn(float2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(float2);\n"
"char2 __ovld __cnfn convert_char2(float2);\n"
"char2 __ovld __cnfn convert_char2_sat(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(char2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(uchar2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(short2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(ushort2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(int2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(uint2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(long2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(ulong2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2(float2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(float2);\n"
"short2 __ovld __cnfn convert_short2_rte(char2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(char2);\n"
"short2 __ovld __cnfn convert_short2_rtz(char2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(char2);\n"
"short2 __ovld __cnfn convert_short2_rtp(char2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(char2);\n"
"short2 __ovld __cnfn convert_short2_rtn(char2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(char2);\n"
"short2 __ovld __cnfn convert_short2(char2);\n"
"short2 __ovld __cnfn convert_short2_sat(char2);\n"
"short2 __ovld __cnfn convert_short2_rte(uchar2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(uchar2);\n"
"short2 __ovld __cnfn convert_short2_rtz(uchar2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(uchar2);\n"
"short2 __ovld __cnfn convert_short2_rtp(uchar2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(uchar2);\n"
"short2 __ovld __cnfn convert_short2_rtn(uchar2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(uchar2);\n"
"short2 __ovld __cnfn convert_short2(uchar2);\n"
"short2 __ovld __cnfn convert_short2_sat(uchar2);\n"
"short2 __ovld __cnfn convert_short2_rte(short2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(short2);\n"
"short2 __ovld __cnfn convert_short2_rtz(short2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(short2);\n"
"short2 __ovld __cnfn convert_short2_rtp(short2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(short2);\n"
"short2 __ovld __cnfn convert_short2_rtn(short2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(short2);\n"
"short2 __ovld __cnfn convert_short2(short2);\n"
"short2 __ovld __cnfn convert_short2_sat(short2);\n"
"short2 __ovld __cnfn convert_short2_rte(ushort2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(ushort2);\n"
"short2 __ovld __cnfn convert_short2_rtz(ushort2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(ushort2);\n"
"short2 __ovld __cnfn convert_short2_rtp(ushort2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(ushort2);\n"
"short2 __ovld __cnfn convert_short2_rtn(ushort2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(ushort2);\n"
"short2 __ovld __cnfn convert_short2(ushort2);\n"
"short2 __ovld __cnfn convert_short2_sat(ushort2);\n"
"short2 __ovld __cnfn convert_short2_rte(int2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(int2);\n"
"short2 __ovld __cnfn convert_short2_rtz(int2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(int2);\n"
"short2 __ovld __cnfn convert_short2_rtp(int2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(int2);\n"
"short2 __ovld __cnfn convert_short2_rtn(int2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(int2);\n"
"short2 __ovld __cnfn convert_short2(int2);\n"
"short2 __ovld __cnfn convert_short2_sat(int2);\n"
"short2 __ovld __cnfn convert_short2_rte(uint2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(uint2);\n"
"short2 __ovld __cnfn convert_short2_rtz(uint2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(uint2);\n"
"short2 __ovld __cnfn convert_short2_rtp(uint2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(uint2);\n"
"short2 __ovld __cnfn convert_short2_rtn(uint2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(uint2);\n"
"short2 __ovld __cnfn convert_short2(uint2);\n"
"short2 __ovld __cnfn convert_short2_sat(uint2);\n"
"short2 __ovld __cnfn convert_short2_rte(long2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(long2);\n"
"short2 __ovld __cnfn convert_short2_rtz(long2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(long2);\n"
"short2 __ovld __cnfn convert_short2_rtp(long2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(long2);\n"
"short2 __ovld __cnfn convert_short2_rtn(long2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(long2);\n"
"short2 __ovld __cnfn convert_short2(long2);\n"
"short2 __ovld __cnfn convert_short2_sat(long2);\n"
"short2 __ovld __cnfn convert_short2_rte(ulong2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(ulong2);\n"
"short2 __ovld __cnfn convert_short2_rtz(ulong2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(ulong2);\n"
"short2 __ovld __cnfn convert_short2_rtp(ulong2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(ulong2);\n"
"short2 __ovld __cnfn convert_short2_rtn(ulong2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(ulong2);\n"
"short2 __ovld __cnfn convert_short2(ulong2);\n"
"short2 __ovld __cnfn convert_short2_sat(ulong2);\n"
"short2 __ovld __cnfn convert_short2_rte(float2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(float2);\n"
"short2 __ovld __cnfn convert_short2_rtz(float2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(float2);\n"
"short2 __ovld __cnfn convert_short2_rtp(float2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(float2);\n"
"short2 __ovld __cnfn convert_short2_rtn(float2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(float2);\n"
"short2 __ovld __cnfn convert_short2(float2);\n"
"short2 __ovld __cnfn convert_short2_sat(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(char2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(uchar2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(short2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(ushort2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(int2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(uint2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(long2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(ulong2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2(float2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(float2);\n"
"int2 __ovld __cnfn convert_int2_rte(char2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(char2);\n"
"int2 __ovld __cnfn convert_int2_rtz(char2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(char2);\n"
"int2 __ovld __cnfn convert_int2_rtp(char2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(char2);\n"
"int2 __ovld __cnfn convert_int2_rtn(char2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(char2);\n"
"int2 __ovld __cnfn convert_int2(char2);\n"
"int2 __ovld __cnfn convert_int2_sat(char2);\n"
"int2 __ovld __cnfn convert_int2_rte(uchar2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(uchar2);\n"
"int2 __ovld __cnfn convert_int2_rtz(uchar2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(uchar2);\n"
"int2 __ovld __cnfn convert_int2_rtp(uchar2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(uchar2);\n"
"int2 __ovld __cnfn convert_int2_rtn(uchar2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(uchar2);\n"
"int2 __ovld __cnfn convert_int2(uchar2);\n"
"int2 __ovld __cnfn convert_int2_sat(uchar2);\n"
"int2 __ovld __cnfn convert_int2_rte(short2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(short2);\n"
"int2 __ovld __cnfn convert_int2_rtz(short2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(short2);\n"
"int2 __ovld __cnfn convert_int2_rtp(short2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(short2);\n"
"int2 __ovld __cnfn convert_int2_rtn(short2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(short2);\n"
"int2 __ovld __cnfn convert_int2(short2);\n"
"int2 __ovld __cnfn convert_int2_sat(short2);\n"
"int2 __ovld __cnfn convert_int2_rte(ushort2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(ushort2);\n"
"int2 __ovld __cnfn convert_int2_rtz(ushort2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(ushort2);\n"
"int2 __ovld __cnfn convert_int2_rtp(ushort2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(ushort2);\n"
"int2 __ovld __cnfn convert_int2_rtn(ushort2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(ushort2);\n"
"int2 __ovld __cnfn convert_int2(ushort2);\n"
"int2 __ovld __cnfn convert_int2_sat(ushort2);\n"
"int2 __ovld __cnfn convert_int2_rte(int2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(int2);\n"
"int2 __ovld __cnfn convert_int2_rtz(int2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(int2);\n"
"int2 __ovld __cnfn convert_int2_rtp(int2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(int2);\n"
"int2 __ovld __cnfn convert_int2_rtn(int2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(int2);\n"
"int2 __ovld __cnfn convert_int2(int2);\n"
"int2 __ovld __cnfn convert_int2_sat(int2);\n"
"int2 __ovld __cnfn convert_int2_rte(uint2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(uint2);\n"
"int2 __ovld __cnfn convert_int2_rtz(uint2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(uint2);\n"
"int2 __ovld __cnfn convert_int2_rtp(uint2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(uint2);\n"
"int2 __ovld __cnfn convert_int2_rtn(uint2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(uint2);\n"
"int2 __ovld __cnfn convert_int2(uint2);\n"
"int2 __ovld __cnfn convert_int2_sat(uint2);\n"
"int2 __ovld __cnfn convert_int2_rte(long2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(long2);\n"
"int2 __ovld __cnfn convert_int2_rtz(long2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(long2);\n"
"int2 __ovld __cnfn convert_int2_rtp(long2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(long2);\n"
"int2 __ovld __cnfn convert_int2_rtn(long2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(long2);\n"
"int2 __ovld __cnfn convert_int2(long2);\n"
"int2 __ovld __cnfn convert_int2_sat(long2);\n"
"int2 __ovld __cnfn convert_int2_rte(ulong2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(ulong2);\n"
"int2 __ovld __cnfn convert_int2_rtz(ulong2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(ulong2);\n"
"int2 __ovld __cnfn convert_int2_rtp(ulong2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(ulong2);\n"
"int2 __ovld __cnfn convert_int2_rtn(ulong2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(ulong2);\n"
"int2 __ovld __cnfn convert_int2(ulong2);\n"
"int2 __ovld __cnfn convert_int2_sat(ulong2);\n"
"int2 __ovld __cnfn convert_int2_rte(float2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(float2);\n"
"int2 __ovld __cnfn convert_int2_rtz(float2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(float2);\n"
"int2 __ovld __cnfn convert_int2_rtp(float2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(float2);\n"
"int2 __ovld __cnfn convert_int2_rtn(float2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(float2);\n"
"int2 __ovld __cnfn convert_int2(float2);\n"
"int2 __ovld __cnfn convert_int2_sat(float2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(char2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(char2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(char2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(char2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(char2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(char2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(char2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(char2);\n"
"uint2 __ovld __cnfn convert_uint2(char2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(char2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(uchar2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(short2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(short2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(short2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(short2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(short2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(short2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(short2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(short2);\n"
"uint2 __ovld __cnfn convert_uint2(short2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(short2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(ushort2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(int2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(int2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(int2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(int2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(int2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(int2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(int2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(int2);\n"
"uint2 __ovld __cnfn convert_uint2(int2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(int2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(uint2);\n"
"uint2 __ovld __cnfn convert_uint2(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(uint2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(long2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(long2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(long2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(long2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(long2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(long2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(long2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(long2);\n"
"uint2 __ovld __cnfn convert_uint2(long2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(long2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(ulong2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(float2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(float2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(float2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(float2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(float2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(float2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(float2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(float2);\n"
"uint2 __ovld __cnfn convert_uint2(float2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(float2);\n"
"long2 __ovld __cnfn convert_long2_rte(char2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(char2);\n"
"long2 __ovld __cnfn convert_long2_rtz(char2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(char2);\n"
"long2 __ovld __cnfn convert_long2_rtp(char2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(char2);\n"
"long2 __ovld __cnfn convert_long2_rtn(char2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(char2);\n"
"long2 __ovld __cnfn convert_long2(char2);\n"
"long2 __ovld __cnfn convert_long2_sat(char2);\n"
"long2 __ovld __cnfn convert_long2_rte(uchar2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(uchar2);\n"
"long2 __ovld __cnfn convert_long2_rtz(uchar2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(uchar2);\n"
"long2 __ovld __cnfn convert_long2_rtp(uchar2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(uchar2);\n"
"long2 __ovld __cnfn convert_long2_rtn(uchar2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(uchar2);\n"
"long2 __ovld __cnfn convert_long2(uchar2);\n"
"long2 __ovld __cnfn convert_long2_sat(uchar2);\n"
"long2 __ovld __cnfn convert_long2_rte(short2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(short2);\n"
"long2 __ovld __cnfn convert_long2_rtz(short2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(short2);\n"
"long2 __ovld __cnfn convert_long2_rtp(short2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(short2);\n"
"long2 __ovld __cnfn convert_long2_rtn(short2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(short2);\n"
"long2 __ovld __cnfn convert_long2(short2);\n"
"long2 __ovld __cnfn convert_long2_sat(short2);\n"
"long2 __ovld __cnfn convert_long2_rte(ushort2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(ushort2);\n"
"long2 __ovld __cnfn convert_long2_rtz(ushort2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(ushort2);\n"
"long2 __ovld __cnfn convert_long2_rtp(ushort2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(ushort2);\n"
"long2 __ovld __cnfn convert_long2_rtn(ushort2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(ushort2);\n"
"long2 __ovld __cnfn convert_long2(ushort2);\n"
"long2 __ovld __cnfn convert_long2_sat(ushort2);\n"
"long2 __ovld __cnfn convert_long2_rte(int2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(int2);\n"
"long2 __ovld __cnfn convert_long2_rtz(int2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(int2);\n"
"long2 __ovld __cnfn convert_long2_rtp(int2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(int2);\n"
"long2 __ovld __cnfn convert_long2_rtn(int2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(int2);\n"
"long2 __ovld __cnfn convert_long2(int2);\n"
"long2 __ovld __cnfn convert_long2_sat(int2);\n"
"long2 __ovld __cnfn convert_long2_rte(uint2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(uint2);\n"
"long2 __ovld __cnfn convert_long2_rtz(uint2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(uint2);\n"
"long2 __ovld __cnfn convert_long2_rtp(uint2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(uint2);\n"
"long2 __ovld __cnfn convert_long2_rtn(uint2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(uint2);\n"
"long2 __ovld __cnfn convert_long2(uint2);\n"
"long2 __ovld __cnfn convert_long2_sat(uint2);\n"
"long2 __ovld __cnfn convert_long2_rte(long2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(long2);\n"
"long2 __ovld __cnfn convert_long2_rtz(long2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(long2);\n"
"long2 __ovld __cnfn convert_long2_rtp(long2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(long2);\n"
"long2 __ovld __cnfn convert_long2_rtn(long2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(long2);\n"
"long2 __ovld __cnfn convert_long2(long2);\n"
"long2 __ovld __cnfn convert_long2_sat(long2);\n"
"long2 __ovld __cnfn convert_long2_rte(ulong2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(ulong2);\n"
"long2 __ovld __cnfn convert_long2_rtz(ulong2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(ulong2);\n"
"long2 __ovld __cnfn convert_long2_rtp(ulong2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(ulong2);\n"
"long2 __ovld __cnfn convert_long2_rtn(ulong2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(ulong2);\n"
"long2 __ovld __cnfn convert_long2(ulong2);\n"
"long2 __ovld __cnfn convert_long2_sat(ulong2);\n"
"long2 __ovld __cnfn convert_long2_rte(float2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(float2);\n"
"long2 __ovld __cnfn convert_long2_rtz(float2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(float2);\n"
"long2 __ovld __cnfn convert_long2_rtp(float2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(float2);\n"
"long2 __ovld __cnfn convert_long2_rtn(float2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(float2);\n"
"long2 __ovld __cnfn convert_long2(float2);\n"
"long2 __ovld __cnfn convert_long2_sat(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(char2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(uchar2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(short2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(ushort2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(int2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(uint2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(long2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(ulong2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2(float2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(float2);\n"
"float2 __ovld __cnfn convert_float2_rte(char2);\n"
"float2 __ovld __cnfn convert_float2_rtz(char2);\n"
"float2 __ovld __cnfn convert_float2_rtp(char2);\n"
"float2 __ovld __cnfn convert_float2_rtn(char2);\n"
"float2 __ovld __cnfn convert_float2(char2);\n"
"float2 __ovld __cnfn convert_float2_rte(uchar2);\n"
"float2 __ovld __cnfn convert_float2_rtz(uchar2);\n"
"float2 __ovld __cnfn convert_float2_rtp(uchar2);\n"
"float2 __ovld __cnfn convert_float2_rtn(uchar2);\n"
"float2 __ovld __cnfn convert_float2(uchar2);\n"
"float2 __ovld __cnfn convert_float2_rte(short2);\n"
"float2 __ovld __cnfn convert_float2_rtz(short2);\n"
"float2 __ovld __cnfn convert_float2_rtp(short2);\n"
"float2 __ovld __cnfn convert_float2_rtn(short2);\n"
"float2 __ovld __cnfn convert_float2(short2);\n"
"float2 __ovld __cnfn convert_float2_rte(ushort2);\n"
"float2 __ovld __cnfn convert_float2_rtz(ushort2);\n"
"float2 __ovld __cnfn convert_float2_rtp(ushort2);\n"
"float2 __ovld __cnfn convert_float2_rtn(ushort2);\n"
"float2 __ovld __cnfn convert_float2(ushort2);\n"
"float2 __ovld __cnfn convert_float2_rte(int2);\n"
"float2 __ovld __cnfn convert_float2_rtz(int2);\n"
"float2 __ovld __cnfn convert_float2_rtp(int2);\n"
"float2 __ovld __cnfn convert_float2_rtn(int2);\n"
"float2 __ovld __cnfn convert_float2(int2);\n"
"float2 __ovld __cnfn convert_float2_rte(uint2);\n"
"float2 __ovld __cnfn convert_float2_rtz(uint2);\n"
"float2 __ovld __cnfn convert_float2_rtp(uint2);\n"
"float2 __ovld __cnfn convert_float2_rtn(uint2);\n"
"float2 __ovld __cnfn convert_float2(uint2);\n"
"float2 __ovld __cnfn convert_float2_rte(long2);\n"
"float2 __ovld __cnfn convert_float2_rtz(long2);\n"
"float2 __ovld __cnfn convert_float2_rtp(long2);\n"
"float2 __ovld __cnfn convert_float2_rtn(long2);\n"
"float2 __ovld __cnfn convert_float2(long2);\n"
"float2 __ovld __cnfn convert_float2_rte(ulong2);\n"
"float2 __ovld __cnfn convert_float2_rtz(ulong2);\n"
"float2 __ovld __cnfn convert_float2_rtp(ulong2);\n"
"float2 __ovld __cnfn convert_float2_rtn(ulong2);\n"
"float2 __ovld __cnfn convert_float2(ulong2);\n"
"float2 __ovld __cnfn convert_float2_rte(float2);\n"
"float2 __ovld __cnfn convert_float2_rtz(float2);\n"
"float2 __ovld __cnfn convert_float2_rtp(float2);\n"
"float2 __ovld __cnfn convert_float2_rtn(float2);\n"
"float2 __ovld __cnfn convert_float2(float2);\n"
"char3 __ovld __cnfn convert_char3_rte(char3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(char3);\n"
"char3 __ovld __cnfn convert_char3_rtz(char3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(char3);\n"
"char3 __ovld __cnfn convert_char3_rtp(char3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(char3);\n"
"char3 __ovld __cnfn convert_char3_rtn(char3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(char3);\n"
"char3 __ovld __cnfn convert_char3(char3);\n"
"char3 __ovld __cnfn convert_char3_sat(char3);\n"
"char3 __ovld __cnfn convert_char3_rte(uchar3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(uchar3);\n"
"char3 __ovld __cnfn convert_char3_rtz(uchar3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(uchar3);\n"
"char3 __ovld __cnfn convert_char3_rtp(uchar3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(uchar3);\n"
"char3 __ovld __cnfn convert_char3_rtn(uchar3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(uchar3);\n"
"char3 __ovld __cnfn convert_char3(uchar3);\n"
"char3 __ovld __cnfn convert_char3_sat(uchar3);\n"
"char3 __ovld __cnfn convert_char3_rte(short3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(short3);\n"
"char3 __ovld __cnfn convert_char3_rtz(short3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(short3);\n"
"char3 __ovld __cnfn convert_char3_rtp(short3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(short3);\n"
"char3 __ovld __cnfn convert_char3_rtn(short3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(short3);\n"
"char3 __ovld __cnfn convert_char3(short3);\n"
"char3 __ovld __cnfn convert_char3_sat(short3);\n"
"char3 __ovld __cnfn convert_char3_rte(ushort3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(ushort3);\n"
"char3 __ovld __cnfn convert_char3_rtz(ushort3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(ushort3);\n"
"char3 __ovld __cnfn convert_char3_rtp(ushort3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(ushort3);\n"
"char3 __ovld __cnfn convert_char3_rtn(ushort3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(ushort3);\n"
"char3 __ovld __cnfn convert_char3(ushort3);\n"
"char3 __ovld __cnfn convert_char3_sat(ushort3);\n"
"char3 __ovld __cnfn convert_char3_rte(int3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(int3);\n"
"char3 __ovld __cnfn convert_char3_rtz(int3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(int3);\n"
"char3 __ovld __cnfn convert_char3_rtp(int3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(int3);\n"
"char3 __ovld __cnfn convert_char3_rtn(int3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(int3);\n"
"char3 __ovld __cnfn convert_char3(int3);\n"
"char3 __ovld __cnfn convert_char3_sat(int3);\n"
"char3 __ovld __cnfn convert_char3_rte(uint3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(uint3);\n"
"char3 __ovld __cnfn convert_char3_rtz(uint3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(uint3);\n"
"char3 __ovld __cnfn convert_char3_rtp(uint3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(uint3);\n"
"char3 __ovld __cnfn convert_char3_rtn(uint3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(uint3);\n"
"char3 __ovld __cnfn convert_char3(uint3);\n"
"char3 __ovld __cnfn convert_char3_sat(uint3);\n"
"char3 __ovld __cnfn convert_char3_rte(long3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(long3);\n"
"char3 __ovld __cnfn convert_char3_rtz(long3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(long3);\n"
"char3 __ovld __cnfn convert_char3_rtp(long3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(long3);\n"
"char3 __ovld __cnfn convert_char3_rtn(long3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(long3);\n"
"char3 __ovld __cnfn convert_char3(long3);\n"
"char3 __ovld __cnfn convert_char3_sat(long3);\n"
"char3 __ovld __cnfn convert_char3_rte(ulong3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(ulong3);\n"
"char3 __ovld __cnfn convert_char3_rtz(ulong3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(ulong3);\n"
"char3 __ovld __cnfn convert_char3_rtp(ulong3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(ulong3);\n"
"char3 __ovld __cnfn convert_char3_rtn(ulong3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(ulong3);\n"
"char3 __ovld __cnfn convert_char3(ulong3);\n"
"char3 __ovld __cnfn convert_char3_sat(ulong3);\n"
"char3 __ovld __cnfn convert_char3_rte(float3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(float3);\n"
"char3 __ovld __cnfn convert_char3_rtz(float3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(float3);\n"
"char3 __ovld __cnfn convert_char3_rtp(float3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(float3);\n"
"char3 __ovld __cnfn convert_char3_rtn(float3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(float3);\n"
"char3 __ovld __cnfn convert_char3(float3);\n"
"char3 __ovld __cnfn convert_char3_sat(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(char3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(uchar3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(short3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(ushort3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(int3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(uint3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(long3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(ulong3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3(float3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(float3);\n"
"short3 __ovld __cnfn convert_short3_rte(char3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(char3);\n"
"short3 __ovld __cnfn convert_short3_rtz(char3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(char3);\n"
"short3 __ovld __cnfn convert_short3_rtp(char3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(char3);\n"
"short3 __ovld __cnfn convert_short3_rtn(char3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(char3);\n"
"short3 __ovld __cnfn convert_short3(char3);\n"
"short3 __ovld __cnfn convert_short3_sat(char3);\n"
"short3 __ovld __cnfn convert_short3_rte(uchar3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(uchar3);\n"
"short3 __ovld __cnfn convert_short3_rtz(uchar3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(uchar3);\n"
"short3 __ovld __cnfn convert_short3_rtp(uchar3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(uchar3);\n"
"short3 __ovld __cnfn convert_short3_rtn(uchar3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(uchar3);\n"
"short3 __ovld __cnfn convert_short3(uchar3);\n"
"short3 __ovld __cnfn convert_short3_sat(uchar3);\n"
"short3 __ovld __cnfn convert_short3_rte(short3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(short3);\n"
"short3 __ovld __cnfn convert_short3_rtz(short3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(short3);\n"
"short3 __ovld __cnfn convert_short3_rtp(short3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(short3);\n"
"short3 __ovld __cnfn convert_short3_rtn(short3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(short3);\n"
"short3 __ovld __cnfn convert_short3(short3);\n"
"short3 __ovld __cnfn convert_short3_sat(short3);\n"
"short3 __ovld __cnfn convert_short3_rte(ushort3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(ushort3);\n"
"short3 __ovld __cnfn convert_short3_rtz(ushort3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(ushort3);\n"
"short3 __ovld __cnfn convert_short3_rtp(ushort3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(ushort3);\n"
"short3 __ovld __cnfn convert_short3_rtn(ushort3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(ushort3);\n"
"short3 __ovld __cnfn convert_short3(ushort3);\n"
"short3 __ovld __cnfn convert_short3_sat(ushort3);\n"
"short3 __ovld __cnfn convert_short3_rte(int3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(int3);\n"
"short3 __ovld __cnfn convert_short3_rtz(int3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(int3);\n"
"short3 __ovld __cnfn convert_short3_rtp(int3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(int3);\n"
"short3 __ovld __cnfn convert_short3_rtn(int3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(int3);\n"
"short3 __ovld __cnfn convert_short3(int3);\n"
"short3 __ovld __cnfn convert_short3_sat(int3);\n"
"short3 __ovld __cnfn convert_short3_rte(uint3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(uint3);\n"
"short3 __ovld __cnfn convert_short3_rtz(uint3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(uint3);\n"
"short3 __ovld __cnfn convert_short3_rtp(uint3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(uint3);\n"
"short3 __ovld __cnfn convert_short3_rtn(uint3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(uint3);\n"
"short3 __ovld __cnfn convert_short3(uint3);\n"
"short3 __ovld __cnfn convert_short3_sat(uint3);\n"
"short3 __ovld __cnfn convert_short3_rte(long3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(long3);\n"
"short3 __ovld __cnfn convert_short3_rtz(long3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(long3);\n"
"short3 __ovld __cnfn convert_short3_rtp(long3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(long3);\n"
"short3 __ovld __cnfn convert_short3_rtn(long3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(long3);\n"
"short3 __ovld __cnfn convert_short3(long3);\n"
"short3 __ovld __cnfn convert_short3_sat(long3);\n"
"short3 __ovld __cnfn convert_short3_rte(ulong3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(ulong3);\n"
"short3 __ovld __cnfn convert_short3_rtz(ulong3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(ulong3);\n"
"short3 __ovld __cnfn convert_short3_rtp(ulong3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(ulong3);\n"
"short3 __ovld __cnfn convert_short3_rtn(ulong3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(ulong3);\n"
"short3 __ovld __cnfn convert_short3(ulong3);\n"
"short3 __ovld __cnfn convert_short3_sat(ulong3);\n"
"short3 __ovld __cnfn convert_short3_rte(float3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(float3);\n"
"short3 __ovld __cnfn convert_short3_rtz(float3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(float3);\n"
"short3 __ovld __cnfn convert_short3_rtp(float3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(float3);\n"
"short3 __ovld __cnfn convert_short3_rtn(float3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(float3);\n"
"short3 __ovld __cnfn convert_short3(float3);\n"
"short3 __ovld __cnfn convert_short3_sat(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(char3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(uchar3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(short3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(ushort3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(int3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(uint3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(long3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(ulong3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3(float3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(float3);\n"
"int3 __ovld __cnfn convert_int3_rte(char3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(char3);\n"
"int3 __ovld __cnfn convert_int3_rtz(char3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(char3);\n"
"int3 __ovld __cnfn convert_int3_rtp(char3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(char3);\n"
"int3 __ovld __cnfn convert_int3_rtn(char3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(char3);\n"
"int3 __ovld __cnfn convert_int3(char3);\n"
"int3 __ovld __cnfn convert_int3_sat(char3);\n"
"int3 __ovld __cnfn convert_int3_rte(uchar3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(uchar3);\n"
"int3 __ovld __cnfn convert_int3_rtz(uchar3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(uchar3);\n"
"int3 __ovld __cnfn convert_int3_rtp(uchar3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(uchar3);\n"
"int3 __ovld __cnfn convert_int3_rtn(uchar3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(uchar3);\n"
"int3 __ovld __cnfn convert_int3(uchar3);\n"
"int3 __ovld __cnfn convert_int3_sat(uchar3);\n"
"int3 __ovld __cnfn convert_int3_rte(short3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(short3);\n"
"int3 __ovld __cnfn convert_int3_rtz(short3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(short3);\n"
"int3 __ovld __cnfn convert_int3_rtp(short3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(short3);\n"
"int3 __ovld __cnfn convert_int3_rtn(short3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(short3);\n"
"int3 __ovld __cnfn convert_int3(short3);\n"
"int3 __ovld __cnfn convert_int3_sat(short3);\n"
"int3 __ovld __cnfn convert_int3_rte(ushort3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(ushort3);\n"
"int3 __ovld __cnfn convert_int3_rtz(ushort3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(ushort3);\n"
"int3 __ovld __cnfn convert_int3_rtp(ushort3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(ushort3);\n"
"int3 __ovld __cnfn convert_int3_rtn(ushort3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(ushort3);\n"
"int3 __ovld __cnfn convert_int3(ushort3);\n"
"int3 __ovld __cnfn convert_int3_sat(ushort3);\n"
"int3 __ovld __cnfn convert_int3_rte(int3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(int3);\n"
"int3 __ovld __cnfn convert_int3_rtz(int3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(int3);\n"
"int3 __ovld __cnfn convert_int3_rtp(int3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(int3);\n"
"int3 __ovld __cnfn convert_int3_rtn(int3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(int3);\n"
"int3 __ovld __cnfn convert_int3(int3);\n"
"int3 __ovld __cnfn convert_int3_sat(int3);\n"
"int3 __ovld __cnfn convert_int3_rte(uint3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(uint3);\n"
"int3 __ovld __cnfn convert_int3_rtz(uint3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(uint3);\n"
"int3 __ovld __cnfn convert_int3_rtp(uint3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(uint3);\n"
"int3 __ovld __cnfn convert_int3_rtn(uint3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(uint3);\n"
"int3 __ovld __cnfn convert_int3(uint3);\n"
"int3 __ovld __cnfn convert_int3_sat(uint3);\n"
"int3 __ovld __cnfn convert_int3_rte(long3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(long3);\n"
"int3 __ovld __cnfn convert_int3_rtz(long3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(long3);\n"
"int3 __ovld __cnfn convert_int3_rtp(long3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(long3);\n"
"int3 __ovld __cnfn convert_int3_rtn(long3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(long3);\n"
"int3 __ovld __cnfn convert_int3(long3);\n"
"int3 __ovld __cnfn convert_int3_sat(long3);\n"
"int3 __ovld __cnfn convert_int3_rte(ulong3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(ulong3);\n"
"int3 __ovld __cnfn convert_int3_rtz(ulong3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(ulong3);\n"
"int3 __ovld __cnfn convert_int3_rtp(ulong3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(ulong3);\n"
"int3 __ovld __cnfn convert_int3_rtn(ulong3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(ulong3);\n"
"int3 __ovld __cnfn convert_int3(ulong3);\n"
"int3 __ovld __cnfn convert_int3_sat(ulong3);\n"
"int3 __ovld __cnfn convert_int3_rte(float3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(float3);\n"
"int3 __ovld __cnfn convert_int3_rtz(float3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(float3);\n"
"int3 __ovld __cnfn convert_int3_rtp(float3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(float3);\n"
"int3 __ovld __cnfn convert_int3_rtn(float3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(float3);\n"
"int3 __ovld __cnfn convert_int3(float3);\n"
"int3 __ovld __cnfn convert_int3_sat(float3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(char3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(char3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(char3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(char3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(char3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(char3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(char3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(char3);\n"
"uint3 __ovld __cnfn convert_uint3(char3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(char3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(uchar3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(short3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(short3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(short3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(short3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(short3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(short3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(short3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(short3);\n"
"uint3 __ovld __cnfn convert_uint3(short3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(short3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(ushort3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(int3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(int3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(int3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(int3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(int3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(int3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(int3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(int3);\n"
"uint3 __ovld __cnfn convert_uint3(int3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(int3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(uint3);\n"
"uint3 __ovld __cnfn convert_uint3(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(uint3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(long3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(long3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(long3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(long3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(long3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(long3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(long3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(long3);\n"
"uint3 __ovld __cnfn convert_uint3(long3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(long3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(ulong3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(float3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(float3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(float3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(float3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(float3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(float3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(float3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(float3);\n"
"uint3 __ovld __cnfn convert_uint3(float3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(float3);\n"
"long3 __ovld __cnfn convert_long3_rte(char3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(char3);\n"
"long3 __ovld __cnfn convert_long3_rtz(char3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(char3);\n"
"long3 __ovld __cnfn convert_long3_rtp(char3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(char3);\n"
"long3 __ovld __cnfn convert_long3_rtn(char3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(char3);\n"
"long3 __ovld __cnfn convert_long3(char3);\n"
"long3 __ovld __cnfn convert_long3_sat(char3);\n"
"long3 __ovld __cnfn convert_long3_rte(uchar3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(uchar3);\n"
"long3 __ovld __cnfn convert_long3_rtz(uchar3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(uchar3);\n"
"long3 __ovld __cnfn convert_long3_rtp(uchar3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(uchar3);\n"
"long3 __ovld __cnfn convert_long3_rtn(uchar3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(uchar3);\n"
"long3 __ovld __cnfn convert_long3(uchar3);\n"
"long3 __ovld __cnfn convert_long3_sat(uchar3);\n"
"long3 __ovld __cnfn convert_long3_rte(short3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(short3);\n"
"long3 __ovld __cnfn convert_long3_rtz(short3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(short3);\n"
"long3 __ovld __cnfn convert_long3_rtp(short3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(short3);\n"
"long3 __ovld __cnfn convert_long3_rtn(short3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(short3);\n"
"long3 __ovld __cnfn convert_long3(short3);\n"
"long3 __ovld __cnfn convert_long3_sat(short3);\n"
"long3 __ovld __cnfn convert_long3_rte(ushort3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(ushort3);\n"
"long3 __ovld __cnfn convert_long3_rtz(ushort3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(ushort3);\n"
"long3 __ovld __cnfn convert_long3_rtp(ushort3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(ushort3);\n"
"long3 __ovld __cnfn convert_long3_rtn(ushort3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(ushort3);\n"
"long3 __ovld __cnfn convert_long3(ushort3);\n"
"long3 __ovld __cnfn convert_long3_sat(ushort3);\n"
"long3 __ovld __cnfn convert_long3_rte(int3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(int3);\n"
"long3 __ovld __cnfn convert_long3_rtz(int3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(int3);\n"
"long3 __ovld __cnfn convert_long3_rtp(int3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(int3);\n"
"long3 __ovld __cnfn convert_long3_rtn(int3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(int3);\n"
"long3 __ovld __cnfn convert_long3(int3);\n"
"long3 __ovld __cnfn convert_long3_sat(int3);\n"
"long3 __ovld __cnfn convert_long3_rte(uint3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(uint3);\n"
"long3 __ovld __cnfn convert_long3_rtz(uint3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(uint3);\n"
"long3 __ovld __cnfn convert_long3_rtp(uint3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(uint3);\n"
"long3 __ovld __cnfn convert_long3_rtn(uint3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(uint3);\n"
"long3 __ovld __cnfn convert_long3(uint3);\n"
"long3 __ovld __cnfn convert_long3_sat(uint3);\n"
"long3 __ovld __cnfn convert_long3_rte(long3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(long3);\n"
"long3 __ovld __cnfn convert_long3_rtz(long3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(long3);\n"
"long3 __ovld __cnfn convert_long3_rtp(long3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(long3);\n"
"long3 __ovld __cnfn convert_long3_rtn(long3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(long3);\n"
"long3 __ovld __cnfn convert_long3(long3);\n"
"long3 __ovld __cnfn convert_long3_sat(long3);\n"
"long3 __ovld __cnfn convert_long3_rte(ulong3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(ulong3);\n"
"long3 __ovld __cnfn convert_long3_rtz(ulong3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(ulong3);\n"
"long3 __ovld __cnfn convert_long3_rtp(ulong3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(ulong3);\n"
"long3 __ovld __cnfn convert_long3_rtn(ulong3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(ulong3);\n"
"long3 __ovld __cnfn convert_long3(ulong3);\n"
"long3 __ovld __cnfn convert_long3_sat(ulong3);\n"
"long3 __ovld __cnfn convert_long3_rte(float3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(float3);\n"
"long3 __ovld __cnfn convert_long3_rtz(float3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(float3);\n"
"long3 __ovld __cnfn convert_long3_rtp(float3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(float3);\n"
"long3 __ovld __cnfn convert_long3_rtn(float3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(float3);\n"
"long3 __ovld __cnfn convert_long3(float3);\n"
"long3 __ovld __cnfn convert_long3_sat(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(char3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(uchar3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(short3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(ushort3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(int3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(uint3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(long3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(ulong3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3(float3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(float3);\n"
"float3 __ovld __cnfn convert_float3_rte(char3);\n"
"float3 __ovld __cnfn convert_float3_rtz(char3);\n"
"float3 __ovld __cnfn convert_float3_rtp(char3);\n"
"float3 __ovld __cnfn convert_float3_rtn(char3);\n"
"float3 __ovld __cnfn convert_float3(char3);\n"
"float3 __ovld __cnfn convert_float3_rte(uchar3);\n"
"float3 __ovld __cnfn convert_float3_rtz(uchar3);\n"
"float3 __ovld __cnfn convert_float3_rtp(uchar3);\n"
"float3 __ovld __cnfn convert_float3_rtn(uchar3);\n"
"float3 __ovld __cnfn convert_float3(uchar3);\n"
"float3 __ovld __cnfn convert_float3_rte(short3);\n"
"float3 __ovld __cnfn convert_float3_rtz(short3);\n"
"float3 __ovld __cnfn convert_float3_rtp(short3);\n"
"float3 __ovld __cnfn convert_float3_rtn(short3);\n"
"float3 __ovld __cnfn convert_float3(short3);\n"
"float3 __ovld __cnfn convert_float3_rte(ushort3);\n"
"float3 __ovld __cnfn convert_float3_rtz(ushort3);\n"
"float3 __ovld __cnfn convert_float3_rtp(ushort3);\n"
"float3 __ovld __cnfn convert_float3_rtn(ushort3);\n"
"float3 __ovld __cnfn convert_float3(ushort3);\n"
"float3 __ovld __cnfn convert_float3_rte(int3);\n"
"float3 __ovld __cnfn convert_float3_rtz(int3);\n"
"float3 __ovld __cnfn convert_float3_rtp(int3);\n"
"float3 __ovld __cnfn convert_float3_rtn(int3);\n"
"float3 __ovld __cnfn convert_float3(int3);\n"
"float3 __ovld __cnfn convert_float3_rte(uint3);\n"
"float3 __ovld __cnfn convert_float3_rtz(uint3);\n"
"float3 __ovld __cnfn convert_float3_rtp(uint3);\n"
"float3 __ovld __cnfn convert_float3_rtn(uint3);\n"
"float3 __ovld __cnfn convert_float3(uint3);\n"
"float3 __ovld __cnfn convert_float3_rte(long3);\n"
"float3 __ovld __cnfn convert_float3_rtz(long3);\n"
"float3 __ovld __cnfn convert_float3_rtp(long3);\n"
"float3 __ovld __cnfn convert_float3_rtn(long3);\n"
"float3 __ovld __cnfn convert_float3(long3);\n"
"float3 __ovld __cnfn convert_float3_rte(ulong3);\n"
"float3 __ovld __cnfn convert_float3_rtz(ulong3);\n"
"float3 __ovld __cnfn convert_float3_rtp(ulong3);\n"
"float3 __ovld __cnfn convert_float3_rtn(ulong3);\n"
"float3 __ovld __cnfn convert_float3(ulong3);\n"
"float3 __ovld __cnfn convert_float3_rte(float3);\n"
"float3 __ovld __cnfn convert_float3_rtz(float3);\n"
"float3 __ovld __cnfn convert_float3_rtp(float3);\n"
"float3 __ovld __cnfn convert_float3_rtn(float3);\n"
"float3 __ovld __cnfn convert_float3(float3);\n"
"char4 __ovld __cnfn convert_char4_rte(char4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(char4);\n"
"char4 __ovld __cnfn convert_char4_rtz(char4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(char4);\n"
"char4 __ovld __cnfn convert_char4_rtp(char4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(char4);\n"
"char4 __ovld __cnfn convert_char4_rtn(char4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(char4);\n"
"char4 __ovld __cnfn convert_char4(char4);\n"
"char4 __ovld __cnfn convert_char4_sat(char4);\n"
"char4 __ovld __cnfn convert_char4_rte(uchar4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(uchar4);\n"
"char4 __ovld __cnfn convert_char4_rtz(uchar4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(uchar4);\n"
"char4 __ovld __cnfn convert_char4_rtp(uchar4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(uchar4);\n"
"char4 __ovld __cnfn convert_char4_rtn(uchar4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(uchar4);\n"
"char4 __ovld __cnfn convert_char4(uchar4);\n"
"char4 __ovld __cnfn convert_char4_sat(uchar4);\n"
"char4 __ovld __cnfn convert_char4_rte(short4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(short4);\n"
"char4 __ovld __cnfn convert_char4_rtz(short4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(short4);\n"
"char4 __ovld __cnfn convert_char4_rtp(short4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(short4);\n"
"char4 __ovld __cnfn convert_char4_rtn(short4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(short4);\n"
"char4 __ovld __cnfn convert_char4(short4);\n"
"char4 __ovld __cnfn convert_char4_sat(short4);\n"
"char4 __ovld __cnfn convert_char4_rte(ushort4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(ushort4);\n"
"char4 __ovld __cnfn convert_char4_rtz(ushort4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(ushort4);\n"
"char4 __ovld __cnfn convert_char4_rtp(ushort4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(ushort4);\n"
"char4 __ovld __cnfn convert_char4_rtn(ushort4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(ushort4);\n"
"char4 __ovld __cnfn convert_char4(ushort4);\n"
"char4 __ovld __cnfn convert_char4_sat(ushort4);\n"
"char4 __ovld __cnfn convert_char4_rte(int4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(int4);\n"
"char4 __ovld __cnfn convert_char4_rtz(int4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(int4);\n"
"char4 __ovld __cnfn convert_char4_rtp(int4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(int4);\n"
"char4 __ovld __cnfn convert_char4_rtn(int4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(int4);\n"
"char4 __ovld __cnfn convert_char4(int4);\n"
"char4 __ovld __cnfn convert_char4_sat(int4);\n"
"char4 __ovld __cnfn convert_char4_rte(uint4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(uint4);\n"
"char4 __ovld __cnfn convert_char4_rtz(uint4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(uint4);\n"
"char4 __ovld __cnfn convert_char4_rtp(uint4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(uint4);\n"
"char4 __ovld __cnfn convert_char4_rtn(uint4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(uint4);\n"
"char4 __ovld __cnfn convert_char4(uint4);\n"
"char4 __ovld __cnfn convert_char4_sat(uint4);\n"
"char4 __ovld __cnfn convert_char4_rte(long4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(long4);\n"
"char4 __ovld __cnfn convert_char4_rtz(long4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(long4);\n"
"char4 __ovld __cnfn convert_char4_rtp(long4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(long4);\n"
"char4 __ovld __cnfn convert_char4_rtn(long4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(long4);\n"
"char4 __ovld __cnfn convert_char4(long4);\n"
"char4 __ovld __cnfn convert_char4_sat(long4);\n"
"char4 __ovld __cnfn convert_char4_rte(ulong4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(ulong4);\n"
"char4 __ovld __cnfn convert_char4_rtz(ulong4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(ulong4);\n"
"char4 __ovld __cnfn convert_char4_rtp(ulong4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(ulong4);\n"
"char4 __ovld __cnfn convert_char4_rtn(ulong4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(ulong4);\n"
"char4 __ovld __cnfn convert_char4(ulong4);\n"
"char4 __ovld __cnfn convert_char4_sat(ulong4);\n"
"char4 __ovld __cnfn convert_char4_rte(float4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(float4);\n"
"char4 __ovld __cnfn convert_char4_rtz(float4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(float4);\n"
"char4 __ovld __cnfn convert_char4_rtp(float4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(float4);\n"
"char4 __ovld __cnfn convert_char4_rtn(float4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(float4);\n"
"char4 __ovld __cnfn convert_char4(float4);\n"
"char4 __ovld __cnfn convert_char4_sat(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(char4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(uchar4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(short4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(ushort4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(int4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(uint4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(long4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(ulong4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4(float4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(float4);\n"
"short4 __ovld __cnfn convert_short4_rte(char4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(char4);\n"
"short4 __ovld __cnfn convert_short4_rtz(char4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(char4);\n"
"short4 __ovld __cnfn convert_short4_rtp(char4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(char4);\n"
"short4 __ovld __cnfn convert_short4_rtn(char4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(char4);\n"
"short4 __ovld __cnfn convert_short4(char4);\n"
"short4 __ovld __cnfn convert_short4_sat(char4);\n"
"short4 __ovld __cnfn convert_short4_rte(uchar4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(uchar4);\n"
"short4 __ovld __cnfn convert_short4_rtz(uchar4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(uchar4);\n"
"short4 __ovld __cnfn convert_short4_rtp(uchar4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(uchar4);\n"
"short4 __ovld __cnfn convert_short4_rtn(uchar4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(uchar4);\n"
"short4 __ovld __cnfn convert_short4(uchar4);\n"
"short4 __ovld __cnfn convert_short4_sat(uchar4);\n"
"short4 __ovld __cnfn convert_short4_rte(short4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(short4);\n"
"short4 __ovld __cnfn convert_short4_rtz(short4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(short4);\n"
"short4 __ovld __cnfn convert_short4_rtp(short4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(short4);\n"
"short4 __ovld __cnfn convert_short4_rtn(short4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(short4);\n"
"short4 __ovld __cnfn convert_short4(short4);\n"
"short4 __ovld __cnfn convert_short4_sat(short4);\n"
"short4 __ovld __cnfn convert_short4_rte(ushort4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(ushort4);\n"
"short4 __ovld __cnfn convert_short4_rtz(ushort4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(ushort4);\n"
"short4 __ovld __cnfn convert_short4_rtp(ushort4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(ushort4);\n"
"short4 __ovld __cnfn convert_short4_rtn(ushort4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(ushort4);\n"
"short4 __ovld __cnfn convert_short4(ushort4);\n"
"short4 __ovld __cnfn convert_short4_sat(ushort4);\n"
"short4 __ovld __cnfn convert_short4_rte(int4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(int4);\n"
"short4 __ovld __cnfn convert_short4_rtz(int4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(int4);\n"
"short4 __ovld __cnfn convert_short4_rtp(int4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(int4);\n"
"short4 __ovld __cnfn convert_short4_rtn(int4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(int4);\n"
"short4 __ovld __cnfn convert_short4(int4);\n"
"short4 __ovld __cnfn convert_short4_sat(int4);\n"
"short4 __ovld __cnfn convert_short4_rte(uint4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(uint4);\n"
"short4 __ovld __cnfn convert_short4_rtz(uint4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(uint4);\n"
"short4 __ovld __cnfn convert_short4_rtp(uint4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(uint4);\n"
"short4 __ovld __cnfn convert_short4_rtn(uint4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(uint4);\n"
"short4 __ovld __cnfn convert_short4(uint4);\n"
"short4 __ovld __cnfn convert_short4_sat(uint4);\n"
"short4 __ovld __cnfn convert_short4_rte(long4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(long4);\n"
"short4 __ovld __cnfn convert_short4_rtz(long4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(long4);\n"
"short4 __ovld __cnfn convert_short4_rtp(long4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(long4);\n"
"short4 __ovld __cnfn convert_short4_rtn(long4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(long4);\n"
"short4 __ovld __cnfn convert_short4(long4);\n"
"short4 __ovld __cnfn convert_short4_sat(long4);\n"
"short4 __ovld __cnfn convert_short4_rte(ulong4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(ulong4);\n"
"short4 __ovld __cnfn convert_short4_rtz(ulong4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(ulong4);\n"
"short4 __ovld __cnfn convert_short4_rtp(ulong4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(ulong4);\n"
"short4 __ovld __cnfn convert_short4_rtn(ulong4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(ulong4);\n"
"short4 __ovld __cnfn convert_short4(ulong4);\n"
"short4 __ovld __cnfn convert_short4_sat(ulong4);\n"
"short4 __ovld __cnfn convert_short4_rte(float4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(float4);\n"
"short4 __ovld __cnfn convert_short4_rtz(float4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(float4);\n"
"short4 __ovld __cnfn convert_short4_rtp(float4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(float4);\n"
"short4 __ovld __cnfn convert_short4_rtn(float4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(float4);\n"
"short4 __ovld __cnfn convert_short4(float4);\n"
"short4 __ovld __cnfn convert_short4_sat(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(char4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(uchar4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(short4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(ushort4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(int4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(uint4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(long4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(ulong4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4(float4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(float4);\n"
"int4 __ovld __cnfn convert_int4_rte(char4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(char4);\n"
"int4 __ovld __cnfn convert_int4_rtz(char4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(char4);\n"
"int4 __ovld __cnfn convert_int4_rtp(char4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(char4);\n"
"int4 __ovld __cnfn convert_int4_rtn(char4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(char4);\n"
"int4 __ovld __cnfn convert_int4(char4);\n"
"int4 __ovld __cnfn convert_int4_sat(char4);\n"
"int4 __ovld __cnfn convert_int4_rte(uchar4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(uchar4);\n"
"int4 __ovld __cnfn convert_int4_rtz(uchar4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(uchar4);\n"
"int4 __ovld __cnfn convert_int4_rtp(uchar4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(uchar4);\n"
"int4 __ovld __cnfn convert_int4_rtn(uchar4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(uchar4);\n"
"int4 __ovld __cnfn convert_int4(uchar4);\n"
"int4 __ovld __cnfn convert_int4_sat(uchar4);\n"
"int4 __ovld __cnfn convert_int4_rte(short4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(short4);\n"
"int4 __ovld __cnfn convert_int4_rtz(short4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(short4);\n"
"int4 __ovld __cnfn convert_int4_rtp(short4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(short4);\n"
"int4 __ovld __cnfn convert_int4_rtn(short4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(short4);\n"
"int4 __ovld __cnfn convert_int4(short4);\n"
"int4 __ovld __cnfn convert_int4_sat(short4);\n"
"int4 __ovld __cnfn convert_int4_rte(ushort4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(ushort4);\n"
"int4 __ovld __cnfn convert_int4_rtz(ushort4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(ushort4);\n"
"int4 __ovld __cnfn convert_int4_rtp(ushort4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(ushort4);\n"
"int4 __ovld __cnfn convert_int4_rtn(ushort4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(ushort4);\n"
"int4 __ovld __cnfn convert_int4(ushort4);\n"
"int4 __ovld __cnfn convert_int4_sat(ushort4);\n"
"int4 __ovld __cnfn convert_int4_rte(int4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(int4);\n"
"int4 __ovld __cnfn convert_int4_rtz(int4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(int4);\n"
"int4 __ovld __cnfn convert_int4_rtp(int4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(int4);\n"
"int4 __ovld __cnfn convert_int4_rtn(int4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(int4);\n"
"int4 __ovld __cnfn convert_int4(int4);\n"
"int4 __ovld __cnfn convert_int4_sat(int4);\n"
"int4 __ovld __cnfn convert_int4_rte(uint4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(uint4);\n"
"int4 __ovld __cnfn convert_int4_rtz(uint4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(uint4);\n"
"int4 __ovld __cnfn convert_int4_rtp(uint4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(uint4);\n"
"int4 __ovld __cnfn convert_int4_rtn(uint4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(uint4);\n"
"int4 __ovld __cnfn convert_int4(uint4);\n"
"int4 __ovld __cnfn convert_int4_sat(uint4);\n"
"int4 __ovld __cnfn convert_int4_rte(long4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(long4);\n"
"int4 __ovld __cnfn convert_int4_rtz(long4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(long4);\n"
"int4 __ovld __cnfn convert_int4_rtp(long4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(long4);\n"
"int4 __ovld __cnfn convert_int4_rtn(long4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(long4);\n"
"int4 __ovld __cnfn convert_int4(long4);\n"
"int4 __ovld __cnfn convert_int4_sat(long4);\n"
"int4 __ovld __cnfn convert_int4_rte(ulong4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(ulong4);\n"
"int4 __ovld __cnfn convert_int4_rtz(ulong4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(ulong4);\n"
"int4 __ovld __cnfn convert_int4_rtp(ulong4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(ulong4);\n"
"int4 __ovld __cnfn convert_int4_rtn(ulong4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(ulong4);\n"
"int4 __ovld __cnfn convert_int4(ulong4);\n"
"int4 __ovld __cnfn convert_int4_sat(ulong4);\n"
"int4 __ovld __cnfn convert_int4_rte(float4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(float4);\n"
"int4 __ovld __cnfn convert_int4_rtz(float4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(float4);\n"
"int4 __ovld __cnfn convert_int4_rtp(float4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(float4);\n"
"int4 __ovld __cnfn convert_int4_rtn(float4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(float4);\n"
"int4 __ovld __cnfn convert_int4(float4);\n"
"int4 __ovld __cnfn convert_int4_sat(float4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(char4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(char4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(char4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(char4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(char4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(char4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(char4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(char4);\n"
"uint4 __ovld __cnfn convert_uint4(char4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(char4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(uchar4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(short4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(short4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(short4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(short4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(short4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(short4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(short4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(short4);\n"
"uint4 __ovld __cnfn convert_uint4(short4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(short4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(ushort4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(int4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(int4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(int4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(int4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(int4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(int4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(int4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(int4);\n"
"uint4 __ovld __cnfn convert_uint4(int4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(int4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(uint4);\n"
"uint4 __ovld __cnfn convert_uint4(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(uint4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(long4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(long4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(long4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(long4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(long4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(long4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(long4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(long4);\n"
"uint4 __ovld __cnfn convert_uint4(long4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(long4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(ulong4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(float4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(float4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(float4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(float4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(float4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(float4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(float4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(float4);\n"
"uint4 __ovld __cnfn convert_uint4(float4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(float4);\n"
"long4 __ovld __cnfn convert_long4_rte(char4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(char4);\n"
"long4 __ovld __cnfn convert_long4_rtz(char4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(char4);\n"
"long4 __ovld __cnfn convert_long4_rtp(char4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(char4);\n"
"long4 __ovld __cnfn convert_long4_rtn(char4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(char4);\n"
"long4 __ovld __cnfn convert_long4(char4);\n"
"long4 __ovld __cnfn convert_long4_sat(char4);\n"
"long4 __ovld __cnfn convert_long4_rte(uchar4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(uchar4);\n"
"long4 __ovld __cnfn convert_long4_rtz(uchar4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(uchar4);\n"
"long4 __ovld __cnfn convert_long4_rtp(uchar4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(uchar4);\n"
"long4 __ovld __cnfn convert_long4_rtn(uchar4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(uchar4);\n"
"long4 __ovld __cnfn convert_long4(uchar4);\n"
"long4 __ovld __cnfn convert_long4_sat(uchar4);\n"
"long4 __ovld __cnfn convert_long4_rte(short4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(short4);\n"
"long4 __ovld __cnfn convert_long4_rtz(short4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(short4);\n"
"long4 __ovld __cnfn convert_long4_rtp(short4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(short4);\n"
"long4 __ovld __cnfn convert_long4_rtn(short4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(short4);\n"
"long4 __ovld __cnfn convert_long4(short4);\n"
"long4 __ovld __cnfn convert_long4_sat(short4);\n"
"long4 __ovld __cnfn convert_long4_rte(ushort4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(ushort4);\n"
"long4 __ovld __cnfn convert_long4_rtz(ushort4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(ushort4);\n"
"long4 __ovld __cnfn convert_long4_rtp(ushort4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(ushort4);\n"
"long4 __ovld __cnfn convert_long4_rtn(ushort4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(ushort4);\n"
"long4 __ovld __cnfn convert_long4(ushort4);\n"
"long4 __ovld __cnfn convert_long4_sat(ushort4);\n"
"long4 __ovld __cnfn convert_long4_rte(int4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(int4);\n"
"long4 __ovld __cnfn convert_long4_rtz(int4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(int4);\n"
"long4 __ovld __cnfn convert_long4_rtp(int4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(int4);\n"
"long4 __ovld __cnfn convert_long4_rtn(int4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(int4);\n"
"long4 __ovld __cnfn convert_long4(int4);\n"
"long4 __ovld __cnfn convert_long4_sat(int4);\n"
"long4 __ovld __cnfn convert_long4_rte(uint4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(uint4);\n"
"long4 __ovld __cnfn convert_long4_rtz(uint4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(uint4);\n"
"long4 __ovld __cnfn convert_long4_rtp(uint4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(uint4);\n"
"long4 __ovld __cnfn convert_long4_rtn(uint4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(uint4);\n"
"long4 __ovld __cnfn convert_long4(uint4);\n"
"long4 __ovld __cnfn convert_long4_sat(uint4);\n"
"long4 __ovld __cnfn convert_long4_rte(long4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(long4);\n"
"long4 __ovld __cnfn convert_long4_rtz(long4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(long4);\n"
"long4 __ovld __cnfn convert_long4_rtp(long4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(long4);\n"
"long4 __ovld __cnfn convert_long4_rtn(long4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(long4);\n"
"long4 __ovld __cnfn convert_long4(long4);\n"
"long4 __ovld __cnfn convert_long4_sat(long4);\n"
"long4 __ovld __cnfn convert_long4_rte(ulong4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(ulong4);\n"
"long4 __ovld __cnfn convert_long4_rtz(ulong4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(ulong4);\n"
"long4 __ovld __cnfn convert_long4_rtp(ulong4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(ulong4);\n"
"long4 __ovld __cnfn convert_long4_rtn(ulong4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(ulong4);\n"
"long4 __ovld __cnfn convert_long4(ulong4);\n"
"long4 __ovld __cnfn convert_long4_sat(ulong4);\n"
"long4 __ovld __cnfn convert_long4_rte(float4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(float4);\n"
"long4 __ovld __cnfn convert_long4_rtz(float4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(float4);\n"
"long4 __ovld __cnfn convert_long4_rtp(float4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(float4);\n"
"long4 __ovld __cnfn convert_long4_rtn(float4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(float4);\n"
"long4 __ovld __cnfn convert_long4(float4);\n"
"long4 __ovld __cnfn convert_long4_sat(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(char4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(uchar4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(short4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(ushort4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(int4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(uint4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(long4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(ulong4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4(float4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(float4);\n"
"float4 __ovld __cnfn convert_float4_rte(char4);\n"
"float4 __ovld __cnfn convert_float4_rtz(char4);\n"
"float4 __ovld __cnfn convert_float4_rtp(char4);\n"
"float4 __ovld __cnfn convert_float4_rtn(char4);\n"
"float4 __ovld __cnfn convert_float4(char4);\n"
"float4 __ovld __cnfn convert_float4_rte(uchar4);\n"
"float4 __ovld __cnfn convert_float4_rtz(uchar4);\n"
"float4 __ovld __cnfn convert_float4_rtp(uchar4);\n"
"float4 __ovld __cnfn convert_float4_rtn(uchar4);\n"
"float4 __ovld __cnfn convert_float4(uchar4);\n"
"float4 __ovld __cnfn convert_float4_rte(short4);\n"
"float4 __ovld __cnfn convert_float4_rtz(short4);\n"
"float4 __ovld __cnfn convert_float4_rtp(short4);\n"
"float4 __ovld __cnfn convert_float4_rtn(short4);\n"
"float4 __ovld __cnfn convert_float4(short4);\n"
"float4 __ovld __cnfn convert_float4_rte(ushort4);\n"
"float4 __ovld __cnfn convert_float4_rtz(ushort4);\n"
"float4 __ovld __cnfn convert_float4_rtp(ushort4);\n"
"float4 __ovld __cnfn convert_float4_rtn(ushort4);\n"
"float4 __ovld __cnfn convert_float4(ushort4);\n"
"float4 __ovld __cnfn convert_float4_rte(int4);\n"
"float4 __ovld __cnfn convert_float4_rtz(int4);\n"
"float4 __ovld __cnfn convert_float4_rtp(int4);\n"
"float4 __ovld __cnfn convert_float4_rtn(int4);\n"
"float4 __ovld __cnfn convert_float4(int4);\n"
"float4 __ovld __cnfn convert_float4_rte(uint4);\n"
"float4 __ovld __cnfn convert_float4_rtz(uint4);\n"
"float4 __ovld __cnfn convert_float4_rtp(uint4);\n"
"float4 __ovld __cnfn convert_float4_rtn(uint4);\n"
"float4 __ovld __cnfn convert_float4(uint4);\n"
"float4 __ovld __cnfn convert_float4_rte(long4);\n"
"float4 __ovld __cnfn convert_float4_rtz(long4);\n"
"float4 __ovld __cnfn convert_float4_rtp(long4);\n"
"float4 __ovld __cnfn convert_float4_rtn(long4);\n"
"float4 __ovld __cnfn convert_float4(long4);\n"
"float4 __ovld __cnfn convert_float4_rte(ulong4);\n"
"float4 __ovld __cnfn convert_float4_rtz(ulong4);\n"
"float4 __ovld __cnfn convert_float4_rtp(ulong4);\n"
"float4 __ovld __cnfn convert_float4_rtn(ulong4);\n"
"float4 __ovld __cnfn convert_float4(ulong4);\n"
"float4 __ovld __cnfn convert_float4_rte(float4);\n"
"float4 __ovld __cnfn convert_float4_rtz(float4);\n"
"float4 __ovld __cnfn convert_float4_rtp(float4);\n"
"float4 __ovld __cnfn convert_float4_rtn(float4);\n"
"float4 __ovld __cnfn convert_float4(float4);\n"
"char8 __ovld __cnfn convert_char8_rte(char8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(char8);\n"
"char8 __ovld __cnfn convert_char8_rtz(char8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(char8);\n"
"char8 __ovld __cnfn convert_char8_rtp(char8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(char8);\n"
"char8 __ovld __cnfn convert_char8_rtn(char8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(char8);\n"
"char8 __ovld __cnfn convert_char8(char8);\n"
"char8 __ovld __cnfn convert_char8_sat(char8);\n"
"char8 __ovld __cnfn convert_char8_rte(uchar8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(uchar8);\n"
"char8 __ovld __cnfn convert_char8_rtz(uchar8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(uchar8);\n"
"char8 __ovld __cnfn convert_char8_rtp(uchar8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(uchar8);\n"
"char8 __ovld __cnfn convert_char8_rtn(uchar8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(uchar8);\n"
"char8 __ovld __cnfn convert_char8(uchar8);\n"
"char8 __ovld __cnfn convert_char8_sat(uchar8);\n"
"char8 __ovld __cnfn convert_char8_rte(short8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(short8);\n"
"char8 __ovld __cnfn convert_char8_rtz(short8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(short8);\n"
"char8 __ovld __cnfn convert_char8_rtp(short8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(short8);\n"
"char8 __ovld __cnfn convert_char8_rtn(short8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(short8);\n"
"char8 __ovld __cnfn convert_char8(short8);\n"
"char8 __ovld __cnfn convert_char8_sat(short8);\n"
"char8 __ovld __cnfn convert_char8_rte(ushort8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(ushort8);\n"
"char8 __ovld __cnfn convert_char8_rtz(ushort8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(ushort8);\n"
"char8 __ovld __cnfn convert_char8_rtp(ushort8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(ushort8);\n"
"char8 __ovld __cnfn convert_char8_rtn(ushort8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(ushort8);\n"
"char8 __ovld __cnfn convert_char8(ushort8);\n"
"char8 __ovld __cnfn convert_char8_sat(ushort8);\n"
"char8 __ovld __cnfn convert_char8_rte(int8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(int8);\n"
"char8 __ovld __cnfn convert_char8_rtz(int8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(int8);\n"
"char8 __ovld __cnfn convert_char8_rtp(int8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(int8);\n"
"char8 __ovld __cnfn convert_char8_rtn(int8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(int8);\n"
"char8 __ovld __cnfn convert_char8(int8);\n"
"char8 __ovld __cnfn convert_char8_sat(int8);\n"
"char8 __ovld __cnfn convert_char8_rte(uint8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(uint8);\n"
"char8 __ovld __cnfn convert_char8_rtz(uint8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(uint8);\n"
"char8 __ovld __cnfn convert_char8_rtp(uint8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(uint8);\n"
"char8 __ovld __cnfn convert_char8_rtn(uint8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(uint8);\n"
"char8 __ovld __cnfn convert_char8(uint8);\n"
"char8 __ovld __cnfn convert_char8_sat(uint8);\n"
"char8 __ovld __cnfn convert_char8_rte(long8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(long8);\n"
"char8 __ovld __cnfn convert_char8_rtz(long8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(long8);\n"
"char8 __ovld __cnfn convert_char8_rtp(long8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(long8);\n"
"char8 __ovld __cnfn convert_char8_rtn(long8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(long8);\n"
"char8 __ovld __cnfn convert_char8(long8);\n"
"char8 __ovld __cnfn convert_char8_sat(long8);\n"
"char8 __ovld __cnfn convert_char8_rte(ulong8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(ulong8);\n"
"char8 __ovld __cnfn convert_char8_rtz(ulong8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(ulong8);\n"
"char8 __ovld __cnfn convert_char8_rtp(ulong8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(ulong8);\n"
"char8 __ovld __cnfn convert_char8_rtn(ulong8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(ulong8);\n"
"char8 __ovld __cnfn convert_char8(ulong8);\n"
"char8 __ovld __cnfn convert_char8_sat(ulong8);\n"
"char8 __ovld __cnfn convert_char8_rte(float8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(float8);\n"
"char8 __ovld __cnfn convert_char8_rtz(float8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(float8);\n"
"char8 __ovld __cnfn convert_char8_rtp(float8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(float8);\n"
"char8 __ovld __cnfn convert_char8_rtn(float8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(float8);\n"
"char8 __ovld __cnfn convert_char8(float8);\n"
"char8 __ovld __cnfn convert_char8_sat(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(char8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(uchar8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(short8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(ushort8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(int8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(uint8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(long8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(ulong8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8(float8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(float8);\n"
"short8 __ovld __cnfn convert_short8_rte(char8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(char8);\n"
"short8 __ovld __cnfn convert_short8_rtz(char8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(char8);\n"
"short8 __ovld __cnfn convert_short8_rtp(char8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(char8);\n"
"short8 __ovld __cnfn convert_short8_rtn(char8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(char8);\n"
"short8 __ovld __cnfn convert_short8(char8);\n"
"short8 __ovld __cnfn convert_short8_sat(char8);\n"
"short8 __ovld __cnfn convert_short8_rte(uchar8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(uchar8);\n"
"short8 __ovld __cnfn convert_short8_rtz(uchar8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(uchar8);\n"
"short8 __ovld __cnfn convert_short8_rtp(uchar8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(uchar8);\n"
"short8 __ovld __cnfn convert_short8_rtn(uchar8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(uchar8);\n"
"short8 __ovld __cnfn convert_short8(uchar8);\n"
"short8 __ovld __cnfn convert_short8_sat(uchar8);\n"
"short8 __ovld __cnfn convert_short8_rte(short8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(short8);\n"
"short8 __ovld __cnfn convert_short8_rtz(short8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(short8);\n"
"short8 __ovld __cnfn convert_short8_rtp(short8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(short8);\n"
"short8 __ovld __cnfn convert_short8_rtn(short8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(short8);\n"
"short8 __ovld __cnfn convert_short8(short8);\n"
"short8 __ovld __cnfn convert_short8_sat(short8);\n"
"short8 __ovld __cnfn convert_short8_rte(ushort8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(ushort8);\n"
"short8 __ovld __cnfn convert_short8_rtz(ushort8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(ushort8);\n"
"short8 __ovld __cnfn convert_short8_rtp(ushort8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(ushort8);\n"
"short8 __ovld __cnfn convert_short8_rtn(ushort8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(ushort8);\n"
"short8 __ovld __cnfn convert_short8(ushort8);\n"
"short8 __ovld __cnfn convert_short8_sat(ushort8);\n"
"short8 __ovld __cnfn convert_short8_rte(int8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(int8);\n"
"short8 __ovld __cnfn convert_short8_rtz(int8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(int8);\n"
"short8 __ovld __cnfn convert_short8_rtp(int8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(int8);\n"
"short8 __ovld __cnfn convert_short8_rtn(int8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(int8);\n"
"short8 __ovld __cnfn convert_short8(int8);\n"
"short8 __ovld __cnfn convert_short8_sat(int8);\n"
"short8 __ovld __cnfn convert_short8_rte(uint8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(uint8);\n"
"short8 __ovld __cnfn convert_short8_rtz(uint8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(uint8);\n"
"short8 __ovld __cnfn convert_short8_rtp(uint8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(uint8);\n"
"short8 __ovld __cnfn convert_short8_rtn(uint8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(uint8);\n"
"short8 __ovld __cnfn convert_short8(uint8);\n"
"short8 __ovld __cnfn convert_short8_sat(uint8);\n"
"short8 __ovld __cnfn convert_short8_rte(long8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(long8);\n"
"short8 __ovld __cnfn convert_short8_rtz(long8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(long8);\n"
"short8 __ovld __cnfn convert_short8_rtp(long8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(long8);\n"
"short8 __ovld __cnfn convert_short8_rtn(long8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(long8);\n"
"short8 __ovld __cnfn convert_short8(long8);\n"
"short8 __ovld __cnfn convert_short8_sat(long8);\n"
"short8 __ovld __cnfn convert_short8_rte(ulong8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(ulong8);\n"
"short8 __ovld __cnfn convert_short8_rtz(ulong8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(ulong8);\n"
"short8 __ovld __cnfn convert_short8_rtp(ulong8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(ulong8);\n"
"short8 __ovld __cnfn convert_short8_rtn(ulong8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(ulong8);\n"
"short8 __ovld __cnfn convert_short8(ulong8);\n"
"short8 __ovld __cnfn convert_short8_sat(ulong8);\n"
"short8 __ovld __cnfn convert_short8_rte(float8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(float8);\n"
"short8 __ovld __cnfn convert_short8_rtz(float8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(float8);\n"
"short8 __ovld __cnfn convert_short8_rtp(float8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(float8);\n"
"short8 __ovld __cnfn convert_short8_rtn(float8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(float8);\n"
"short8 __ovld __cnfn convert_short8(float8);\n"
"short8 __ovld __cnfn convert_short8_sat(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(char8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(uchar8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(short8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(ushort8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(int8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(uint8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(long8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(ulong8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8(float8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(float8);\n"
"int8 __ovld __cnfn convert_int8_rte(char8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(char8);\n"
"int8 __ovld __cnfn convert_int8_rtz(char8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(char8);\n"
"int8 __ovld __cnfn convert_int8_rtp(char8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(char8);\n"
"int8 __ovld __cnfn convert_int8_rtn(char8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(char8);\n"
"int8 __ovld __cnfn convert_int8(char8);\n"
"int8 __ovld __cnfn convert_int8_sat(char8);\n"
"int8 __ovld __cnfn convert_int8_rte(uchar8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(uchar8);\n"
"int8 __ovld __cnfn convert_int8_rtz(uchar8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(uchar8);\n"
"int8 __ovld __cnfn convert_int8_rtp(uchar8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(uchar8);\n"
"int8 __ovld __cnfn convert_int8_rtn(uchar8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(uchar8);\n"
"int8 __ovld __cnfn convert_int8(uchar8);\n"
"int8 __ovld __cnfn convert_int8_sat(uchar8);\n"
"int8 __ovld __cnfn convert_int8_rte(short8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(short8);\n"
"int8 __ovld __cnfn convert_int8_rtz(short8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(short8);\n"
"int8 __ovld __cnfn convert_int8_rtp(short8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(short8);\n"
"int8 __ovld __cnfn convert_int8_rtn(short8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(short8);\n"
"int8 __ovld __cnfn convert_int8(short8);\n"
"int8 __ovld __cnfn convert_int8_sat(short8);\n"
"int8 __ovld __cnfn convert_int8_rte(ushort8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(ushort8);\n"
"int8 __ovld __cnfn convert_int8_rtz(ushort8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(ushort8);\n"
"int8 __ovld __cnfn convert_int8_rtp(ushort8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(ushort8);\n"
"int8 __ovld __cnfn convert_int8_rtn(ushort8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(ushort8);\n"
"int8 __ovld __cnfn convert_int8(ushort8);\n"
"int8 __ovld __cnfn convert_int8_sat(ushort8);\n"
"int8 __ovld __cnfn convert_int8_rte(int8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(int8);\n"
"int8 __ovld __cnfn convert_int8_rtz(int8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(int8);\n"
"int8 __ovld __cnfn convert_int8_rtp(int8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(int8);\n"
"int8 __ovld __cnfn convert_int8_rtn(int8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(int8);\n"
"int8 __ovld __cnfn convert_int8(int8);\n"
"int8 __ovld __cnfn convert_int8_sat(int8);\n"
"int8 __ovld __cnfn convert_int8_rte(uint8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(uint8);\n"
"int8 __ovld __cnfn convert_int8_rtz(uint8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(uint8);\n"
"int8 __ovld __cnfn convert_int8_rtp(uint8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(uint8);\n"
"int8 __ovld __cnfn convert_int8_rtn(uint8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(uint8);\n"
"int8 __ovld __cnfn convert_int8(uint8);\n"
"int8 __ovld __cnfn convert_int8_sat(uint8);\n"
"int8 __ovld __cnfn convert_int8_rte(long8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(long8);\n"
"int8 __ovld __cnfn convert_int8_rtz(long8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(long8);\n"
"int8 __ovld __cnfn convert_int8_rtp(long8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(long8);\n"
"int8 __ovld __cnfn convert_int8_rtn(long8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(long8);\n"
"int8 __ovld __cnfn convert_int8(long8);\n"
"int8 __ovld __cnfn convert_int8_sat(long8);\n"
"int8 __ovld __cnfn convert_int8_rte(ulong8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(ulong8);\n"
"int8 __ovld __cnfn convert_int8_rtz(ulong8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(ulong8);\n"
"int8 __ovld __cnfn convert_int8_rtp(ulong8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(ulong8);\n"
"int8 __ovld __cnfn convert_int8_rtn(ulong8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(ulong8);\n"
"int8 __ovld __cnfn convert_int8(ulong8);\n"
"int8 __ovld __cnfn convert_int8_sat(ulong8);\n"
"int8 __ovld __cnfn convert_int8_rte(float8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(float8);\n"
"int8 __ovld __cnfn convert_int8_rtz(float8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(float8);\n"
"int8 __ovld __cnfn convert_int8_rtp(float8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(float8);\n"
"int8 __ovld __cnfn convert_int8_rtn(float8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(float8);\n"
"int8 __ovld __cnfn convert_int8(float8);\n"
"int8 __ovld __cnfn convert_int8_sat(float8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(char8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(char8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(char8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(char8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(char8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(char8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(char8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(char8);\n"
"uint8 __ovld __cnfn convert_uint8(char8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(char8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(uchar8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(short8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(short8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(short8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(short8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(short8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(short8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(short8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(short8);\n"
"uint8 __ovld __cnfn convert_uint8(short8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(short8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(ushort8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(int8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(int8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(int8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(int8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(int8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(int8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(int8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(int8);\n"
"uint8 __ovld __cnfn convert_uint8(int8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(int8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(uint8);\n"
"uint8 __ovld __cnfn convert_uint8(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(uint8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(long8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(long8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(long8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(long8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(long8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(long8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(long8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(long8);\n"
"uint8 __ovld __cnfn convert_uint8(long8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(long8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(ulong8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(float8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(float8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(float8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(float8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(float8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(float8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(float8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(float8);\n"
"uint8 __ovld __cnfn convert_uint8(float8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(float8);\n"
"long8 __ovld __cnfn convert_long8_rte(char8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(char8);\n"
"long8 __ovld __cnfn convert_long8_rtz(char8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(char8);\n"
"long8 __ovld __cnfn convert_long8_rtp(char8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(char8);\n"
"long8 __ovld __cnfn convert_long8_rtn(char8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(char8);\n"
"long8 __ovld __cnfn convert_long8(char8);\n"
"long8 __ovld __cnfn convert_long8_sat(char8);\n"
"long8 __ovld __cnfn convert_long8_rte(uchar8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(uchar8);\n"
"long8 __ovld __cnfn convert_long8_rtz(uchar8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(uchar8);\n"
"long8 __ovld __cnfn convert_long8_rtp(uchar8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(uchar8);\n"
"long8 __ovld __cnfn convert_long8_rtn(uchar8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(uchar8);\n"
"long8 __ovld __cnfn convert_long8(uchar8);\n"
"long8 __ovld __cnfn convert_long8_sat(uchar8);\n"
"long8 __ovld __cnfn convert_long8_rte(short8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(short8);\n"
"long8 __ovld __cnfn convert_long8_rtz(short8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(short8);\n"
"long8 __ovld __cnfn convert_long8_rtp(short8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(short8);\n"
"long8 __ovld __cnfn convert_long8_rtn(short8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(short8);\n"
"long8 __ovld __cnfn convert_long8(short8);\n"
"long8 __ovld __cnfn convert_long8_sat(short8);\n"
"long8 __ovld __cnfn convert_long8_rte(ushort8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(ushort8);\n"
"long8 __ovld __cnfn convert_long8_rtz(ushort8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(ushort8);\n"
"long8 __ovld __cnfn convert_long8_rtp(ushort8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(ushort8);\n"
"long8 __ovld __cnfn convert_long8_rtn(ushort8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(ushort8);\n"
"long8 __ovld __cnfn convert_long8(ushort8);\n"
"long8 __ovld __cnfn convert_long8_sat(ushort8);\n"
"long8 __ovld __cnfn convert_long8_rte(int8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(int8);\n"
"long8 __ovld __cnfn convert_long8_rtz(int8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(int8);\n"
"long8 __ovld __cnfn convert_long8_rtp(int8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(int8);\n"
"long8 __ovld __cnfn convert_long8_rtn(int8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(int8);\n"
"long8 __ovld __cnfn convert_long8(int8);\n"
"long8 __ovld __cnfn convert_long8_sat(int8);\n"
"long8 __ovld __cnfn convert_long8_rte(uint8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(uint8);\n"
"long8 __ovld __cnfn convert_long8_rtz(uint8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(uint8);\n"
"long8 __ovld __cnfn convert_long8_rtp(uint8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(uint8);\n"
"long8 __ovld __cnfn convert_long8_rtn(uint8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(uint8);\n"
"long8 __ovld __cnfn convert_long8(uint8);\n"
"long8 __ovld __cnfn convert_long8_sat(uint8);\n"
"long8 __ovld __cnfn convert_long8_rte(long8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(long8);\n"
"long8 __ovld __cnfn convert_long8_rtz(long8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(long8);\n"
"long8 __ovld __cnfn convert_long8_rtp(long8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(long8);\n"
"long8 __ovld __cnfn convert_long8_rtn(long8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(long8);\n"
"long8 __ovld __cnfn convert_long8(long8);\n"
"long8 __ovld __cnfn convert_long8_sat(long8);\n"
"long8 __ovld __cnfn convert_long8_rte(ulong8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(ulong8);\n"
"long8 __ovld __cnfn convert_long8_rtz(ulong8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(ulong8);\n"
"long8 __ovld __cnfn convert_long8_rtp(ulong8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(ulong8);\n"
"long8 __ovld __cnfn convert_long8_rtn(ulong8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(ulong8);\n"
"long8 __ovld __cnfn convert_long8(ulong8);\n"
"long8 __ovld __cnfn convert_long8_sat(ulong8);\n"
"long8 __ovld __cnfn convert_long8_rte(float8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(float8);\n"
"long8 __ovld __cnfn convert_long8_rtz(float8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(float8);\n"
"long8 __ovld __cnfn convert_long8_rtp(float8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(float8);\n"
"long8 __ovld __cnfn convert_long8_rtn(float8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(float8);\n"
"long8 __ovld __cnfn convert_long8(float8);\n"
"long8 __ovld __cnfn convert_long8_sat(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(char8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(uchar8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(short8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(ushort8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(int8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(uint8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(long8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(ulong8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8(float8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(float8);\n"
"float8 __ovld __cnfn convert_float8_rte(char8);\n"
"float8 __ovld __cnfn convert_float8_rtz(char8);\n"
"float8 __ovld __cnfn convert_float8_rtp(char8);\n"
"float8 __ovld __cnfn convert_float8_rtn(char8);\n"
"float8 __ovld __cnfn convert_float8(char8);\n"
"float8 __ovld __cnfn convert_float8_rte(uchar8);\n"
"float8 __ovld __cnfn convert_float8_rtz(uchar8);\n"
"float8 __ovld __cnfn convert_float8_rtp(uchar8);\n"
"float8 __ovld __cnfn convert_float8_rtn(uchar8);\n"
"float8 __ovld __cnfn convert_float8(uchar8);\n"
"float8 __ovld __cnfn convert_float8_rte(short8);\n"
"float8 __ovld __cnfn convert_float8_rtz(short8);\n"
"float8 __ovld __cnfn convert_float8_rtp(short8);\n"
"float8 __ovld __cnfn convert_float8_rtn(short8);\n"
"float8 __ovld __cnfn convert_float8(short8);\n"
"float8 __ovld __cnfn convert_float8_rte(ushort8);\n"
"float8 __ovld __cnfn convert_float8_rtz(ushort8);\n"
"float8 __ovld __cnfn convert_float8_rtp(ushort8);\n"
"float8 __ovld __cnfn convert_float8_rtn(ushort8);\n"
"float8 __ovld __cnfn convert_float8(ushort8);\n"
"float8 __ovld __cnfn convert_float8_rte(int8);\n"
"float8 __ovld __cnfn convert_float8_rtz(int8);\n"
"float8 __ovld __cnfn convert_float8_rtp(int8);\n"
"float8 __ovld __cnfn convert_float8_rtn(int8);\n"
"float8 __ovld __cnfn convert_float8(int8);\n"
"float8 __ovld __cnfn convert_float8_rte(uint8);\n"
"float8 __ovld __cnfn convert_float8_rtz(uint8);\n"
"float8 __ovld __cnfn convert_float8_rtp(uint8);\n"
"float8 __ovld __cnfn convert_float8_rtn(uint8);\n"
"float8 __ovld __cnfn convert_float8(uint8);\n"
"float8 __ovld __cnfn convert_float8_rte(long8);\n"
"float8 __ovld __cnfn convert_float8_rtz(long8);\n"
"float8 __ovld __cnfn convert_float8_rtp(long8);\n"
"float8 __ovld __cnfn convert_float8_rtn(long8);\n"
"float8 __ovld __cnfn convert_float8(long8);\n"
"float8 __ovld __cnfn convert_float8_rte(ulong8);\n"
"float8 __ovld __cnfn convert_float8_rtz(ulong8);\n"
"float8 __ovld __cnfn convert_float8_rtp(ulong8);\n"
"float8 __ovld __cnfn convert_float8_rtn(ulong8);\n"
"float8 __ovld __cnfn convert_float8(ulong8);\n"
"float8 __ovld __cnfn convert_float8_rte(float8);\n"
"float8 __ovld __cnfn convert_float8_rtz(float8);\n"
"float8 __ovld __cnfn convert_float8_rtp(float8);\n"
"float8 __ovld __cnfn convert_float8_rtn(float8);\n"
"float8 __ovld __cnfn convert_float8(float8);\n"
"char16 __ovld __cnfn convert_char16_rte(char16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(char16);\n"
"char16 __ovld __cnfn convert_char16_rtz(char16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(char16);\n"
"char16 __ovld __cnfn convert_char16_rtp(char16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(char16);\n"
"char16 __ovld __cnfn convert_char16_rtn(char16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(char16);\n"
"char16 __ovld __cnfn convert_char16(char16);\n"
"char16 __ovld __cnfn convert_char16_sat(char16);\n"
"char16 __ovld __cnfn convert_char16_rte(uchar16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(uchar16);\n"
"char16 __ovld __cnfn convert_char16_rtz(uchar16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(uchar16);\n"
"char16 __ovld __cnfn convert_char16_rtp(uchar16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(uchar16);\n"
"char16 __ovld __cnfn convert_char16_rtn(uchar16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(uchar16);\n"
"char16 __ovld __cnfn convert_char16(uchar16);\n"
"char16 __ovld __cnfn convert_char16_sat(uchar16);\n"
"char16 __ovld __cnfn convert_char16_rte(short16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(short16);\n"
"char16 __ovld __cnfn convert_char16_rtz(short16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(short16);\n"
"char16 __ovld __cnfn convert_char16_rtp(short16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(short16);\n"
"char16 __ovld __cnfn convert_char16_rtn(short16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(short16);\n"
"char16 __ovld __cnfn convert_char16(short16);\n"
"char16 __ovld __cnfn convert_char16_sat(short16);\n"
"char16 __ovld __cnfn convert_char16_rte(ushort16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(ushort16);\n"
"char16 __ovld __cnfn convert_char16_rtz(ushort16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(ushort16);\n"
"char16 __ovld __cnfn convert_char16_rtp(ushort16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(ushort16);\n"
"char16 __ovld __cnfn convert_char16_rtn(ushort16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(ushort16);\n"
"char16 __ovld __cnfn convert_char16(ushort16);\n"
"char16 __ovld __cnfn convert_char16_sat(ushort16);\n"
"char16 __ovld __cnfn convert_char16_rte(int16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(int16);\n"
"char16 __ovld __cnfn convert_char16_rtz(int16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(int16);\n"
"char16 __ovld __cnfn convert_char16_rtp(int16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(int16);\n"
"char16 __ovld __cnfn convert_char16_rtn(int16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(int16);\n"
"char16 __ovld __cnfn convert_char16(int16);\n"
"char16 __ovld __cnfn convert_char16_sat(int16);\n"
"char16 __ovld __cnfn convert_char16_rte(uint16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(uint16);\n"
"char16 __ovld __cnfn convert_char16_rtz(uint16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(uint16);\n"
"char16 __ovld __cnfn convert_char16_rtp(uint16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(uint16);\n"
"char16 __ovld __cnfn convert_char16_rtn(uint16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(uint16);\n"
"char16 __ovld __cnfn convert_char16(uint16);\n"
"char16 __ovld __cnfn convert_char16_sat(uint16);\n"
"char16 __ovld __cnfn convert_char16_rte(long16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(long16);\n"
"char16 __ovld __cnfn convert_char16_rtz(long16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(long16);\n"
"char16 __ovld __cnfn convert_char16_rtp(long16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(long16);\n"
"char16 __ovld __cnfn convert_char16_rtn(long16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(long16);\n"
"char16 __ovld __cnfn convert_char16(long16);\n"
"char16 __ovld __cnfn convert_char16_sat(long16);\n"
"char16 __ovld __cnfn convert_char16_rte(ulong16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(ulong16);\n"
"char16 __ovld __cnfn convert_char16_rtz(ulong16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(ulong16);\n"
"char16 __ovld __cnfn convert_char16_rtp(ulong16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(ulong16);\n"
"char16 __ovld __cnfn convert_char16_rtn(ulong16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(ulong16);\n"
"char16 __ovld __cnfn convert_char16(ulong16);\n"
"char16 __ovld __cnfn convert_char16_sat(ulong16);\n"
"char16 __ovld __cnfn convert_char16_rte(float16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(float16);\n"
"char16 __ovld __cnfn convert_char16_rtz(float16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(float16);\n"
"char16 __ovld __cnfn convert_char16_rtp(float16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(float16);\n"
"char16 __ovld __cnfn convert_char16_rtn(float16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(float16);\n"
"char16 __ovld __cnfn convert_char16(float16);\n"
"char16 __ovld __cnfn convert_char16_sat(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(char16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(uchar16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(short16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(ushort16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(int16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(uint16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(long16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(ulong16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16(float16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(float16);\n"
"short16 __ovld __cnfn convert_short16_rte(char16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(char16);\n"
"short16 __ovld __cnfn convert_short16_rtz(char16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(char16);\n"
"short16 __ovld __cnfn convert_short16_rtp(char16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(char16);\n"
"short16 __ovld __cnfn convert_short16_rtn(char16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(char16);\n"
"short16 __ovld __cnfn convert_short16(char16);\n"
"short16 __ovld __cnfn convert_short16_sat(char16);\n"
"short16 __ovld __cnfn convert_short16_rte(uchar16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(uchar16);\n"
"short16 __ovld __cnfn convert_short16_rtz(uchar16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(uchar16);\n"
"short16 __ovld __cnfn convert_short16_rtp(uchar16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(uchar16);\n"
"short16 __ovld __cnfn convert_short16_rtn(uchar16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(uchar16);\n"
"short16 __ovld __cnfn convert_short16(uchar16);\n"
"short16 __ovld __cnfn convert_short16_sat(uchar16);\n"
"short16 __ovld __cnfn convert_short16_rte(short16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(short16);\n"
"short16 __ovld __cnfn convert_short16_rtz(short16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(short16);\n"
"short16 __ovld __cnfn convert_short16_rtp(short16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(short16);\n"
"short16 __ovld __cnfn convert_short16_rtn(short16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(short16);\n"
"short16 __ovld __cnfn convert_short16(short16);\n"
"short16 __ovld __cnfn convert_short16_sat(short16);\n"
"short16 __ovld __cnfn convert_short16_rte(ushort16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(ushort16);\n"
"short16 __ovld __cnfn convert_short16_rtz(ushort16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(ushort16);\n"
"short16 __ovld __cnfn convert_short16_rtp(ushort16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(ushort16);\n"
"short16 __ovld __cnfn convert_short16_rtn(ushort16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(ushort16);\n"
"short16 __ovld __cnfn convert_short16(ushort16);\n"
"short16 __ovld __cnfn convert_short16_sat(ushort16);\n"
"short16 __ovld __cnfn convert_short16_rte(int16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(int16);\n"
"short16 __ovld __cnfn convert_short16_rtz(int16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(int16);\n"
"short16 __ovld __cnfn convert_short16_rtp(int16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(int16);\n"
"short16 __ovld __cnfn convert_short16_rtn(int16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(int16);\n"
"short16 __ovld __cnfn convert_short16(int16);\n"
"short16 __ovld __cnfn convert_short16_sat(int16);\n"
"short16 __ovld __cnfn convert_short16_rte(uint16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(uint16);\n"
"short16 __ovld __cnfn convert_short16_rtz(uint16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(uint16);\n"
"short16 __ovld __cnfn convert_short16_rtp(uint16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(uint16);\n"
"short16 __ovld __cnfn convert_short16_rtn(uint16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(uint16);\n"
"short16 __ovld __cnfn convert_short16(uint16);\n"
"short16 __ovld __cnfn convert_short16_sat(uint16);\n"
"short16 __ovld __cnfn convert_short16_rte(long16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(long16);\n"
"short16 __ovld __cnfn convert_short16_rtz(long16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(long16);\n"
"short16 __ovld __cnfn convert_short16_rtp(long16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(long16);\n"
"short16 __ovld __cnfn convert_short16_rtn(long16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(long16);\n"
"short16 __ovld __cnfn convert_short16(long16);\n"
"short16 __ovld __cnfn convert_short16_sat(long16);\n"
"short16 __ovld __cnfn convert_short16_rte(ulong16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(ulong16);\n"
"short16 __ovld __cnfn convert_short16_rtz(ulong16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(ulong16);\n"
"short16 __ovld __cnfn convert_short16_rtp(ulong16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(ulong16);\n"
"short16 __ovld __cnfn convert_short16_rtn(ulong16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(ulong16);\n"
"short16 __ovld __cnfn convert_short16(ulong16);\n"
"short16 __ovld __cnfn convert_short16_sat(ulong16);\n"
"short16 __ovld __cnfn convert_short16_rte(float16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(float16);\n"
"short16 __ovld __cnfn convert_short16_rtz(float16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(float16);\n"
"short16 __ovld __cnfn convert_short16_rtp(float16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(float16);\n"
"short16 __ovld __cnfn convert_short16_rtn(float16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(float16);\n"
"short16 __ovld __cnfn convert_short16(float16);\n"
"short16 __ovld __cnfn convert_short16_sat(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(char16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(uchar16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(short16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(ushort16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(int16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(uint16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(long16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(ulong16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16(float16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(float16);\n"
"int16 __ovld __cnfn convert_int16_rte(char16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(char16);\n"
"int16 __ovld __cnfn convert_int16_rtz(char16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(char16);\n"
"int16 __ovld __cnfn convert_int16_rtp(char16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(char16);\n"
"int16 __ovld __cnfn convert_int16_rtn(char16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(char16);\n"
"int16 __ovld __cnfn convert_int16(char16);\n"
"int16 __ovld __cnfn convert_int16_sat(char16);\n"
"int16 __ovld __cnfn convert_int16_rte(uchar16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(uchar16);\n"
"int16 __ovld __cnfn convert_int16_rtz(uchar16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(uchar16);\n"
"int16 __ovld __cnfn convert_int16_rtp(uchar16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(uchar16);\n"
"int16 __ovld __cnfn convert_int16_rtn(uchar16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(uchar16);\n"
"int16 __ovld __cnfn convert_int16(uchar16);\n"
"int16 __ovld __cnfn convert_int16_sat(uchar16);\n"
"int16 __ovld __cnfn convert_int16_rte(short16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(short16);\n"
"int16 __ovld __cnfn convert_int16_rtz(short16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(short16);\n"
"int16 __ovld __cnfn convert_int16_rtp(short16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(short16);\n"
"int16 __ovld __cnfn convert_int16_rtn(short16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(short16);\n"
"int16 __ovld __cnfn convert_int16(short16);\n"
"int16 __ovld __cnfn convert_int16_sat(short16);\n"
"int16 __ovld __cnfn convert_int16_rte(ushort16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(ushort16);\n"
"int16 __ovld __cnfn convert_int16_rtz(ushort16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(ushort16);\n"
"int16 __ovld __cnfn convert_int16_rtp(ushort16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(ushort16);\n"
"int16 __ovld __cnfn convert_int16_rtn(ushort16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(ushort16);\n"
"int16 __ovld __cnfn convert_int16(ushort16);\n"
"int16 __ovld __cnfn convert_int16_sat(ushort16);\n"
"int16 __ovld __cnfn convert_int16_rte(int16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(int16);\n"
"int16 __ovld __cnfn convert_int16_rtz(int16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(int16);\n"
"int16 __ovld __cnfn convert_int16_rtp(int16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(int16);\n"
"int16 __ovld __cnfn convert_int16_rtn(int16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(int16);\n"
"int16 __ovld __cnfn convert_int16(int16);\n"
"int16 __ovld __cnfn convert_int16_sat(int16);\n"
"int16 __ovld __cnfn convert_int16_rte(uint16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(uint16);\n"
"int16 __ovld __cnfn convert_int16_rtz(uint16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(uint16);\n"
"int16 __ovld __cnfn convert_int16_rtp(uint16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(uint16);\n"
"int16 __ovld __cnfn convert_int16_rtn(uint16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(uint16);\n"
"int16 __ovld __cnfn convert_int16(uint16);\n"
"int16 __ovld __cnfn convert_int16_sat(uint16);\n"
"int16 __ovld __cnfn convert_int16_rte(long16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(long16);\n"
"int16 __ovld __cnfn convert_int16_rtz(long16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(long16);\n"
"int16 __ovld __cnfn convert_int16_rtp(long16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(long16);\n"
"int16 __ovld __cnfn convert_int16_rtn(long16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(long16);\n"
"int16 __ovld __cnfn convert_int16(long16);\n"
"int16 __ovld __cnfn convert_int16_sat(long16);\n"
"int16 __ovld __cnfn convert_int16_rte(ulong16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(ulong16);\n"
"int16 __ovld __cnfn convert_int16_rtz(ulong16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(ulong16);\n"
"int16 __ovld __cnfn convert_int16_rtp(ulong16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(ulong16);\n"
"int16 __ovld __cnfn convert_int16_rtn(ulong16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(ulong16);\n"
"int16 __ovld __cnfn convert_int16(ulong16);\n"
"int16 __ovld __cnfn convert_int16_sat(ulong16);\n"
"int16 __ovld __cnfn convert_int16_rte(float16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(float16);\n"
"int16 __ovld __cnfn convert_int16_rtz(float16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(float16);\n"
"int16 __ovld __cnfn convert_int16_rtp(float16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(float16);\n"
"int16 __ovld __cnfn convert_int16_rtn(float16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(float16);\n"
"int16 __ovld __cnfn convert_int16(float16);\n"
"int16 __ovld __cnfn convert_int16_sat(float16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(char16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(char16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(char16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(char16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(char16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(char16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(char16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(char16);\n"
"uint16 __ovld __cnfn convert_uint16(char16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(char16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(uchar16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(short16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(short16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(short16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(short16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(short16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(short16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(short16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(short16);\n"
"uint16 __ovld __cnfn convert_uint16(short16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(short16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(ushort16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(int16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(int16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(int16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(int16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(int16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(int16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(int16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(int16);\n"
"uint16 __ovld __cnfn convert_uint16(int16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(int16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(uint16);\n"
"uint16 __ovld __cnfn convert_uint16(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(uint16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(long16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(long16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(long16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(long16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(long16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(long16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(long16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(long16);\n"
"uint16 __ovld __cnfn convert_uint16(long16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(long16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(ulong16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(float16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(float16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(float16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(float16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(float16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(float16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(float16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(float16);\n"
"uint16 __ovld __cnfn convert_uint16(float16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(float16);\n"
"long16 __ovld __cnfn convert_long16_rte(char16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(char16);\n"
"long16 __ovld __cnfn convert_long16_rtz(char16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(char16);\n"
"long16 __ovld __cnfn convert_long16_rtp(char16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(char16);\n"
"long16 __ovld __cnfn convert_long16_rtn(char16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(char16);\n"
"long16 __ovld __cnfn convert_long16(char16);\n"
"long16 __ovld __cnfn convert_long16_sat(char16);\n"
"long16 __ovld __cnfn convert_long16_rte(uchar16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(uchar16);\n"
"long16 __ovld __cnfn convert_long16_rtz(uchar16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(uchar16);\n"
"long16 __ovld __cnfn convert_long16_rtp(uchar16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(uchar16);\n"
"long16 __ovld __cnfn convert_long16_rtn(uchar16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(uchar16);\n"
"long16 __ovld __cnfn convert_long16(uchar16);\n"
"long16 __ovld __cnfn convert_long16_sat(uchar16);\n"
"long16 __ovld __cnfn convert_long16_rte(short16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(short16);\n"
"long16 __ovld __cnfn convert_long16_rtz(short16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(short16);\n"
"long16 __ovld __cnfn convert_long16_rtp(short16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(short16);\n"
"long16 __ovld __cnfn convert_long16_rtn(short16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(short16);\n"
"long16 __ovld __cnfn convert_long16(short16);\n"
"long16 __ovld __cnfn convert_long16_sat(short16);\n"
"long16 __ovld __cnfn convert_long16_rte(ushort16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(ushort16);\n"
"long16 __ovld __cnfn convert_long16_rtz(ushort16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(ushort16);\n"
"long16 __ovld __cnfn convert_long16_rtp(ushort16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(ushort16);\n"
"long16 __ovld __cnfn convert_long16_rtn(ushort16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(ushort16);\n"
"long16 __ovld __cnfn convert_long16(ushort16);\n"
"long16 __ovld __cnfn convert_long16_sat(ushort16);\n"
"long16 __ovld __cnfn convert_long16_rte(int16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(int16);\n"
"long16 __ovld __cnfn convert_long16_rtz(int16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(int16);\n"
"long16 __ovld __cnfn convert_long16_rtp(int16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(int16);\n"
"long16 __ovld __cnfn convert_long16_rtn(int16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(int16);\n"
"long16 __ovld __cnfn convert_long16(int16);\n"
"long16 __ovld __cnfn convert_long16_sat(int16);\n"
"long16 __ovld __cnfn convert_long16_rte(uint16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(uint16);\n"
"long16 __ovld __cnfn convert_long16_rtz(uint16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(uint16);\n"
"long16 __ovld __cnfn convert_long16_rtp(uint16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(uint16);\n"
"long16 __ovld __cnfn convert_long16_rtn(uint16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(uint16);\n"
"long16 __ovld __cnfn convert_long16(uint16);\n"
"long16 __ovld __cnfn convert_long16_sat(uint16);\n"
"long16 __ovld __cnfn convert_long16_rte(long16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(long16);\n"
"long16 __ovld __cnfn convert_long16_rtz(long16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(long16);\n"
"long16 __ovld __cnfn convert_long16_rtp(long16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(long16);\n"
"long16 __ovld __cnfn convert_long16_rtn(long16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(long16);\n"
"long16 __ovld __cnfn convert_long16(long16);\n"
"long16 __ovld __cnfn convert_long16_sat(long16);\n"
"long16 __ovld __cnfn convert_long16_rte(ulong16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(ulong16);\n"
"long16 __ovld __cnfn convert_long16_rtz(ulong16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(ulong16);\n"
"long16 __ovld __cnfn convert_long16_rtp(ulong16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(ulong16);\n"
"long16 __ovld __cnfn convert_long16_rtn(ulong16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(ulong16);\n"
"long16 __ovld __cnfn convert_long16(ulong16);\n"
"long16 __ovld __cnfn convert_long16_sat(ulong16);\n"
"long16 __ovld __cnfn convert_long16_rte(float16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(float16);\n"
"long16 __ovld __cnfn convert_long16_rtz(float16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(float16);\n"
"long16 __ovld __cnfn convert_long16_rtp(float16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(float16);\n"
"long16 __ovld __cnfn convert_long16_rtn(float16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(float16);\n"
"long16 __ovld __cnfn convert_long16(float16);\n"
"long16 __ovld __cnfn convert_long16_sat(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(char16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(uchar16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(short16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(ushort16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(int16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(uint16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(long16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(ulong16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16(float16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(float16);\n"
"float16 __ovld __cnfn convert_float16_rte(char16);\n"
"float16 __ovld __cnfn convert_float16_rtz(char16);\n"
"float16 __ovld __cnfn convert_float16_rtp(char16);\n"
"float16 __ovld __cnfn convert_float16_rtn(char16);\n"
"float16 __ovld __cnfn convert_float16(char16);\n"
"float16 __ovld __cnfn convert_float16_rte(uchar16);\n"
"float16 __ovld __cnfn convert_float16_rtz(uchar16);\n"
"float16 __ovld __cnfn convert_float16_rtp(uchar16);\n"
"float16 __ovld __cnfn convert_float16_rtn(uchar16);\n"
"float16 __ovld __cnfn convert_float16(uchar16);\n"
"float16 __ovld __cnfn convert_float16_rte(short16);\n"
"float16 __ovld __cnfn convert_float16_rtz(short16);\n"
"float16 __ovld __cnfn convert_float16_rtp(short16);\n"
"float16 __ovld __cnfn convert_float16_rtn(short16);\n"
"float16 __ovld __cnfn convert_float16(short16);\n"
"float16 __ovld __cnfn convert_float16_rte(ushort16);\n"
"float16 __ovld __cnfn convert_float16_rtz(ushort16);\n"
"float16 __ovld __cnfn convert_float16_rtp(ushort16);\n"
"float16 __ovld __cnfn convert_float16_rtn(ushort16);\n"
"float16 __ovld __cnfn convert_float16(ushort16);\n"
"float16 __ovld __cnfn convert_float16_rte(int16);\n"
"float16 __ovld __cnfn convert_float16_rtz(int16);\n"
"float16 __ovld __cnfn convert_float16_rtp(int16);\n"
"float16 __ovld __cnfn convert_float16_rtn(int16);\n"
"float16 __ovld __cnfn convert_float16(int16);\n"
"float16 __ovld __cnfn convert_float16_rte(uint16);\n"
"float16 __ovld __cnfn convert_float16_rtz(uint16);\n"
"float16 __ovld __cnfn convert_float16_rtp(uint16);\n"
"float16 __ovld __cnfn convert_float16_rtn(uint16);\n"
"float16 __ovld __cnfn convert_float16(uint16);\n"
"float16 __ovld __cnfn convert_float16_rte(long16);\n"
"float16 __ovld __cnfn convert_float16_rtz(long16);\n"
"float16 __ovld __cnfn convert_float16_rtp(long16);\n"
"float16 __ovld __cnfn convert_float16_rtn(long16);\n"
"float16 __ovld __cnfn convert_float16(long16);\n"
"float16 __ovld __cnfn convert_float16_rte(ulong16);\n"
"float16 __ovld __cnfn convert_float16_rtz(ulong16);\n"
"float16 __ovld __cnfn convert_float16_rtp(ulong16);\n"
"float16 __ovld __cnfn convert_float16_rtn(ulong16);\n"
"float16 __ovld __cnfn convert_float16(ulong16);\n"
"float16 __ovld __cnfn convert_float16_rte(float16);\n"
"float16 __ovld __cnfn convert_float16_rtz(float16);\n"
"float16 __ovld __cnfn convert_float16_rtp(float16);\n"
"float16 __ovld __cnfn convert_float16_rtn(float16);\n"
"float16 __ovld __cnfn convert_float16(float16);\n"
"\n"
"// Conversions with double data type parameters or return value.\n"
"\n"
"#ifdef cl_khr_fp64\n"
"char __ovld __cnfn convert_char(double);\n"
"char __ovld __cnfn convert_char_rte(double);\n"
"char __ovld __cnfn convert_char_rtn(double);\n"
"char __ovld __cnfn convert_char_rtp(double);\n"
"char __ovld __cnfn convert_char_rtz(double);\n"
"char __ovld __cnfn convert_char_sat(double);\n"
"char __ovld __cnfn convert_char_sat_rte(double);\n"
"char __ovld __cnfn convert_char_sat_rtn(double);\n"
"char __ovld __cnfn convert_char_sat_rtp(double);\n"
"char __ovld __cnfn convert_char_sat_rtz(double);\n"
"char2 __ovld __cnfn convert_char2(double2);\n"
"char2 __ovld __cnfn convert_char2_rte(double2);\n"
"char2 __ovld __cnfn convert_char2_rtn(double2);\n"
"char2 __ovld __cnfn convert_char2_rtp(double2);\n"
"char2 __ovld __cnfn convert_char2_rtz(double2);\n"
"char2 __ovld __cnfn convert_char2_sat(double2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(double2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(double2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(double2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(double2);\n"
"char3 __ovld __cnfn convert_char3(double3);\n"
"char3 __ovld __cnfn convert_char3_rte(double3);\n"
"char3 __ovld __cnfn convert_char3_rtn(double3);\n"
"char3 __ovld __cnfn convert_char3_rtp(double3);\n"
"char3 __ovld __cnfn convert_char3_rtz(double3);\n"
"char3 __ovld __cnfn convert_char3_sat(double3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(double3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(double3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(double3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(double3);\n"
"char4 __ovld __cnfn convert_char4(double4);\n"
"char4 __ovld __cnfn convert_char4_rte(double4);\n"
"char4 __ovld __cnfn convert_char4_rtn(double4);\n"
"char4 __ovld __cnfn convert_char4_rtp(double4);\n"
"char4 __ovld __cnfn convert_char4_rtz(double4);\n"
"char4 __ovld __cnfn convert_char4_sat(double4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(double4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(double4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(double4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(double4);\n"
"char8 __ovld __cnfn convert_char8(double8);\n"
"char8 __ovld __cnfn convert_char8_rte(double8);\n"
"char8 __ovld __cnfn convert_char8_rtn(double8);\n"
"char8 __ovld __cnfn convert_char8_rtp(double8);\n"
"char8 __ovld __cnfn convert_char8_rtz(double8);\n"
"char8 __ovld __cnfn convert_char8_sat(double8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(double8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(double8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(double8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(double8);\n"
"char16 __ovld __cnfn convert_char16(double16);\n"
"char16 __ovld __cnfn convert_char16_rte(double16);\n"
"char16 __ovld __cnfn convert_char16_rtn(double16);\n"
"char16 __ovld __cnfn convert_char16_rtp(double16);\n"
"char16 __ovld __cnfn convert_char16_rtz(double16);\n"
"char16 __ovld __cnfn convert_char16_sat(double16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(double16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(double16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(double16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(double16);\n"
"\n"
"uchar __ovld __cnfn convert_uchar(double);\n"
"uchar __ovld __cnfn convert_uchar_rte(double);\n"
"uchar __ovld __cnfn convert_uchar_rtn(double);\n"
"uchar __ovld __cnfn convert_uchar_rtp(double);\n"
"uchar __ovld __cnfn convert_uchar_rtz(double);\n"
"uchar __ovld __cnfn convert_uchar_sat(double);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(double);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(double);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(double);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(double);\n"
"uchar2 __ovld __cnfn convert_uchar2(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(double2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(double2);\n"
"uchar3 __ovld __cnfn convert_uchar3(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(double3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(double3);\n"
"uchar4 __ovld __cnfn convert_uchar4(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(double4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(double4);\n"
"uchar8 __ovld __cnfn convert_uchar8(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(double8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(double8);\n"
"uchar16 __ovld __cnfn convert_uchar16(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(double16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(double16);\n"
"\n"
"short __ovld __cnfn convert_short(double);\n"
"short __ovld __cnfn convert_short_rte(double);\n"
"short __ovld __cnfn convert_short_rtn(double);\n"
"short __ovld __cnfn convert_short_rtp(double);\n"
"short __ovld __cnfn convert_short_rtz(double);\n"
"short __ovld __cnfn convert_short_sat(double);\n"
"short __ovld __cnfn convert_short_sat_rte(double);\n"
"short __ovld __cnfn convert_short_sat_rtn(double);\n"
"short __ovld __cnfn convert_short_sat_rtp(double);\n"
"short __ovld __cnfn convert_short_sat_rtz(double);\n"
"short2 __ovld __cnfn convert_short2(double2);\n"
"short2 __ovld __cnfn convert_short2_rte(double2);\n"
"short2 __ovld __cnfn convert_short2_rtn(double2);\n"
"short2 __ovld __cnfn convert_short2_rtp(double2);\n"
"short2 __ovld __cnfn convert_short2_rtz(double2);\n"
"short2 __ovld __cnfn convert_short2_sat(double2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(double2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(double2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(double2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(double2);\n"
"short3 __ovld __cnfn convert_short3(double3);\n"
"short3 __ovld __cnfn convert_short3_rte(double3);\n"
"short3 __ovld __cnfn convert_short3_rtn(double3);\n"
"short3 __ovld __cnfn convert_short3_rtp(double3);\n"
"short3 __ovld __cnfn convert_short3_rtz(double3);\n"
"short3 __ovld __cnfn convert_short3_sat(double3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(double3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(double3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(double3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(double3);\n"
"short4 __ovld __cnfn convert_short4(double4);\n"
"short4 __ovld __cnfn convert_short4_rte(double4);\n"
"short4 __ovld __cnfn convert_short4_rtn(double4);\n"
"short4 __ovld __cnfn convert_short4_rtp(double4);\n"
"short4 __ovld __cnfn convert_short4_rtz(double4);\n"
"short4 __ovld __cnfn convert_short4_sat(double4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(double4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(double4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(double4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(double4);\n"
"short8 __ovld __cnfn convert_short8(double8);\n"
"short8 __ovld __cnfn convert_short8_rte(double8);\n"
"short8 __ovld __cnfn convert_short8_rtn(double8);\n"
"short8 __ovld __cnfn convert_short8_rtp(double8);\n"
"short8 __ovld __cnfn convert_short8_rtz(double8);\n"
"short8 __ovld __cnfn convert_short8_sat(double8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(double8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(double8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(double8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(double8);\n"
"short16 __ovld __cnfn convert_short16(double16);\n"
"short16 __ovld __cnfn convert_short16_rte(double16);\n"
"short16 __ovld __cnfn convert_short16_rtn(double16);\n"
"short16 __ovld __cnfn convert_short16_rtp(double16);\n"
"short16 __ovld __cnfn convert_short16_rtz(double16);\n"
"short16 __ovld __cnfn convert_short16_sat(double16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(double16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(double16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(double16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(double16);\n"
"\n"
"ushort __ovld __cnfn convert_ushort(double);\n"
"ushort __ovld __cnfn convert_ushort_rte(double);\n"
"ushort __ovld __cnfn convert_ushort_rtn(double);\n"
"ushort __ovld __cnfn convert_ushort_rtp(double);\n"
"ushort __ovld __cnfn convert_ushort_rtz(double);\n"
"ushort __ovld __cnfn convert_ushort_sat(double);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(double);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(double);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(double);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(double);\n"
"ushort2 __ovld __cnfn convert_ushort2(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(double2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(double2);\n"
"ushort3 __ovld __cnfn convert_ushort3(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(double3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(double3);\n"
"ushort4 __ovld __cnfn convert_ushort4(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(double4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(double4);\n"
"ushort8 __ovld __cnfn convert_ushort8(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(double8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(double8);\n"
"ushort16 __ovld __cnfn convert_ushort16(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(double16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(double16);\n"
"\n"
"int __ovld __cnfn convert_int(double);\n"
"int __ovld __cnfn convert_int_rte(double);\n"
"int __ovld __cnfn convert_int_rtn(double);\n"
"int __ovld __cnfn convert_int_rtp(double);\n"
"int __ovld __cnfn convert_int_rtz(double);\n"
"int __ovld __cnfn convert_int_sat(double);\n"
"int __ovld __cnfn convert_int_sat_rte(double);\n"
"int __ovld __cnfn convert_int_sat_rtn(double);\n"
"int __ovld __cnfn convert_int_sat_rtp(double);\n"
"int __ovld __cnfn convert_int_sat_rtz(double);\n"
"int2 __ovld __cnfn convert_int2(double2);\n"
"int2 __ovld __cnfn convert_int2_rte(double2);\n"
"int2 __ovld __cnfn convert_int2_rtn(double2);\n"
"int2 __ovld __cnfn convert_int2_rtp(double2);\n"
"int2 __ovld __cnfn convert_int2_rtz(double2);\n"
"int2 __ovld __cnfn convert_int2_sat(double2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(double2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(double2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(double2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(double2);\n"
"int3 __ovld __cnfn convert_int3(double3);\n"
"int3 __ovld __cnfn convert_int3_rte(double3);\n"
"int3 __ovld __cnfn convert_int3_rtn(double3);\n"
"int3 __ovld __cnfn convert_int3_rtp(double3);\n"
"int3 __ovld __cnfn convert_int3_rtz(double3);\n"
"int3 __ovld __cnfn convert_int3_sat(double3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(double3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(double3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(double3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(double3);\n"
"int4 __ovld __cnfn convert_int4(double4);\n"
"int4 __ovld __cnfn convert_int4_rte(double4);\n"
"int4 __ovld __cnfn convert_int4_rtn(double4);\n"
"int4 __ovld __cnfn convert_int4_rtp(double4);\n"
"int4 __ovld __cnfn convert_int4_rtz(double4);\n"
"int4 __ovld __cnfn convert_int4_sat(double4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(double4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(double4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(double4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(double4);\n"
"int8 __ovld __cnfn convert_int8(double8);\n"
"int8 __ovld __cnfn convert_int8_rte(double8);\n"
"int8 __ovld __cnfn convert_int8_rtn(double8);\n"
"int8 __ovld __cnfn convert_int8_rtp(double8);\n"
"int8 __ovld __cnfn convert_int8_rtz(double8);\n"
"int8 __ovld __cnfn convert_int8_sat(double8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(double8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(double8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(double8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(double8);\n"
"int16 __ovld __cnfn convert_int16(double16);\n"
"int16 __ovld __cnfn convert_int16_rte(double16);\n"
"int16 __ovld __cnfn convert_int16_rtn(double16);\n"
"int16 __ovld __cnfn convert_int16_rtp(double16);\n"
"int16 __ovld __cnfn convert_int16_rtz(double16);\n"
"int16 __ovld __cnfn convert_int16_sat(double16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(double16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(double16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(double16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(double16);\n"
"\n"
"uint __ovld __cnfn convert_uint(double);\n"
"uint __ovld __cnfn convert_uint_rte(double);\n"
"uint __ovld __cnfn convert_uint_rtn(double);\n"
"uint __ovld __cnfn convert_uint_rtp(double);\n"
"uint __ovld __cnfn convert_uint_rtz(double);\n"
"uint __ovld __cnfn convert_uint_sat(double);\n"
"uint __ovld __cnfn convert_uint_sat_rte(double);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(double);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(double);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(double);\n"
"uint2 __ovld __cnfn convert_uint2(double2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(double2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(double2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(double2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(double2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(double2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(double2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(double2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(double2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(double2);\n"
"uint3 __ovld __cnfn convert_uint3(double3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(double3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(double3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(double3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(double3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(double3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(double3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(double3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(double3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(double3);\n"
"uint4 __ovld __cnfn convert_uint4(double4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(double4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(double4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(double4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(double4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(double4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(double4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(double4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(double4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(double4);\n"
"uint8 __ovld __cnfn convert_uint8(double8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(double8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(double8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(double8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(double8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(double8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(double8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(double8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(double8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(double8);\n"
"uint16 __ovld __cnfn convert_uint16(double16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(double16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(double16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(double16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(double16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(double16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(double16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(double16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(double16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(double16);\n"
"\n"
"long __ovld __cnfn convert_long(double);\n"
"long __ovld __cnfn convert_long_rte(double);\n"
"long __ovld __cnfn convert_long_rtn(double);\n"
"long __ovld __cnfn convert_long_rtp(double);\n"
"long __ovld __cnfn convert_long_rtz(double);\n"
"long __ovld __cnfn convert_long_sat(double);\n"
"long __ovld __cnfn convert_long_sat_rte(double);\n"
"long __ovld __cnfn convert_long_sat_rtn(double);\n"
"long __ovld __cnfn convert_long_sat_rtp(double);\n"
"long __ovld __cnfn convert_long_sat_rtz(double);\n"
"long2 __ovld __cnfn convert_long2(double2);\n"
"long2 __ovld __cnfn convert_long2_rte(double2);\n"
"long2 __ovld __cnfn convert_long2_rtn(double2);\n"
"long2 __ovld __cnfn convert_long2_rtp(double2);\n"
"long2 __ovld __cnfn convert_long2_rtz(double2);\n"
"long2 __ovld __cnfn convert_long2_sat(double2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(double2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(double2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(double2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(double2);\n"
"long3 __ovld __cnfn convert_long3(double3);\n"
"long3 __ovld __cnfn convert_long3_rte(double3);\n"
"long3 __ovld __cnfn convert_long3_rtn(double3);\n"
"long3 __ovld __cnfn convert_long3_rtp(double3);\n"
"long3 __ovld __cnfn convert_long3_rtz(double3);\n"
"long3 __ovld __cnfn convert_long3_sat(double3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(double3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(double3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(double3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(double3);\n"
"long4 __ovld __cnfn convert_long4(double4);\n"
"long4 __ovld __cnfn convert_long4_rte(double4);\n"
"long4 __ovld __cnfn convert_long4_rtn(double4);\n"
"long4 __ovld __cnfn convert_long4_rtp(double4);\n"
"long4 __ovld __cnfn convert_long4_rtz(double4);\n"
"long4 __ovld __cnfn convert_long4_sat(double4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(double4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(double4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(double4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(double4);\n"
"long8 __ovld __cnfn convert_long8(double8);\n"
"long8 __ovld __cnfn convert_long8_rte(double8);\n"
"long8 __ovld __cnfn convert_long8_rtn(double8);\n"
"long8 __ovld __cnfn convert_long8_rtp(double8);\n"
"long8 __ovld __cnfn convert_long8_rtz(double8);\n"
"long8 __ovld __cnfn convert_long8_sat(double8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(double8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(double8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(double8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(double8);\n"
"long16 __ovld __cnfn convert_long16(double16);\n"
"long16 __ovld __cnfn convert_long16_rte(double16);\n"
"long16 __ovld __cnfn convert_long16_rtn(double16);\n"
"long16 __ovld __cnfn convert_long16_rtp(double16);\n"
"long16 __ovld __cnfn convert_long16_rtz(double16);\n"
"long16 __ovld __cnfn convert_long16_sat(double16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(double16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(double16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(double16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(double16);\n"
"\n"
"ulong __ovld __cnfn convert_ulong(double);\n"
"ulong __ovld __cnfn convert_ulong_rte(double);\n"
"ulong __ovld __cnfn convert_ulong_rtn(double);\n"
"ulong __ovld __cnfn convert_ulong_rtp(double);\n"
"ulong __ovld __cnfn convert_ulong_rtz(double);\n"
"ulong __ovld __cnfn convert_ulong_sat(double);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(double);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(double);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(double);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(double);\n"
"ulong2 __ovld __cnfn convert_ulong2(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(double2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(double2);\n"
"ulong3 __ovld __cnfn convert_ulong3(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(double3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(double3);\n"
"ulong4 __ovld __cnfn convert_ulong4(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(double4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(double4);\n"
"ulong8 __ovld __cnfn convert_ulong8(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(double8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(double8);\n"
"ulong16 __ovld __cnfn convert_ulong16(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(double16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(double16);\n"
"\n"
"float __ovld __cnfn convert_float(double);\n"
"float __ovld __cnfn convert_float_rte(double);\n"
"float __ovld __cnfn convert_float_rtn(double);\n"
"float __ovld __cnfn convert_float_rtp(double);\n"
"float __ovld __cnfn convert_float_rtz(double);\n"
"float2 __ovld __cnfn convert_float2(double2);\n"
"float2 __ovld __cnfn convert_float2_rte(double2);\n"
"float2 __ovld __cnfn convert_float2_rtn(double2);\n"
"float2 __ovld __cnfn convert_float2_rtp(double2);\n"
"float2 __ovld __cnfn convert_float2_rtz(double2);\n"
"float3 __ovld __cnfn convert_float3(double3);\n"
"float3 __ovld __cnfn convert_float3_rte(double3);\n"
"float3 __ovld __cnfn convert_float3_rtn(double3);\n"
"float3 __ovld __cnfn convert_float3_rtp(double3);\n"
"float3 __ovld __cnfn convert_float3_rtz(double3);\n"
"float4 __ovld __cnfn convert_float4(double4);\n"
"float4 __ovld __cnfn convert_float4_rte(double4);\n"
"float4 __ovld __cnfn convert_float4_rtn(double4);\n"
"float4 __ovld __cnfn convert_float4_rtp(double4);\n"
"float4 __ovld __cnfn convert_float4_rtz(double4);\n"
"float8 __ovld __cnfn convert_float8(double8);\n"
"float8 __ovld __cnfn convert_float8_rte(double8);\n"
"float8 __ovld __cnfn convert_float8_rtn(double8);\n"
"float8 __ovld __cnfn convert_float8_rtp(double8);\n"
"float8 __ovld __cnfn convert_float8_rtz(double8);\n"
"float16 __ovld __cnfn convert_float16(double16);\n"
"float16 __ovld __cnfn convert_float16_rte(double16);\n"
"float16 __ovld __cnfn convert_float16_rtn(double16);\n"
"float16 __ovld __cnfn convert_float16_rtp(double16);\n"
"float16 __ovld __cnfn convert_float16_rtz(double16);\n"
"\n"
"double __ovld __cnfn convert_double(char);\n"
"double __ovld __cnfn convert_double(double);\n"
"double __ovld __cnfn convert_double(float);\n"
"double __ovld __cnfn convert_double(int);\n"
"double __ovld __cnfn convert_double(long);\n"
"double __ovld __cnfn convert_double(short);\n"
"double __ovld __cnfn convert_double(uchar);\n"
"double __ovld __cnfn convert_double(uint);\n"
"double __ovld __cnfn convert_double(ulong);\n"
"double __ovld __cnfn convert_double(ushort);\n"
"double __ovld __cnfn convert_double_rte(char);\n"
"double __ovld __cnfn convert_double_rte(double);\n"
"double __ovld __cnfn convert_double_rte(float);\n"
"double __ovld __cnfn convert_double_rte(int);\n"
"double __ovld __cnfn convert_double_rte(long);\n"
"double __ovld __cnfn convert_double_rte(short);\n"
"double __ovld __cnfn convert_double_rte(uchar);\n"
"double __ovld __cnfn convert_double_rte(uint);\n"
"double __ovld __cnfn convert_double_rte(ulong);\n"
"double __ovld __cnfn convert_double_rte(ushort);\n"
"double __ovld __cnfn convert_double_rtn(char);\n"
"double __ovld __cnfn convert_double_rtn(double);\n"
"double __ovld __cnfn convert_double_rtn(float);\n"
"double __ovld __cnfn convert_double_rtn(int);\n"
"double __ovld __cnfn convert_double_rtn(long);\n"
"double __ovld __cnfn convert_double_rtn(short);\n"
"double __ovld __cnfn convert_double_rtn(uchar);\n"
"double __ovld __cnfn convert_double_rtn(uint);\n"
"double __ovld __cnfn convert_double_rtn(ulong);\n"
"double __ovld __cnfn convert_double_rtn(ushort);\n"
"double __ovld __cnfn convert_double_rtp(char);\n"
"double __ovld __cnfn convert_double_rtp(double);\n"
"double __ovld __cnfn convert_double_rtp(float);\n"
"double __ovld __cnfn convert_double_rtp(int);\n"
"double __ovld __cnfn convert_double_rtp(long);\n"
"double __ovld __cnfn convert_double_rtp(short);\n"
"double __ovld __cnfn convert_double_rtp(uchar);\n"
"double __ovld __cnfn convert_double_rtp(uint);\n"
"double __ovld __cnfn convert_double_rtp(ulong);\n"
"double __ovld __cnfn convert_double_rtp(ushort);\n"
"double __ovld __cnfn convert_double_rtz(char);\n"
"double __ovld __cnfn convert_double_rtz(double);\n"
"double __ovld __cnfn convert_double_rtz(float);\n"
"double __ovld __cnfn convert_double_rtz(int);\n"
"double __ovld __cnfn convert_double_rtz(long);\n"
"double __ovld __cnfn convert_double_rtz(short);\n"
"double __ovld __cnfn convert_double_rtz(uchar);\n"
"double __ovld __cnfn convert_double_rtz(uint);\n"
"double __ovld __cnfn convert_double_rtz(ulong);\n"
"double __ovld __cnfn convert_double_rtz(ushort);\n"
"double2 __ovld __cnfn convert_double2(char2);\n"
"double2 __ovld __cnfn convert_double2(double2);\n"
"double2 __ovld __cnfn convert_double2(float2);\n"
"double2 __ovld __cnfn convert_double2(int2);\n"
"double2 __ovld __cnfn convert_double2(long2);\n"
"double2 __ovld __cnfn convert_double2(short2);\n"
"double2 __ovld __cnfn convert_double2(uchar2);\n"
"double2 __ovld __cnfn convert_double2(uint2);\n"
"double2 __ovld __cnfn convert_double2(ulong2);\n"
"double2 __ovld __cnfn convert_double2(ushort2);\n"
"double2 __ovld __cnfn convert_double2_rte(char2);\n"
"double2 __ovld __cnfn convert_double2_rte(double2);\n"
"double2 __ovld __cnfn convert_double2_rte(float2);\n"
"double2 __ovld __cnfn convert_double2_rte(int2);\n"
"double2 __ovld __cnfn convert_double2_rte(long2);\n"
"double2 __ovld __cnfn convert_double2_rte(short2);\n"
"double2 __ovld __cnfn convert_double2_rte(uchar2);\n"
"double2 __ovld __cnfn convert_double2_rte(uint2);\n"
"double2 __ovld __cnfn convert_double2_rte(ulong2);\n"
"double2 __ovld __cnfn convert_double2_rte(ushort2);\n"
"double2 __ovld __cnfn convert_double2_rtn(char2);\n"
"double2 __ovld __cnfn convert_double2_rtn(double2);\n"
"double2 __ovld __cnfn convert_double2_rtn(float2);\n"
"double2 __ovld __cnfn convert_double2_rtn(int2);\n"
"double2 __ovld __cnfn convert_double2_rtn(long2);\n"
"double2 __ovld __cnfn convert_double2_rtn(short2);\n"
"double2 __ovld __cnfn convert_double2_rtn(uchar2);\n"
"double2 __ovld __cnfn convert_double2_rtn(uint2);\n"
"double2 __ovld __cnfn convert_double2_rtn(ulong2);\n"
"double2 __ovld __cnfn convert_double2_rtn(ushort2);\n"
"double2 __ovld __cnfn convert_double2_rtp(char2);\n"
"double2 __ovld __cnfn convert_double2_rtp(double2);\n"
"double2 __ovld __cnfn convert_double2_rtp(float2);\n"
"double2 __ovld __cnfn convert_double2_rtp(int2);\n"
"double2 __ovld __cnfn convert_double2_rtp(long2);\n"
"double2 __ovld __cnfn convert_double2_rtp(short2);\n"
"double2 __ovld __cnfn convert_double2_rtp(uchar2);\n"
"double2 __ovld __cnfn convert_double2_rtp(uint2);\n"
"double2 __ovld __cnfn convert_double2_rtp(ulong2);\n"
"double2 __ovld __cnfn convert_double2_rtp(ushort2);\n"
"double2 __ovld __cnfn convert_double2_rtz(char2);\n"
"double2 __ovld __cnfn convert_double2_rtz(double2);\n"
"double2 __ovld __cnfn convert_double2_rtz(float2);\n"
"double2 __ovld __cnfn convert_double2_rtz(int2);\n"
"double2 __ovld __cnfn convert_double2_rtz(long2);\n"
"double2 __ovld __cnfn convert_double2_rtz(short2);\n"
"double2 __ovld __cnfn convert_double2_rtz(uchar2);\n"
"double2 __ovld __cnfn convert_double2_rtz(uint2);\n"
"double2 __ovld __cnfn convert_double2_rtz(ulong2);\n"
"double2 __ovld __cnfn convert_double2_rtz(ushort2);\n"
"double3 __ovld __cnfn convert_double3(char3);\n"
"double3 __ovld __cnfn convert_double3(double3);\n"
"double3 __ovld __cnfn convert_double3(float3);\n"
"double3 __ovld __cnfn convert_double3(int3);\n"
"double3 __ovld __cnfn convert_double3(long3);\n"
"double3 __ovld __cnfn convert_double3(short3);\n"
"double3 __ovld __cnfn convert_double3(uchar3);\n"
"double3 __ovld __cnfn convert_double3(uint3);\n"
"double3 __ovld __cnfn convert_double3(ulong3);\n"
"double3 __ovld __cnfn convert_double3(ushort3);\n"
"double3 __ovld __cnfn convert_double3_rte(char3);\n"
"double3 __ovld __cnfn convert_double3_rte(double3);\n"
"double3 __ovld __cnfn convert_double3_rte(float3);\n"
"double3 __ovld __cnfn convert_double3_rte(int3);\n"
"double3 __ovld __cnfn convert_double3_rte(long3);\n"
"double3 __ovld __cnfn convert_double3_rte(short3);\n"
"double3 __ovld __cnfn convert_double3_rte(uchar3);\n"
"double3 __ovld __cnfn convert_double3_rte(uint3);\n"
"double3 __ovld __cnfn convert_double3_rte(ulong3);\n"
"double3 __ovld __cnfn convert_double3_rte(ushort3);\n"
"double3 __ovld __cnfn convert_double3_rtn(char3);\n"
"double3 __ovld __cnfn convert_double3_rtn(double3);\n"
"double3 __ovld __cnfn convert_double3_rtn(float3);\n"
"double3 __ovld __cnfn convert_double3_rtn(int3);\n"
"double3 __ovld __cnfn convert_double3_rtn(long3);\n"
"double3 __ovld __cnfn convert_double3_rtn(short3);\n"
"double3 __ovld __cnfn convert_double3_rtn(uchar3);\n"
"double3 __ovld __cnfn convert_double3_rtn(uint3);\n"
"double3 __ovld __cnfn convert_double3_rtn(ulong3);\n"
"double3 __ovld __cnfn convert_double3_rtn(ushort3);\n"
"double3 __ovld __cnfn convert_double3_rtp(char3);\n"
"double3 __ovld __cnfn convert_double3_rtp(double3);\n"
"double3 __ovld __cnfn convert_double3_rtp(float3);\n"
"double3 __ovld __cnfn convert_double3_rtp(int3);\n"
"double3 __ovld __cnfn convert_double3_rtp(long3);\n"
"double3 __ovld __cnfn convert_double3_rtp(short3);\n"
"double3 __ovld __cnfn convert_double3_rtp(uchar3);\n"
"double3 __ovld __cnfn convert_double3_rtp(uint3);\n"
"double3 __ovld __cnfn convert_double3_rtp(ulong3);\n"
"double3 __ovld __cnfn convert_double3_rtp(ushort3);\n"
"double3 __ovld __cnfn convert_double3_rtz(char3);\n"
"double3 __ovld __cnfn convert_double3_rtz(double3);\n"
"double3 __ovld __cnfn convert_double3_rtz(float3);\n"
"double3 __ovld __cnfn convert_double3_rtz(int3);\n"
"double3 __ovld __cnfn convert_double3_rtz(long3);\n"
"double3 __ovld __cnfn convert_double3_rtz(short3);\n"
"double3 __ovld __cnfn convert_double3_rtz(uchar3);\n"
"double3 __ovld __cnfn convert_double3_rtz(uint3);\n"
"double3 __ovld __cnfn convert_double3_rtz(ulong3);\n"
"double3 __ovld __cnfn convert_double3_rtz(ushort3);\n"
"double4 __ovld __cnfn convert_double4(char4);\n"
"double4 __ovld __cnfn convert_double4(double4);\n"
"double4 __ovld __cnfn convert_double4(float4);\n"
"double4 __ovld __cnfn convert_double4(int4);\n"
"double4 __ovld __cnfn convert_double4(long4);\n"
"double4 __ovld __cnfn convert_double4(short4);\n"
"double4 __ovld __cnfn convert_double4(uchar4);\n"
"double4 __ovld __cnfn convert_double4(uint4);\n"
"double4 __ovld __cnfn convert_double4(ulong4);\n"
"double4 __ovld __cnfn convert_double4(ushort4);\n"
"double4 __ovld __cnfn convert_double4_rte(char4);\n"
"double4 __ovld __cnfn convert_double4_rte(double4);\n"
"double4 __ovld __cnfn convert_double4_rte(float4);\n"
"double4 __ovld __cnfn convert_double4_rte(int4);\n"
"double4 __ovld __cnfn convert_double4_rte(long4);\n"
"double4 __ovld __cnfn convert_double4_rte(short4);\n"
"double4 __ovld __cnfn convert_double4_rte(uchar4);\n"
"double4 __ovld __cnfn convert_double4_rte(uint4);\n"
"double4 __ovld __cnfn convert_double4_rte(ulong4);\n"
"double4 __ovld __cnfn convert_double4_rte(ushort4);\n"
"double4 __ovld __cnfn convert_double4_rtn(char4);\n"
"double4 __ovld __cnfn convert_double4_rtn(double4);\n"
"double4 __ovld __cnfn convert_double4_rtn(float4);\n"
"double4 __ovld __cnfn convert_double4_rtn(int4);\n"
"double4 __ovld __cnfn convert_double4_rtn(long4);\n"
"double4 __ovld __cnfn convert_double4_rtn(short4);\n"
"double4 __ovld __cnfn convert_double4_rtn(uchar4);\n"
"double4 __ovld __cnfn convert_double4_rtn(uint4);\n"
"double4 __ovld __cnfn convert_double4_rtn(ulong4);\n"
"double4 __ovld __cnfn convert_double4_rtn(ushort4);\n"
"double4 __ovld __cnfn convert_double4_rtp(char4);\n"
"double4 __ovld __cnfn convert_double4_rtp(double4);\n"
"double4 __ovld __cnfn convert_double4_rtp(float4);\n"
"double4 __ovld __cnfn convert_double4_rtp(int4);\n"
"double4 __ovld __cnfn convert_double4_rtp(long4);\n"
"double4 __ovld __cnfn convert_double4_rtp(short4);\n"
"double4 __ovld __cnfn convert_double4_rtp(uchar4);\n"
"double4 __ovld __cnfn convert_double4_rtp(uint4);\n"
"double4 __ovld __cnfn convert_double4_rtp(ulong4);\n"
"double4 __ovld __cnfn convert_double4_rtp(ushort4);\n"
"double4 __ovld __cnfn convert_double4_rtz(char4);\n"
"double4 __ovld __cnfn convert_double4_rtz(double4);\n"
"double4 __ovld __cnfn convert_double4_rtz(float4);\n"
"double4 __ovld __cnfn convert_double4_rtz(int4);\n"
"double4 __ovld __cnfn convert_double4_rtz(long4);\n"
"double4 __ovld __cnfn convert_double4_rtz(short4);\n"
"double4 __ovld __cnfn convert_double4_rtz(uchar4);\n"
"double4 __ovld __cnfn convert_double4_rtz(uint4);\n"
"double4 __ovld __cnfn convert_double4_rtz(ulong4);\n"
"double4 __ovld __cnfn convert_double4_rtz(ushort4);\n"
"double8 __ovld __cnfn convert_double8(char8);\n"
"double8 __ovld __cnfn convert_double8(double8);\n"
"double8 __ovld __cnfn convert_double8(float8);\n"
"double8 __ovld __cnfn convert_double8(int8);\n"
"double8 __ovld __cnfn convert_double8(long8);\n"
"double8 __ovld __cnfn convert_double8(short8);\n"
"double8 __ovld __cnfn convert_double8(uchar8);\n"
"double8 __ovld __cnfn convert_double8(uint8);\n"
"double8 __ovld __cnfn convert_double8(ulong8);\n"
"double8 __ovld __cnfn convert_double8(ushort8);\n"
"double8 __ovld __cnfn convert_double8_rte(char8);\n"
"double8 __ovld __cnfn convert_double8_rte(double8);\n"
"double8 __ovld __cnfn convert_double8_rte(float8);\n"
"double8 __ovld __cnfn convert_double8_rte(int8);\n"
"double8 __ovld __cnfn convert_double8_rte(long8);\n"
"double8 __ovld __cnfn convert_double8_rte(short8);\n"
"double8 __ovld __cnfn convert_double8_rte(uchar8);\n"
"double8 __ovld __cnfn convert_double8_rte(uint8);\n"
"double8 __ovld __cnfn convert_double8_rte(ulong8);\n"
"double8 __ovld __cnfn convert_double8_rte(ushort8);\n"
"double8 __ovld __cnfn convert_double8_rtn(char8);\n"
"double8 __ovld __cnfn convert_double8_rtn(double8);\n"
"double8 __ovld __cnfn convert_double8_rtn(float8);\n"
"double8 __ovld __cnfn convert_double8_rtn(int8);\n"
"double8 __ovld __cnfn convert_double8_rtn(long8);\n"
"double8 __ovld __cnfn convert_double8_rtn(short8);\n"
"double8 __ovld __cnfn convert_double8_rtn(uchar8);\n"
"double8 __ovld __cnfn convert_double8_rtn(uint8);\n"
"double8 __ovld __cnfn convert_double8_rtn(ulong8);\n"
"double8 __ovld __cnfn convert_double8_rtn(ushort8);\n"
"double8 __ovld __cnfn convert_double8_rtp(char8);\n"
"double8 __ovld __cnfn convert_double8_rtp(double8);\n"
"double8 __ovld __cnfn convert_double8_rtp(float8);\n"
"double8 __ovld __cnfn convert_double8_rtp(int8);\n"
"double8 __ovld __cnfn convert_double8_rtp(long8);\n"
"double8 __ovld __cnfn convert_double8_rtp(short8);\n"
"double8 __ovld __cnfn convert_double8_rtp(uchar8);\n"
"double8 __ovld __cnfn convert_double8_rtp(uint8);\n"
"double8 __ovld __cnfn convert_double8_rtp(ulong8);\n"
"double8 __ovld __cnfn convert_double8_rtp(ushort8);\n"
"double8 __ovld __cnfn convert_double8_rtz(char8);\n"
"double8 __ovld __cnfn convert_double8_rtz(double8);\n"
"double8 __ovld __cnfn convert_double8_rtz(float8);\n"
"double8 __ovld __cnfn convert_double8_rtz(int8);\n"
"double8 __ovld __cnfn convert_double8_rtz(long8);\n"
"double8 __ovld __cnfn convert_double8_rtz(short8);\n"
"double8 __ovld __cnfn convert_double8_rtz(uchar8);\n"
"double8 __ovld __cnfn convert_double8_rtz(uint8);\n"
"double8 __ovld __cnfn convert_double8_rtz(ulong8);\n"
"double8 __ovld __cnfn convert_double8_rtz(ushort8);\n"
"double16 __ovld __cnfn convert_double16(char16);\n"
"double16 __ovld __cnfn convert_double16(double16);\n"
"double16 __ovld __cnfn convert_double16(float16);\n"
"double16 __ovld __cnfn convert_double16(int16);\n"
"double16 __ovld __cnfn convert_double16(long16);\n"
"double16 __ovld __cnfn convert_double16(short16);\n"
"double16 __ovld __cnfn convert_double16(uchar16);\n"
"double16 __ovld __cnfn convert_double16(uint16);\n"
"double16 __ovld __cnfn convert_double16(ulong16);\n"
"double16 __ovld __cnfn convert_double16(ushort16);\n"
"double16 __ovld __cnfn convert_double16_rte(char16);\n"
"double16 __ovld __cnfn convert_double16_rte(double16);\n"
"double16 __ovld __cnfn convert_double16_rte(float16);\n"
"double16 __ovld __cnfn convert_double16_rte(int16);\n"
"double16 __ovld __cnfn convert_double16_rte(long16);\n"
"double16 __ovld __cnfn convert_double16_rte(short16);\n"
"double16 __ovld __cnfn convert_double16_rte(uchar16);\n"
"double16 __ovld __cnfn convert_double16_rte(uint16);\n"
"double16 __ovld __cnfn convert_double16_rte(ulong16);\n"
"double16 __ovld __cnfn convert_double16_rte(ushort16);\n"
"double16 __ovld __cnfn convert_double16_rtn(char16);\n"
"double16 __ovld __cnfn convert_double16_rtn(double16);\n"
"double16 __ovld __cnfn convert_double16_rtn(float16);\n"
"double16 __ovld __cnfn convert_double16_rtn(int16);\n"
"double16 __ovld __cnfn convert_double16_rtn(long16);\n"
"double16 __ovld __cnfn convert_double16_rtn(short16);\n"
"double16 __ovld __cnfn convert_double16_rtn(uchar16);\n"
"double16 __ovld __cnfn convert_double16_rtn(uint16);\n"
"double16 __ovld __cnfn convert_double16_rtn(ulong16);\n"
"double16 __ovld __cnfn convert_double16_rtn(ushort16);\n"
"double16 __ovld __cnfn convert_double16_rtp(char16);\n"
"double16 __ovld __cnfn convert_double16_rtp(double16);\n"
"double16 __ovld __cnfn convert_double16_rtp(float16);\n"
"double16 __ovld __cnfn convert_double16_rtp(int16);\n"
"double16 __ovld __cnfn convert_double16_rtp(long16);\n"
"double16 __ovld __cnfn convert_double16_rtp(short16);\n"
"double16 __ovld __cnfn convert_double16_rtp(uchar16);\n"
"double16 __ovld __cnfn convert_double16_rtp(uint16);\n"
"double16 __ovld __cnfn convert_double16_rtp(ulong16);\n"
"double16 __ovld __cnfn convert_double16_rtp(ushort16);\n"
"double16 __ovld __cnfn convert_double16_rtz(char16);\n"
"double16 __ovld __cnfn convert_double16_rtz(double16);\n"
"double16 __ovld __cnfn convert_double16_rtz(float16);\n"
"double16 __ovld __cnfn convert_double16_rtz(int16);\n"
"double16 __ovld __cnfn convert_double16_rtz(long16);\n"
"double16 __ovld __cnfn convert_double16_rtz(short16);\n"
"double16 __ovld __cnfn convert_double16_rtz(uchar16);\n"
"double16 __ovld __cnfn convert_double16_rtz(uint16);\n"
"double16 __ovld __cnfn convert_double16_rtz(ulong16);\n"
"double16 __ovld __cnfn convert_double16_rtz(ushort16);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"// Convert half types to non-double types.\n"
"uchar __ovld __cnfn convert_uchar(half);\n"
"uchar __ovld __cnfn convert_uchar_rte(half);\n"
"uchar __ovld __cnfn convert_uchar_rtp(half);\n"
"uchar __ovld __cnfn convert_uchar_rtn(half);\n"
"uchar __ovld __cnfn convert_uchar_rtz(half);\n"
"uchar __ovld __cnfn convert_uchar_sat(half);\n"
"uchar __ovld __cnfn convert_uchar_sat_rte(half);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtp(half);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtn(half);\n"
"uchar __ovld __cnfn convert_uchar_sat_rtz(half);\n"
"uchar2 __ovld __cnfn convert_uchar2(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rte(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtp(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtn(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_rtz(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rte(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtp(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtn(half2);\n"
"uchar2 __ovld __cnfn convert_uchar2_sat_rtz(half2);\n"
"uchar3 __ovld __cnfn convert_uchar3(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rte(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtp(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtn(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_rtz(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rte(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtp(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtn(half3);\n"
"uchar3 __ovld __cnfn convert_uchar3_sat_rtz(half3);\n"
"uchar4 __ovld __cnfn convert_uchar4(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rte(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtp(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtn(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_rtz(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rte(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtp(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtn(half4);\n"
"uchar4 __ovld __cnfn convert_uchar4_sat_rtz(half4);\n"
"uchar8 __ovld __cnfn convert_uchar8(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rte(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtp(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtn(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_rtz(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rte(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtp(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtn(half8);\n"
"uchar8 __ovld __cnfn convert_uchar8_sat_rtz(half8);\n"
"uchar16 __ovld __cnfn convert_uchar16(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rte(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtp(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtn(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_rtz(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rte(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtp(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtn(half16);\n"
"uchar16 __ovld __cnfn convert_uchar16_sat_rtz(half16);\n"
"ushort __ovld __cnfn convert_ushort(half);\n"
"ushort __ovld __cnfn convert_ushort_rte(half);\n"
"ushort __ovld __cnfn convert_ushort_rtp(half);\n"
"ushort __ovld __cnfn convert_ushort_rtn(half);\n"
"ushort __ovld __cnfn convert_ushort_rtz(half);\n"
"ushort __ovld __cnfn convert_ushort_sat(half);\n"
"ushort __ovld __cnfn convert_ushort_sat_rte(half);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtp(half);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtn(half);\n"
"ushort __ovld __cnfn convert_ushort_sat_rtz(half);\n"
"ushort2 __ovld __cnfn convert_ushort2(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rte(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtp(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtn(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_rtz(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rte(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtp(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtn(half2);\n"
"ushort2 __ovld __cnfn convert_ushort2_sat_rtz(half2);\n"
"ushort3 __ovld __cnfn convert_ushort3(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rte(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtp(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtn(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_rtz(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rte(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtp(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtn(half3);\n"
"ushort3 __ovld __cnfn convert_ushort3_sat_rtz(half3);\n"
"ushort4 __ovld __cnfn convert_ushort4(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rte(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtp(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtn(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_rtz(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rte(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtp(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtn(half4);\n"
"ushort4 __ovld __cnfn convert_ushort4_sat_rtz(half4);\n"
"ushort8 __ovld __cnfn convert_ushort8(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rte(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtp(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtn(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_rtz(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rte(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtp(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtn(half8);\n"
"ushort8 __ovld __cnfn convert_ushort8_sat_rtz(half8);\n"
"ushort16 __ovld __cnfn convert_ushort16(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rte(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtp(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtn(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_rtz(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rte(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtp(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtn(half16);\n"
"ushort16 __ovld __cnfn convert_ushort16_sat_rtz(half16);\n"
"uint __ovld __cnfn convert_uint(half);\n"
"uint __ovld __cnfn convert_uint_rte(half);\n"
"uint __ovld __cnfn convert_uint_rtp(half);\n"
"uint __ovld __cnfn convert_uint_rtn(half);\n"
"uint __ovld __cnfn convert_uint_rtz(half);\n"
"uint __ovld __cnfn convert_uint_sat(half);\n"
"uint __ovld __cnfn convert_uint_sat_rte(half);\n"
"uint __ovld __cnfn convert_uint_sat_rtp(half);\n"
"uint __ovld __cnfn convert_uint_sat_rtn(half);\n"
"uint __ovld __cnfn convert_uint_sat_rtz(half);\n"
"uint2 __ovld __cnfn convert_uint2(half2);\n"
"uint2 __ovld __cnfn convert_uint2_rte(half2);\n"
"uint2 __ovld __cnfn convert_uint2_rtp(half2);\n"
"uint2 __ovld __cnfn convert_uint2_rtn(half2);\n"
"uint2 __ovld __cnfn convert_uint2_rtz(half2);\n"
"uint2 __ovld __cnfn convert_uint2_sat(half2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rte(half2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtp(half2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtn(half2);\n"
"uint2 __ovld __cnfn convert_uint2_sat_rtz(half2);\n"
"uint3 __ovld __cnfn convert_uint3(half3);\n"
"uint3 __ovld __cnfn convert_uint3_rte(half3);\n"
"uint3 __ovld __cnfn convert_uint3_rtp(half3);\n"
"uint3 __ovld __cnfn convert_uint3_rtn(half3);\n"
"uint3 __ovld __cnfn convert_uint3_rtz(half3);\n"
"uint3 __ovld __cnfn convert_uint3_sat(half3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rte(half3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtp(half3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtn(half3);\n"
"uint3 __ovld __cnfn convert_uint3_sat_rtz(half3);\n"
"uint4 __ovld __cnfn convert_uint4(half4);\n"
"uint4 __ovld __cnfn convert_uint4_rte(half4);\n"
"uint4 __ovld __cnfn convert_uint4_rtp(half4);\n"
"uint4 __ovld __cnfn convert_uint4_rtn(half4);\n"
"uint4 __ovld __cnfn convert_uint4_rtz(half4);\n"
"uint4 __ovld __cnfn convert_uint4_sat(half4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rte(half4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtp(half4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtn(half4);\n"
"uint4 __ovld __cnfn convert_uint4_sat_rtz(half4);\n"
"uint8 __ovld __cnfn convert_uint8(half8);\n"
"uint8 __ovld __cnfn convert_uint8_rte(half8);\n"
"uint8 __ovld __cnfn convert_uint8_rtp(half8);\n"
"uint8 __ovld __cnfn convert_uint8_rtn(half8);\n"
"uint8 __ovld __cnfn convert_uint8_rtz(half8);\n"
"uint8 __ovld __cnfn convert_uint8_sat(half8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rte(half8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtp(half8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtn(half8);\n"
"uint8 __ovld __cnfn convert_uint8_sat_rtz(half8);\n"
"uint16 __ovld __cnfn convert_uint16(half16);\n"
"uint16 __ovld __cnfn convert_uint16_rte(half16);\n"
"uint16 __ovld __cnfn convert_uint16_rtp(half16);\n"
"uint16 __ovld __cnfn convert_uint16_rtn(half16);\n"
"uint16 __ovld __cnfn convert_uint16_rtz(half16);\n"
"uint16 __ovld __cnfn convert_uint16_sat(half16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rte(half16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtp(half16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtn(half16);\n"
"uint16 __ovld __cnfn convert_uint16_sat_rtz(half16);\n"
"ulong __ovld __cnfn convert_ulong(half);\n"
"ulong __ovld __cnfn convert_ulong_rte(half);\n"
"ulong __ovld __cnfn convert_ulong_rtp(half);\n"
"ulong __ovld __cnfn convert_ulong_rtn(half);\n"
"ulong __ovld __cnfn convert_ulong_rtz(half);\n"
"ulong __ovld __cnfn convert_ulong_sat(half);\n"
"ulong __ovld __cnfn convert_ulong_sat_rte(half);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtp(half);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtn(half);\n"
"ulong __ovld __cnfn convert_ulong_sat_rtz(half);\n"
"ulong2 __ovld __cnfn convert_ulong2(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rte(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtp(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtn(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_rtz(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rte(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtp(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtn(half2);\n"
"ulong2 __ovld __cnfn convert_ulong2_sat_rtz(half2);\n"
"ulong3 __ovld __cnfn convert_ulong3(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rte(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtp(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtn(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_rtz(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rte(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtp(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtn(half3);\n"
"ulong3 __ovld __cnfn convert_ulong3_sat_rtz(half3);\n"
"ulong4 __ovld __cnfn convert_ulong4(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rte(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtp(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtn(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_rtz(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rte(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtp(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtn(half4);\n"
"ulong4 __ovld __cnfn convert_ulong4_sat_rtz(half4);\n"
"ulong8 __ovld __cnfn convert_ulong8(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rte(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtp(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtn(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_rtz(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rte(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtp(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtn(half8);\n"
"ulong8 __ovld __cnfn convert_ulong8_sat_rtz(half8);\n"
"ulong16 __ovld __cnfn convert_ulong16(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rte(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtp(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtn(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_rtz(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rte(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtp(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtn(half16);\n"
"ulong16 __ovld __cnfn convert_ulong16_sat_rtz(half16);\n"
"char __ovld __cnfn convert_char(half);\n"
"char __ovld __cnfn convert_char_rte(half);\n"
"char __ovld __cnfn convert_char_rtp(half);\n"
"char __ovld __cnfn convert_char_rtn(half);\n"
"char __ovld __cnfn convert_char_rtz(half);\n"
"char __ovld __cnfn convert_char_sat(half);\n"
"char __ovld __cnfn convert_char_sat_rte(half);\n"
"char __ovld __cnfn convert_char_sat_rtp(half);\n"
"char __ovld __cnfn convert_char_sat_rtn(half);\n"
"char __ovld __cnfn convert_char_sat_rtz(half);\n"
"char2 __ovld __cnfn convert_char2(half2);\n"
"char2 __ovld __cnfn convert_char2_rte(half2);\n"
"char2 __ovld __cnfn convert_char2_rtp(half2);\n"
"char2 __ovld __cnfn convert_char2_rtn(half2);\n"
"char2 __ovld __cnfn convert_char2_rtz(half2);\n"
"char2 __ovld __cnfn convert_char2_sat(half2);\n"
"char2 __ovld __cnfn convert_char2_sat_rte(half2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtp(half2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtn(half2);\n"
"char2 __ovld __cnfn convert_char2_sat_rtz(half2);\n"
"char3 __ovld __cnfn convert_char3(half3);\n"
"char3 __ovld __cnfn convert_char3_rte(half3);\n"
"char3 __ovld __cnfn convert_char3_rtp(half3);\n"
"char3 __ovld __cnfn convert_char3_rtn(half3);\n"
"char3 __ovld __cnfn convert_char3_rtz(half3);\n"
"char3 __ovld __cnfn convert_char3_sat(half3);\n"
"char3 __ovld __cnfn convert_char3_sat_rte(half3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtp(half3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtn(half3);\n"
"char3 __ovld __cnfn convert_char3_sat_rtz(half3);\n"
"char4 __ovld __cnfn convert_char4(half4);\n"
"char4 __ovld __cnfn convert_char4_rte(half4);\n"
"char4 __ovld __cnfn convert_char4_rtp(half4);\n"
"char4 __ovld __cnfn convert_char4_rtn(half4);\n"
"char4 __ovld __cnfn convert_char4_rtz(half4);\n"
"char4 __ovld __cnfn convert_char4_sat(half4);\n"
"char4 __ovld __cnfn convert_char4_sat_rte(half4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtp(half4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtn(half4);\n"
"char4 __ovld __cnfn convert_char4_sat_rtz(half4);\n"
"char8 __ovld __cnfn convert_char8(half8);\n"
"char8 __ovld __cnfn convert_char8_rte(half8);\n"
"char8 __ovld __cnfn convert_char8_rtp(half8);\n"
"char8 __ovld __cnfn convert_char8_rtn(half8);\n"
"char8 __ovld __cnfn convert_char8_rtz(half8);\n"
"char8 __ovld __cnfn convert_char8_sat(half8);\n"
"char8 __ovld __cnfn convert_char8_sat_rte(half8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtp(half8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtn(half8);\n"
"char8 __ovld __cnfn convert_char8_sat_rtz(half8);\n"
"char16 __ovld __cnfn convert_char16(half16);\n"
"char16 __ovld __cnfn convert_char16_rte(half16);\n"
"char16 __ovld __cnfn convert_char16_rtp(half16);\n"
"char16 __ovld __cnfn convert_char16_rtn(half16);\n"
"char16 __ovld __cnfn convert_char16_rtz(half16);\n"
"char16 __ovld __cnfn convert_char16_sat(half16);\n"
"char16 __ovld __cnfn convert_char16_sat_rte(half16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtp(half16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtn(half16);\n"
"char16 __ovld __cnfn convert_char16_sat_rtz(half16);\n"
"short __ovld __cnfn convert_short(half);\n"
"short __ovld __cnfn convert_short_rte(half);\n"
"short __ovld __cnfn convert_short_rtp(half);\n"
"short __ovld __cnfn convert_short_rtn(half);\n"
"short __ovld __cnfn convert_short_rtz(half);\n"
"short __ovld __cnfn convert_short_sat(half);\n"
"short __ovld __cnfn convert_short_sat_rte(half);\n"
"short __ovld __cnfn convert_short_sat_rtp(half);\n"
"short __ovld __cnfn convert_short_sat_rtn(half);\n"
"short __ovld __cnfn convert_short_sat_rtz(half);\n"
"short2 __ovld __cnfn convert_short2(half2);\n"
"short2 __ovld __cnfn convert_short2_rte(half2);\n"
"short2 __ovld __cnfn convert_short2_rtp(half2);\n"
"short2 __ovld __cnfn convert_short2_rtn(half2);\n"
"short2 __ovld __cnfn convert_short2_rtz(half2);\n"
"short2 __ovld __cnfn convert_short2_sat(half2);\n"
"short2 __ovld __cnfn convert_short2_sat_rte(half2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtp(half2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtn(half2);\n"
"short2 __ovld __cnfn convert_short2_sat_rtz(half2);\n"
"short3 __ovld __cnfn convert_short3(half3);\n"
"short3 __ovld __cnfn convert_short3_rte(half3);\n"
"short3 __ovld __cnfn convert_short3_rtp(half3);\n"
"short3 __ovld __cnfn convert_short3_rtn(half3);\n"
"short3 __ovld __cnfn convert_short3_rtz(half3);\n"
"short3 __ovld __cnfn convert_short3_sat(half3);\n"
"short3 __ovld __cnfn convert_short3_sat_rte(half3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtp(half3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtn(half3);\n"
"short3 __ovld __cnfn convert_short3_sat_rtz(half3);\n"
"short4 __ovld __cnfn convert_short4(half4);\n"
"short4 __ovld __cnfn convert_short4_rte(half4);\n"
"short4 __ovld __cnfn convert_short4_rtp(half4);\n"
"short4 __ovld __cnfn convert_short4_rtn(half4);\n"
"short4 __ovld __cnfn convert_short4_rtz(half4);\n"
"short4 __ovld __cnfn convert_short4_sat(half4);\n"
"short4 __ovld __cnfn convert_short4_sat_rte(half4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtp(half4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtn(half4);\n"
"short4 __ovld __cnfn convert_short4_sat_rtz(half4);\n"
"short8 __ovld __cnfn convert_short8(half8);\n"
"short8 __ovld __cnfn convert_short8_rte(half8);\n"
"short8 __ovld __cnfn convert_short8_rtp(half8);\n"
"short8 __ovld __cnfn convert_short8_rtn(half8);\n"
"short8 __ovld __cnfn convert_short8_rtz(half8);\n"
"short8 __ovld __cnfn convert_short8_sat(half8);\n"
"short8 __ovld __cnfn convert_short8_sat_rte(half8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtp(half8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtn(half8);\n"
"short8 __ovld __cnfn convert_short8_sat_rtz(half8);\n"
"short16 __ovld __cnfn convert_short16(half16);\n"
"short16 __ovld __cnfn convert_short16_rte(half16);\n"
"short16 __ovld __cnfn convert_short16_rtp(half16);\n"
"short16 __ovld __cnfn convert_short16_rtn(half16);\n"
"short16 __ovld __cnfn convert_short16_rtz(half16);\n"
"short16 __ovld __cnfn convert_short16_sat(half16);\n"
"short16 __ovld __cnfn convert_short16_sat_rte(half16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtp(half16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtn(half16);\n"
"short16 __ovld __cnfn convert_short16_sat_rtz(half16);\n"
"int __ovld __cnfn convert_int(half);\n"
"int __ovld __cnfn convert_int_rte(half);\n"
"int __ovld __cnfn convert_int_rtp(half);\n"
"int __ovld __cnfn convert_int_rtn(half);\n"
"int __ovld __cnfn convert_int_rtz(half);\n"
"int __ovld __cnfn convert_int_sat(half);\n"
"int __ovld __cnfn convert_int_sat_rte(half);\n"
"int __ovld __cnfn convert_int_sat_rtp(half);\n"
"int __ovld __cnfn convert_int_sat_rtn(half);\n"
"int __ovld __cnfn convert_int_sat_rtz(half);\n"
"int2 __ovld __cnfn convert_int2(half2);\n"
"int2 __ovld __cnfn convert_int2_rte(half2);\n"
"int2 __ovld __cnfn convert_int2_rtp(half2);\n"
"int2 __ovld __cnfn convert_int2_rtn(half2);\n"
"int2 __ovld __cnfn convert_int2_rtz(half2);\n"
"int2 __ovld __cnfn convert_int2_sat(half2);\n"
"int2 __ovld __cnfn convert_int2_sat_rte(half2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtp(half2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtn(half2);\n"
"int2 __ovld __cnfn convert_int2_sat_rtz(half2);\n"
"int3 __ovld __cnfn convert_int3(half3);\n"
"int3 __ovld __cnfn convert_int3_rte(half3);\n"
"int3 __ovld __cnfn convert_int3_rtp(half3);\n"
"int3 __ovld __cnfn convert_int3_rtn(half3);\n"
"int3 __ovld __cnfn convert_int3_rtz(half3);\n"
"int3 __ovld __cnfn convert_int3_sat(half3);\n"
"int3 __ovld __cnfn convert_int3_sat_rte(half3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtp(half3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtn(half3);\n"
"int3 __ovld __cnfn convert_int3_sat_rtz(half3);\n"
"int4 __ovld __cnfn convert_int4(half4);\n"
"int4 __ovld __cnfn convert_int4_rte(half4);\n"
"int4 __ovld __cnfn convert_int4_rtp(half4);\n"
"int4 __ovld __cnfn convert_int4_rtn(half4);\n"
"int4 __ovld __cnfn convert_int4_rtz(half4);\n"
"int4 __ovld __cnfn convert_int4_sat(half4);\n"
"int4 __ovld __cnfn convert_int4_sat_rte(half4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtp(half4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtn(half4);\n"
"int4 __ovld __cnfn convert_int4_sat_rtz(half4);\n"
"int8 __ovld __cnfn convert_int8(half8);\n"
"int8 __ovld __cnfn convert_int8_rte(half8);\n"
"int8 __ovld __cnfn convert_int8_rtp(half8);\n"
"int8 __ovld __cnfn convert_int8_rtn(half8);\n"
"int8 __ovld __cnfn convert_int8_rtz(half8);\n"
"int8 __ovld __cnfn convert_int8_sat(half8);\n"
"int8 __ovld __cnfn convert_int8_sat_rte(half8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtp(half8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtn(half8);\n"
"int8 __ovld __cnfn convert_int8_sat_rtz(half8);\n"
"int16 __ovld __cnfn convert_int16(half16);\n"
"int16 __ovld __cnfn convert_int16_rte(half16);\n"
"int16 __ovld __cnfn convert_int16_rtp(half16);\n"
"int16 __ovld __cnfn convert_int16_rtn(half16);\n"
"int16 __ovld __cnfn convert_int16_rtz(half16);\n"
"int16 __ovld __cnfn convert_int16_sat(half16);\n"
"int16 __ovld __cnfn convert_int16_sat_rte(half16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtp(half16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtn(half16);\n"
"int16 __ovld __cnfn convert_int16_sat_rtz(half16);\n"
"long __ovld __cnfn convert_long(half);\n"
"long __ovld __cnfn convert_long_rte(half);\n"
"long __ovld __cnfn convert_long_rtp(half);\n"
"long __ovld __cnfn convert_long_rtn(half);\n"
"long __ovld __cnfn convert_long_rtz(half);\n"
"long __ovld __cnfn convert_long_sat(half);\n"
"long __ovld __cnfn convert_long_sat_rte(half);\n"
"long __ovld __cnfn convert_long_sat_rtp(half);\n"
"long __ovld __cnfn convert_long_sat_rtn(half);\n"
"long __ovld __cnfn convert_long_sat_rtz(half);\n"
"long2 __ovld __cnfn convert_long2(half2);\n"
"long2 __ovld __cnfn convert_long2_rte(half2);\n"
"long2 __ovld __cnfn convert_long2_rtp(half2);\n"
"long2 __ovld __cnfn convert_long2_rtn(half2);\n"
"long2 __ovld __cnfn convert_long2_rtz(half2);\n"
"long2 __ovld __cnfn convert_long2_sat(half2);\n"
"long2 __ovld __cnfn convert_long2_sat_rte(half2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtp(half2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtn(half2);\n"
"long2 __ovld __cnfn convert_long2_sat_rtz(half2);\n"
"long3 __ovld __cnfn convert_long3(half3);\n"
"long3 __ovld __cnfn convert_long3_rte(half3);\n"
"long3 __ovld __cnfn convert_long3_rtp(half3);\n"
"long3 __ovld __cnfn convert_long3_rtn(half3);\n"
"long3 __ovld __cnfn convert_long3_rtz(half3);\n"
"long3 __ovld __cnfn convert_long3_sat(half3);\n"
"long3 __ovld __cnfn convert_long3_sat_rte(half3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtp(half3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtn(half3);\n"
"long3 __ovld __cnfn convert_long3_sat_rtz(half3);\n"
"long4 __ovld __cnfn convert_long4(half4);\n"
"long4 __ovld __cnfn convert_long4_rte(half4);\n"
"long4 __ovld __cnfn convert_long4_rtp(half4);\n"
"long4 __ovld __cnfn convert_long4_rtn(half4);\n"
"long4 __ovld __cnfn convert_long4_rtz(half4);\n"
"long4 __ovld __cnfn convert_long4_sat(half4);\n"
"long4 __ovld __cnfn convert_long4_sat_rte(half4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtp(half4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtn(half4);\n"
"long4 __ovld __cnfn convert_long4_sat_rtz(half4);\n"
"long8 __ovld __cnfn convert_long8(half8);\n"
"long8 __ovld __cnfn convert_long8_rte(half8);\n"
"long8 __ovld __cnfn convert_long8_rtp(half8);\n"
"long8 __ovld __cnfn convert_long8_rtn(half8);\n"
"long8 __ovld __cnfn convert_long8_rtz(half8);\n"
"long8 __ovld __cnfn convert_long8_sat(half8);\n"
"long8 __ovld __cnfn convert_long8_sat_rte(half8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtp(half8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtn(half8);\n"
"long8 __ovld __cnfn convert_long8_sat_rtz(half8);\n"
"long16 __ovld __cnfn convert_long16(half16);\n"
"long16 __ovld __cnfn convert_long16_rte(half16);\n"
"long16 __ovld __cnfn convert_long16_rtp(half16);\n"
"long16 __ovld __cnfn convert_long16_rtn(half16);\n"
"long16 __ovld __cnfn convert_long16_rtz(half16);\n"
"long16 __ovld __cnfn convert_long16_sat(half16);\n"
"long16 __ovld __cnfn convert_long16_sat_rte(half16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtp(half16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtn(half16);\n"
"long16 __ovld __cnfn convert_long16_sat_rtz(half16);\n"
"float __ovld __cnfn convert_float(half);\n"
"float __ovld __cnfn convert_float_rte(half);\n"
"float __ovld __cnfn convert_float_rtp(half);\n"
"float __ovld __cnfn convert_float_rtn(half);\n"
"float __ovld __cnfn convert_float_rtz(half);\n"
"float2 __ovld __cnfn convert_float2(half2);\n"
"float2 __ovld __cnfn convert_float2_rte(half2);\n"
"float2 __ovld __cnfn convert_float2_rtp(half2);\n"
"float2 __ovld __cnfn convert_float2_rtn(half2);\n"
"float2 __ovld __cnfn convert_float2_rtz(half2);\n"
"float3 __ovld __cnfn convert_float3(half3);\n"
"float3 __ovld __cnfn convert_float3_rte(half3);\n"
"float3 __ovld __cnfn convert_float3_rtp(half3);\n"
"float3 __ovld __cnfn convert_float3_rtn(half3);\n"
"float3 __ovld __cnfn convert_float3_rtz(half3);\n"
"float4 __ovld __cnfn convert_float4(half4);\n"
"float4 __ovld __cnfn convert_float4_rte(half4);\n"
"float4 __ovld __cnfn convert_float4_rtp(half4);\n"
"float4 __ovld __cnfn convert_float4_rtn(half4);\n"
"float4 __ovld __cnfn convert_float4_rtz(half4);\n"
"float8 __ovld __cnfn convert_float8(half8);\n"
"float8 __ovld __cnfn convert_float8_rte(half8);\n"
"float8 __ovld __cnfn convert_float8_rtp(half8);\n"
"float8 __ovld __cnfn convert_float8_rtn(half8);\n"
"float8 __ovld __cnfn convert_float8_rtz(half8);\n"
"float16 __ovld __cnfn convert_float16(half16);\n"
"float16 __ovld __cnfn convert_float16_rte(half16);\n"
"float16 __ovld __cnfn convert_float16_rtp(half16);\n"
"float16 __ovld __cnfn convert_float16_rtn(half16);\n"
"float16 __ovld __cnfn convert_float16_rtz(half16);\n"
"\n"
"// Convert non-double types to half types.\n"
"half __ovld __cnfn convert_half(uchar);\n"
"half __ovld __cnfn convert_half(ushort);\n"
"half __ovld __cnfn convert_half(uint);\n"
"half __ovld __cnfn convert_half(ulong);\n"
"half __ovld __cnfn convert_half(char);\n"
"half __ovld __cnfn convert_half(short);\n"
"half __ovld __cnfn convert_half(int);\n"
"half __ovld __cnfn convert_half(long);\n"
"half __ovld __cnfn convert_half(float);\n"
"half __ovld __cnfn convert_half(half);\n"
"half __ovld __cnfn convert_half_rte(uchar);\n"
"half __ovld __cnfn convert_half_rte(ushort);\n"
"half __ovld __cnfn convert_half_rte(uint);\n"
"half __ovld __cnfn convert_half_rte(ulong);\n"
"half __ovld __cnfn convert_half_rte(char);\n"
"half __ovld __cnfn convert_half_rte(short);\n"
"half __ovld __cnfn convert_half_rte(int);\n"
"half __ovld __cnfn convert_half_rte(long);\n"
"half __ovld __cnfn convert_half_rte(float);\n"
"half __ovld __cnfn convert_half_rte(half);\n"
"half __ovld __cnfn convert_half_rtp(uchar);\n"
"half __ovld __cnfn convert_half_rtp(ushort);\n"
"half __ovld __cnfn convert_half_rtp(uint);\n"
"half __ovld __cnfn convert_half_rtp(ulong);\n"
"half __ovld __cnfn convert_half_rtp(char);\n"
"half __ovld __cnfn convert_half_rtp(short);\n"
"half __ovld __cnfn convert_half_rtp(int);\n"
"half __ovld __cnfn convert_half_rtp(long);\n"
"half __ovld __cnfn convert_half_rtp(float);\n"
"half __ovld __cnfn convert_half_rtp(half);\n"
"half __ovld __cnfn convert_half_rtn(uchar);\n"
"half __ovld __cnfn convert_half_rtn(ushort);\n"
"half __ovld __cnfn convert_half_rtn(uint);\n"
"half __ovld __cnfn convert_half_rtn(ulong);\n"
"half __ovld __cnfn convert_half_rtn(char);\n"
"half __ovld __cnfn convert_half_rtn(short);\n"
"half __ovld __cnfn convert_half_rtn(int);\n"
"half __ovld __cnfn convert_half_rtn(long);\n"
"half __ovld __cnfn convert_half_rtn(float);\n"
"half __ovld __cnfn convert_half_rtn(half);\n"
"half __ovld __cnfn convert_half_rtz(uchar);\n"
"half __ovld __cnfn convert_half_rtz(ushort);\n"
"half __ovld __cnfn convert_half_rtz(uint);\n"
"half __ovld __cnfn convert_half_rtz(ulong);\n"
"half __ovld __cnfn convert_half_rtz(char);\n"
"half __ovld __cnfn convert_half_rtz(short);\n"
"half __ovld __cnfn convert_half_rtz(int);\n"
"half __ovld __cnfn convert_half_rtz(long);\n"
"half __ovld __cnfn convert_half_rtz(float);\n"
"half __ovld __cnfn convert_half_rtz(half);\n"
"half2 __ovld __cnfn convert_half2(char2);\n"
"half2 __ovld __cnfn convert_half2(uchar2);\n"
"half2 __ovld __cnfn convert_half2(short2);\n"
"half2 __ovld __cnfn convert_half2(ushort2);\n"
"half2 __ovld __cnfn convert_half2(int2);\n"
"half2 __ovld __cnfn convert_half2(uint2);\n"
"half2 __ovld __cnfn convert_half2(long2);\n"
"half2 __ovld __cnfn convert_half2(ulong2);\n"
"half2 __ovld __cnfn convert_half2(float2);\n"
"half2 __ovld __cnfn convert_half2(half2);\n"
"half2 __ovld __cnfn convert_half2_rte(char2);\n"
"half2 __ovld __cnfn convert_half2_rte(uchar2);\n"
"half2 __ovld __cnfn convert_half2_rte(short2);\n"
"half2 __ovld __cnfn convert_half2_rte(ushort2);\n"
"half2 __ovld __cnfn convert_half2_rte(int2);\n"
"half2 __ovld __cnfn convert_half2_rte(uint2);\n"
"half2 __ovld __cnfn convert_half2_rte(long2);\n"
"half2 __ovld __cnfn convert_half2_rte(ulong2);\n"
"half2 __ovld __cnfn convert_half2_rte(float2);\n"
"half2 __ovld __cnfn convert_half2_rte(half2);\n"
"half2 __ovld __cnfn convert_half2_rtp(char2);\n"
"half2 __ovld __cnfn convert_half2_rtp(uchar2);\n"
"half2 __ovld __cnfn convert_half2_rtp(short2);\n"
"half2 __ovld __cnfn convert_half2_rtp(ushort2);\n"
"half2 __ovld __cnfn convert_half2_rtp(int2);\n"
"half2 __ovld __cnfn convert_half2_rtp(uint2);\n"
"half2 __ovld __cnfn convert_half2_rtp(long2);\n"
"half2 __ovld __cnfn convert_half2_rtp(ulong2);\n"
"half2 __ovld __cnfn convert_half2_rtp(float2);\n"
"half2 __ovld __cnfn convert_half2_rtp(half2);\n"
"half2 __ovld __cnfn convert_half2_rtn(char2);\n"
"half2 __ovld __cnfn convert_half2_rtn(uchar2);\n"
"half2 __ovld __cnfn convert_half2_rtn(short2);\n"
"half2 __ovld __cnfn convert_half2_rtn(ushort2);\n"
"half2 __ovld __cnfn convert_half2_rtn(int2);\n"
"half2 __ovld __cnfn convert_half2_rtn(uint2);\n"
"half2 __ovld __cnfn convert_half2_rtn(long2);\n"
"half2 __ovld __cnfn convert_half2_rtn(ulong2);\n"
"half2 __ovld __cnfn convert_half2_rtn(float2);\n"
"half2 __ovld __cnfn convert_half2_rtn(half2);\n"
"half2 __ovld __cnfn convert_half2_rtz(char2);\n"
"half2 __ovld __cnfn convert_half2_rtz(uchar2);\n"
"half2 __ovld __cnfn convert_half2_rtz(short2);\n"
"half2 __ovld __cnfn convert_half2_rtz(ushort2);\n"
"half2 __ovld __cnfn convert_half2_rtz(int2);\n"
"half2 __ovld __cnfn convert_half2_rtz(uint2);\n"
"half2 __ovld __cnfn convert_half2_rtz(long2);\n"
"half2 __ovld __cnfn convert_half2_rtz(ulong2);\n"
"half2 __ovld __cnfn convert_half2_rtz(float2);\n"
"half2 __ovld __cnfn convert_half2_rtz(half2);\n"
"half3 __ovld __cnfn convert_half3(char3);\n"
"half3 __ovld __cnfn convert_half3(uchar3);\n"
"half3 __ovld __cnfn convert_half3(short3);\n"
"half3 __ovld __cnfn convert_half3(ushort3);\n"
"half3 __ovld __cnfn convert_half3(int3);\n"
"half3 __ovld __cnfn convert_half3(uint3);\n"
"half3 __ovld __cnfn convert_half3(long3);\n"
"half3 __ovld __cnfn convert_half3(ulong3);\n"
"half3 __ovld __cnfn convert_half3(float3);\n"
"half3 __ovld __cnfn convert_half3(half3);\n"
"half3 __ovld __cnfn convert_half3_rte(char3);\n"
"half3 __ovld __cnfn convert_half3_rte(uchar3);\n"
"half3 __ovld __cnfn convert_half3_rte(short3);\n"
"half3 __ovld __cnfn convert_half3_rte(ushort3);\n"
"half3 __ovld __cnfn convert_half3_rte(int3);\n"
"half3 __ovld __cnfn convert_half3_rte(uint3);\n"
"half3 __ovld __cnfn convert_half3_rte(long3);\n"
"half3 __ovld __cnfn convert_half3_rte(ulong3);\n"
"half3 __ovld __cnfn convert_half3_rte(float3);\n"
"half3 __ovld __cnfn convert_half3_rte(half3);\n"
"half3 __ovld __cnfn convert_half3_rtp(char3);\n"
"half3 __ovld __cnfn convert_half3_rtp(uchar3);\n"
"half3 __ovld __cnfn convert_half3_rtp(short3);\n"
"half3 __ovld __cnfn convert_half3_rtp(ushort3);\n"
"half3 __ovld __cnfn convert_half3_rtp(int3);\n"
"half3 __ovld __cnfn convert_half3_rtp(uint3);\n"
"half3 __ovld __cnfn convert_half3_rtp(long3);\n"
"half3 __ovld __cnfn convert_half3_rtp(ulong3);\n"
"half3 __ovld __cnfn convert_half3_rtp(float3);\n"
"half3 __ovld __cnfn convert_half3_rtp(half3);\n"
"half3 __ovld __cnfn convert_half3_rtn(char3);\n"
"half3 __ovld __cnfn convert_half3_rtn(uchar3);\n"
"half3 __ovld __cnfn convert_half3_rtn(short3);\n"
"half3 __ovld __cnfn convert_half3_rtn(ushort3);\n"
"half3 __ovld __cnfn convert_half3_rtn(int3);\n"
"half3 __ovld __cnfn convert_half3_rtn(uint3);\n"
"half3 __ovld __cnfn convert_half3_rtn(long3);\n"
"half3 __ovld __cnfn convert_half3_rtn(ulong3);\n"
"half3 __ovld __cnfn convert_half3_rtn(float3);\n"
"half3 __ovld __cnfn convert_half3_rtn(half3);\n"
"half3 __ovld __cnfn convert_half3_rtz(char3);\n"
"half3 __ovld __cnfn convert_half3_rtz(uchar3);\n"
"half3 __ovld __cnfn convert_half3_rtz(short3);\n"
"half3 __ovld __cnfn convert_half3_rtz(ushort3);\n"
"half3 __ovld __cnfn convert_half3_rtz(int3);\n"
"half3 __ovld __cnfn convert_half3_rtz(uint3);\n"
"half3 __ovld __cnfn convert_half3_rtz(long3);\n"
"half3 __ovld __cnfn convert_half3_rtz(ulong3);\n"
"half3 __ovld __cnfn convert_half3_rtz(float3);\n"
"half3 __ovld __cnfn convert_half3_rtz(half3);\n"
"half4 __ovld __cnfn convert_half4(char4);\n"
"half4 __ovld __cnfn convert_half4(uchar4);\n"
"half4 __ovld __cnfn convert_half4(short4);\n"
"half4 __ovld __cnfn convert_half4(ushort4);\n"
"half4 __ovld __cnfn convert_half4(int4);\n"
"half4 __ovld __cnfn convert_half4(uint4);\n"
"half4 __ovld __cnfn convert_half4(long4);\n"
"half4 __ovld __cnfn convert_half4(ulong4);\n"
"half4 __ovld __cnfn convert_half4(float4);\n"
"half4 __ovld __cnfn convert_half4(half4);\n"
"half4 __ovld __cnfn convert_half4_rte(char4);\n"
"half4 __ovld __cnfn convert_half4_rte(uchar4);\n"
"half4 __ovld __cnfn convert_half4_rte(short4);\n"
"half4 __ovld __cnfn convert_half4_rte(ushort4);\n"
"half4 __ovld __cnfn convert_half4_rte(int4);\n"
"half4 __ovld __cnfn convert_half4_rte(uint4);\n"
"half4 __ovld __cnfn convert_half4_rte(long4);\n"
"half4 __ovld __cnfn convert_half4_rte(ulong4);\n"
"half4 __ovld __cnfn convert_half4_rte(float4);\n"
"half4 __ovld __cnfn convert_half4_rte(half4);\n"
"half4 __ovld __cnfn convert_half4_rtp(char4);\n"
"half4 __ovld __cnfn convert_half4_rtp(uchar4);\n"
"half4 __ovld __cnfn convert_half4_rtp(short4);\n"
"half4 __ovld __cnfn convert_half4_rtp(ushort4);\n"
"half4 __ovld __cnfn convert_half4_rtp(int4);\n"
"half4 __ovld __cnfn convert_half4_rtp(uint4);\n"
"half4 __ovld __cnfn convert_half4_rtp(long4);\n"
"half4 __ovld __cnfn convert_half4_rtp(ulong4);\n"
"half4 __ovld __cnfn convert_half4_rtp(float4);\n"
"half4 __ovld __cnfn convert_half4_rtp(half4);\n"
"half4 __ovld __cnfn convert_half4_rtn(char4);\n"
"half4 __ovld __cnfn convert_half4_rtn(uchar4);\n"
"half4 __ovld __cnfn convert_half4_rtn(short4);\n"
"half4 __ovld __cnfn convert_half4_rtn(ushort4);\n"
"half4 __ovld __cnfn convert_half4_rtn(int4);\n"
"half4 __ovld __cnfn convert_half4_rtn(uint4);\n"
"half4 __ovld __cnfn convert_half4_rtn(long4);\n"
"half4 __ovld __cnfn convert_half4_rtn(ulong4);\n"
"half4 __ovld __cnfn convert_half4_rtn(float4);\n"
"half4 __ovld __cnfn convert_half4_rtn(half4);\n"
"half4 __ovld __cnfn convert_half4_rtz(char4);\n"
"half4 __ovld __cnfn convert_half4_rtz(uchar4);\n"
"half4 __ovld __cnfn convert_half4_rtz(short4);\n"
"half4 __ovld __cnfn convert_half4_rtz(ushort4);\n"
"half4 __ovld __cnfn convert_half4_rtz(int4);\n"
"half4 __ovld __cnfn convert_half4_rtz(uint4);\n"
"half4 __ovld __cnfn convert_half4_rtz(long4);\n"
"half4 __ovld __cnfn convert_half4_rtz(ulong4);\n"
"half4 __ovld __cnfn convert_half4_rtz(float4);\n"
"half4 __ovld __cnfn convert_half4_rtz(half4);\n"
"half8 __ovld __cnfn convert_half8(char8);\n"
"half8 __ovld __cnfn convert_half8(uchar8);\n"
"half8 __ovld __cnfn convert_half8(short8);\n"
"half8 __ovld __cnfn convert_half8(ushort8);\n"
"half8 __ovld __cnfn convert_half8(int8);\n"
"half8 __ovld __cnfn convert_half8(uint8);\n"
"half8 __ovld __cnfn convert_half8(long8);\n"
"half8 __ovld __cnfn convert_half8(ulong8);\n"
"half8 __ovld __cnfn convert_half8(float8);\n"
"half8 __ovld __cnfn convert_half8(half8);\n"
"half8 __ovld __cnfn convert_half8_rte(char8);\n"
"half8 __ovld __cnfn convert_half8_rte(uchar8);\n"
"half8 __ovld __cnfn convert_half8_rte(short8);\n"
"half8 __ovld __cnfn convert_half8_rte(ushort8);\n"
"half8 __ovld __cnfn convert_half8_rte(int8);\n"
"half8 __ovld __cnfn convert_half8_rte(uint8);\n"
"half8 __ovld __cnfn convert_half8_rte(long8);\n"
"half8 __ovld __cnfn convert_half8_rte(ulong8);\n"
"half8 __ovld __cnfn convert_half8_rte(float8);\n"
"half8 __ovld __cnfn convert_half8_rte(half8);\n"
"half8 __ovld __cnfn convert_half8_rtp(char8);\n"
"half8 __ovld __cnfn convert_half8_rtp(uchar8);\n"
"half8 __ovld __cnfn convert_half8_rtp(short8);\n"
"half8 __ovld __cnfn convert_half8_rtp(ushort8);\n"
"half8 __ovld __cnfn convert_half8_rtp(int8);\n"
"half8 __ovld __cnfn convert_half8_rtp(uint8);\n"
"half8 __ovld __cnfn convert_half8_rtp(long8);\n"
"half8 __ovld __cnfn convert_half8_rtp(ulong8);\n"
"half8 __ovld __cnfn convert_half8_rtp(float8);\n"
"half8 __ovld __cnfn convert_half8_rtp(half8);\n"
"half8 __ovld __cnfn convert_half8_rtn(char8);\n"
"half8 __ovld __cnfn convert_half8_rtn(uchar8);\n"
"half8 __ovld __cnfn convert_half8_rtn(short8);\n"
"half8 __ovld __cnfn convert_half8_rtn(ushort8);\n"
"half8 __ovld __cnfn convert_half8_rtn(int8);\n"
"half8 __ovld __cnfn convert_half8_rtn(uint8);\n"
"half8 __ovld __cnfn convert_half8_rtn(long8);\n"
"half8 __ovld __cnfn convert_half8_rtn(ulong8);\n"
"half8 __ovld __cnfn convert_half8_rtn(float8);\n"
"half8 __ovld __cnfn convert_half8_rtn(half8);\n"
"half8 __ovld __cnfn convert_half8_rtz(char8);\n"
"half8 __ovld __cnfn convert_half8_rtz(uchar8);\n"
"half8 __ovld __cnfn convert_half8_rtz(short8);\n"
"half8 __ovld __cnfn convert_half8_rtz(ushort8);\n"
"half8 __ovld __cnfn convert_half8_rtz(int8);\n"
"half8 __ovld __cnfn convert_half8_rtz(uint8);\n"
"half8 __ovld __cnfn convert_half8_rtz(long8);\n"
"half8 __ovld __cnfn convert_half8_rtz(ulong8);\n"
"half8 __ovld __cnfn convert_half8_rtz(float8);\n"
"half8 __ovld __cnfn convert_half8_rtz(half8);\n"
"half16 __ovld __cnfn convert_half16(char16);\n"
"half16 __ovld __cnfn convert_half16(uchar16);\n"
"half16 __ovld __cnfn convert_half16(short16);\n"
"half16 __ovld __cnfn convert_half16(ushort16);\n"
"half16 __ovld __cnfn convert_half16(int16);\n"
"half16 __ovld __cnfn convert_half16(uint16);\n"
"half16 __ovld __cnfn convert_half16(long16);\n"
"half16 __ovld __cnfn convert_half16(ulong16);\n"
"half16 __ovld __cnfn convert_half16(float16);\n"
"half16 __ovld __cnfn convert_half16(half16);\n"
"half16 __ovld __cnfn convert_half16_rte(char16);\n"
"half16 __ovld __cnfn convert_half16_rte(uchar16);\n"
"half16 __ovld __cnfn convert_half16_rte(short16);\n"
"half16 __ovld __cnfn convert_half16_rte(ushort16);\n"
"half16 __ovld __cnfn convert_half16_rte(int16);\n"
"half16 __ovld __cnfn convert_half16_rte(uint16);\n"
"half16 __ovld __cnfn convert_half16_rte(long16);\n"
"half16 __ovld __cnfn convert_half16_rte(ulong16);\n"
"half16 __ovld __cnfn convert_half16_rte(float16);\n"
"half16 __ovld __cnfn convert_half16_rte(half16);\n"
"half16 __ovld __cnfn convert_half16_rtp(char16);\n"
"half16 __ovld __cnfn convert_half16_rtp(uchar16);\n"
"half16 __ovld __cnfn convert_half16_rtp(short16);\n"
"half16 __ovld __cnfn convert_half16_rtp(ushort16);\n"
"half16 __ovld __cnfn convert_half16_rtp(int16);\n"
"half16 __ovld __cnfn convert_half16_rtp(uint16);\n"
"half16 __ovld __cnfn convert_half16_rtp(long16);\n"
"half16 __ovld __cnfn convert_half16_rtp(ulong16);\n"
"half16 __ovld __cnfn convert_half16_rtp(float16);\n"
"half16 __ovld __cnfn convert_half16_rtp(half16);\n"
"half16 __ovld __cnfn convert_half16_rtn(char16);\n"
"half16 __ovld __cnfn convert_half16_rtn(uchar16);\n"
"half16 __ovld __cnfn convert_half16_rtn(short16);\n"
"half16 __ovld __cnfn convert_half16_rtn(ushort16);\n"
"half16 __ovld __cnfn convert_half16_rtn(int16);\n"
"half16 __ovld __cnfn convert_half16_rtn(uint16);\n"
"half16 __ovld __cnfn convert_half16_rtn(long16);\n"
"half16 __ovld __cnfn convert_half16_rtn(ulong16);\n"
"half16 __ovld __cnfn convert_half16_rtn(float16);\n"
"half16 __ovld __cnfn convert_half16_rtn(half16);\n"
"half16 __ovld __cnfn convert_half16_rtz(char16);\n"
"half16 __ovld __cnfn convert_half16_rtz(uchar16);\n"
"half16 __ovld __cnfn convert_half16_rtz(short16);\n"
"half16 __ovld __cnfn convert_half16_rtz(ushort16);\n"
"half16 __ovld __cnfn convert_half16_rtz(int16);\n"
"half16 __ovld __cnfn convert_half16_rtz(uint16);\n"
"half16 __ovld __cnfn convert_half16_rtz(long16);\n"
"half16 __ovld __cnfn convert_half16_rtz(ulong16);\n"
"half16 __ovld __cnfn convert_half16_rtz(float16);\n"
"half16 __ovld __cnfn convert_half16_rtz(half16);\n"
"\n"
"// Convert half types to double types.\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn convert_double(half);\n"
"double __ovld __cnfn convert_double_rte(half);\n"
"double __ovld __cnfn convert_double_rtp(half);\n"
"double __ovld __cnfn convert_double_rtn(half);\n"
"double __ovld __cnfn convert_double_rtz(half);\n"
"double2 __ovld __cnfn convert_double2(half2);\n"
"double2 __ovld __cnfn convert_double2_rte(half2);\n"
"double2 __ovld __cnfn convert_double2_rtp(half2);\n"
"double2 __ovld __cnfn convert_double2_rtn(half2);\n"
"double2 __ovld __cnfn convert_double2_rtz(half2);\n"
"double3 __ovld __cnfn convert_double3(half3);\n"
"double3 __ovld __cnfn convert_double3_rte(half3);\n"
"double3 __ovld __cnfn convert_double3_rtp(half3);\n"
"double3 __ovld __cnfn convert_double3_rtn(half3);\n"
"double3 __ovld __cnfn convert_double3_rtz(half3);\n"
"double4 __ovld __cnfn convert_double4(half4);\n"
"double4 __ovld __cnfn convert_double4_rte(half4);\n"
"double4 __ovld __cnfn convert_double4_rtp(half4);\n"
"double4 __ovld __cnfn convert_double4_rtn(half4);\n"
"double4 __ovld __cnfn convert_double4_rtz(half4);\n"
"double8 __ovld __cnfn convert_double8(half8);\n"
"double8 __ovld __cnfn convert_double8_rte(half8);\n"
"double8 __ovld __cnfn convert_double8_rtp(half8);\n"
"double8 __ovld __cnfn convert_double8_rtn(half8);\n"
"double8 __ovld __cnfn convert_double8_rtz(half8);\n"
"double16 __ovld __cnfn convert_double16(half16);\n"
"double16 __ovld __cnfn convert_double16_rte(half16);\n"
"double16 __ovld __cnfn convert_double16_rtp(half16);\n"
"double16 __ovld __cnfn convert_double16_rtn(half16);\n"
"double16 __ovld __cnfn convert_double16_rtz(half16);\n"
"\n"
"// Convert double types to half types.\n"
"half __ovld __cnfn convert_half(double);\n"
"half __ovld __cnfn convert_half_rte(double);\n"
"half __ovld __cnfn convert_half_rtp(double);\n"
"half __ovld __cnfn convert_half_rtn(double);\n"
"half __ovld __cnfn convert_half_rtz(double);\n"
"half2 __ovld __cnfn convert_half2(double2);\n"
"half2 __ovld __cnfn convert_half2_rte(double2);\n"
"half2 __ovld __cnfn convert_half2_rtp(double2);\n"
"half2 __ovld __cnfn convert_half2_rtn(double2);\n"
"half2 __ovld __cnfn convert_half2_rtz(double2);\n"
"half3 __ovld __cnfn convert_half3(double3);\n"
"half3 __ovld __cnfn convert_half3_rte(double3);\n"
"half3 __ovld __cnfn convert_half3_rtp(double3);\n"
"half3 __ovld __cnfn convert_half3_rtn(double3);\n"
"half3 __ovld __cnfn convert_half3_rtz(double3);\n"
"half4 __ovld __cnfn convert_half4(double4);\n"
"half4 __ovld __cnfn convert_half4_rte(double4);\n"
"half4 __ovld __cnfn convert_half4_rtp(double4);\n"
"half4 __ovld __cnfn convert_half4_rtn(double4);\n"
"half4 __ovld __cnfn convert_half4_rtz(double4);\n"
"half8 __ovld __cnfn convert_half8(double8);\n"
"half8 __ovld __cnfn convert_half8_rte(double8);\n"
"half8 __ovld __cnfn convert_half8_rtp(double8);\n"
"half8 __ovld __cnfn convert_half8_rtn(double8);\n"
"half8 __ovld __cnfn convert_half8_rtz(double8);\n"
"half16 __ovld __cnfn convert_half16(double16);\n"
"half16 __ovld __cnfn convert_half16_rte(double16);\n"
"half16 __ovld __cnfn convert_half16_rtp(double16);\n"
"half16 __ovld __cnfn convert_half16_rtn(double16);\n"
"half16 __ovld __cnfn convert_half16_rtz(double16);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#endif // cl_khr_fp16\n"
"\n"
"/**\n"
" * OpenCL v1.1/1.2/2.0 s6.2.4.2 - as_type operators\n"
" * Reinterprets a data type as another data type of the same size\n"
" */\n"
"#define as_char(x) __builtin_astype((x),   char)\n"
"#define as_char2(x) __builtin_astype((x),  char2)\n"
"#define as_char3(x) __builtin_astype((x),  char3)\n"
"#define as_char4(x) __builtin_astype((x),  char4)\n"
"#define as_char8(x) __builtin_astype((x),  char8)\n"
"#define as_char16(x) __builtin_astype((x), char16)\n"
"\n"
"#define as_uchar(x) __builtin_astype((x),   uchar)\n"
"#define as_uchar2(x) __builtin_astype((x),  uchar2)\n"
"#define as_uchar3(x) __builtin_astype((x),  uchar3)\n"
"#define as_uchar4(x) __builtin_astype((x),  uchar4)\n"
"#define as_uchar8(x) __builtin_astype((x),  uchar8)\n"
"#define as_uchar16(x) __builtin_astype((x), uchar16)\n"
"\n"
"#define as_short(x) __builtin_astype((x),   short)\n"
"#define as_short2(x) __builtin_astype((x),  short2)\n"
"#define as_short3(x) __builtin_astype((x),  short3)\n"
"#define as_short4(x) __builtin_astype((x),  short4)\n"
"#define as_short8(x) __builtin_astype((x),  short8)\n"
"#define as_short16(x) __builtin_astype((x), short16)\n"
"\n"
"#define as_ushort(x) __builtin_astype((x),   ushort)\n"
"#define as_ushort2(x) __builtin_astype((x),  ushort2)\n"
"#define as_ushort3(x) __builtin_astype((x),  ushort3)\n"
"#define as_ushort4(x) __builtin_astype((x),  ushort4)\n"
"#define as_ushort8(x) __builtin_astype((x),  ushort8)\n"
"#define as_ushort16(x) __builtin_astype((x), ushort16)\n"
"\n"
"#define as_int(x) __builtin_astype((x),   int)\n"
"#define as_int2(x) __builtin_astype((x),  int2)\n"
"#define as_int3(x) __builtin_astype((x),  int3)\n"
"#define as_int4(x) __builtin_astype((x),  int4)\n"
"#define as_int8(x) __builtin_astype((x),  int8)\n"
"#define as_int16(x) __builtin_astype((x), int16)\n"
"\n"
"#define as_uint(x) __builtin_astype((x),   uint)\n"
"#define as_uint2(x) __builtin_astype((x),  uint2)\n"
"#define as_uint3(x) __builtin_astype((x),  uint3)\n"
"#define as_uint4(x) __builtin_astype((x),  uint4)\n"
"#define as_uint8(x) __builtin_astype((x),  uint8)\n"
"#define as_uint16(x) __builtin_astype((x), uint16)\n"
"\n"
"#define as_long(x) __builtin_astype((x),   long)\n"
"#define as_long2(x) __builtin_astype((x),  long2)\n"
"#define as_long3(x) __builtin_astype((x),  long3)\n"
"#define as_long4(x) __builtin_astype((x),  long4)\n"
"#define as_long8(x) __builtin_astype((x),  long8)\n"
"#define as_long16(x) __builtin_astype((x), long16)\n"
"\n"
"#define as_ulong(x) __builtin_astype((x),   ulong)\n"
"#define as_ulong2(x) __builtin_astype((x),  ulong2)\n"
"#define as_ulong3(x) __builtin_astype((x),  ulong3)\n"
"#define as_ulong4(x) __builtin_astype((x),  ulong4)\n"
"#define as_ulong8(x) __builtin_astype((x),  ulong8)\n"
"#define as_ulong16(x) __builtin_astype((x), ulong16)\n"
"\n"
"#define as_float(x) __builtin_astype((x),   float)\n"
"#define as_float2(x) __builtin_astype((x),  float2)\n"
"#define as_float3(x) __builtin_astype((x),  float3)\n"
"#define as_float4(x) __builtin_astype((x),  float4)\n"
"#define as_float8(x) __builtin_astype((x),  float8)\n"
"#define as_float16(x) __builtin_astype((x), float16)\n"
"\n"
"#ifdef cl_khr_fp64\n"
"#define as_double(x) __builtin_astype((x),   double)\n"
"#define as_double2(x) __builtin_astype((x),  double2)\n"
"#define as_double3(x) __builtin_astype((x),  double3)\n"
"#define as_double4(x) __builtin_astype((x),  double4)\n"
"#define as_double8(x) __builtin_astype((x),  double8)\n"
"#define as_double16(x) __builtin_astype((x), double16)\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"#define as_half(x) __builtin_astype((x),   half)\n"
"#define as_half2(x) __builtin_astype((x),  half2)\n"
"#define as_half3(x) __builtin_astype((x),  half3)\n"
"#define as_half4(x) __builtin_astype((x),  half4)\n"
"#define as_half8(x) __builtin_astype((x),  half8)\n"
"#define as_half16(x) __builtin_astype((x), half16)\n"
"#endif //cl_khr_fp16\n"
"\n"
"// OpenCL v1.1 s6.9, v1.2/2.0 s6.10 - Function qualifiers\n"
"\n"
"#define __kernel_exec(X, typen) __kernel \\\n"
"	__attribute__((work_group_size_hint(X, 1, 1))) \\\n"
"	__attribute__((vec_type_hint(typen)))\n"
"\n"
"#define kernel_exec(X, typen) __kernel \\\n"
"	__attribute__((work_group_size_hint(X, 1, 1))) \\\n"
"	__attribute__((vec_type_hint(typen)))\n"
"\n"
"// OpenCL v1.1 s6.11.1, v1.2 s6.12.1, v2.0 s6.13.1 - Work-item Functions\n"
"\n"
"/**\n"
" * Returns the number of dimensions in use. This is the\n"
" * value given to the work_dim argument specified in\n"
" * clEnqueueNDRangeKernel.\n"
" * For clEnqueueTask, this returns 1.\n"
" */\n"
"uint __ovld __cnfn get_work_dim(void);\n"
"\n"
"/**\n"
" * Returns the number of global work-items specified for\n"
" * dimension identified by dimindx. This value is given by\n"
" * the global_work_size argument to\n"
" * clEnqueueNDRangeKernel. Valid values of dimindx\n"
" * are 0 to get_work_dim() - 1. For other values of\n"
" * dimindx, get_global_size() returns 1.\n"
" * For clEnqueueTask, this always returns 1.\n"
" */\n"
"size_t __ovld __cnfn get_global_size(uint dimindx);\n"
"\n"
"/**\n"
" * Returns the unique global work-item ID value for\n"
" * dimension identified by dimindx. The global work-item\n"
" * ID specifies the work-item ID based on the number of\n"
" * global work-items specified to execute the kernel. Valid\n"
" * values of dimindx are 0 to get_work_dim() - 1. For\n"
" * other values of dimindx, get_global_id() returns 0.\n"
" * For clEnqueueTask, this returns 0.\n"
" */\n"
"size_t __ovld __cnfn get_global_id(uint dimindx);\n"
"\n"
"/**\n"
" * Returns the number of local work-items specified in\n"
" * dimension identified by dimindx. This value is given by\n"
" * the local_work_size argument to\n"
" * clEnqueueNDRangeKernel if local_work_size is not\n"
" * NULL; otherwise the OpenCL implementation chooses\n"
" * an appropriate local_work_size value which is returned\n"
" * by this function. Valid values of dimindx are 0 to\n"
" * get_work_dim() - 1. For other values of dimindx,\n"
" * get_local_size() returns 1.\n"
" * For clEnqueueTask, this always returns 1.\n"
" */\n"
"size_t __ovld __cnfn get_local_size(uint dimindx);\n"
"\n"
"/**\n"
" * Returns the unique local work-item ID i.e. a work-item\n"
" * within a specific work-group for dimension identified by\n"
" * dimindx. Valid values of dimindx are 0 to\n"
" * get_work_dim() - 1. For other values of dimindx,\n"
" * get_local_id() returns 0.\n"
" * For clEnqueueTask, this returns 0.\n"
" */\n"
"size_t __ovld __cnfn get_local_id(uint dimindx);\n"
"\n"
"/**\n"
" * Returns the number of work-groups that will execute a\n"
" * kernel for dimension identified by dimindx.\n"
" * Valid values of dimindx are 0 to get_work_dim() - 1.\n"
" * For other values of dimindx, get_num_groups () returns\n"
" * 1.\n"
" * For clEnqueueTask, this always returns 1.\n"
" */\n"
"size_t __ovld __cnfn get_num_groups(uint dimindx);\n"
"\n"
"/**\n"
" * get_group_id returns the work-group ID which is a\n"
" * number from 0 .. get_num_groups(dimindx) - 1.\n"
" * Valid values of dimindx are 0 to get_work_dim() - 1.\n"
" * For other values, get_group_id() returns 0.\n"
" * For clEnqueueTask, this returns 0.\n"
" */\n"
"size_t __ovld __cnfn get_group_id(uint dimindx);\n"
"\n"
"/**\n"
" * get_global_offset returns the offset values specified in\n"
" * global_work_offset argument to\n"
" * clEnqueueNDRangeKernel.\n"
" * Valid values of dimindx are 0 to get_work_dim() - 1.\n"
" * For other values, get_global_offset() returns 0.\n"
" * For clEnqueueTask, this returns 0.\n"
" */\n"
"size_t __ovld __cnfn get_global_offset(uint dimindx);\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"size_t __ovld get_enqueued_local_size(uint dimindx);\n"
"size_t __ovld get_global_linear_id(void);\n"
"size_t __ovld get_local_linear_id(void);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL v1.1 s6.11.2, v1.2 s6.12.2, v2.0 s6.13.2 - Math functions\n"
"\n"
"/**\n"
" * Arc cosine function.\n"
" */\n"
"float __ovld __cnfn acos(float);\n"
"float2 __ovld __cnfn acos(float2);\n"
"float3 __ovld __cnfn acos(float3);\n"
"float4 __ovld __cnfn acos(float4);\n"
"float8 __ovld __cnfn acos(float8);\n"
"float16 __ovld __cnfn acos(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn acos(double);\n"
"double2 __ovld __cnfn acos(double2);\n"
"double3 __ovld __cnfn acos(double3);\n"
"double4 __ovld __cnfn acos(double4);\n"
"double8 __ovld __cnfn acos(double8);\n"
"double16 __ovld __cnfn acos(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn acos(half);\n"
"half2 __ovld __cnfn acos(half2);\n"
"half3 __ovld __cnfn acos(half3);\n"
"half4 __ovld __cnfn acos(half4);\n"
"half8 __ovld __cnfn acos(half8);\n"
"half16 __ovld __cnfn acos(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Inverse hyperbolic cosine.\n"
" */\n"
"float __ovld __cnfn acosh(float);\n"
"float2 __ovld __cnfn acosh(float2);\n"
"float3 __ovld __cnfn acosh(float3);\n"
"float4 __ovld __cnfn acosh(float4);\n"
"float8 __ovld __cnfn acosh(float8);\n"
"float16 __ovld __cnfn acosh(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn acosh(double);\n"
"double2 __ovld __cnfn acosh(double2);\n"
"double3 __ovld __cnfn acosh(double3);\n"
"double4 __ovld __cnfn acosh(double4);\n"
"double8 __ovld __cnfn acosh(double8);\n"
"double16 __ovld __cnfn acosh(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn acosh(half);\n"
"half2 __ovld __cnfn acosh(half2);\n"
"half3 __ovld __cnfn acosh(half3);\n"
"half4 __ovld __cnfn acosh(half4);\n"
"half8 __ovld __cnfn acosh(half8);\n"
"half16 __ovld __cnfn acosh(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute acos (x) / PI.\n"
" */\n"
"float __ovld __cnfn acospi(float x);\n"
"float2 __ovld __cnfn acospi(float2 x);\n"
"float3 __ovld __cnfn acospi(float3 x);\n"
"float4 __ovld __cnfn acospi(float4 x);\n"
"float8 __ovld __cnfn acospi(float8 x);\n"
"float16 __ovld __cnfn acospi(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn acospi(double x);\n"
"double2 __ovld __cnfn acospi(double2 x);\n"
"double3 __ovld __cnfn acospi(double3 x);\n"
"double4 __ovld __cnfn acospi(double4 x);\n"
"double8 __ovld __cnfn acospi(double8 x);\n"
"double16 __ovld __cnfn acospi(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn acospi(half x);\n"
"half2 __ovld __cnfn acospi(half2 x);\n"
"half3 __ovld __cnfn acospi(half3 x);\n"
"half4 __ovld __cnfn acospi(half4 x);\n"
"half8 __ovld __cnfn acospi(half8 x);\n"
"half16 __ovld __cnfn acospi(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Arc sine function.\n"
" */\n"
"float __ovld __cnfn asin(float);\n"
"float2 __ovld __cnfn asin(float2);\n"
"float3 __ovld __cnfn asin(float3);\n"
"float4 __ovld __cnfn asin(float4);\n"
"float8 __ovld __cnfn asin(float8);\n"
"float16 __ovld __cnfn asin(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn asin(double);\n"
"double2 __ovld __cnfn asin(double2);\n"
"double3 __ovld __cnfn asin(double3);\n"
"double4 __ovld __cnfn asin(double4);\n"
"double8 __ovld __cnfn asin(double8);\n"
"double16 __ovld __cnfn asin(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn asin(half);\n"
"half2 __ovld __cnfn asin(half2);\n"
"half3 __ovld __cnfn asin(half3);\n"
"half4 __ovld __cnfn asin(half4);\n"
"half8 __ovld __cnfn asin(half8);\n"
"half16 __ovld __cnfn asin(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Inverse hyperbolic sine.\n"
" */\n"
"float __ovld __cnfn asinh(float);\n"
"float2 __ovld __cnfn asinh(float2);\n"
"float3 __ovld __cnfn asinh(float3);\n"
"float4 __ovld __cnfn asinh(float4);\n"
"float8 __ovld __cnfn asinh(float8);\n"
"float16 __ovld __cnfn asinh(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn asinh(double);\n"
"double2 __ovld __cnfn asinh(double2);\n"
"double3 __ovld __cnfn asinh(double3);\n"
"double4 __ovld __cnfn asinh(double4);\n"
"double8 __ovld __cnfn asinh(double8);\n"
"double16 __ovld __cnfn asinh(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn asinh(half);\n"
"half2 __ovld __cnfn asinh(half2);\n"
"half3 __ovld __cnfn asinh(half3);\n"
"half4 __ovld __cnfn asinh(half4);\n"
"half8 __ovld __cnfn asinh(half8);\n"
"half16 __ovld __cnfn asinh(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute asin (x) / PI.\n"
" */\n"
"float __ovld __cnfn asinpi(float x);\n"
"float2 __ovld __cnfn asinpi(float2 x);\n"
"float3 __ovld __cnfn asinpi(float3 x);\n"
"float4 __ovld __cnfn asinpi(float4 x);\n"
"float8 __ovld __cnfn asinpi(float8 x);\n"
"float16 __ovld __cnfn asinpi(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn asinpi(double x);\n"
"double2 __ovld __cnfn asinpi(double2 x);\n"
"double3 __ovld __cnfn asinpi(double3 x);\n"
"double4 __ovld __cnfn asinpi(double4 x);\n"
"double8 __ovld __cnfn asinpi(double8 x);\n"
"double16 __ovld __cnfn asinpi(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn asinpi(half x);\n"
"half2 __ovld __cnfn asinpi(half2 x);\n"
"half3 __ovld __cnfn asinpi(half3 x);\n"
"half4 __ovld __cnfn asinpi(half4 x);\n"
"half8 __ovld __cnfn asinpi(half8 x);\n"
"half16 __ovld __cnfn asinpi(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Arc tangent function.\n"
" */\n"
"float __ovld __cnfn atan(float y_over_x);\n"
"float2 __ovld __cnfn atan(float2 y_over_x);\n"
"float3 __ovld __cnfn atan(float3 y_over_x);\n"
"float4 __ovld __cnfn atan(float4 y_over_x);\n"
"float8 __ovld __cnfn atan(float8 y_over_x);\n"
"float16 __ovld __cnfn atan(float16 y_over_x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn atan(double y_over_x);\n"
"double2 __ovld __cnfn atan(double2 y_over_x);\n"
"double3 __ovld __cnfn atan(double3 y_over_x);\n"
"double4 __ovld __cnfn atan(double4 y_over_x);\n"
"double8 __ovld __cnfn atan(double8 y_over_x);\n"
"double16 __ovld __cnfn atan(double16 y_over_x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn atan(half y_over_x);\n"
"half2 __ovld __cnfn atan(half2 y_over_x);\n"
"half3 __ovld __cnfn atan(half3 y_over_x);\n"
"half4 __ovld __cnfn atan(half4 y_over_x);\n"
"half8 __ovld __cnfn atan(half8 y_over_x);\n"
"half16 __ovld __cnfn atan(half16 y_over_x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Arc tangent of y / x.\n"
" */\n"
"float __ovld __cnfn atan2(float y, float x);\n"
"float2 __ovld __cnfn atan2(float2 y, float2 x);\n"
"float3 __ovld __cnfn atan2(float3 y, float3 x);\n"
"float4 __ovld __cnfn atan2(float4 y, float4 x);\n"
"float8 __ovld __cnfn atan2(float8 y, float8 x);\n"
"float16 __ovld __cnfn atan2(float16 y, float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn atan2(double y, double x);\n"
"double2 __ovld __cnfn atan2(double2 y, double2 x);\n"
"double3 __ovld __cnfn atan2(double3 y, double3 x);\n"
"double4 __ovld __cnfn atan2(double4 y, double4 x);\n"
"double8 __ovld __cnfn atan2(double8 y, double8 x);\n"
"double16 __ovld __cnfn atan2(double16 y, double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn atan2(half y, half x);\n"
"half2 __ovld __cnfn atan2(half2 y, half2 x);\n"
"half3 __ovld __cnfn atan2(half3 y, half3 x);\n"
"half4 __ovld __cnfn atan2(half4 y, half4 x);\n"
"half8 __ovld __cnfn atan2(half8 y, half8 x);\n"
"half16 __ovld __cnfn atan2(half16 y, half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Hyperbolic arc tangent.\n"
" */\n"
"float __ovld __cnfn atanh(float);\n"
"float2 __ovld __cnfn atanh(float2);\n"
"float3 __ovld __cnfn atanh(float3);\n"
"float4 __ovld __cnfn atanh(float4);\n"
"float8 __ovld __cnfn atanh(float8);\n"
"float16 __ovld __cnfn atanh(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn atanh(double);\n"
"double2 __ovld __cnfn atanh(double2);\n"
"double3 __ovld __cnfn atanh(double3);\n"
"double4 __ovld __cnfn atanh(double4);\n"
"double8 __ovld __cnfn atanh(double8);\n"
"double16 __ovld __cnfn atanh(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn atanh(half);\n"
"half2 __ovld __cnfn atanh(half2);\n"
"half3 __ovld __cnfn atanh(half3);\n"
"half4 __ovld __cnfn atanh(half4);\n"
"half8 __ovld __cnfn atanh(half8);\n"
"half16 __ovld __cnfn atanh(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute atan (x) / PI.\n"
" */\n"
"float __ovld __cnfn atanpi(float x);\n"
"float2 __ovld __cnfn atanpi(float2 x);\n"
"float3 __ovld __cnfn atanpi(float3 x);\n"
"float4 __ovld __cnfn atanpi(float4 x);\n"
"float8 __ovld __cnfn atanpi(float8 x);\n"
"float16 __ovld __cnfn atanpi(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn atanpi(double x);\n"
"double2 __ovld __cnfn atanpi(double2 x);\n"
"double3 __ovld __cnfn atanpi(double3 x);\n"
"double4 __ovld __cnfn atanpi(double4 x);\n"
"double8 __ovld __cnfn atanpi(double8 x);\n"
"double16 __ovld __cnfn atanpi(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn atanpi(half x);\n"
"half2 __ovld __cnfn atanpi(half2 x);\n"
"half3 __ovld __cnfn atanpi(half3 x);\n"
"half4 __ovld __cnfn atanpi(half4 x);\n"
"half8 __ovld __cnfn atanpi(half8 x);\n"
"half16 __ovld __cnfn atanpi(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute atan2 (y, x) / PI.\n"
" */\n"
"float __ovld __cnfn atan2pi(float y, float x);\n"
"float2 __ovld __cnfn atan2pi(float2 y, float2 x);\n"
"float3 __ovld __cnfn atan2pi(float3 y, float3 x);\n"
"float4 __ovld __cnfn atan2pi(float4 y, float4 x);\n"
"float8 __ovld __cnfn atan2pi(float8 y, float8 x);\n"
"float16 __ovld __cnfn atan2pi(float16 y, float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn atan2pi(double y, double x);\n"
"double2 __ovld __cnfn atan2pi(double2 y, double2 x);\n"
"double3 __ovld __cnfn atan2pi(double3 y, double3 x);\n"
"double4 __ovld __cnfn atan2pi(double4 y, double4 x);\n"
"double8 __ovld __cnfn atan2pi(double8 y, double8 x);\n"
"double16 __ovld __cnfn atan2pi(double16 y, double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn atan2pi(half y, half x);\n"
"half2 __ovld __cnfn atan2pi(half2 y, half2 x);\n"
"half3 __ovld __cnfn atan2pi(half3 y, half3 x);\n"
"half4 __ovld __cnfn atan2pi(half4 y, half4 x);\n"
"half8 __ovld __cnfn atan2pi(half8 y, half8 x);\n"
"half16 __ovld __cnfn atan2pi(half16 y, half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute cube-root.\n"
" */\n"
"float __ovld __cnfn cbrt(float);\n"
"float2 __ovld __cnfn cbrt(float2);\n"
"float3 __ovld __cnfn cbrt(float3);\n"
"float4 __ovld __cnfn cbrt(float4);\n"
"float8 __ovld __cnfn cbrt(float8);\n"
"float16 __ovld __cnfn cbrt(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn cbrt(double);\n"
"double2 __ovld __cnfn cbrt(double2);\n"
"double3 __ovld __cnfn cbrt(double3);\n"
"double4 __ovld __cnfn cbrt(double4);\n"
"double8 __ovld __cnfn cbrt(double8);\n"
"double16 __ovld __cnfn cbrt(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn cbrt(half);\n"
"half2 __ovld __cnfn cbrt(half2);\n"
"half3 __ovld __cnfn cbrt(half3);\n"
"half4 __ovld __cnfn cbrt(half4);\n"
"half8 __ovld __cnfn cbrt(half8);\n"
"half16 __ovld __cnfn cbrt(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Round to integral value using the round to positive\n"
" * infinity rounding mode.\n"
" */\n"
"float __ovld __cnfn ceil(float);\n"
"float2 __ovld __cnfn ceil(float2);\n"
"float3 __ovld __cnfn ceil(float3);\n"
"float4 __ovld __cnfn ceil(float4);\n"
"float8 __ovld __cnfn ceil(float8);\n"
"float16 __ovld __cnfn ceil(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn ceil(double);\n"
"double2 __ovld __cnfn ceil(double2);\n"
"double3 __ovld __cnfn ceil(double3);\n"
"double4 __ovld __cnfn ceil(double4);\n"
"double8 __ovld __cnfn ceil(double8);\n"
"double16 __ovld __cnfn ceil(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn ceil(half);\n"
"half2 __ovld __cnfn ceil(half2);\n"
"half3 __ovld __cnfn ceil(half3);\n"
"half4 __ovld __cnfn ceil(half4);\n"
"half8 __ovld __cnfn ceil(half8);\n"
"half16 __ovld __cnfn ceil(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns x with its sign changed to match the sign of y.\n"
" */\n"
"float __ovld __cnfn copysign(float x, float y);\n"
"float2 __ovld __cnfn copysign(float2 x, float2 y);\n"
"float3 __ovld __cnfn copysign(float3 x, float3 y);\n"
"float4 __ovld __cnfn copysign(float4 x, float4 y);\n"
"float8 __ovld __cnfn copysign(float8 x, float8 y);\n"
"float16 __ovld __cnfn copysign(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn copysign(double x, double y);\n"
"double2 __ovld __cnfn copysign(double2 x, double2 y);\n"
"double3 __ovld __cnfn copysign(double3 x, double3 y);\n"
"double4 __ovld __cnfn copysign(double4 x, double4 y);\n"
"double8 __ovld __cnfn copysign(double8 x, double8 y);\n"
"double16 __ovld __cnfn copysign(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn copysign(half x, half y);\n"
"half2 __ovld __cnfn copysign(half2 x, half2 y);\n"
"half3 __ovld __cnfn copysign(half3 x, half3 y);\n"
"half4 __ovld __cnfn copysign(half4 x, half4 y);\n"
"half8 __ovld __cnfn copysign(half8 x, half8 y);\n"
"half16 __ovld __cnfn copysign(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute cosine.\n"
" */\n"
"float __ovld __cnfn cos(float);\n"
"float2 __ovld __cnfn cos(float2);\n"
"float3 __ovld __cnfn cos(float3);\n"
"float4 __ovld __cnfn cos(float4);\n"
"float8 __ovld __cnfn cos(float8);\n"
"float16 __ovld __cnfn cos(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn cos(double);\n"
"double2 __ovld __cnfn cos(double2);\n"
"double3 __ovld __cnfn cos(double3);\n"
"double4 __ovld __cnfn cos(double4);\n"
"double8 __ovld __cnfn cos(double8);\n"
"double16 __ovld __cnfn cos(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn cos(half);\n"
"half2 __ovld __cnfn cos(half2);\n"
"half3 __ovld __cnfn cos(half3);\n"
"half4 __ovld __cnfn cos(half4);\n"
"half8 __ovld __cnfn cos(half8);\n"
"half16 __ovld __cnfn cos(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute hyperbolic cosine.\n"
" */\n"
"float __ovld __cnfn cosh(float);\n"
"float2 __ovld __cnfn cosh(float2);\n"
"float3 __ovld __cnfn cosh(float3);\n"
"float4 __ovld __cnfn cosh(float4);\n"
"float8 __ovld __cnfn cosh(float8);\n"
"float16 __ovld __cnfn cosh(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn cosh(double);\n"
"double2 __ovld __cnfn cosh(double2);\n"
"double3 __ovld __cnfn cosh(double3);\n"
"double4 __ovld __cnfn cosh(double4);\n"
"double8 __ovld __cnfn cosh(double8);\n"
"double16 __ovld __cnfn cosh(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn cosh(half);\n"
"half2 __ovld __cnfn cosh(half2);\n"
"half3 __ovld __cnfn cosh(half3);\n"
"half4 __ovld __cnfn cosh(half4);\n"
"half8 __ovld __cnfn cosh(half8);\n"
"half16 __ovld __cnfn cosh(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute cos (PI * x).\n"
" */\n"
"float __ovld __cnfn cospi(float x);\n"
"float2 __ovld __cnfn cospi(float2 x);\n"
"float3 __ovld __cnfn cospi(float3 x);\n"
"float4 __ovld __cnfn cospi(float4 x);\n"
"float8 __ovld __cnfn cospi(float8 x);\n"
"float16 __ovld __cnfn cospi(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn cospi(double x);\n"
"double2 __ovld __cnfn cospi(double2 x);\n"
"double3 __ovld __cnfn cospi(double3 x);\n"
"double4 __ovld __cnfn cospi(double4 x);\n"
"double8 __ovld __cnfn cospi(double8 x);\n"
"double16 __ovld __cnfn cospi(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn cospi(half x);\n"
"half2 __ovld __cnfn cospi(half2 x);\n"
"half3 __ovld __cnfn cospi(half3 x);\n"
"half4 __ovld __cnfn cospi(half4 x);\n"
"half8 __ovld __cnfn cospi(half8 x);\n"
"half16 __ovld __cnfn cospi(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Complementary error function.\n"
" */\n"
"float __ovld __cnfn erfc(float);\n"
"float2 __ovld __cnfn erfc(float2);\n"
"float3 __ovld __cnfn erfc(float3);\n"
"float4 __ovld __cnfn erfc(float4);\n"
"float8 __ovld __cnfn erfc(float8);\n"
"float16 __ovld __cnfn erfc(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn erfc(double);\n"
"double2 __ovld __cnfn erfc(double2);\n"
"double3 __ovld __cnfn erfc(double3);\n"
"double4 __ovld __cnfn erfc(double4);\n"
"double8 __ovld __cnfn erfc(double8);\n"
"double16 __ovld __cnfn erfc(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn erfc(half);\n"
"half2 __ovld __cnfn erfc(half2);\n"
"half3 __ovld __cnfn erfc(half3);\n"
"half4 __ovld __cnfn erfc(half4);\n"
"half8 __ovld __cnfn erfc(half8);\n"
"half16 __ovld __cnfn erfc(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Error function encountered in integrating the\n"
" * normal distribution.\n"
" */\n"
"float __ovld __cnfn erf(float);\n"
"float2 __ovld __cnfn erf(float2);\n"
"float3 __ovld __cnfn erf(float3);\n"
"float4 __ovld __cnfn erf(float4);\n"
"float8 __ovld __cnfn erf(float8);\n"
"float16 __ovld __cnfn erf(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn erf(double);\n"
"double2 __ovld __cnfn erf(double2);\n"
"double3 __ovld __cnfn erf(double3);\n"
"double4 __ovld __cnfn erf(double4);\n"
"double8 __ovld __cnfn erf(double8);\n"
"double16 __ovld __cnfn erf(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn erf(half);\n"
"half2 __ovld __cnfn erf(half2);\n"
"half3 __ovld __cnfn erf(half3);\n"
"half4 __ovld __cnfn erf(half4);\n"
"half8 __ovld __cnfn erf(half8);\n"
"half16 __ovld __cnfn erf(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute the base e exponential function of x.\n"
" */\n"
"float __ovld __cnfn exp(float x);\n"
"float2 __ovld __cnfn exp(float2 x);\n"
"float3 __ovld __cnfn exp(float3 x);\n"
"float4 __ovld __cnfn exp(float4 x);\n"
"float8 __ovld __cnfn exp(float8 x);\n"
"float16 __ovld __cnfn exp(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn exp(double x);\n"
"double2 __ovld __cnfn exp(double2 x);\n"
"double3 __ovld __cnfn exp(double3 x);\n"
"double4 __ovld __cnfn exp(double4 x);\n"
"double8 __ovld __cnfn exp(double8 x);\n"
"double16 __ovld __cnfn exp(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn exp(half x);\n"
"half2 __ovld __cnfn exp(half2 x);\n"
"half3 __ovld __cnfn exp(half3 x);\n"
"half4 __ovld __cnfn exp(half4 x);\n"
"half8 __ovld __cnfn exp(half8 x);\n"
"half16 __ovld __cnfn exp(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Exponential base 2 function.\n"
" */\n"
"float __ovld __cnfn exp2(float);\n"
"float2 __ovld __cnfn exp2(float2);\n"
"float3 __ovld __cnfn exp2(float3);\n"
"float4 __ovld __cnfn exp2(float4);\n"
"float8 __ovld __cnfn exp2(float8);\n"
"float16 __ovld __cnfn exp2(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn exp2(double);\n"
"double2 __ovld __cnfn exp2(double2);\n"
"double3 __ovld __cnfn exp2(double3);\n"
"double4 __ovld __cnfn exp2(double4);\n"
"double8 __ovld __cnfn exp2(double8);\n"
"double16 __ovld __cnfn exp2(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn exp2(half);\n"
"half2 __ovld __cnfn exp2(half2);\n"
"half3 __ovld __cnfn exp2(half3);\n"
"half4 __ovld __cnfn exp2(half4);\n"
"half8 __ovld __cnfn exp2(half8);\n"
"half16 __ovld __cnfn exp2(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Exponential base 10 function.\n"
" */\n"
"float __ovld __cnfn exp10(float);\n"
"float2 __ovld __cnfn exp10(float2);\n"
"float3 __ovld __cnfn exp10(float3);\n"
"float4 __ovld __cnfn exp10(float4);\n"
"float8 __ovld __cnfn exp10(float8);\n"
"float16 __ovld __cnfn exp10(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn exp10(double);\n"
"double2 __ovld __cnfn exp10(double2);\n"
"double3 __ovld __cnfn exp10(double3);\n"
"double4 __ovld __cnfn exp10(double4);\n"
"double8 __ovld __cnfn exp10(double8);\n"
"double16 __ovld __cnfn exp10(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn exp10(half);\n"
"half2 __ovld __cnfn exp10(half2);\n"
"half3 __ovld __cnfn exp10(half3);\n"
"half4 __ovld __cnfn exp10(half4);\n"
"half8 __ovld __cnfn exp10(half8);\n"
"half16 __ovld __cnfn exp10(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute e^x- 1.0.\n"
" */\n"
"float __ovld __cnfn expm1(float x);\n"
"float2 __ovld __cnfn expm1(float2 x);\n"
"float3 __ovld __cnfn expm1(float3 x);\n"
"float4 __ovld __cnfn expm1(float4 x);\n"
"float8 __ovld __cnfn expm1(float8 x);\n"
"float16 __ovld __cnfn expm1(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn expm1(double x);\n"
"double2 __ovld __cnfn expm1(double2 x);\n"
"double3 __ovld __cnfn expm1(double3 x);\n"
"double4 __ovld __cnfn expm1(double4 x);\n"
"double8 __ovld __cnfn expm1(double8 x);\n"
"double16 __ovld __cnfn expm1(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn expm1(half x);\n"
"half2 __ovld __cnfn expm1(half2 x);\n"
"half3 __ovld __cnfn expm1(half3 x);\n"
"half4 __ovld __cnfn expm1(half4 x);\n"
"half8 __ovld __cnfn expm1(half8 x);\n"
"half16 __ovld __cnfn expm1(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute absolute value of a floating-point number.\n"
" */\n"
"float __ovld __cnfn fabs(float);\n"
"float2 __ovld __cnfn fabs(float2);\n"
"float3 __ovld __cnfn fabs(float3);\n"
"float4 __ovld __cnfn fabs(float4);\n"
"float8 __ovld __cnfn fabs(float8);\n"
"float16 __ovld __cnfn fabs(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn fabs(double);\n"
"double2 __ovld __cnfn fabs(double2);\n"
"double3 __ovld __cnfn fabs(double3);\n"
"double4 __ovld __cnfn fabs(double4);\n"
"double8 __ovld __cnfn fabs(double8);\n"
"double16 __ovld __cnfn fabs(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fabs(half);\n"
"half2 __ovld __cnfn fabs(half2);\n"
"half3 __ovld __cnfn fabs(half3);\n"
"half4 __ovld __cnfn fabs(half4);\n"
"half8 __ovld __cnfn fabs(half8);\n"
"half16 __ovld __cnfn fabs(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * x - y if x > y, +0 if x is less than or equal to y.\n"
" */\n"
"float __ovld __cnfn fdim(float x, float y);\n"
"float2 __ovld __cnfn fdim(float2 x, float2 y);\n"
"float3 __ovld __cnfn fdim(float3 x, float3 y);\n"
"float4 __ovld __cnfn fdim(float4 x, float4 y);\n"
"float8 __ovld __cnfn fdim(float8 x, float8 y);\n"
"float16 __ovld __cnfn fdim(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn fdim(double x, double y);\n"
"double2 __ovld __cnfn fdim(double2 x, double2 y);\n"
"double3 __ovld __cnfn fdim(double3 x, double3 y);\n"
"double4 __ovld __cnfn fdim(double4 x, double4 y);\n"
"double8 __ovld __cnfn fdim(double8 x, double8 y);\n"
"double16 __ovld __cnfn fdim(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fdim(half x, half y);\n"
"half2 __ovld __cnfn fdim(half2 x, half2 y);\n"
"half3 __ovld __cnfn fdim(half3 x, half3 y);\n"
"half4 __ovld __cnfn fdim(half4 x, half4 y);\n"
"half8 __ovld __cnfn fdim(half8 x, half8 y);\n"
"half16 __ovld __cnfn fdim(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Round to integral value using the round to -ve\n"
" * infinity rounding mode.\n"
" */\n"
"float __ovld __cnfn floor(float);\n"
"float2 __ovld __cnfn floor(float2);\n"
"float3 __ovld __cnfn floor(float3);\n"
"float4 __ovld __cnfn floor(float4);\n"
"float8 __ovld __cnfn floor(float8);\n"
"float16 __ovld __cnfn floor(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn floor(double);\n"
"double2 __ovld __cnfn floor(double2);\n"
"double3 __ovld __cnfn floor(double3);\n"
"double4 __ovld __cnfn floor(double4);\n"
"double8 __ovld __cnfn floor(double8);\n"
"double16 __ovld __cnfn floor(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn floor(half);\n"
"half2 __ovld __cnfn floor(half2);\n"
"half3 __ovld __cnfn floor(half3);\n"
"half4 __ovld __cnfn floor(half4);\n"
"half8 __ovld __cnfn floor(half8);\n"
"half16 __ovld __cnfn floor(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the correctly rounded floating-point\n"
" * representation of the sum of c with the infinitely\n"
" * precise product of a and b. Rounding of\n"
" * intermediate products shall not occur. Edge case\n"
" * behavior is per the IEEE 754-2008 standard.\n"
" */\n"
"float __ovld __cnfn fma(float a, float b, float c);\n"
"float2 __ovld __cnfn fma(float2 a, float2 b, float2 c);\n"
"float3 __ovld __cnfn fma(float3 a, float3 b, float3 c);\n"
"float4 __ovld __cnfn fma(float4 a, float4 b, float4 c);\n"
"float8 __ovld __cnfn fma(float8 a, float8 b, float8 c);\n"
"float16 __ovld __cnfn fma(float16 a, float16 b, float16 c);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn fma(double a, double b, double c);\n"
"double2 __ovld __cnfn fma(double2 a, double2 b, double2 c);\n"
"double3 __ovld __cnfn fma(double3 a, double3 b, double3 c);\n"
"double4 __ovld __cnfn fma(double4 a, double4 b, double4 c);\n"
"double8 __ovld __cnfn fma(double8 a, double8 b, double8 c);\n"
"double16 __ovld __cnfn fma(double16 a, double16 b, double16 c);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fma(half a, half b, half c);\n"
"half2 __ovld __cnfn fma(half2 a, half2 b, half2 c);\n"
"half3 __ovld __cnfn fma(half3 a, half3 b, half3 c);\n"
"half4 __ovld __cnfn fma(half4 a, half4 b, half4 c);\n"
"half8 __ovld __cnfn fma(half8 a, half8 b, half8 c);\n"
"half16 __ovld __cnfn fma(half16 a, half16 b, half16 c);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns y if x < y, otherwise it returns x. If one\n"
" * argument is a NaN, fmax() returns the other\n"
" * argument. If both arguments are NaNs, fmax()\n"
" * returns a NaN.\n"
" */\n"
"float __ovld __cnfn fmax(float x, float y);\n"
"float2 __ovld __cnfn fmax(float2 x, float2 y);\n"
"float3 __ovld __cnfn fmax(float3 x, float3 y);\n"
"float4 __ovld __cnfn fmax(float4 x, float4 y);\n"
"float8 __ovld __cnfn fmax(float8 x, float8 y);\n"
"float16 __ovld __cnfn fmax(float16 x, float16 y);\n"
"float2 __ovld __cnfn fmax(float2 x, float y);\n"
"float3 __ovld __cnfn fmax(float3 x, float y);\n"
"float4 __ovld __cnfn fmax(float4 x, float y);\n"
"float8 __ovld __cnfn fmax(float8 x, float y);\n"
"float16 __ovld __cnfn fmax(float16 x, float y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn fmax(double x, double y);\n"
"double2 __ovld __cnfn fmax(double2 x, double2 y);\n"
"double3 __ovld __cnfn fmax(double3 x, double3 y);\n"
"double4 __ovld __cnfn fmax(double4 x, double4 y);\n"
"double8 __ovld __cnfn fmax(double8 x, double8 y);\n"
"double16 __ovld __cnfn fmax(double16 x, double16 y);\n"
"double2 __ovld __cnfn fmax(double2 x, double y);\n"
"double3 __ovld __cnfn fmax(double3 x, double y);\n"
"double4 __ovld __cnfn fmax(double4 x, double y);\n"
"double8 __ovld __cnfn fmax(double8 x, double y);\n"
"double16 __ovld __cnfn fmax(double16 x, double y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fmax(half x, half y);\n"
"half2 __ovld __cnfn fmax(half2 x, half2 y);\n"
"half3 __ovld __cnfn fmax(half3 x, half3 y);\n"
"half4 __ovld __cnfn fmax(half4 x, half4 y);\n"
"half8 __ovld __cnfn fmax(half8 x, half8 y);\n"
"half16 __ovld __cnfn fmax(half16 x, half16 y);\n"
"half2 __ovld __cnfn fmax(half2 x, half y);\n"
"half3 __ovld __cnfn fmax(half3 x, half y);\n"
"half4 __ovld __cnfn fmax(half4 x, half y);\n"
"half8 __ovld __cnfn fmax(half8 x, half y);\n"
"half16 __ovld __cnfn fmax(half16 x, half y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns y if y < x, otherwise it returns x. If one\n"
" * argument is a NaN, fmin() returns the other\n"
" * argument. If both arguments are NaNs, fmin()\n"
" * returns a NaN.\n"
" */\n"
"float __ovld __cnfn fmin(float x, float y);\n"
"float2 __ovld __cnfn fmin(float2 x, float2 y);\n"
"float3 __ovld __cnfn fmin(float3 x, float3 y);\n"
"float4 __ovld __cnfn fmin(float4 x, float4 y);\n"
"float8 __ovld __cnfn fmin(float8 x, float8 y);\n"
"float16 __ovld __cnfn fmin(float16 x, float16 y);\n"
"float2 __ovld __cnfn fmin(float2 x, float y);\n"
"float3 __ovld __cnfn fmin(float3 x, float y);\n"
"float4 __ovld __cnfn fmin(float4 x, float y);\n"
"float8 __ovld __cnfn fmin(float8 x, float y);\n"
"float16 __ovld __cnfn fmin(float16 x, float y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn fmin(double x, double y);\n"
"double2 __ovld __cnfn fmin(double2 x, double2 y);\n"
"double3 __ovld __cnfn fmin(double3 x, double3 y);\n"
"double4 __ovld __cnfn fmin(double4 x, double4 y);\n"
"double8 __ovld __cnfn fmin(double8 x, double8 y);\n"
"double16 __ovld __cnfn fmin(double16 x, double16 y);\n"
"double2 __ovld __cnfn fmin(double2 x, double y);\n"
"double3 __ovld __cnfn fmin(double3 x, double y);\n"
"double4 __ovld __cnfn fmin(double4 x, double y);\n"
"double8 __ovld __cnfn fmin(double8 x, double y);\n"
"double16 __ovld __cnfn fmin(double16 x, double y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fmin(half x, half y);\n"
"half2 __ovld __cnfn fmin(half2 x, half2 y);\n"
"half3 __ovld __cnfn fmin(half3 x, half3 y);\n"
"half4 __ovld __cnfn fmin(half4 x, half4 y);\n"
"half8 __ovld __cnfn fmin(half8 x, half8 y);\n"
"half16 __ovld __cnfn fmin(half16 x, half16 y);\n"
"half2 __ovld __cnfn fmin(half2 x, half y);\n"
"half3 __ovld __cnfn fmin(half3 x, half y);\n"
"half4 __ovld __cnfn fmin(half4 x, half y);\n"
"half8 __ovld __cnfn fmin(half8 x, half y);\n"
"half16 __ovld __cnfn fmin(half16 x, half y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Modulus. Returns x - y * trunc (x/y).\n"
" */\n"
"float __ovld __cnfn fmod(float x, float y);\n"
"float2 __ovld __cnfn fmod(float2 x, float2 y);\n"
"float3 __ovld __cnfn fmod(float3 x, float3 y);\n"
"float4 __ovld __cnfn fmod(float4 x, float4 y);\n"
"float8 __ovld __cnfn fmod(float8 x, float8 y);\n"
"float16 __ovld __cnfn fmod(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn fmod(double x, double y);\n"
"double2 __ovld __cnfn fmod(double2 x, double2 y);\n"
"double3 __ovld __cnfn fmod(double3 x, double3 y);\n"
"double4 __ovld __cnfn fmod(double4 x, double4 y);\n"
"double8 __ovld __cnfn fmod(double8 x, double8 y);\n"
"double16 __ovld __cnfn fmod(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fmod(half x, half y);\n"
"half2 __ovld __cnfn fmod(half2 x, half2 y);\n"
"half3 __ovld __cnfn fmod(half3 x, half3 y);\n"
"half4 __ovld __cnfn fmod(half4 x, half4 y);\n"
"half8 __ovld __cnfn fmod(half8 x, half8 y);\n"
"half16 __ovld __cnfn fmod(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns fmin(x - floor (x), 0x1.fffffep-1f ).\n"
" * floor(x) is returned in iptr.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld fract(float x, float *iptr);\n"
"float2 __ovld fract(float2 x, float2 *iptr);\n"
"float3 __ovld fract(float3 x, float3 *iptr);\n"
"float4 __ovld fract(float4 x, float4 *iptr);\n"
"float8 __ovld fract(float8 x, float8 *iptr);\n"
"float16 __ovld fract(float16 x, float16 *iptr);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld fract(double x, double *iptr);\n"
"double2 __ovld fract(double2 x, double2 *iptr);\n"
"double3 __ovld fract(double3 x, double3 *iptr);\n"
"double4 __ovld fract(double4 x, double4 *iptr);\n"
"double8 __ovld fract(double8 x, double8 *iptr);\n"
"double16 __ovld fract(double16 x, double16 *iptr);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld fract(half x, half *iptr);\n"
"half2 __ovld fract(half2 x, half2 *iptr);\n"
"half3 __ovld fract(half3 x, half3 *iptr);\n"
"half4 __ovld fract(half4 x, half4 *iptr);\n"
"half8 __ovld fract(half8 x, half8 *iptr);\n"
"half16 __ovld fract(half16 x, half16 *iptr);\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"float __ovld fract(float x, __global float *iptr);\n"
"float2 __ovld fract(float2 x, __global float2 *iptr);\n"
"float3 __ovld fract(float3 x, __global float3 *iptr);\n"
"float4 __ovld fract(float4 x, __global float4 *iptr);\n"
"float8 __ovld fract(float8 x, __global float8 *iptr);\n"
"float16 __ovld fract(float16 x, __global float16 *iptr);\n"
"float __ovld fract(float x, __local float *iptr);\n"
"float2 __ovld fract(float2 x, __local float2 *iptr);\n"
"float3 __ovld fract(float3 x, __local float3 *iptr);\n"
"float4 __ovld fract(float4 x, __local float4 *iptr);\n"
"float8 __ovld fract(float8 x, __local float8 *iptr);\n"
"float16 __ovld fract(float16 x, __local float16 *iptr);\n"
"float __ovld fract(float x, __private float *iptr);\n"
"float2 __ovld fract(float2 x, __private float2 *iptr);\n"
"float3 __ovld fract(float3 x, __private float3 *iptr);\n"
"float4 __ovld fract(float4 x, __private float4 *iptr);\n"
"float8 __ovld fract(float8 x, __private float8 *iptr);\n"
"float16 __ovld fract(float16 x, __private float16 *iptr);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld fract(double x, __global double *iptr);\n"
"double2 __ovld fract(double2 x, __global double2 *iptr);\n"
"double3 __ovld fract(double3 x, __global double3 *iptr);\n"
"double4 __ovld fract(double4 x, __global double4 *iptr);\n"
"double8 __ovld fract(double8 x, __global double8 *iptr);\n"
"double16 __ovld fract(double16 x, __global double16 *iptr);\n"
"double __ovld fract(double x, __local double *iptr);\n"
"double2 __ovld fract(double2 x, __local double2 *iptr);\n"
"double3 __ovld fract(double3 x, __local double3 *iptr);\n"
"double4 __ovld fract(double4 x, __local double4 *iptr);\n"
"double8 __ovld fract(double8 x, __local double8 *iptr);\n"
"double16 __ovld fract(double16 x, __local double16 *iptr);\n"
"double __ovld fract(double x, __private double *iptr);\n"
"double2 __ovld fract(double2 x, __private double2 *iptr);\n"
"double3 __ovld fract(double3 x, __private double3 *iptr);\n"
"double4 __ovld fract(double4 x, __private double4 *iptr);\n"
"double8 __ovld fract(double8 x, __private double8 *iptr);\n"
"double16 __ovld fract(double16 x, __private double16 *iptr);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld fract(half x, __global half *iptr);\n"
"half2 __ovld fract(half2 x, __global half2 *iptr);\n"
"half3 __ovld fract(half3 x, __global half3 *iptr);\n"
"half4 __ovld fract(half4 x, __global half4 *iptr);\n"
"half8 __ovld fract(half8 x, __global half8 *iptr);\n"
"half16 __ovld fract(half16 x, __global half16 *iptr);\n"
"half __ovld fract(half x, __local half *iptr);\n"
"half2 __ovld fract(half2 x, __local half2 *iptr);\n"
"half3 __ovld fract(half3 x, __local half3 *iptr);\n"
"half4 __ovld fract(half4 x, __local half4 *iptr);\n"
"half8 __ovld fract(half8 x, __local half8 *iptr);\n"
"half16 __ovld fract(half16 x, __local half16 *iptr);\n"
"half __ovld fract(half x, __private half *iptr);\n"
"half2 __ovld fract(half2 x, __private half2 *iptr);\n"
"half3 __ovld fract(half3 x, __private half3 *iptr);\n"
"half4 __ovld fract(half4 x, __private half4 *iptr);\n"
"half8 __ovld fract(half8 x, __private half8 *iptr);\n"
"half16 __ovld fract(half16 x, __private half16 *iptr);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Extract mantissa and exponent from x. For each\n"
" * component the mantissa returned is a float with\n"
" * magnitude in the interval [1/2, 1) or 0. Each\n"
" * component of x equals mantissa returned * 2^exp.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld frexp(float x, int *exp);\n"
"float2 __ovld frexp(float2 x, int2 *exp);\n"
"float3 __ovld frexp(float3 x, int3 *exp);\n"
"float4 __ovld frexp(float4 x, int4 *exp);\n"
"float8 __ovld frexp(float8 x, int8 *exp);\n"
"float16 __ovld frexp(float16 x, int16 *exp);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld frexp(double x, int *exp);\n"
"double2 __ovld frexp(double2 x, int2 *exp);\n"
"double3 __ovld frexp(double3 x, int3 *exp);\n"
"double4 __ovld frexp(double4 x, int4 *exp);\n"
"double8 __ovld frexp(double8 x, int8 *exp);\n"
"double16 __ovld frexp(double16 x, int16 *exp);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld frexp(half x, int *exp);\n"
"half2 __ovld frexp(half2 x, int2 *exp);\n"
"half3 __ovld frexp(half3 x, int3 *exp);\n"
"half4 __ovld frexp(half4 x, int4 *exp);\n"
"half8 __ovld frexp(half8 x, int8 *exp);\n"
"half16 __ovld frexp(half16 x, int16 *exp);\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"float __ovld frexp(float x, __global int *exp);\n"
"float2 __ovld frexp(float2 x, __global int2 *exp);\n"
"float3 __ovld frexp(float3 x, __global int3 *exp);\n"
"float4 __ovld frexp(float4 x, __global int4 *exp);\n"
"float8 __ovld frexp(float8 x, __global int8 *exp);\n"
"float16 __ovld frexp(float16 x, __global int16 *exp);\n"
"float __ovld frexp(float x, __local int *exp);\n"
"float2 __ovld frexp(float2 x, __local int2 *exp);\n"
"float3 __ovld frexp(float3 x, __local int3 *exp);\n"
"float4 __ovld frexp(float4 x, __local int4 *exp);\n"
"float8 __ovld frexp(float8 x, __local int8 *exp);\n"
"float16 __ovld frexp(float16 x, __local int16 *exp);\n"
"float __ovld frexp(float x, __private int *exp);\n"
"float2 __ovld frexp(float2 x, __private int2 *exp);\n"
"float3 __ovld frexp(float3 x, __private int3 *exp);\n"
"float4 __ovld frexp(float4 x, __private int4 *exp);\n"
"float8 __ovld frexp(float8 x, __private int8 *exp);\n"
"float16 __ovld frexp(float16 x, __private int16 *exp);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld frexp(double x, __global int *exp);\n"
"double2 __ovld frexp(double2 x, __global int2 *exp);\n"
"double3 __ovld frexp(double3 x, __global int3 *exp);\n"
"double4 __ovld frexp(double4 x, __global int4 *exp);\n"
"double8 __ovld frexp(double8 x, __global int8 *exp);\n"
"double16 __ovld frexp(double16 x, __global int16 *exp);\n"
"double __ovld frexp(double x, __local int *exp);\n"
"double2 __ovld frexp(double2 x, __local int2 *exp);\n"
"double3 __ovld frexp(double3 x, __local int3 *exp);\n"
"double4 __ovld frexp(double4 x, __local int4 *exp);\n"
"double8 __ovld frexp(double8 x, __local int8 *exp);\n"
"double16 __ovld frexp(double16 x, __local int16 *exp);\n"
"double __ovld frexp(double x, __private int *exp);\n"
"double2 __ovld frexp(double2 x, __private int2 *exp);\n"
"double3 __ovld frexp(double3 x, __private int3 *exp);\n"
"double4 __ovld frexp(double4 x, __private int4 *exp);\n"
"double8 __ovld frexp(double8 x, __private int8 *exp);\n"
"double16 __ovld frexp(double16 x, __private int16 *exp);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld frexp(half x, __global int *exp);\n"
"half2 __ovld frexp(half2 x, __global int2 *exp);\n"
"half3 __ovld frexp(half3 x, __global int3 *exp);\n"
"half4 __ovld frexp(half4 x, __global int4 *exp);\n"
"half8 __ovld frexp(half8 x, __global int8 *exp);\n"
"half16 __ovld frexp(half16 x, __global int16 *exp);\n"
"half __ovld frexp(half x, __local int *exp);\n"
"half2 __ovld frexp(half2 x, __local int2 *exp);\n"
"half3 __ovld frexp(half3 x, __local int3 *exp);\n"
"half4 __ovld frexp(half4 x, __local int4 *exp);\n"
"half8 __ovld frexp(half8 x, __local int8 *exp);\n"
"half16 __ovld frexp(half16 x, __local int16 *exp);\n"
"half __ovld frexp(half x, __private int *exp);\n"
"half2 __ovld frexp(half2 x, __private int2 *exp);\n"
"half3 __ovld frexp(half3 x, __private int3 *exp);\n"
"half4 __ovld frexp(half4 x, __private int4 *exp);\n"
"half8 __ovld frexp(half8 x, __private int8 *exp);\n"
"half16 __ovld frexp(half16 x, __private int16 *exp);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Compute the value of the square root of x^2 + y^2\n"
" * without undue overflow or underflow.\n"
" */\n"
"float __ovld __cnfn hypot(float x, float y);\n"
"float2 __ovld __cnfn hypot(float2 x, float2 y);\n"
"float3 __ovld __cnfn hypot(float3 x, float3 y);\n"
"float4 __ovld __cnfn hypot(float4 x, float4 y);\n"
"float8 __ovld __cnfn hypot(float8 x, float8 y);\n"
"float16 __ovld __cnfn hypot(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn hypot(double x, double y);\n"
"double2 __ovld __cnfn hypot(double2 x, double2 y);\n"
"double3 __ovld __cnfn hypot(double3 x, double3 y);\n"
"double4 __ovld __cnfn hypot(double4 x, double4 y);\n"
"double8 __ovld __cnfn hypot(double8 x, double8 y);\n"
"double16 __ovld __cnfn hypot(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn hypot(half x, half y);\n"
"half2 __ovld __cnfn hypot(half2 x, half2 y);\n"
"half3 __ovld __cnfn hypot(half3 x, half3 y);\n"
"half4 __ovld __cnfn hypot(half4 x, half4 y);\n"
"half8 __ovld __cnfn hypot(half8 x, half8 y);\n"
"half16 __ovld __cnfn hypot(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Return the exponent as an integer value.\n"
" */\n"
"int __ovld __cnfn ilogb(float x);\n"
"int2 __ovld __cnfn ilogb(float2 x);\n"
"int3 __ovld __cnfn ilogb(float3 x);\n"
"int4 __ovld __cnfn ilogb(float4 x);\n"
"int8 __ovld __cnfn ilogb(float8 x);\n"
"int16 __ovld __cnfn ilogb(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn ilogb(double x);\n"
"int2 __ovld __cnfn ilogb(double2 x);\n"
"int3 __ovld __cnfn ilogb(double3 x);\n"
"int4 __ovld __cnfn ilogb(double4 x);\n"
"int8 __ovld __cnfn ilogb(double8 x);\n"
"int16 __ovld __cnfn ilogb(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn ilogb(half x);\n"
"int2 __ovld __cnfn ilogb(half2 x);\n"
"int3 __ovld __cnfn ilogb(half3 x);\n"
"int4 __ovld __cnfn ilogb(half4 x);\n"
"int8 __ovld __cnfn ilogb(half8 x);\n"
"int16 __ovld __cnfn ilogb(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Multiply x by 2 to the power n.\n"
" */\n"
"float __ovld __cnfn ldexp(float x, int n);\n"
"float2 __ovld __cnfn ldexp(float2 x, int2 n);\n"
"float3 __ovld __cnfn ldexp(float3 x, int3 n);\n"
"float4 __ovld __cnfn ldexp(float4 x, int4 n);\n"
"float8 __ovld __cnfn ldexp(float8 x, int8 n);\n"
"float16 __ovld __cnfn ldexp(float16 x, int16 n);\n"
"float2 __ovld __cnfn ldexp(float2 x, int n);\n"
"float3 __ovld __cnfn ldexp(float3 x, int n);\n"
"float4 __ovld __cnfn ldexp(float4 x, int n);\n"
"float8 __ovld __cnfn ldexp(float8 x, int n);\n"
"float16 __ovld __cnfn ldexp(float16 x, int n);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn ldexp(double x, int n);\n"
"double2 __ovld __cnfn ldexp(double2 x, int2 n);\n"
"double3 __ovld __cnfn ldexp(double3 x, int3 n);\n"
"double4 __ovld __cnfn ldexp(double4 x, int4 n);\n"
"double8 __ovld __cnfn ldexp(double8 x, int8 n);\n"
"double16 __ovld __cnfn ldexp(double16 x, int16 n);\n"
"double2 __ovld __cnfn ldexp(double2 x, int n);\n"
"double3 __ovld __cnfn ldexp(double3 x, int n);\n"
"double4 __ovld __cnfn ldexp(double4 x, int n);\n"
"double8 __ovld __cnfn ldexp(double8 x, int n);\n"
"double16 __ovld __cnfn ldexp(double16 x, int n);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn ldexp(half x, int n);\n"
"half2 __ovld __cnfn ldexp(half2 x, int2 n);\n"
"half3 __ovld __cnfn ldexp(half3 x, int3 n);\n"
"half4 __ovld __cnfn ldexp(half4 x, int4 n);\n"
"half8 __ovld __cnfn ldexp(half8 x, int8 n);\n"
"half16 __ovld __cnfn ldexp(half16 x, int16 n);\n"
"half2 __ovld __cnfn ldexp(half2 x, int n);\n"
"half3 __ovld __cnfn ldexp(half3 x, int n);\n"
"half4 __ovld __cnfn ldexp(half4 x, int n);\n"
"half8 __ovld __cnfn ldexp(half8 x, int n);\n"
"half16 __ovld __cnfn ldexp(half16 x, int n);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Log gamma function. Returns the natural\n"
" * logarithm of the absolute value of the gamma\n"
" * function. The sign of the gamma function is\n"
" * returned in the signp argument of lgamma_r.\n"
" */\n"
"float __ovld __cnfn lgamma(float x);\n"
"float2 __ovld __cnfn lgamma(float2 x);\n"
"float3 __ovld __cnfn lgamma(float3 x);\n"
"float4 __ovld __cnfn lgamma(float4 x);\n"
"float8 __ovld __cnfn lgamma(float8 x);\n"
"float16 __ovld __cnfn lgamma(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn lgamma(double x);\n"
"double2 __ovld __cnfn lgamma(double2 x);\n"
"double3 __ovld __cnfn lgamma(double3 x);\n"
"double4 __ovld __cnfn lgamma(double4 x);\n"
"double8 __ovld __cnfn lgamma(double8 x);\n"
"double16 __ovld __cnfn lgamma(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn lgamma(half x);\n"
"half2 __ovld __cnfn lgamma(half2 x);\n"
"half3 __ovld __cnfn lgamma(half3 x);\n"
"half4 __ovld __cnfn lgamma(half4 x);\n"
"half8 __ovld __cnfn lgamma(half8 x);\n"
"half16 __ovld __cnfn lgamma(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld lgamma_r(float x, int *signp);\n"
"float2 __ovld lgamma_r(float2 x, int2 *signp);\n"
"float3 __ovld lgamma_r(float3 x, int3 *signp);\n"
"float4 __ovld lgamma_r(float4 x, int4 *signp);\n"
"float8 __ovld lgamma_r(float8 x, int8 *signp);\n"
"float16 __ovld lgamma_r(float16 x, int16 *signp);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld lgamma_r(double x, int *signp);\n"
"double2 __ovld lgamma_r(double2 x, int2 *signp);\n"
"double3 __ovld lgamma_r(double3 x, int3 *signp);\n"
"double4 __ovld lgamma_r(double4 x, int4 *signp);\n"
"double8 __ovld lgamma_r(double8 x, int8 *signp);\n"
"double16 __ovld lgamma_r(double16 x, int16 *signp);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld lgamma_r(half x, int *signp);\n"
"half2 __ovld lgamma_r(half2 x, int2 *signp);\n"
"half3 __ovld lgamma_r(half3 x, int3 *signp);\n"
"half4 __ovld lgamma_r(half4 x, int4 *signp);\n"
"half8 __ovld lgamma_r(half8 x, int8 *signp);\n"
"half16 __ovld lgamma_r(half16 x, int16 *signp);\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"float __ovld lgamma_r(float x, __global int *signp);\n"
"float2 __ovld lgamma_r(float2 x, __global int2 *signp);\n"
"float3 __ovld lgamma_r(float3 x, __global int3 *signp);\n"
"float4 __ovld lgamma_r(float4 x, __global int4 *signp);\n"
"float8 __ovld lgamma_r(float8 x, __global int8 *signp);\n"
"float16 __ovld lgamma_r(float16 x, __global int16 *signp);\n"
"float __ovld lgamma_r(float x, __local int *signp);\n"
"float2 __ovld lgamma_r(float2 x, __local int2 *signp);\n"
"float3 __ovld lgamma_r(float3 x, __local int3 *signp);\n"
"float4 __ovld lgamma_r(float4 x, __local int4 *signp);\n"
"float8 __ovld lgamma_r(float8 x, __local int8 *signp);\n"
"float16 __ovld lgamma_r(float16 x, __local int16 *signp);\n"
"float __ovld lgamma_r(float x, __private int *signp);\n"
"float2 __ovld lgamma_r(float2 x, __private int2 *signp);\n"
"float3 __ovld lgamma_r(float3 x, __private int3 *signp);\n"
"float4 __ovld lgamma_r(float4 x, __private int4 *signp);\n"
"float8 __ovld lgamma_r(float8 x, __private int8 *signp);\n"
"float16 __ovld lgamma_r(float16 x, __private int16 *signp);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld lgamma_r(double x, __global int *signp);\n"
"double2 __ovld lgamma_r(double2 x, __global int2 *signp);\n"
"double3 __ovld lgamma_r(double3 x, __global int3 *signp);\n"
"double4 __ovld lgamma_r(double4 x, __global int4 *signp);\n"
"double8 __ovld lgamma_r(double8 x, __global int8 *signp);\n"
"double16 __ovld lgamma_r(double16 x, __global int16 *signp);\n"
"double __ovld lgamma_r(double x, __local int *signp);\n"
"double2 __ovld lgamma_r(double2 x, __local int2 *signp);\n"
"double3 __ovld lgamma_r(double3 x, __local int3 *signp);\n"
"double4 __ovld lgamma_r(double4 x, __local int4 *signp);\n"
"double8 __ovld lgamma_r(double8 x, __local int8 *signp);\n"
"double16 __ovld lgamma_r(double16 x, __local int16 *signp);\n"
"double __ovld lgamma_r(double x, __private int *signp);\n"
"double2 __ovld lgamma_r(double2 x, __private int2 *signp);\n"
"double3 __ovld lgamma_r(double3 x, __private int3 *signp);\n"
"double4 __ovld lgamma_r(double4 x, __private int4 *signp);\n"
"double8 __ovld lgamma_r(double8 x, __private int8 *signp);\n"
"double16 __ovld lgamma_r(double16 x, __private int16 *signp);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld lgamma_r(half x, __global int *signp);\n"
"half2 __ovld lgamma_r(half2 x, __global int2 *signp);\n"
"half3 __ovld lgamma_r(half3 x, __global int3 *signp);\n"
"half4 __ovld lgamma_r(half4 x, __global int4 *signp);\n"
"half8 __ovld lgamma_r(half8 x, __global int8 *signp);\n"
"half16 __ovld lgamma_r(half16 x, __global int16 *signp);\n"
"half __ovld lgamma_r(half x, __local int *signp);\n"
"half2 __ovld lgamma_r(half2 x, __local int2 *signp);\n"
"half3 __ovld lgamma_r(half3 x, __local int3 *signp);\n"
"half4 __ovld lgamma_r(half4 x, __local int4 *signp);\n"
"half8 __ovld lgamma_r(half8 x, __local int8 *signp);\n"
"half16 __ovld lgamma_r(half16 x, __local int16 *signp);\n"
"half __ovld lgamma_r(half x, __private int *signp);\n"
"half2 __ovld lgamma_r(half2 x, __private int2 *signp);\n"
"half3 __ovld lgamma_r(half3 x, __private int3 *signp);\n"
"half4 __ovld lgamma_r(half4 x, __private int4 *signp);\n"
"half8 __ovld lgamma_r(half8 x, __private int8 *signp);\n"
"half16 __ovld lgamma_r(half16 x, __private int16 *signp);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Compute natural logarithm.\n"
" */\n"
"float __ovld __cnfn log(float);\n"
"float2 __ovld __cnfn log(float2);\n"
"float3 __ovld __cnfn log(float3);\n"
"float4 __ovld __cnfn log(float4);\n"
"float8 __ovld __cnfn log(float8);\n"
"float16 __ovld __cnfn log(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn log(double);\n"
"double2 __ovld __cnfn log(double2);\n"
"double3 __ovld __cnfn log(double3);\n"
"double4 __ovld __cnfn log(double4);\n"
"double8 __ovld __cnfn log(double8);\n"
"double16 __ovld __cnfn log(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn log(half);\n"
"half2 __ovld __cnfn log(half2);\n"
"half3 __ovld __cnfn log(half3);\n"
"half4 __ovld __cnfn log(half4);\n"
"half8 __ovld __cnfn log(half8);\n"
"half16 __ovld __cnfn log(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute a base 2 logarithm.\n"
" */\n"
"float __ovld __cnfn log2(float);\n"
"float2 __ovld __cnfn log2(float2);\n"
"float3 __ovld __cnfn log2(float3);\n"
"float4 __ovld __cnfn log2(float4);\n"
"float8 __ovld __cnfn log2(float8);\n"
"float16 __ovld __cnfn log2(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn log2(double);\n"
"double2 __ovld __cnfn log2(double2);\n"
"double3 __ovld __cnfn log2(double3);\n"
"double4 __ovld __cnfn log2(double4);\n"
"double8 __ovld __cnfn log2(double8);\n"
"double16 __ovld __cnfn log2(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn log2(half);\n"
"half2 __ovld __cnfn log2(half2);\n"
"half3 __ovld __cnfn log2(half3);\n"
"half4 __ovld __cnfn log2(half4);\n"
"half8 __ovld __cnfn log2(half8);\n"
"half16 __ovld __cnfn log2(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute a base 10 logarithm.\n"
" */\n"
"float __ovld __cnfn log10(float);\n"
"float2 __ovld __cnfn log10(float2);\n"
"float3 __ovld __cnfn log10(float3);\n"
"float4 __ovld __cnfn log10(float4);\n"
"float8 __ovld __cnfn log10(float8);\n"
"float16 __ovld __cnfn log10(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn log10(double);\n"
"double2 __ovld __cnfn log10(double2);\n"
"double3 __ovld __cnfn log10(double3);\n"
"double4 __ovld __cnfn log10(double4);\n"
"double8 __ovld __cnfn log10(double8);\n"
"double16 __ovld __cnfn log10(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn log10(half);\n"
"half2 __ovld __cnfn log10(half2);\n"
"half3 __ovld __cnfn log10(half3);\n"
"half4 __ovld __cnfn log10(half4);\n"
"half8 __ovld __cnfn log10(half8);\n"
"half16 __ovld __cnfn log10(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute a base e logarithm of (1.0 + x).\n"
" */\n"
"float __ovld __cnfn log1p(float x);\n"
"float2 __ovld __cnfn log1p(float2 x);\n"
"float3 __ovld __cnfn log1p(float3 x);\n"
"float4 __ovld __cnfn log1p(float4 x);\n"
"float8 __ovld __cnfn log1p(float8 x);\n"
"float16 __ovld __cnfn log1p(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn log1p(double x);\n"
"double2 __ovld __cnfn log1p(double2 x);\n"
"double3 __ovld __cnfn log1p(double3 x);\n"
"double4 __ovld __cnfn log1p(double4 x);\n"
"double8 __ovld __cnfn log1p(double8 x);\n"
"double16 __ovld __cnfn log1p(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn log1p(half x);\n"
"half2 __ovld __cnfn log1p(half2 x);\n"
"half3 __ovld __cnfn log1p(half3 x);\n"
"half4 __ovld __cnfn log1p(half4 x);\n"
"half8 __ovld __cnfn log1p(half8 x);\n"
"half16 __ovld __cnfn log1p(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute the exponent of x, which is the integral\n"
" * part of logr | x |.\n"
" */\n"
"float __ovld __cnfn logb(float x);\n"
"float2 __ovld __cnfn logb(float2 x);\n"
"float3 __ovld __cnfn logb(float3 x);\n"
"float4 __ovld __cnfn logb(float4 x);\n"
"float8 __ovld __cnfn logb(float8 x);\n"
"float16 __ovld __cnfn logb(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn logb(double x);\n"
"double2 __ovld __cnfn logb(double2 x);\n"
"double3 __ovld __cnfn logb(double3 x);\n"
"double4 __ovld __cnfn logb(double4 x);\n"
"double8 __ovld __cnfn logb(double8 x);\n"
"double16 __ovld __cnfn logb(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn logb(half x);\n"
"half2 __ovld __cnfn logb(half2 x);\n"
"half3 __ovld __cnfn logb(half3 x);\n"
"half4 __ovld __cnfn logb(half4 x);\n"
"half8 __ovld __cnfn logb(half8 x);\n"
"half16 __ovld __cnfn logb(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * mad approximates a * b + c. Whether or how the\n"
" * product of a * b is rounded and how supernormal or\n"
" * subnormal intermediate products are handled is not\n"
" * defined. mad is intended to be used where speed is\n"
" * preferred over accuracy.\n"
" */\n"
"float __ovld __cnfn mad(float a, float b, float c);\n"
"float2 __ovld __cnfn mad(float2 a, float2 b, float2 c);\n"
"float3 __ovld __cnfn mad(float3 a, float3 b, float3 c);\n"
"float4 __ovld __cnfn mad(float4 a, float4 b, float4 c);\n"
"float8 __ovld __cnfn mad(float8 a, float8 b, float8 c);\n"
"float16 __ovld __cnfn mad(float16 a, float16 b, float16 c);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn mad(double a, double b, double c);\n"
"double2 __ovld __cnfn mad(double2 a, double2 b, double2 c);\n"
"double3 __ovld __cnfn mad(double3 a, double3 b, double3 c);\n"
"double4 __ovld __cnfn mad(double4 a, double4 b, double4 c);\n"
"double8 __ovld __cnfn mad(double8 a, double8 b, double8 c);\n"
"double16 __ovld __cnfn mad(double16 a, double16 b, double16 c);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn mad(half a, half b, half c);\n"
"half2 __ovld __cnfn mad(half2 a, half2 b, half2 c);\n"
"half3 __ovld __cnfn mad(half3 a, half3 b, half3 c);\n"
"half4 __ovld __cnfn mad(half4 a, half4 b, half4 c);\n"
"half8 __ovld __cnfn mad(half8 a, half8 b, half8 c);\n"
"half16 __ovld __cnfn mad(half16 a, half16 b, half16 c);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns x if | x | > | y |, y if | y | > | x |, otherwise\n"
" * fmax(x, y).\n"
" */\n"
"float __ovld __cnfn maxmag(float x, float y);\n"
"float2 __ovld __cnfn maxmag(float2 x, float2 y);\n"
"float3 __ovld __cnfn maxmag(float3 x, float3 y);\n"
"float4 __ovld __cnfn maxmag(float4 x, float4 y);\n"
"float8 __ovld __cnfn maxmag(float8 x, float8 y);\n"
"float16 __ovld __cnfn maxmag(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn maxmag(double x, double y);\n"
"double2 __ovld __cnfn maxmag(double2 x, double2 y);\n"
"double3 __ovld __cnfn maxmag(double3 x, double3 y);\n"
"double4 __ovld __cnfn maxmag(double4 x, double4 y);\n"
"double8 __ovld __cnfn maxmag(double8 x, double8 y);\n"
"double16 __ovld __cnfn maxmag(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn maxmag(half x, half y);\n"
"half2 __ovld __cnfn maxmag(half2 x, half2 y);\n"
"half3 __ovld __cnfn maxmag(half3 x, half3 y);\n"
"half4 __ovld __cnfn maxmag(half4 x, half4 y);\n"
"half8 __ovld __cnfn maxmag(half8 x, half8 y);\n"
"half16 __ovld __cnfn maxmag(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns x if | x | < | y |, y if | y | < | x |, otherwise\n"
" * fmin(x, y).\n"
" */\n"
"float __ovld __cnfn minmag(float x, float y);\n"
"float2 __ovld __cnfn minmag(float2 x, float2 y);\n"
"float3 __ovld __cnfn minmag(float3 x, float3 y);\n"
"float4 __ovld __cnfn minmag(float4 x, float4 y);\n"
"float8 __ovld __cnfn minmag(float8 x, float8 y);\n"
"float16 __ovld __cnfn minmag(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn minmag(double x, double y);\n"
"double2 __ovld __cnfn minmag(double2 x, double2 y);\n"
"double3 __ovld __cnfn minmag(double3 x, double3 y);\n"
"double4 __ovld __cnfn minmag(double4 x, double4 y);\n"
"double8 __ovld __cnfn minmag(double8 x, double8 y);\n"
"double16 __ovld __cnfn minmag(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn minmag(half x, half y);\n"
"half2 __ovld __cnfn minmag(half2 x, half2 y);\n"
"half3 __ovld __cnfn minmag(half3 x, half3 y);\n"
"half4 __ovld __cnfn minmag(half4 x, half4 y);\n"
"half8 __ovld __cnfn minmag(half8 x, half8 y);\n"
"half16 __ovld __cnfn minmag(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Decompose a floating-point number. The modf\n"
" * function breaks the argument x into integral and\n"
" * fractional parts, each of which has the same sign as\n"
" * the argument. It stores the integral part in the object\n"
" * pointed to by iptr.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld modf(float x, float *iptr);\n"
"float2 __ovld modf(float2 x, float2 *iptr);\n"
"float3 __ovld modf(float3 x, float3 *iptr);\n"
"float4 __ovld modf(float4 x, float4 *iptr);\n"
"float8 __ovld modf(float8 x, float8 *iptr);\n"
"float16 __ovld modf(float16 x, float16 *iptr);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld modf(double x, double *iptr);\n"
"double2 __ovld modf(double2 x, double2 *iptr);\n"
"double3 __ovld modf(double3 x, double3 *iptr);\n"
"double4 __ovld modf(double4 x, double4 *iptr);\n"
"double8 __ovld modf(double8 x, double8 *iptr);\n"
"double16 __ovld modf(double16 x, double16 *iptr);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld modf(half x, half *iptr);\n"
"half2 __ovld modf(half2 x, half2 *iptr);\n"
"half3 __ovld modf(half3 x, half3 *iptr);\n"
"half4 __ovld modf(half4 x, half4 *iptr);\n"
"half8 __ovld modf(half8 x, half8 *iptr);\n"
"half16 __ovld modf(half16 x, half16 *iptr);\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"float __ovld modf(float x, __global float *iptr);\n"
"float2 __ovld modf(float2 x, __global float2 *iptr);\n"
"float3 __ovld modf(float3 x, __global float3 *iptr);\n"
"float4 __ovld modf(float4 x, __global float4 *iptr);\n"
"float8 __ovld modf(float8 x, __global float8 *iptr);\n"
"float16 __ovld modf(float16 x, __global float16 *iptr);\n"
"float __ovld modf(float x, __local float *iptr);\n"
"float2 __ovld modf(float2 x, __local float2 *iptr);\n"
"float3 __ovld modf(float3 x, __local float3 *iptr);\n"
"float4 __ovld modf(float4 x, __local float4 *iptr);\n"
"float8 __ovld modf(float8 x, __local float8 *iptr);\n"
"float16 __ovld modf(float16 x, __local float16 *iptr);\n"
"float __ovld modf(float x, __private float *iptr);\n"
"float2 __ovld modf(float2 x, __private float2 *iptr);\n"
"float3 __ovld modf(float3 x, __private float3 *iptr);\n"
"float4 __ovld modf(float4 x, __private float4 *iptr);\n"
"float8 __ovld modf(float8 x, __private float8 *iptr);\n"
"float16 __ovld modf(float16 x, __private float16 *iptr);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld modf(double x, __global double *iptr);\n"
"double2 __ovld modf(double2 x, __global double2 *iptr);\n"
"double3 __ovld modf(double3 x, __global double3 *iptr);\n"
"double4 __ovld modf(double4 x, __global double4 *iptr);\n"
"double8 __ovld modf(double8 x, __global double8 *iptr);\n"
"double16 __ovld modf(double16 x, __global double16 *iptr);\n"
"double __ovld modf(double x, __local double *iptr);\n"
"double2 __ovld modf(double2 x, __local double2 *iptr);\n"
"double3 __ovld modf(double3 x, __local double3 *iptr);\n"
"double4 __ovld modf(double4 x, __local double4 *iptr);\n"
"double8 __ovld modf(double8 x, __local double8 *iptr);\n"
"double16 __ovld modf(double16 x, __local double16 *iptr);\n"
"double __ovld modf(double x, __private double *iptr);\n"
"double2 __ovld modf(double2 x, __private double2 *iptr);\n"
"double3 __ovld modf(double3 x, __private double3 *iptr);\n"
"double4 __ovld modf(double4 x, __private double4 *iptr);\n"
"double8 __ovld modf(double8 x, __private double8 *iptr);\n"
"double16 __ovld modf(double16 x, __private double16 *iptr);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld modf(half x, __global half *iptr);\n"
"half2 __ovld modf(half2 x, __global half2 *iptr);\n"
"half3 __ovld modf(half3 x, __global half3 *iptr);\n"
"half4 __ovld modf(half4 x, __global half4 *iptr);\n"
"half8 __ovld modf(half8 x, __global half8 *iptr);\n"
"half16 __ovld modf(half16 x, __global half16 *iptr);\n"
"half __ovld modf(half x, __local half *iptr);\n"
"half2 __ovld modf(half2 x, __local half2 *iptr);\n"
"half3 __ovld modf(half3 x, __local half3 *iptr);\n"
"half4 __ovld modf(half4 x, __local half4 *iptr);\n"
"half8 __ovld modf(half8 x, __local half8 *iptr);\n"
"half16 __ovld modf(half16 x, __local half16 *iptr);\n"
"half __ovld modf(half x, __private half *iptr);\n"
"half2 __ovld modf(half2 x, __private half2 *iptr);\n"
"half3 __ovld modf(half3 x, __private half3 *iptr);\n"
"half4 __ovld modf(half4 x, __private half4 *iptr);\n"
"half8 __ovld modf(half8 x, __private half8 *iptr);\n"
"half16 __ovld modf(half16 x, __private half16 *iptr);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Returns a quiet NaN. The nancode may be placed\n"
" * in the significand of the resulting NaN.\n"
" */\n"
"float __ovld __cnfn nan(uint nancode);\n"
"float2 __ovld __cnfn nan(uint2 nancode);\n"
"float3 __ovld __cnfn nan(uint3 nancode);\n"
"float4 __ovld __cnfn nan(uint4 nancode);\n"
"float8 __ovld __cnfn nan(uint8 nancode);\n"
"float16 __ovld __cnfn nan(uint16 nancode);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn nan(ulong nancode);\n"
"double2 __ovld __cnfn nan(ulong2 nancode);\n"
"double3 __ovld __cnfn nan(ulong3 nancode);\n"
"double4 __ovld __cnfn nan(ulong4 nancode);\n"
"double8 __ovld __cnfn nan(ulong8 nancode);\n"
"double16 __ovld __cnfn nan(ulong16 nancode);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn nan(ushort nancode);\n"
"half2 __ovld __cnfn nan(ushort2 nancode);\n"
"half3 __ovld __cnfn nan(ushort3 nancode);\n"
"half4 __ovld __cnfn nan(ushort4 nancode);\n"
"half8 __ovld __cnfn nan(ushort8 nancode);\n"
"half16 __ovld __cnfn nan(ushort16 nancode);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Computes the next representable single-precision\n"
" * floating-point value following x in the direction of\n"
" * y. Thus, if y is less than x, nextafter() returns the\n"
" * largest representable floating-point number less\n"
" * than x.\n"
" */\n"
"float __ovld __cnfn nextafter(float x, float y);\n"
"float2 __ovld __cnfn nextafter(float2 x, float2 y);\n"
"float3 __ovld __cnfn nextafter(float3 x, float3 y);\n"
"float4 __ovld __cnfn nextafter(float4 x, float4 y);\n"
"float8 __ovld __cnfn nextafter(float8 x, float8 y);\n"
"float16 __ovld __cnfn nextafter(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn nextafter(double x, double y);\n"
"double2 __ovld __cnfn nextafter(double2 x, double2 y);\n"
"double3 __ovld __cnfn nextafter(double3 x, double3 y);\n"
"double4 __ovld __cnfn nextafter(double4 x, double4 y);\n"
"double8 __ovld __cnfn nextafter(double8 x, double8 y);\n"
"double16 __ovld __cnfn nextafter(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn nextafter(half x, half y);\n"
"half2 __ovld __cnfn nextafter(half2 x, half2 y);\n"
"half3 __ovld __cnfn nextafter(half3 x, half3 y);\n"
"half4 __ovld __cnfn nextafter(half4 x, half4 y);\n"
"half8 __ovld __cnfn nextafter(half8 x, half8 y);\n"
"half16 __ovld __cnfn nextafter(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute x to the power y.\n"
" */\n"
"float __ovld __cnfn pow(float x, float y);\n"
"float2 __ovld __cnfn pow(float2 x, float2 y);\n"
"float3 __ovld __cnfn pow(float3 x, float3 y);\n"
"float4 __ovld __cnfn pow(float4 x, float4 y);\n"
"float8 __ovld __cnfn pow(float8 x, float8 y);\n"
"float16 __ovld __cnfn pow(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn pow(double x, double y);\n"
"double2 __ovld __cnfn pow(double2 x, double2 y);\n"
"double3 __ovld __cnfn pow(double3 x, double3 y);\n"
"double4 __ovld __cnfn pow(double4 x, double4 y);\n"
"double8 __ovld __cnfn pow(double8 x, double8 y);\n"
"double16 __ovld __cnfn pow(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn pow(half x, half y);\n"
"half2 __ovld __cnfn pow(half2 x, half2 y);\n"
"half3 __ovld __cnfn pow(half3 x, half3 y);\n"
"half4 __ovld __cnfn pow(half4 x, half4 y);\n"
"half8 __ovld __cnfn pow(half8 x, half8 y);\n"
"half16 __ovld __cnfn pow(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute x to the power y, where y is an integer.\n"
" */\n"
"float __ovld __cnfn pown(float x, int y);\n"
"float2 __ovld __cnfn pown(float2 x, int2 y);\n"
"float3 __ovld __cnfn pown(float3 x, int3 y);\n"
"float4 __ovld __cnfn pown(float4 x, int4 y);\n"
"float8 __ovld __cnfn pown(float8 x, int8 y);\n"
"float16 __ovld __cnfn pown(float16 x, int16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn pown(double x, int y);\n"
"double2 __ovld __cnfn pown(double2 x, int2 y);\n"
"double3 __ovld __cnfn pown(double3 x, int3 y);\n"
"double4 __ovld __cnfn pown(double4 x, int4 y);\n"
"double8 __ovld __cnfn pown(double8 x, int8 y);\n"
"double16 __ovld __cnfn pown(double16 x, int16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn pown(half x, int y);\n"
"half2 __ovld __cnfn pown(half2 x, int2 y);\n"
"half3 __ovld __cnfn pown(half3 x, int3 y);\n"
"half4 __ovld __cnfn pown(half4 x, int4 y);\n"
"half8 __ovld __cnfn pown(half8 x, int8 y);\n"
"half16 __ovld __cnfn pown(half16 x, int16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute x to the power y, where x is >= 0.\n"
" */\n"
"float __ovld __cnfn powr(float x, float y);\n"
"float2 __ovld __cnfn powr(float2 x, float2 y);\n"
"float3 __ovld __cnfn powr(float3 x, float3 y);\n"
"float4 __ovld __cnfn powr(float4 x, float4 y);\n"
"float8 __ovld __cnfn powr(float8 x, float8 y);\n"
"float16 __ovld __cnfn powr(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn powr(double x, double y);\n"
"double2 __ovld __cnfn powr(double2 x, double2 y);\n"
"double3 __ovld __cnfn powr(double3 x, double3 y);\n"
"double4 __ovld __cnfn powr(double4 x, double4 y);\n"
"double8 __ovld __cnfn powr(double8 x, double8 y);\n"
"double16 __ovld __cnfn powr(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn powr(half x, half y);\n"
"half2 __ovld __cnfn powr(half2 x, half2 y);\n"
"half3 __ovld __cnfn powr(half3 x, half3 y);\n"
"half4 __ovld __cnfn powr(half4 x, half4 y);\n"
"half8 __ovld __cnfn powr(half8 x, half8 y);\n"
"half16 __ovld __cnfn powr(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute the value r such that r = x - n*y, where n\n"
" * is the integer nearest the exact value of x/y. If there\n"
" * are two integers closest to x/y, n shall be the even\n"
" * one. If r is zero, it is given the same sign as x.\n"
" */\n"
"float __ovld __cnfn remainder(float x, float y);\n"
"float2 __ovld __cnfn remainder(float2 x, float2 y);\n"
"float3 __ovld __cnfn remainder(float3 x, float3 y);\n"
"float4 __ovld __cnfn remainder(float4 x, float4 y);\n"
"float8 __ovld __cnfn remainder(float8 x, float8 y);\n"
"float16 __ovld __cnfn remainder(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn remainder(double x, double y);\n"
"double2 __ovld __cnfn remainder(double2 x, double2 y);\n"
"double3 __ovld __cnfn remainder(double3 x, double3 y);\n"
"double4 __ovld __cnfn remainder(double4 x, double4 y);\n"
"double8 __ovld __cnfn remainder(double8 x, double8 y);\n"
"double16 __ovld __cnfn remainder(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn remainder(half x, half y);\n"
"half2 __ovld __cnfn remainder(half2 x, half2 y);\n"
"half3 __ovld __cnfn remainder(half3 x, half3 y);\n"
"half4 __ovld __cnfn remainder(half4 x, half4 y);\n"
"half8 __ovld __cnfn remainder(half8 x, half8 y);\n"
"half16 __ovld __cnfn remainder(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * The remquo function computes the value r such\n"
" * that r = x - n*y, where n is the integer nearest the\n"
" * exact value of x/y. If there are two integers closest\n"
" * to x/y, n shall be the even one. If r is zero, it is\n"
" * given the same sign as x. This is the same value\n"
" * that is returned by the remainder function.\n"
" * remquo also calculates the lower seven bits of the\n"
" * integral quotient x/y, and gives that value the same\n"
" * sign as x/y. It stores this signed value in the object\n"
" * pointed to by quo.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld remquo(float x, float y, int *quo);\n"
"float2 __ovld remquo(float2 x, float2 y, int2 *quo);\n"
"float3 __ovld remquo(float3 x, float3 y, int3 *quo);\n"
"float4 __ovld remquo(float4 x, float4 y, int4 *quo);\n"
"float8 __ovld remquo(float8 x, float8 y, int8 *quo);\n"
"float16 __ovld remquo(float16 x, float16 y, int16 *quo);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld remquo(double x, double y, int *quo);\n"
"double2 __ovld remquo(double2 x, double2 y, int2 *quo);\n"
"double3 __ovld remquo(double3 x, double3 y, int3 *quo);\n"
"double4 __ovld remquo(double4 x, double4 y, int4 *quo);\n"
"double8 __ovld remquo(double8 x, double8 y, int8 *quo);\n"
"double16 __ovld remquo(double16 x, double16 y, int16 *quo);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld remquo(half x, half y, int *quo);\n"
"half2 __ovld remquo(half2 x, half2 y, int2 *quo);\n"
"half3 __ovld remquo(half3 x, half3 y, int3 *quo);\n"
"half4 __ovld remquo(half4 x, half4 y, int4 *quo);\n"
"half8 __ovld remquo(half8 x, half8 y, int8 *quo);\n"
"half16 __ovld remquo(half16 x, half16 y, int16 *quo);\n"
"\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"float __ovld remquo(float x, float y, __global int *quo);\n"
"float2 __ovld remquo(float2 x, float2 y, __global int2 *quo);\n"
"float3 __ovld remquo(float3 x, float3 y, __global int3 *quo);\n"
"float4 __ovld remquo(float4 x, float4 y, __global int4 *quo);\n"
"float8 __ovld remquo(float8 x, float8 y, __global int8 *quo);\n"
"float16 __ovld remquo(float16 x, float16 y, __global int16 *quo);\n"
"float __ovld remquo(float x, float y, __local int *quo);\n"
"float2 __ovld remquo(float2 x, float2 y, __local int2 *quo);\n"
"float3 __ovld remquo(float3 x, float3 y, __local int3 *quo);\n"
"float4 __ovld remquo(float4 x, float4 y, __local int4 *quo);\n"
"float8 __ovld remquo(float8 x, float8 y, __local int8 *quo);\n"
"float16 __ovld remquo(float16 x, float16 y, __local int16 *quo);\n"
"float __ovld remquo(float x, float y, __private int *quo);\n"
"float2 __ovld remquo(float2 x, float2 y, __private int2 *quo);\n"
"float3 __ovld remquo(float3 x, float3 y, __private int3 *quo);\n"
"float4 __ovld remquo(float4 x, float4 y, __private int4 *quo);\n"
"float8 __ovld remquo(float8 x, float8 y, __private int8 *quo);\n"
"float16 __ovld remquo(float16 x, float16 y, __private int16 *quo);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld remquo(double x, double y, __global int *quo);\n"
"double2 __ovld remquo(double2 x, double2 y, __global int2 *quo);\n"
"double3 __ovld remquo(double3 x, double3 y, __global int3 *quo);\n"
"double4 __ovld remquo(double4 x, double4 y, __global int4 *quo);\n"
"double8 __ovld remquo(double8 x, double8 y, __global int8 *quo);\n"
"double16 __ovld remquo(double16 x, double16 y, __global int16 *quo);\n"
"double __ovld remquo(double x, double y, __local int *quo);\n"
"double2 __ovld remquo(double2 x, double2 y, __local int2 *quo);\n"
"double3 __ovld remquo(double3 x, double3 y, __local int3 *quo);\n"
"double4 __ovld remquo(double4 x, double4 y, __local int4 *quo);\n"
"double8 __ovld remquo(double8 x, double8 y, __local int8 *quo);\n"
"double16 __ovld remquo(double16 x, double16 y, __local int16 *quo);\n"
"double __ovld remquo(double x, double y, __private int *quo);\n"
"double2 __ovld remquo(double2 x, double2 y, __private int2 *quo);\n"
"double3 __ovld remquo(double3 x, double3 y, __private int3 *quo);\n"
"double4 __ovld remquo(double4 x, double4 y, __private int4 *quo);\n"
"double8 __ovld remquo(double8 x, double8 y, __private int8 *quo);\n"
"double16 __ovld remquo(double16 x, double16 y, __private int16 *quo);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld remquo(half x, half y, __global int *quo);\n"
"half2 __ovld remquo(half2 x, half2 y, __global int2 *quo);\n"
"half3 __ovld remquo(half3 x, half3 y, __global int3 *quo);\n"
"half4 __ovld remquo(half4 x, half4 y, __global int4 *quo);\n"
"half8 __ovld remquo(half8 x, half8 y, __global int8 *quo);\n"
"half16 __ovld remquo(half16 x, half16 y, __global int16 *quo);\n"
"half __ovld remquo(half x, half y, __local int *quo);\n"
"half2 __ovld remquo(half2 x, half2 y, __local int2 *quo);\n"
"half3 __ovld remquo(half3 x, half3 y, __local int3 *quo);\n"
"half4 __ovld remquo(half4 x, half4 y, __local int4 *quo);\n"
"half8 __ovld remquo(half8 x, half8 y, __local int8 *quo);\n"
"half16 __ovld remquo(half16 x, half16 y, __local int16 *quo);\n"
"half __ovld remquo(half x, half y, __private int *quo);\n"
"half2 __ovld remquo(half2 x, half2 y, __private int2 *quo);\n"
"half3 __ovld remquo(half3 x, half3 y, __private int3 *quo);\n"
"half4 __ovld remquo(half4 x, half4 y, __private int4 *quo);\n"
"half8 __ovld remquo(half8 x, half8 y, __private int8 *quo);\n"
"half16 __ovld remquo(half16 x, half16 y, __private int16 *quo);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"/**\n"
" * Round to integral value (using round to nearest\n"
" * even rounding mode) in floating-point format.\n"
" * Refer to section 7.1 for description of rounding\n"
" * modes.\n"
" */\n"
"float __ovld __cnfn rint(float);\n"
"float2 __ovld __cnfn rint(float2);\n"
"float3 __ovld __cnfn rint(float3);\n"
"float4 __ovld __cnfn rint(float4);\n"
"float8 __ovld __cnfn rint(float8);\n"
"float16 __ovld __cnfn rint(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn rint(double);\n"
"double2 __ovld __cnfn rint(double2);\n"
"double3 __ovld __cnfn rint(double3);\n"
"double4 __ovld __cnfn rint(double4);\n"
"double8 __ovld __cnfn rint(double8);\n"
"double16 __ovld __cnfn rint(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn rint(half);\n"
"half2 __ovld __cnfn rint(half2);\n"
"half3 __ovld __cnfn rint(half3);\n"
"half4 __ovld __cnfn rint(half4);\n"
"half8 __ovld __cnfn rint(half8);\n"
"half16 __ovld __cnfn rint(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute x to the power 1/y.\n"
" */\n"
"float __ovld __cnfn rootn(float x, int y);\n"
"float2 __ovld __cnfn rootn(float2 x, int2 y);\n"
"float3 __ovld __cnfn rootn(float3 x, int3 y);\n"
"float4 __ovld __cnfn rootn(float4 x, int4 y);\n"
"float8 __ovld __cnfn rootn(float8 x, int8 y);\n"
"float16 __ovld __cnfn rootn(float16 x, int16 y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn rootn(double x, int y);\n"
"double2 __ovld __cnfn rootn(double2 x, int2 y);\n"
"double3 __ovld __cnfn rootn(double3 x, int3 y);\n"
"double4 __ovld __cnfn rootn(double4 x, int4 y);\n"
"double8 __ovld __cnfn rootn(double8 x, int8 y);\n"
"double16 __ovld __cnfn rootn(double16 x, int16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn rootn(half x, int y);\n"
"half2 __ovld __cnfn rootn(half2 x, int2 y);\n"
"half3 __ovld __cnfn rootn(half3 x, int3 y);\n"
"half4 __ovld __cnfn rootn(half4 x, int4 y);\n"
"half8 __ovld __cnfn rootn(half8 x, int8 y);\n"
"half16 __ovld __cnfn rootn(half16 x, int16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Return the integral value nearest to x rounding\n"
" * halfway cases away from zero, regardless of the\n"
" * current rounding direction.\n"
" */\n"
"float __ovld __cnfn round(float x);\n"
"float2 __ovld __cnfn round(float2 x);\n"
"float3 __ovld __cnfn round(float3 x);\n"
"float4 __ovld __cnfn round(float4 x);\n"
"float8 __ovld __cnfn round(float8 x);\n"
"float16 __ovld __cnfn round(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn round(double x);\n"
"double2 __ovld __cnfn round(double2 x);\n"
"double3 __ovld __cnfn round(double3 x);\n"
"double4 __ovld __cnfn round(double4 x);\n"
"double8 __ovld __cnfn round(double8 x);\n"
"double16 __ovld __cnfn round(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn round(half x);\n"
"half2 __ovld __cnfn round(half2 x);\n"
"half3 __ovld __cnfn round(half3 x);\n"
"half4 __ovld __cnfn round(half4 x);\n"
"half8 __ovld __cnfn round(half8 x);\n"
"half16 __ovld __cnfn round(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute inverse square root.\n"
" */\n"
"float __ovld __cnfn rsqrt(float);\n"
"float2 __ovld __cnfn rsqrt(float2);\n"
"float3 __ovld __cnfn rsqrt(float3);\n"
"float4 __ovld __cnfn rsqrt(float4);\n"
"float8 __ovld __cnfn rsqrt(float8);\n"
"float16 __ovld __cnfn rsqrt(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn rsqrt(double);\n"
"double2 __ovld __cnfn rsqrt(double2);\n"
"double3 __ovld __cnfn rsqrt(double3);\n"
"double4 __ovld __cnfn rsqrt(double4);\n"
"double8 __ovld __cnfn rsqrt(double8);\n"
"double16 __ovld __cnfn rsqrt(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn rsqrt(half);\n"
"half2 __ovld __cnfn rsqrt(half2);\n"
"half3 __ovld __cnfn rsqrt(half3);\n"
"half4 __ovld __cnfn rsqrt(half4);\n"
"half8 __ovld __cnfn rsqrt(half8);\n"
"half16 __ovld __cnfn rsqrt(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute sine.\n"
" */\n"
"float __ovld __cnfn sin(float);\n"
"float2 __ovld __cnfn sin(float2);\n"
"float3 __ovld __cnfn sin(float3);\n"
"float4 __ovld __cnfn sin(float4);\n"
"float8 __ovld __cnfn sin(float8);\n"
"float16 __ovld __cnfn sin(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn sin(double);\n"
"double2 __ovld __cnfn sin(double2);\n"
"double3 __ovld __cnfn sin(double3);\n"
"double4 __ovld __cnfn sin(double4);\n"
"double8 __ovld __cnfn sin(double8);\n"
"double16 __ovld __cnfn sin(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn sin(half);\n"
"half2 __ovld __cnfn sin(half2);\n"
"half3 __ovld __cnfn sin(half3);\n"
"half4 __ovld __cnfn sin(half4);\n"
"half8 __ovld __cnfn sin(half8);\n"
"half16 __ovld __cnfn sin(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute sine and cosine of x. The computed sine\n"
" * is the return value and computed cosine is returned\n"
" * in cosval.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld sincos(float x, float *cosval);\n"
"float2 __ovld sincos(float2 x, float2 *cosval);\n"
"float3 __ovld sincos(float3 x, float3 *cosval);\n"
"float4 __ovld sincos(float4 x, float4 *cosval);\n"
"float8 __ovld sincos(float8 x, float8 *cosval);\n"
"float16 __ovld sincos(float16 x, float16 *cosval);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld sincos(double x, double *cosval);\n"
"double2 __ovld sincos(double2 x, double2 *cosval);\n"
"double3 __ovld sincos(double3 x, double3 *cosval);\n"
"double4 __ovld sincos(double4 x, double4 *cosval);\n"
"double8 __ovld sincos(double8 x, double8 *cosval);\n"
"double16 __ovld sincos(double16 x, double16 *cosval);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld sincos(half x, half *cosval);\n"
"half2 __ovld sincos(half2 x, half2 *cosval);\n"
"half3 __ovld sincos(half3 x, half3 *cosval);\n"
"half4 __ovld sincos(half4 x, half4 *cosval);\n"
"half8 __ovld sincos(half8 x, half8 *cosval);\n"
"half16 __ovld sincos(half16 x, half16 *cosval);\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"float __ovld sincos(float x, __global float *cosval);\n"
"float2 __ovld sincos(float2 x, __global float2 *cosval);\n"
"float3 __ovld sincos(float3 x, __global float3 *cosval);\n"
"float4 __ovld sincos(float4 x, __global float4 *cosval);\n"
"float8 __ovld sincos(float8 x, __global float8 *cosval);\n"
"float16 __ovld sincos(float16 x, __global float16 *cosval);\n"
"float __ovld sincos(float x, __local float *cosval);\n"
"float2 __ovld sincos(float2 x, __local float2 *cosval);\n"
"float3 __ovld sincos(float3 x, __local float3 *cosval);\n"
"float4 __ovld sincos(float4 x, __local float4 *cosval);\n"
"float8 __ovld sincos(float8 x, __local float8 *cosval);\n"
"float16 __ovld sincos(float16 x, __local float16 *cosval);\n"
"float __ovld sincos(float x, __private float *cosval);\n"
"float2 __ovld sincos(float2 x, __private float2 *cosval);\n"
"float3 __ovld sincos(float3 x, __private float3 *cosval);\n"
"float4 __ovld sincos(float4 x, __private float4 *cosval);\n"
"float8 __ovld sincos(float8 x, __private float8 *cosval);\n"
"float16 __ovld sincos(float16 x, __private float16 *cosval);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld sincos(double x, __global double *cosval);\n"
"double2 __ovld sincos(double2 x, __global double2 *cosval);\n"
"double3 __ovld sincos(double3 x, __global double3 *cosval);\n"
"double4 __ovld sincos(double4 x, __global double4 *cosval);\n"
"double8 __ovld sincos(double8 x, __global double8 *cosval);\n"
"double16 __ovld sincos(double16 x, __global double16 *cosval);\n"
"double __ovld sincos(double x, __local double *cosval);\n"
"double2 __ovld sincos(double2 x, __local double2 *cosval);\n"
"double3 __ovld sincos(double3 x, __local double3 *cosval);\n"
"double4 __ovld sincos(double4 x, __local double4 *cosval);\n"
"double8 __ovld sincos(double8 x, __local double8 *cosval);\n"
"double16 __ovld sincos(double16 x, __local double16 *cosval);\n"
"double __ovld sincos(double x, __private double *cosval);\n"
"double2 __ovld sincos(double2 x, __private double2 *cosval);\n"
"double3 __ovld sincos(double3 x, __private double3 *cosval);\n"
"double4 __ovld sincos(double4 x, __private double4 *cosval);\n"
"double8 __ovld sincos(double8 x, __private double8 *cosval);\n"
"double16 __ovld sincos(double16 x, __private double16 *cosval);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld sincos(half x, __global half *cosval);\n"
"half2 __ovld sincos(half2 x, __global half2 *cosval);\n"
"half3 __ovld sincos(half3 x, __global half3 *cosval);\n"
"half4 __ovld sincos(half4 x, __global half4 *cosval);\n"
"half8 __ovld sincos(half8 x, __global half8 *cosval);\n"
"half16 __ovld sincos(half16 x, __global half16 *cosval);\n"
"half __ovld sincos(half x, __local half *cosval);\n"
"half2 __ovld sincos(half2 x, __local half2 *cosval);\n"
"half3 __ovld sincos(half3 x, __local half3 *cosval);\n"
"half4 __ovld sincos(half4 x, __local half4 *cosval);\n"
"half8 __ovld sincos(half8 x, __local half8 *cosval);\n"
"half16 __ovld sincos(half16 x, __local half16 *cosval);\n"
"half __ovld sincos(half x, __private half *cosval);\n"
"half2 __ovld sincos(half2 x, __private half2 *cosval);\n"
"half3 __ovld sincos(half3 x, __private half3 *cosval);\n"
"half4 __ovld sincos(half4 x, __private half4 *cosval);\n"
"half8 __ovld sincos(half8 x, __private half8 *cosval);\n"
"half16 __ovld sincos(half16 x, __private half16 *cosval);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Compute hyperbolic sine.\n"
" */\n"
"float __ovld __cnfn sinh(float);\n"
"float2 __ovld __cnfn sinh(float2);\n"
"float3 __ovld __cnfn sinh(float3);\n"
"float4 __ovld __cnfn sinh(float4);\n"
"float8 __ovld __cnfn sinh(float8);\n"
"float16 __ovld __cnfn sinh(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn sinh(double);\n"
"double2 __ovld __cnfn sinh(double2);\n"
"double3 __ovld __cnfn sinh(double3);\n"
"double4 __ovld __cnfn sinh(double4);\n"
"double8 __ovld __cnfn sinh(double8);\n"
"double16 __ovld __cnfn sinh(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn sinh(half);\n"
"half2 __ovld __cnfn sinh(half2);\n"
"half3 __ovld __cnfn sinh(half3);\n"
"half4 __ovld __cnfn sinh(half4);\n"
"half8 __ovld __cnfn sinh(half8);\n"
"half16 __ovld __cnfn sinh(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute sin (PI * x).\n"
" */\n"
"float __ovld __cnfn sinpi(float x);\n"
"float2 __ovld __cnfn sinpi(float2 x);\n"
"float3 __ovld __cnfn sinpi(float3 x);\n"
"float4 __ovld __cnfn sinpi(float4 x);\n"
"float8 __ovld __cnfn sinpi(float8 x);\n"
"float16 __ovld __cnfn sinpi(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn sinpi(double x);\n"
"double2 __ovld __cnfn sinpi(double2 x);\n"
"double3 __ovld __cnfn sinpi(double3 x);\n"
"double4 __ovld __cnfn sinpi(double4 x);\n"
"double8 __ovld __cnfn sinpi(double8 x);\n"
"double16 __ovld __cnfn sinpi(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn sinpi(half x);\n"
"half2 __ovld __cnfn sinpi(half2 x);\n"
"half3 __ovld __cnfn sinpi(half3 x);\n"
"half4 __ovld __cnfn sinpi(half4 x);\n"
"half8 __ovld __cnfn sinpi(half8 x);\n"
"half16 __ovld __cnfn sinpi(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute square root.\n"
" */\n"
"float __ovld __cnfn sqrt(float);\n"
"float2 __ovld __cnfn sqrt(float2);\n"
"float3 __ovld __cnfn sqrt(float3);\n"
"float4 __ovld __cnfn sqrt(float4);\n"
"float8 __ovld __cnfn sqrt(float8);\n"
"float16 __ovld __cnfn sqrt(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn sqrt(double);\n"
"double2 __ovld __cnfn sqrt(double2);\n"
"double3 __ovld __cnfn sqrt(double3);\n"
"double4 __ovld __cnfn sqrt(double4);\n"
"double8 __ovld __cnfn sqrt(double8);\n"
"double16 __ovld __cnfn sqrt(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn sqrt(half);\n"
"half2 __ovld __cnfn sqrt(half2);\n"
"half3 __ovld __cnfn sqrt(half3);\n"
"half4 __ovld __cnfn sqrt(half4);\n"
"half8 __ovld __cnfn sqrt(half8);\n"
"half16 __ovld __cnfn sqrt(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute tangent.\n"
" */\n"
"float __ovld __cnfn tan(float);\n"
"float2 __ovld __cnfn tan(float2);\n"
"float3 __ovld __cnfn tan(float3);\n"
"float4 __ovld __cnfn tan(float4);\n"
"float8 __ovld __cnfn tan(float8);\n"
"float16 __ovld __cnfn tan(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn tan(double);\n"
"double2 __ovld __cnfn tan(double2);\n"
"double3 __ovld __cnfn tan(double3);\n"
"double4 __ovld __cnfn tan(double4);\n"
"double8 __ovld __cnfn tan(double8);\n"
"double16 __ovld __cnfn tan(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn tan(half);\n"
"half2 __ovld __cnfn tan(half2);\n"
"half3 __ovld __cnfn tan(half3);\n"
"half4 __ovld __cnfn tan(half4);\n"
"half8 __ovld __cnfn tan(half8);\n"
"half16 __ovld __cnfn tan(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute hyperbolic tangent.\n"
" */\n"
"float __ovld __cnfn tanh(float);\n"
"float2 __ovld __cnfn tanh(float2);\n"
"float3 __ovld __cnfn tanh(float3);\n"
"float4 __ovld __cnfn tanh(float4);\n"
"float8 __ovld __cnfn tanh(float8);\n"
"float16 __ovld __cnfn tanh(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn tanh(double);\n"
"double2 __ovld __cnfn tanh(double2);\n"
"double3 __ovld __cnfn tanh(double3);\n"
"double4 __ovld __cnfn tanh(double4);\n"
"double8 __ovld __cnfn tanh(double8);\n"
"double16 __ovld __cnfn tanh(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn tanh(half);\n"
"half2 __ovld __cnfn tanh(half2);\n"
"half3 __ovld __cnfn tanh(half3);\n"
"half4 __ovld __cnfn tanh(half4);\n"
"half8 __ovld __cnfn tanh(half8);\n"
"half16 __ovld __cnfn tanh(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute tan (PI * x).\n"
" */\n"
"float __ovld __cnfn tanpi(float x);\n"
"float2 __ovld __cnfn tanpi(float2 x);\n"
"float3 __ovld __cnfn tanpi(float3 x);\n"
"float4 __ovld __cnfn tanpi(float4 x);\n"
"float8 __ovld __cnfn tanpi(float8 x);\n"
"float16 __ovld __cnfn tanpi(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn tanpi(double x);\n"
"double2 __ovld __cnfn tanpi(double2 x);\n"
"double3 __ovld __cnfn tanpi(double3 x);\n"
"double4 __ovld __cnfn tanpi(double4 x);\n"
"double8 __ovld __cnfn tanpi(double8 x);\n"
"double16 __ovld __cnfn tanpi(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn tanpi(half x);\n"
"half2 __ovld __cnfn tanpi(half2 x);\n"
"half3 __ovld __cnfn tanpi(half3 x);\n"
"half4 __ovld __cnfn tanpi(half4 x);\n"
"half8 __ovld __cnfn tanpi(half8 x);\n"
"half16 __ovld __cnfn tanpi(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute the gamma function.\n"
" */\n"
"float __ovld __cnfn tgamma(float);\n"
"float2 __ovld __cnfn tgamma(float2);\n"
"float3 __ovld __cnfn tgamma(float3);\n"
"float4 __ovld __cnfn tgamma(float4);\n"
"float8 __ovld __cnfn tgamma(float8);\n"
"float16 __ovld __cnfn tgamma(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn tgamma(double);\n"
"double2 __ovld __cnfn tgamma(double2);\n"
"double3 __ovld __cnfn tgamma(double3);\n"
"double4 __ovld __cnfn tgamma(double4);\n"
"double8 __ovld __cnfn tgamma(double8);\n"
"double16 __ovld __cnfn tgamma(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn tgamma(half);\n"
"half2 __ovld __cnfn tgamma(half2);\n"
"half3 __ovld __cnfn tgamma(half3);\n"
"half4 __ovld __cnfn tgamma(half4);\n"
"half8 __ovld __cnfn tgamma(half8);\n"
"half16 __ovld __cnfn tgamma(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Round to integral value using the round to zero\n"
" * rounding mode.\n"
" */\n"
"float __ovld __cnfn trunc(float);\n"
"float2 __ovld __cnfn trunc(float2);\n"
"float3 __ovld __cnfn trunc(float3);\n"
"float4 __ovld __cnfn trunc(float4);\n"
"float8 __ovld __cnfn trunc(float8);\n"
"float16 __ovld __cnfn trunc(float16);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn trunc(double);\n"
"double2 __ovld __cnfn trunc(double2);\n"
"double3 __ovld __cnfn trunc(double3);\n"
"double4 __ovld __cnfn trunc(double4);\n"
"double8 __ovld __cnfn trunc(double8);\n"
"double16 __ovld __cnfn trunc(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn trunc(half);\n"
"half2 __ovld __cnfn trunc(half2);\n"
"half3 __ovld __cnfn trunc(half3);\n"
"half4 __ovld __cnfn trunc(half4);\n"
"half8 __ovld __cnfn trunc(half8);\n"
"half16 __ovld __cnfn trunc(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute cosine. x must be in the range -2^16 ... +2^16.\n"
" */\n"
"float __ovld __cnfn half_cos(float x);\n"
"float2 __ovld __cnfn half_cos(float2 x);\n"
"float3 __ovld __cnfn half_cos(float3 x);\n"
"float4 __ovld __cnfn half_cos(float4 x);\n"
"float8 __ovld __cnfn half_cos(float8 x);\n"
"float16 __ovld __cnfn half_cos(float16 x);\n"
"\n"
"/**\n"
" * Compute x / y.\n"
" */\n"
"float __ovld __cnfn half_divide(float x, float y);\n"
"float2 __ovld __cnfn half_divide(float2 x, float2 y);\n"
"float3 __ovld __cnfn half_divide(float3 x, float3 y);\n"
"float4 __ovld __cnfn half_divide(float4 x, float4 y);\n"
"float8 __ovld __cnfn half_divide(float8 x, float8 y);\n"
"float16 __ovld __cnfn half_divide(float16 x, float16 y);\n"
"\n"
"/**\n"
" * Compute the base- e exponential of x.\n"
" */\n"
"float __ovld __cnfn half_exp(float x);\n"
"float2 __ovld __cnfn half_exp(float2 x);\n"
"float3 __ovld __cnfn half_exp(float3 x);\n"
"float4 __ovld __cnfn half_exp(float4 x);\n"
"float8 __ovld __cnfn half_exp(float8 x);\n"
"float16 __ovld __cnfn half_exp(float16 x);\n"
"\n"
"/**\n"
" * Compute the base- 2 exponential of x.\n"
" */\n"
"float __ovld __cnfn half_exp2(float x);\n"
"float2 __ovld __cnfn half_exp2(float2 x);\n"
"float3 __ovld __cnfn half_exp2(float3 x);\n"
"float4 __ovld __cnfn half_exp2(float4 x);\n"
"float8 __ovld __cnfn half_exp2(float8 x);\n"
"float16 __ovld __cnfn half_exp2(float16 x);\n"
"\n"
"/**\n"
" * Compute the base- 10 exponential of x.\n"
" */\n"
"float __ovld __cnfn half_exp10(float x);\n"
"float2 __ovld __cnfn half_exp10(float2 x);\n"
"float3 __ovld __cnfn half_exp10(float3 x);\n"
"float4 __ovld __cnfn half_exp10(float4 x);\n"
"float8 __ovld __cnfn half_exp10(float8 x);\n"
"float16 __ovld __cnfn half_exp10(float16 x);\n"
"\n"
"/**\n"
" * Compute natural logarithm.\n"
" */\n"
"float __ovld __cnfn half_log(float x);\n"
"float2 __ovld __cnfn half_log(float2 x);\n"
"float3 __ovld __cnfn half_log(float3 x);\n"
"float4 __ovld __cnfn half_log(float4 x);\n"
"float8 __ovld __cnfn half_log(float8 x);\n"
"float16 __ovld __cnfn half_log(float16 x);\n"
"\n"
"/**\n"
" * Compute a base 2 logarithm.\n"
" */\n"
"float __ovld __cnfn half_log2(float x);\n"
"float2 __ovld __cnfn half_log2(float2 x);\n"
"float3 __ovld __cnfn half_log2(float3 x);\n"
"float4 __ovld __cnfn half_log2(float4 x);\n"
"float8 __ovld __cnfn half_log2(float8 x);\n"
"float16 __ovld __cnfn half_log2(float16 x);\n"
"\n"
"/**\n"
" * Compute a base 10 logarithm.\n"
" */\n"
"float __ovld __cnfn half_log10(float x);\n"
"float2 __ovld __cnfn half_log10(float2 x);\n"
"float3 __ovld __cnfn half_log10(float3 x);\n"
"float4 __ovld __cnfn half_log10(float4 x);\n"
"float8 __ovld __cnfn half_log10(float8 x);\n"
"float16 __ovld __cnfn half_log10(float16 x);\n"
"\n"
"/**\n"
" * Compute x to the power y, where x is >= 0.\n"
" */\n"
"float __ovld __cnfn half_powr(float x, float y);\n"
"float2 __ovld __cnfn half_powr(float2 x, float2 y);\n"
"float3 __ovld __cnfn half_powr(float3 x, float3 y);\n"
"float4 __ovld __cnfn half_powr(float4 x, float4 y);\n"
"float8 __ovld __cnfn half_powr(float8 x, float8 y);\n"
"float16 __ovld __cnfn half_powr(float16 x, float16 y);\n"
"\n"
"/**\n"
" * Compute reciprocal.\n"
" */\n"
"float __ovld __cnfn half_recip(float x);\n"
"float2 __ovld __cnfn half_recip(float2 x);\n"
"float3 __ovld __cnfn half_recip(float3 x);\n"
"float4 __ovld __cnfn half_recip(float4 x);\n"
"float8 __ovld __cnfn half_recip(float8 x);\n"
"float16 __ovld __cnfn half_recip(float16 x);\n"
"\n"
"/**\n"
" * Compute inverse square root.\n"
" */\n"
"float __ovld __cnfn half_rsqrt(float x);\n"
"float2 __ovld __cnfn half_rsqrt(float2 x);\n"
"float3 __ovld __cnfn half_rsqrt(float3 x);\n"
"float4 __ovld __cnfn half_rsqrt(float4 x);\n"
"float8 __ovld __cnfn half_rsqrt(float8 x);\n"
"float16 __ovld __cnfn half_rsqrt(float16 x);\n"
"\n"
"/**\n"
" * Compute sine. x must be in the range -2^16 ... +2^16.\n"
" */\n"
"float __ovld __cnfn half_sin(float x);\n"
"float2 __ovld __cnfn half_sin(float2 x);\n"
"float3 __ovld __cnfn half_sin(float3 x);\n"
"float4 __ovld __cnfn half_sin(float4 x);\n"
"float8 __ovld __cnfn half_sin(float8 x);\n"
"float16 __ovld __cnfn half_sin(float16 x);\n"
"\n"
"/**\n"
" * Compute square root.\n"
" */\n"
"float __ovld __cnfn half_sqrt(float x);\n"
"float2 __ovld __cnfn half_sqrt(float2 x);\n"
"float3 __ovld __cnfn half_sqrt(float3 x);\n"
"float4 __ovld __cnfn half_sqrt(float4 x);\n"
"float8 __ovld __cnfn half_sqrt(float8 x);\n"
"float16 __ovld __cnfn half_sqrt(float16 x);\n"
"\n"
"/**\n"
" * Compute tangent. x must be in the range -216 ... +216.\n"
" */\n"
"float __ovld __cnfn half_tan(float x);\n"
"float2 __ovld __cnfn half_tan(float2 x);\n"
"float3 __ovld __cnfn half_tan(float3 x);\n"
"float4 __ovld __cnfn half_tan(float4 x);\n"
"float8 __ovld __cnfn half_tan(float8 x);\n"
"float16 __ovld __cnfn half_tan(float16 x);\n"
"\n"
"/**\n"
" * Compute cosine over an implementation-defined range.\n"
" * The maximum error is implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_cos(float x);\n"
"float2 __ovld __cnfn native_cos(float2 x);\n"
"float3 __ovld __cnfn native_cos(float3 x);\n"
"float4 __ovld __cnfn native_cos(float4 x);\n"
"float8 __ovld __cnfn native_cos(float8 x);\n"
"float16 __ovld __cnfn native_cos(float16 x);\n"
"\n"
"/**\n"
" * Compute x / y over an implementation-defined range.\n"
" * The maximum error is implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_divide(float x, float y);\n"
"float2 __ovld __cnfn native_divide(float2 x, float2 y);\n"
"float3 __ovld __cnfn native_divide(float3 x, float3 y);\n"
"float4 __ovld __cnfn native_divide(float4 x, float4 y);\n"
"float8 __ovld __cnfn native_divide(float8 x, float8 y);\n"
"float16 __ovld __cnfn native_divide(float16 x, float16 y);\n"
"\n"
"/**\n"
" * Compute the base- e exponential of x over an\n"
" * implementation-defined range. The maximum error is\n"
" * implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_exp(float x);\n"
"float2 __ovld __cnfn native_exp(float2 x);\n"
"float3 __ovld __cnfn native_exp(float3 x);\n"
"float4 __ovld __cnfn native_exp(float4 x);\n"
"float8 __ovld __cnfn native_exp(float8 x);\n"
"float16 __ovld __cnfn native_exp(float16 x);\n"
"\n"
"/**\n"
" * Compute the base- 2 exponential of x over an\n"
" * implementation-defined range. The maximum error is\n"
" * implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_exp2(float x);\n"
"float2 __ovld __cnfn native_exp2(float2 x);\n"
"float3 __ovld __cnfn native_exp2(float3 x);\n"
"float4 __ovld __cnfn native_exp2(float4 x);\n"
"float8 __ovld __cnfn native_exp2(float8 x);\n"
"float16 __ovld __cnfn native_exp2(float16 x);\n"
"\n"
"/**\n"
" * Compute the base- 10 exponential of x over an\n"
" * implementation-defined range. The maximum error is\n"
" * implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_exp10(float x);\n"
"float2 __ovld __cnfn native_exp10(float2 x);\n"
"float3 __ovld __cnfn native_exp10(float3 x);\n"
"float4 __ovld __cnfn native_exp10(float4 x);\n"
"float8 __ovld __cnfn native_exp10(float8 x);\n"
"float16 __ovld __cnfn native_exp10(float16 x);\n"
"\n"
"/**\n"
" * Compute natural logarithm over an implementationdefined\n"
" * range. The maximum error is implementation\n"
" * defined.\n"
" */\n"
"float __ovld __cnfn native_log(float x);\n"
"float2 __ovld __cnfn native_log(float2 x);\n"
"float3 __ovld __cnfn native_log(float3 x);\n"
"float4 __ovld __cnfn native_log(float4 x);\n"
"float8 __ovld __cnfn native_log(float8 x);\n"
"float16 __ovld __cnfn native_log(float16 x);\n"
"\n"
"/**\n"
" * Compute a base 2 logarithm over an implementationdefined\n"
" * range. The maximum error is implementationdefined.\n"
" */\n"
"float __ovld __cnfn native_log2(float x);\n"
"float2 __ovld __cnfn native_log2(float2 x);\n"
"float3 __ovld __cnfn native_log2(float3 x);\n"
"float4 __ovld __cnfn native_log2(float4 x);\n"
"float8 __ovld __cnfn native_log2(float8 x);\n"
"float16 __ovld __cnfn native_log2(float16 x);\n"
"\n"
"/**\n"
" * Compute a base 10 logarithm over an implementationdefined\n"
" * range. The maximum error is implementationdefined.\n"
" */\n"
"float __ovld __cnfn native_log10(float x);\n"
"float2 __ovld __cnfn native_log10(float2 x);\n"
"float3 __ovld __cnfn native_log10(float3 x);\n"
"float4 __ovld __cnfn native_log10(float4 x);\n"
"float8 __ovld __cnfn native_log10(float8 x);\n"
"float16 __ovld __cnfn native_log10(float16 x);\n"
"\n"
"/**\n"
" * Compute x to the power y, where x is >= 0. The range of\n"
" * x and y are implementation-defined. The maximum error\n"
" * is implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_powr(float x, float y);\n"
"float2 __ovld __cnfn native_powr(float2 x, float2 y);\n"
"float3 __ovld __cnfn native_powr(float3 x, float3 y);\n"
"float4 __ovld __cnfn native_powr(float4 x, float4 y);\n"
"float8 __ovld __cnfn native_powr(float8 x, float8 y);\n"
"float16 __ovld __cnfn native_powr(float16 x, float16 y);\n"
"\n"
"/**\n"
" * Compute reciprocal over an implementation-defined\n"
" * range. The maximum error is implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_recip(float x);\n"
"float2 __ovld __cnfn native_recip(float2 x);\n"
"float3 __ovld __cnfn native_recip(float3 x);\n"
"float4 __ovld __cnfn native_recip(float4 x);\n"
"float8 __ovld __cnfn native_recip(float8 x);\n"
"float16 __ovld __cnfn native_recip(float16 x);\n"
"\n"
"/**\n"
" * Compute inverse square root over an implementationdefined\n"
" * range. The maximum error is implementationdefined.\n"
" */\n"
"float __ovld __cnfn native_rsqrt(float x);\n"
"float2 __ovld __cnfn native_rsqrt(float2 x);\n"
"float3 __ovld __cnfn native_rsqrt(float3 x);\n"
"float4 __ovld __cnfn native_rsqrt(float4 x);\n"
"float8 __ovld __cnfn native_rsqrt(float8 x);\n"
"float16 __ovld __cnfn native_rsqrt(float16 x);\n"
"\n"
"/**\n"
" * Compute sine over an implementation-defined range.\n"
" * The maximum error is implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_sin(float x);\n"
"float2 __ovld __cnfn native_sin(float2 x);\n"
"float3 __ovld __cnfn native_sin(float3 x);\n"
"float4 __ovld __cnfn native_sin(float4 x);\n"
"float8 __ovld __cnfn native_sin(float8 x);\n"
"float16 __ovld __cnfn native_sin(float16 x);\n"
"\n"
"/**\n"
" * Compute square root over an implementation-defined\n"
" * range. The maximum error is implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_sqrt(float x);\n"
"float2 __ovld __cnfn native_sqrt(float2 x);\n"
"float3 __ovld __cnfn native_sqrt(float3 x);\n"
"float4 __ovld __cnfn native_sqrt(float4 x);\n"
"float8 __ovld __cnfn native_sqrt(float8 x);\n"
"float16 __ovld __cnfn native_sqrt(float16 x);\n"
"\n"
"/**\n"
" * Compute tangent over an implementation-defined range.\n"
" * The maximum error is implementation-defined.\n"
" */\n"
"float __ovld __cnfn native_tan(float x);\n"
"float2 __ovld __cnfn native_tan(float2 x);\n"
"float3 __ovld __cnfn native_tan(float3 x);\n"
"float4 __ovld __cnfn native_tan(float4 x);\n"
"float8 __ovld __cnfn native_tan(float8 x);\n"
"float16 __ovld __cnfn native_tan(float16 x);\n"
"\n"
"// OpenCL v1.1 s6.11.3, v1.2 s6.12.3, v2.0 s6.13.3 - Integer Functions\n"
"\n"
"/**\n"
" * Returns | x |.\n"
" */\n"
"uchar __ovld __cnfn abs(char x);\n"
"uchar __ovld __cnfn abs(uchar x);\n"
"uchar2 __ovld __cnfn abs(char2 x);\n"
"uchar2 __ovld __cnfn abs(uchar2 x);\n"
"uchar3 __ovld __cnfn abs(char3 x);\n"
"uchar3 __ovld __cnfn abs(uchar3 x);\n"
"uchar4 __ovld __cnfn abs(char4 x);\n"
"uchar4 __ovld __cnfn abs(uchar4 x);\n"
"uchar8 __ovld __cnfn abs(char8 x);\n"
"uchar8 __ovld __cnfn abs(uchar8 x);\n"
"uchar16 __ovld __cnfn abs(char16 x);\n"
"uchar16 __ovld __cnfn abs(uchar16 x);\n"
"ushort __ovld __cnfn abs(short x);\n"
"ushort __ovld __cnfn abs(ushort x);\n"
"ushort2 __ovld __cnfn abs(short2 x);\n"
"ushort2 __ovld __cnfn abs(ushort2 x);\n"
"ushort3 __ovld __cnfn abs(short3 x);\n"
"ushort3 __ovld __cnfn abs(ushort3 x);\n"
"ushort4 __ovld __cnfn abs(short4 x);\n"
"ushort4 __ovld __cnfn abs(ushort4 x);\n"
"ushort8 __ovld __cnfn abs(short8 x);\n"
"ushort8 __ovld __cnfn abs(ushort8 x);\n"
"ushort16 __ovld __cnfn abs(short16 x);\n"
"ushort16 __ovld __cnfn abs(ushort16 x);\n"
"uint __ovld __cnfn abs(int x);\n"
"uint __ovld __cnfn abs(uint x);\n"
"uint2 __ovld __cnfn abs(int2 x);\n"
"uint2 __ovld __cnfn abs(uint2 x);\n"
"uint3 __ovld __cnfn abs(int3 x);\n"
"uint3 __ovld __cnfn abs(uint3 x);\n"
"uint4 __ovld __cnfn abs(int4 x);\n"
"uint4 __ovld __cnfn abs(uint4 x);\n"
"uint8 __ovld __cnfn abs(int8 x);\n"
"uint8 __ovld __cnfn abs(uint8 x);\n"
"uint16 __ovld __cnfn abs(int16 x);\n"
"uint16 __ovld __cnfn abs(uint16 x);\n"
"ulong __ovld __cnfn abs(long x);\n"
"ulong __ovld __cnfn abs(ulong x);\n"
"ulong2 __ovld __cnfn abs(long2 x);\n"
"ulong2 __ovld __cnfn abs(ulong2 x);\n"
"ulong3 __ovld __cnfn abs(long3 x);\n"
"ulong3 __ovld __cnfn abs(ulong3 x);\n"
"ulong4 __ovld __cnfn abs(long4 x);\n"
"ulong4 __ovld __cnfn abs(ulong4 x);\n"
"ulong8 __ovld __cnfn abs(long8 x);\n"
"ulong8 __ovld __cnfn abs(ulong8 x);\n"
"ulong16 __ovld __cnfn abs(long16 x);\n"
"ulong16 __ovld __cnfn abs(ulong16 x);\n"
"\n"
"/**\n"
" * Returns | x - y | without modulo overflow.\n"
" */\n"
"uchar __ovld __cnfn abs_diff(char x, char y);\n"
"uchar __ovld __cnfn abs_diff(uchar x, uchar y);\n"
"uchar2 __ovld __cnfn abs_diff(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn abs_diff(uchar2 x, uchar2 y);\n"
"uchar3 __ovld __cnfn abs_diff(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn abs_diff(uchar3 x, uchar3 y);\n"
"uchar4 __ovld __cnfn abs_diff(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn abs_diff(uchar4 x, uchar4 y);\n"
"uchar8 __ovld __cnfn abs_diff(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn abs_diff(uchar8 x, uchar8 y);\n"
"uchar16 __ovld __cnfn abs_diff(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn abs_diff(uchar16 x, uchar16 y);\n"
"ushort __ovld __cnfn abs_diff(short x, short y);\n"
"ushort __ovld __cnfn abs_diff(ushort x, ushort y);\n"
"ushort2 __ovld __cnfn abs_diff(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn abs_diff(ushort2 x, ushort2 y);\n"
"ushort3 __ovld __cnfn abs_diff(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn abs_diff(ushort3 x, ushort3 y);\n"
"ushort4 __ovld __cnfn abs_diff(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn abs_diff(ushort4 x, ushort4 y);\n"
"ushort8 __ovld __cnfn abs_diff(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn abs_diff(ushort8 x, ushort8 y);\n"
"ushort16 __ovld __cnfn abs_diff(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn abs_diff(ushort16 x, ushort16 y);\n"
"uint __ovld __cnfn abs_diff(int x, int y);\n"
"uint __ovld __cnfn abs_diff(uint x, uint y);\n"
"uint2 __ovld __cnfn abs_diff(int2 x, int2 y);\n"
"uint2 __ovld __cnfn abs_diff(uint2 x, uint2 y);\n"
"uint3 __ovld __cnfn abs_diff(int3 x, int3 y);\n"
"uint3 __ovld __cnfn abs_diff(uint3 x, uint3 y);\n"
"uint4 __ovld __cnfn abs_diff(int4 x, int4 y);\n"
"uint4 __ovld __cnfn abs_diff(uint4 x, uint4 y);\n"
"uint8 __ovld __cnfn abs_diff(int8 x, int8 y);\n"
"uint8 __ovld __cnfn abs_diff(uint8 x, uint8 y);\n"
"uint16 __ovld __cnfn abs_diff(int16 x, int16 y);\n"
"uint16 __ovld __cnfn abs_diff(uint16 x, uint16 y);\n"
"ulong __ovld __cnfn abs_diff(long x, long y);\n"
"ulong __ovld __cnfn abs_diff(ulong x, ulong y);\n"
"ulong2 __ovld __cnfn abs_diff(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn abs_diff(ulong2 x, ulong2 y);\n"
"ulong3 __ovld __cnfn abs_diff(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn abs_diff(ulong3 x, ulong3 y);\n"
"ulong4 __ovld __cnfn abs_diff(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn abs_diff(ulong4 x, ulong4 y);\n"
"ulong8 __ovld __cnfn abs_diff(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn abs_diff(ulong8 x, ulong8 y);\n"
"ulong16 __ovld __cnfn abs_diff(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn abs_diff(ulong16 x, ulong16 y);\n"
"\n"
"/**\n"
" * Returns x + y and saturates the result.\n"
" */\n"
"char __ovld __cnfn add_sat(char x, char y);\n"
"uchar __ovld __cnfn add_sat(uchar x, uchar y);\n"
"char2 __ovld __cnfn add_sat(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn add_sat(uchar2 x, uchar2 y);\n"
"char3 __ovld __cnfn add_sat(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn add_sat(uchar3 x, uchar3 y);\n"
"char4 __ovld __cnfn add_sat(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn add_sat(uchar4 x, uchar4 y);\n"
"char8 __ovld __cnfn add_sat(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn add_sat(uchar8 x, uchar8 y);\n"
"char16 __ovld __cnfn add_sat(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn add_sat(uchar16 x, uchar16 y);\n"
"short __ovld __cnfn add_sat(short x, short y);\n"
"ushort __ovld __cnfn add_sat(ushort x, ushort y);\n"
"short2 __ovld __cnfn add_sat(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn add_sat(ushort2 x, ushort2 y);\n"
"short3 __ovld __cnfn add_sat(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn add_sat(ushort3 x, ushort3 y);\n"
"short4 __ovld __cnfn add_sat(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn add_sat(ushort4 x, ushort4 y);\n"
"short8 __ovld __cnfn add_sat(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn add_sat(ushort8 x, ushort8 y);\n"
"short16 __ovld __cnfn add_sat(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn add_sat(ushort16 x, ushort16 y);\n"
"int __ovld __cnfn add_sat(int x, int y);\n"
"uint __ovld __cnfn add_sat(uint x, uint y);\n"
"int2 __ovld __cnfn add_sat(int2 x, int2 y);\n"
"uint2 __ovld __cnfn add_sat(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn add_sat(int3 x, int3 y);\n"
"uint3 __ovld __cnfn add_sat(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn add_sat(int4 x, int4 y);\n"
"uint4 __ovld __cnfn add_sat(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn add_sat(int8 x, int8 y);\n"
"uint8 __ovld __cnfn add_sat(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn add_sat(int16 x, int16 y);\n"
"uint16 __ovld __cnfn add_sat(uint16 x, uint16 y);\n"
"long __ovld __cnfn add_sat(long x, long y);\n"
"ulong __ovld __cnfn add_sat(ulong x, ulong y);\n"
"long2 __ovld __cnfn add_sat(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn add_sat(ulong2 x, ulong2 y);\n"
"long3 __ovld __cnfn add_sat(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn add_sat(ulong3 x, ulong3 y);\n"
"long4 __ovld __cnfn add_sat(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn add_sat(ulong4 x, ulong4 y);\n"
"long8 __ovld __cnfn add_sat(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn add_sat(ulong8 x, ulong8 y);\n"
"long16 __ovld __cnfn add_sat(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn add_sat(ulong16 x, ulong16 y);\n"
"\n"
"/**\n"
" * Returns (x + y) >> 1. The intermediate sum does\n"
" * not modulo overflow.\n"
" */\n"
"char __ovld __cnfn hadd(char x, char y);\n"
"uchar __ovld __cnfn hadd(uchar x, uchar y);\n"
"char2 __ovld __cnfn hadd(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn hadd(uchar2 x, uchar2 y);\n"
"char3 __ovld __cnfn hadd(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn hadd(uchar3 x, uchar3 y);\n"
"char4 __ovld __cnfn hadd(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn hadd(uchar4 x, uchar4 y);\n"
"char8 __ovld __cnfn hadd(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn hadd(uchar8 x, uchar8 y);\n"
"char16 __ovld __cnfn hadd(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn hadd(uchar16 x, uchar16 y);\n"
"short __ovld __cnfn hadd(short x, short y);\n"
"ushort __ovld __cnfn hadd(ushort x, ushort y);\n"
"short2 __ovld __cnfn hadd(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn hadd(ushort2 x, ushort2 y);\n"
"short3 __ovld __cnfn hadd(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn hadd(ushort3 x, ushort3 y);\n"
"short4 __ovld __cnfn hadd(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn hadd(ushort4 x, ushort4 y);\n"
"short8 __ovld __cnfn hadd(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn hadd(ushort8 x, ushort8 y);\n"
"short16 __ovld __cnfn hadd(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn hadd(ushort16 x, ushort16 y);\n"
"int __ovld __cnfn hadd(int x, int y);\n"
"uint __ovld __cnfn hadd(uint x, uint y);\n"
"int2 __ovld __cnfn hadd(int2 x, int2 y);\n"
"uint2 __ovld __cnfn hadd(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn hadd(int3 x, int3 y);\n"
"uint3 __ovld __cnfn hadd(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn hadd(int4 x, int4 y);\n"
"uint4 __ovld __cnfn hadd(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn hadd(int8 x, int8 y);\n"
"uint8 __ovld __cnfn hadd(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn hadd(int16 x, int16 y);\n"
"uint16 __ovld __cnfn hadd(uint16 x, uint16 y);\n"
"long __ovld __cnfn hadd(long x, long y);\n"
"ulong __ovld __cnfn hadd(ulong x, ulong y);\n"
"long2 __ovld __cnfn hadd(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn hadd(ulong2 x, ulong2 y);\n"
"long3 __ovld __cnfn hadd(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn hadd(ulong3 x, ulong3 y);\n"
"long4 __ovld __cnfn hadd(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn hadd(ulong4 x, ulong4 y);\n"
"long8 __ovld __cnfn hadd(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn hadd(ulong8 x, ulong8 y);\n"
"long16 __ovld __cnfn hadd(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn hadd(ulong16 x, ulong16 y);\n"
"\n"
"/**\n"
" * Returns (x + y + 1) >> 1. The intermediate sum\n"
" * does not modulo overflow.\n"
" */\n"
"char __ovld __cnfn rhadd(char x, char y);\n"
"uchar __ovld __cnfn rhadd(uchar x, uchar y);\n"
"char2 __ovld __cnfn rhadd(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn rhadd(uchar2 x, uchar2 y);\n"
"char3 __ovld __cnfn rhadd(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn rhadd(uchar3 x, uchar3 y);\n"
"char4 __ovld __cnfn rhadd(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn rhadd(uchar4 x, uchar4 y);\n"
"char8 __ovld __cnfn rhadd(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn rhadd(uchar8 x, uchar8 y);\n"
"char16 __ovld __cnfn rhadd(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn rhadd(uchar16 x, uchar16 y);\n"
"short __ovld __cnfn rhadd(short x, short y);\n"
"ushort __ovld __cnfn rhadd(ushort x, ushort y);\n"
"short2 __ovld __cnfn rhadd(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn rhadd(ushort2 x, ushort2 y);\n"
"short3 __ovld __cnfn rhadd(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn rhadd(ushort3 x, ushort3 y);\n"
"short4 __ovld __cnfn rhadd(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn rhadd(ushort4 x, ushort4 y);\n"
"short8 __ovld __cnfn rhadd(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn rhadd(ushort8 x, ushort8 y);\n"
"short16 __ovld __cnfn rhadd(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn rhadd(ushort16 x, ushort16 y);\n"
"int __ovld __cnfn rhadd(int x, int y);\n"
"uint __ovld __cnfn rhadd(uint x, uint y);\n"
"int2 __ovld __cnfn rhadd(int2 x, int2 y);\n"
"uint2 __ovld __cnfn rhadd(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn rhadd(int3 x, int3 y);\n"
"uint3 __ovld __cnfn rhadd(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn rhadd(int4 x, int4 y);\n"
"uint4 __ovld __cnfn rhadd(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn rhadd(int8 x, int8 y);\n"
"uint8 __ovld __cnfn rhadd(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn rhadd(int16 x, int16 y);\n"
"uint16 __ovld __cnfn rhadd(uint16 x, uint16 y);\n"
"long __ovld __cnfn rhadd(long x, long y);\n"
"ulong __ovld __cnfn rhadd(ulong x, ulong y);\n"
"long2 __ovld __cnfn rhadd(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn rhadd(ulong2 x, ulong2 y);\n"
"long3 __ovld __cnfn rhadd(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn rhadd(ulong3 x, ulong3 y);\n"
"long4 __ovld __cnfn rhadd(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn rhadd(ulong4 x, ulong4 y);\n"
"long8 __ovld __cnfn rhadd(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn rhadd(ulong8 x, ulong8 y);\n"
"long16 __ovld __cnfn rhadd(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn rhadd(ulong16 x, ulong16 y);\n"
"\n"
"/**\n"
" * Returns min(max(x, minval), maxval).\n"
" * Results are undefined if minval > maxval.\n"
" */\n"
"char __ovld __cnfn clamp(char x, char minval, char maxval);\n"
"uchar __ovld __cnfn clamp(uchar x, uchar minval, uchar maxval);\n"
"char2 __ovld __cnfn clamp(char2 x, char2 minval, char2 maxval);\n"
"uchar2 __ovld __cnfn clamp(uchar2 x, uchar2 minval, uchar2 maxval);\n"
"char3 __ovld __cnfn clamp(char3 x, char3 minval, char3 maxval);\n"
"uchar3 __ovld __cnfn clamp(uchar3 x, uchar3 minval, uchar3 maxval);\n"
"char4 __ovld __cnfn clamp(char4 x, char4 minval, char4 maxval);\n"
"uchar4 __ovld __cnfn clamp(uchar4 x, uchar4 minval, uchar4 maxval);\n"
"char8 __ovld __cnfn clamp(char8 x, char8 minval, char8 maxval);\n"
"uchar8 __ovld __cnfn clamp(uchar8 x, uchar8 minval, uchar8 maxval);\n"
"char16 __ovld __cnfn clamp(char16 x, char16 minval, char16 maxval);\n"
"uchar16 __ovld __cnfn clamp(uchar16 x, uchar16 minval, uchar16 maxval);\n"
"short __ovld __cnfn clamp(short x, short minval, short maxval);\n"
"ushort __ovld __cnfn clamp(ushort x, ushort minval, ushort maxval);\n"
"short2 __ovld __cnfn clamp(short2 x, short2 minval, short2 maxval);\n"
"ushort2 __ovld __cnfn clamp(ushort2 x, ushort2 minval, ushort2 maxval);\n"
"short3 __ovld __cnfn clamp(short3 x, short3 minval, short3 maxval);\n"
"ushort3 __ovld __cnfn clamp(ushort3 x, ushort3 minval, ushort3 maxval);\n"
"short4 __ovld __cnfn clamp(short4 x, short4 minval, short4 maxval);\n"
"ushort4 __ovld __cnfn clamp(ushort4 x, ushort4 minval, ushort4 maxval);\n"
"short8 __ovld __cnfn clamp(short8 x, short8 minval, short8 maxval);\n"
"ushort8 __ovld __cnfn clamp(ushort8 x, ushort8 minval, ushort8 maxval);\n"
"short16 __ovld __cnfn clamp(short16 x, short16 minval, short16 maxval);\n"
"ushort16 __ovld __cnfn clamp(ushort16 x, ushort16 minval, ushort16 maxval);\n"
"int __ovld __cnfn clamp(int x, int minval, int maxval);\n"
"uint __ovld __cnfn clamp(uint x, uint minval, uint maxval);\n"
"int2 __ovld __cnfn clamp(int2 x, int2 minval, int2 maxval);\n"
"uint2 __ovld __cnfn clamp(uint2 x, uint2 minval, uint2 maxval);\n"
"int3 __ovld __cnfn clamp(int3 x, int3 minval, int3 maxval);\n"
"uint3 __ovld __cnfn clamp(uint3 x, uint3 minval, uint3 maxval);\n"
"int4 __ovld __cnfn clamp(int4 x, int4 minval, int4 maxval);\n"
"uint4 __ovld __cnfn clamp(uint4 x, uint4 minval, uint4 maxval);\n"
"int8 __ovld __cnfn clamp(int8 x, int8 minval, int8 maxval);\n"
"uint8 __ovld __cnfn clamp(uint8 x, uint8 minval, uint8 maxval);\n"
"int16 __ovld __cnfn clamp(int16 x, int16 minval, int16 maxval);\n"
"uint16 __ovld __cnfn clamp(uint16 x, uint16 minval, uint16 maxval);\n"
"long __ovld __cnfn clamp(long x, long minval, long maxval);\n"
"ulong __ovld __cnfn clamp(ulong x, ulong minval, ulong maxval);\n"
"long2 __ovld __cnfn clamp(long2 x, long2 minval, long2 maxval);\n"
"ulong2 __ovld __cnfn clamp(ulong2 x, ulong2 minval, ulong2 maxval);\n"
"long3 __ovld __cnfn clamp(long3 x, long3 minval, long3 maxval);\n"
"ulong3 __ovld __cnfn clamp(ulong3 x, ulong3 minval, ulong3 maxval);\n"
"long4 __ovld __cnfn clamp(long4 x, long4 minval, long4 maxval);\n"
"ulong4 __ovld __cnfn clamp(ulong4 x, ulong4 minval, ulong4 maxval);\n"
"long8 __ovld __cnfn clamp(long8 x, long8 minval, long8 maxval);\n"
"ulong8 __ovld __cnfn clamp(ulong8 x, ulong8 minval, ulong8 maxval);\n"
"long16 __ovld __cnfn clamp(long16 x, long16 minval, long16 maxval);\n"
"ulong16 __ovld __cnfn clamp(ulong16 x, ulong16 minval, ulong16 maxval);\n"
"char __ovld __cnfn clamp(char x, char minval, char maxval);\n"
"uchar __ovld __cnfn clamp(uchar x, uchar minval, uchar maxval);\n"
"char2 __ovld __cnfn clamp(char2 x, char minval, char maxval);\n"
"uchar2 __ovld __cnfn clamp(uchar2 x, uchar minval, uchar maxval);\n"
"char3 __ovld __cnfn clamp(char3 x, char minval, char maxval);\n"
"uchar3 __ovld __cnfn clamp(uchar3 x, uchar minval, uchar maxval);\n"
"char4 __ovld __cnfn clamp(char4 x, char minval, char maxval);\n"
"uchar4 __ovld __cnfn clamp(uchar4 x, uchar minval, uchar maxval);\n"
"char8 __ovld __cnfn clamp(char8 x, char minval, char maxval);\n"
"uchar8 __ovld __cnfn clamp(uchar8 x, uchar minval, uchar maxval);\n"
"char16 __ovld __cnfn clamp(char16 x, char minval, char maxval);\n"
"uchar16 __ovld __cnfn clamp(uchar16 x, uchar minval, uchar maxval);\n"
"short __ovld __cnfn clamp(short x, short minval, short maxval);\n"
"ushort __ovld __cnfn clamp(ushort x, ushort minval, ushort maxval);\n"
"short2 __ovld __cnfn clamp(short2 x, short minval, short maxval);\n"
"ushort2 __ovld __cnfn clamp(ushort2 x, ushort minval, ushort maxval);\n"
"short3 __ovld __cnfn clamp(short3 x, short minval, short maxval);\n"
"ushort3 __ovld __cnfn clamp(ushort3 x, ushort minval, ushort maxval);\n"
"short4 __ovld __cnfn clamp(short4 x, short minval, short maxval);\n"
"ushort4 __ovld __cnfn clamp(ushort4 x, ushort minval, ushort maxval);\n"
"short8 __ovld __cnfn clamp(short8 x, short minval, short maxval);\n"
"ushort8 __ovld __cnfn clamp(ushort8 x, ushort minval, ushort maxval);\n"
"short16 __ovld __cnfn clamp(short16 x, short minval, short maxval);\n"
"ushort16 __ovld __cnfn clamp(ushort16 x, ushort minval, ushort maxval);\n"
"int __ovld __cnfn clamp(int x, int minval, int maxval);\n"
"uint __ovld __cnfn clamp(uint x, uint minval, uint maxval);\n"
"int2 __ovld __cnfn clamp(int2 x, int minval, int maxval);\n"
"uint2 __ovld __cnfn clamp(uint2 x, uint minval, uint maxval);\n"
"int3 __ovld __cnfn clamp(int3 x, int minval, int maxval);\n"
"uint3 __ovld __cnfn clamp(uint3 x, uint minval, uint maxval);\n"
"int4 __ovld __cnfn clamp(int4 x, int minval, int maxval);\n"
"uint4 __ovld __cnfn clamp(uint4 x, uint minval, uint maxval);\n"
"int8 __ovld __cnfn clamp(int8 x, int minval, int maxval);\n"
"uint8 __ovld __cnfn clamp(uint8 x, uint minval, uint maxval);\n"
"int16 __ovld __cnfn clamp(int16 x, int minval, int maxval);\n"
"uint16 __ovld __cnfn clamp(uint16 x, uint minval, uint maxval);\n"
"long __ovld __cnfn clamp(long x, long minval, long maxval);\n"
"ulong __ovld __cnfn clamp(ulong x, ulong minval, ulong maxval);\n"
"long2 __ovld __cnfn clamp(long2 x, long minval, long maxval);\n"
"ulong2 __ovld __cnfn clamp(ulong2 x, ulong minval, ulong maxval);\n"
"long3 __ovld __cnfn clamp(long3 x, long minval, long maxval);\n"
"ulong3 __ovld __cnfn clamp(ulong3 x, ulong minval, ulong maxval);\n"
"long4 __ovld __cnfn clamp(long4 x, long minval, long maxval);\n"
"ulong4 __ovld __cnfn clamp(ulong4 x, ulong minval, ulong maxval);\n"
"long8 __ovld __cnfn clamp(long8 x, long minval, long maxval);\n"
"ulong8 __ovld __cnfn clamp(ulong8 x, ulong minval, ulong maxval);\n"
"long16 __ovld __cnfn clamp(long16 x, long minval, long maxval);\n"
"ulong16 __ovld __cnfn clamp(ulong16 x, ulong minval, ulong maxval);\n"
"\n"
"/**\n"
" * Returns the number of leading 0-bits in x, starting\n"
" * at the most significant bit position.\n"
" */\n"
"char __ovld __cnfn clz(char x);\n"
"uchar __ovld __cnfn clz(uchar x);\n"
"char2 __ovld __cnfn clz(char2 x);\n"
"uchar2 __ovld __cnfn clz(uchar2 x);\n"
"char3 __ovld __cnfn clz(char3 x);\n"
"uchar3 __ovld __cnfn clz(uchar3 x);\n"
"char4 __ovld __cnfn clz(char4 x);\n"
"uchar4 __ovld __cnfn clz(uchar4 x);\n"
"char8 __ovld __cnfn clz(char8 x);\n"
"uchar8 __ovld __cnfn clz(uchar8 x);\n"
"char16 __ovld __cnfn clz(char16 x);\n"
"uchar16 __ovld __cnfn clz(uchar16 x);\n"
"short __ovld __cnfn clz(short x);\n"
"ushort __ovld __cnfn clz(ushort x);\n"
"short2 __ovld __cnfn clz(short2 x);\n"
"ushort2 __ovld __cnfn clz(ushort2 x);\n"
"short3 __ovld __cnfn clz(short3 x);\n"
"ushort3 __ovld __cnfn clz(ushort3 x);\n"
"short4 __ovld __cnfn clz(short4 x);\n"
"ushort4 __ovld __cnfn clz(ushort4 x);\n"
"short8 __ovld __cnfn clz(short8 x);\n"
"ushort8 __ovld __cnfn clz(ushort8 x);\n"
"short16 __ovld __cnfn clz(short16 x);\n"
"ushort16 __ovld __cnfn clz(ushort16 x);\n"
"int __ovld __cnfn clz(int x);\n"
"uint __ovld __cnfn clz(uint x);\n"
"int2 __ovld __cnfn clz(int2 x);\n"
"uint2 __ovld __cnfn clz(uint2 x);\n"
"int3 __ovld __cnfn clz(int3 x);\n"
"uint3 __ovld __cnfn clz(uint3 x);\n"
"int4 __ovld __cnfn clz(int4 x);\n"
"uint4 __ovld __cnfn clz(uint4 x);\n"
"int8 __ovld __cnfn clz(int8 x);\n"
"uint8 __ovld __cnfn clz(uint8 x);\n"
"int16 __ovld __cnfn clz(int16 x);\n"
"uint16 __ovld __cnfn clz(uint16 x);\n"
"long __ovld __cnfn clz(long x);\n"
"ulong __ovld __cnfn clz(ulong x);\n"
"long2 __ovld __cnfn clz(long2 x);\n"
"ulong2 __ovld __cnfn clz(ulong2 x);\n"
"long3 __ovld __cnfn clz(long3 x);\n"
"ulong3 __ovld __cnfn clz(ulong3 x);\n"
"long4 __ovld __cnfn clz(long4 x);\n"
"ulong4 __ovld __cnfn clz(ulong4 x);\n"
"long8 __ovld __cnfn clz(long8 x);\n"
"ulong8 __ovld __cnfn clz(ulong8 x);\n"
"long16 __ovld __cnfn clz(long16 x);\n"
"ulong16 __ovld __cnfn clz(ulong16 x);\n"
"\n"
"/**\n"
" * Returns the count of trailing 0-bits in x. If x is 0,\n"
" * returns the size in bits of the type of x or\n"
" * component type of x, if x is a vector.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"char __ovld ctz(char x);\n"
"uchar __ovld ctz(uchar x);\n"
"char2 __ovld ctz(char2 x);\n"
"uchar2 __ovld ctz(uchar2 x);\n"
"char3 __ovld ctz(char3 x);\n"
"uchar3 __ovld ctz(uchar3 x);\n"
"char4 __ovld ctz(char4 x);\n"
"uchar4 __ovld ctz(uchar4 x);\n"
"char8 __ovld ctz(char8 x);\n"
"uchar8 __ovld ctz(uchar8 x);\n"
"char16 __ovld ctz(char16 x);\n"
"uchar16 __ovld ctz(uchar16 x);\n"
"short __ovld ctz(short x);\n"
"ushort __ovld ctz(ushort x);\n"
"short2 __ovld ctz(short2 x);\n"
"ushort2 __ovld ctz(ushort2 x);\n"
"short3 __ovld ctz(short3 x);\n"
"ushort3 __ovld ctz(ushort3 x);\n"
"short4 __ovld ctz(short4 x);\n"
"ushort4 __ovld ctz(ushort4 x);\n"
"short8 __ovld ctz(short8 x);\n"
"ushort8 __ovld ctz(ushort8 x);\n"
"short16 __ovld ctz(short16 x);\n"
"ushort16 __ovld ctz(ushort16 x);\n"
"int __ovld ctz(int x);\n"
"uint __ovld ctz(uint x);\n"
"int2 __ovld ctz(int2 x);\n"
"uint2 __ovld ctz(uint2 x);\n"
"int3 __ovld ctz(int3 x);\n"
"uint3 __ovld ctz(uint3 x);\n"
"int4 __ovld ctz(int4 x);\n"
"uint4 __ovld ctz(uint4 x);\n"
"int8 __ovld ctz(int8 x);\n"
"uint8 __ovld ctz(uint8 x);\n"
"int16 __ovld ctz(int16 x);\n"
"uint16 __ovld ctz(uint16 x);\n"
"long __ovld ctz(long x);\n"
"ulong __ovld ctz(ulong x);\n"
"long2 __ovld ctz(long2 x);\n"
"ulong2 __ovld ctz(ulong2 x);\n"
"long3 __ovld ctz(long3 x);\n"
"ulong3 __ovld ctz(ulong3 x);\n"
"long4 __ovld ctz(long4 x);\n"
"ulong4 __ovld ctz(ulong4 x);\n"
"long8 __ovld ctz(long8 x);\n"
"ulong8 __ovld ctz(ulong8 x);\n"
"long16 __ovld ctz(long16 x);\n"
"ulong16 __ovld ctz(ulong16 x);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Returns mul_hi(a, b) + c.\n"
" */\n"
"char __ovld __cnfn mad_hi(char a, char b, char c);\n"
"uchar __ovld __cnfn mad_hi(uchar a, uchar b, uchar c);\n"
"char2 __ovld __cnfn mad_hi(char2 a, char2 b, char2 c);\n"
"uchar2 __ovld __cnfn mad_hi(uchar2 a, uchar2 b, uchar2 c);\n"
"char3 __ovld __cnfn mad_hi(char3 a, char3 b, char3 c);\n"
"uchar3 __ovld __cnfn mad_hi(uchar3 a, uchar3 b, uchar3 c);\n"
"char4 __ovld __cnfn mad_hi(char4 a, char4 b, char4 c);\n"
"uchar4 __ovld __cnfn mad_hi(uchar4 a, uchar4 b, uchar4 c);\n"
"char8 __ovld __cnfn mad_hi(char8 a, char8 b, char8 c);\n"
"uchar8 __ovld __cnfn mad_hi(uchar8 a, uchar8 b, uchar8 c);\n"
"char16 __ovld __cnfn mad_hi(char16 a, char16 b, char16 c);\n"
"uchar16 __ovld __cnfn mad_hi(uchar16 a, uchar16 b, uchar16 c);\n"
"short __ovld __cnfn mad_hi(short a, short b, short c);\n"
"ushort __ovld __cnfn mad_hi(ushort a, ushort b, ushort c);\n"
"short2 __ovld __cnfn mad_hi(short2 a, short2 b, short2 c);\n"
"ushort2 __ovld __cnfn mad_hi(ushort2 a, ushort2 b, ushort2 c);\n"
"short3 __ovld __cnfn mad_hi(short3 a, short3 b, short3 c);\n"
"ushort3 __ovld __cnfn mad_hi(ushort3 a, ushort3 b, ushort3 c);\n"
"short4 __ovld __cnfn mad_hi(short4 a, short4 b, short4 c);\n"
"ushort4 __ovld __cnfn mad_hi(ushort4 a, ushort4 b, ushort4 c);\n"
"short8 __ovld __cnfn mad_hi(short8 a, short8 b, short8 c);\n"
"ushort8 __ovld __cnfn mad_hi(ushort8 a, ushort8 b, ushort8 c);\n"
"short16 __ovld __cnfn mad_hi(short16 a, short16 b, short16 c);\n"
"ushort16 __ovld __cnfn mad_hi(ushort16 a, ushort16 b, ushort16 c);\n"
"int __ovld __cnfn mad_hi(int a, int b, int c);\n"
"uint __ovld __cnfn mad_hi(uint a, uint b, uint c);\n"
"int2 __ovld __cnfn mad_hi(int2 a, int2 b, int2 c);\n"
"uint2 __ovld __cnfn mad_hi(uint2 a, uint2 b, uint2 c);\n"
"int3 __ovld __cnfn mad_hi(int3 a, int3 b, int3 c);\n"
"uint3 __ovld __cnfn mad_hi(uint3 a, uint3 b, uint3 c);\n"
"int4 __ovld __cnfn mad_hi(int4 a, int4 b, int4 c);\n"
"uint4 __ovld __cnfn mad_hi(uint4 a, uint4 b, uint4 c);\n"
"int8 __ovld __cnfn mad_hi(int8 a, int8 b, int8 c);\n"
"uint8 __ovld __cnfn mad_hi(uint8 a, uint8 b, uint8 c);\n"
"int16 __ovld __cnfn mad_hi(int16 a, int16 b, int16 c);\n"
"uint16 __ovld __cnfn mad_hi(uint16 a, uint16 b, uint16 c);\n"
"long __ovld __cnfn mad_hi(long a, long b, long c);\n"
"ulong __ovld __cnfn mad_hi(ulong a, ulong b, ulong c);\n"
"long2 __ovld __cnfn mad_hi(long2 a, long2 b, long2 c);\n"
"ulong2 __ovld __cnfn mad_hi(ulong2 a, ulong2 b, ulong2 c);\n"
"long3 __ovld __cnfn mad_hi(long3 a, long3 b, long3 c);\n"
"ulong3 __ovld __cnfn mad_hi(ulong3 a, ulong3 b, ulong3 c);\n"
"long4 __ovld __cnfn mad_hi(long4 a, long4 b, long4 c);\n"
"ulong4 __ovld __cnfn mad_hi(ulong4 a, ulong4 b, ulong4 c);\n"
"long8 __ovld __cnfn mad_hi(long8 a, long8 b, long8 c);\n"
"ulong8 __ovld __cnfn mad_hi(ulong8 a, ulong8 b, ulong8 c);\n"
"long16 __ovld __cnfn mad_hi(long16 a, long16 b, long16 c);\n"
"ulong16 __ovld __cnfn mad_hi(ulong16 a, ulong16 b, ulong16 c);\n"
"\n"
"/**\n"
" * Returns a * b + c and saturates the result.\n"
" */\n"
"char __ovld __cnfn mad_sat(char a, char b, char c);\n"
"uchar __ovld __cnfn mad_sat(uchar a, uchar b, uchar c);\n"
"char2 __ovld __cnfn mad_sat(char2 a, char2 b, char2 c);\n"
"uchar2 __ovld __cnfn mad_sat(uchar2 a, uchar2 b, uchar2 c);\n"
"char3 __ovld __cnfn mad_sat(char3 a, char3 b, char3 c);\n"
"uchar3 __ovld __cnfn mad_sat(uchar3 a, uchar3 b, uchar3 c);\n"
"char4 __ovld __cnfn mad_sat(char4 a, char4 b, char4 c);\n"
"uchar4 __ovld __cnfn mad_sat(uchar4 a, uchar4 b, uchar4 c);\n"
"char8 __ovld __cnfn mad_sat(char8 a, char8 b, char8 c);\n"
"uchar8 __ovld __cnfn mad_sat(uchar8 a, uchar8 b, uchar8 c);\n"
"char16 __ovld __cnfn mad_sat(char16 a, char16 b, char16 c);\n"
"uchar16 __ovld __cnfn mad_sat(uchar16 a, uchar16 b, uchar16 c);\n"
"short __ovld __cnfn mad_sat(short a, short b, short c);\n"
"ushort __ovld __cnfn mad_sat(ushort a, ushort b, ushort c);\n"
"short2 __ovld __cnfn mad_sat(short2 a, short2 b, short2 c);\n"
"ushort2 __ovld __cnfn mad_sat(ushort2 a, ushort2 b, ushort2 c);\n"
"short3 __ovld __cnfn mad_sat(short3 a, short3 b, short3 c);\n"
"ushort3 __ovld __cnfn mad_sat(ushort3 a, ushort3 b, ushort3 c);\n"
"short4 __ovld __cnfn mad_sat(short4 a, short4 b, short4 c);\n"
"ushort4 __ovld __cnfn mad_sat(ushort4 a, ushort4 b, ushort4 c);\n"
"short8 __ovld __cnfn mad_sat(short8 a, short8 b, short8 c);\n"
"ushort8 __ovld __cnfn mad_sat(ushort8 a, ushort8 b, ushort8 c);\n"
"short16 __ovld __cnfn mad_sat(short16 a, short16 b, short16 c);\n"
"ushort16 __ovld __cnfn mad_sat(ushort16 a, ushort16 b, ushort16 c);\n"
"int __ovld __cnfn mad_sat(int a, int b, int c);\n"
"uint __ovld __cnfn mad_sat(uint a, uint b, uint c);\n"
"int2 __ovld __cnfn mad_sat(int2 a, int2 b, int2 c);\n"
"uint2 __ovld __cnfn mad_sat(uint2 a, uint2 b, uint2 c);\n"
"int3 __ovld __cnfn mad_sat(int3 a, int3 b, int3 c);\n"
"uint3 __ovld __cnfn mad_sat(uint3 a, uint3 b, uint3 c);\n"
"int4 __ovld __cnfn mad_sat(int4 a, int4 b, int4 c);\n"
"uint4 __ovld __cnfn mad_sat(uint4 a, uint4 b, uint4 c);\n"
"int8 __ovld __cnfn mad_sat(int8 a, int8 b, int8 c);\n"
"uint8 __ovld __cnfn mad_sat(uint8 a, uint8 b, uint8 c);\n"
"int16 __ovld __cnfn mad_sat(int16 a, int16 b, int16 c);\n"
"uint16 __ovld __cnfn mad_sat(uint16 a, uint16 b, uint16 c);\n"
"long __ovld __cnfn mad_sat(long a, long b, long c);\n"
"ulong __ovld __cnfn mad_sat(ulong a, ulong b, ulong c);\n"
"long2 __ovld __cnfn mad_sat(long2 a, long2 b, long2 c);\n"
"ulong2 __ovld __cnfn mad_sat(ulong2 a, ulong2 b, ulong2 c);\n"
"long3 __ovld __cnfn mad_sat(long3 a, long3 b, long3 c);\n"
"ulong3 __ovld __cnfn mad_sat(ulong3 a, ulong3 b, ulong3 c);\n"
"long4 __ovld __cnfn mad_sat(long4 a, long4 b, long4 c);\n"
"ulong4 __ovld __cnfn mad_sat(ulong4 a, ulong4 b, ulong4 c);\n"
"long8 __ovld __cnfn mad_sat(long8 a, long8 b, long8 c);\n"
"ulong8 __ovld __cnfn mad_sat(ulong8 a, ulong8 b, ulong8 c);\n"
"long16 __ovld __cnfn mad_sat(long16 a, long16 b, long16 c);\n"
"ulong16 __ovld __cnfn mad_sat(ulong16 a, ulong16 b, ulong16 c);\n"
"\n"
"/**\n"
" * Returns y if x < y, otherwise it returns x.\n"
" */\n"
"char __ovld __cnfn max(char x, char y);\n"
"uchar __ovld __cnfn max(uchar x, uchar y);\n"
"char2 __ovld __cnfn max(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn max(uchar2 x, uchar2 y);\n"
"char3 __ovld __cnfn max(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn max(uchar3 x, uchar3 y);\n"
"char4 __ovld __cnfn max(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn max(uchar4 x, uchar4 y);\n"
"char8 __ovld __cnfn max(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn max(uchar8 x, uchar8 y);\n"
"char16 __ovld __cnfn max(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn max(uchar16 x, uchar16 y);\n"
"short __ovld __cnfn max(short x, short y);\n"
"ushort __ovld __cnfn max(ushort x, ushort y);\n"
"short2 __ovld __cnfn max(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn max(ushort2 x, ushort2 y);\n"
"short3 __ovld __cnfn max(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn max(ushort3 x, ushort3 y);\n"
"short4 __ovld __cnfn max(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn max(ushort4 x, ushort4 y);\n"
"short8 __ovld __cnfn max(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn max(ushort8 x, ushort8 y);\n"
"short16 __ovld __cnfn max(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn max(ushort16 x, ushort16 y);\n"
"int __ovld __cnfn max(int x, int y);\n"
"uint __ovld __cnfn max(uint x, uint y);\n"
"int2 __ovld __cnfn max(int2 x, int2 y);\n"
"uint2 __ovld __cnfn max(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn max(int3 x, int3 y);\n"
"uint3 __ovld __cnfn max(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn max(int4 x, int4 y);\n"
"uint4 __ovld __cnfn max(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn max(int8 x, int8 y);\n"
"uint8 __ovld __cnfn max(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn max(int16 x, int16 y);\n"
"uint16 __ovld __cnfn max(uint16 x, uint16 y);\n"
"long __ovld __cnfn max(long x, long y);\n"
"ulong __ovld __cnfn max(ulong x, ulong y);\n"
"long2 __ovld __cnfn max(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn max(ulong2 x, ulong2 y);\n"
"long3 __ovld __cnfn max(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn max(ulong3 x, ulong3 y);\n"
"long4 __ovld __cnfn max(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn max(ulong4 x, ulong4 y);\n"
"long8 __ovld __cnfn max(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn max(ulong8 x, ulong8 y);\n"
"long16 __ovld __cnfn max(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn max(ulong16 x, ulong16 y);\n"
"char __ovld __cnfn max(char x, char y);\n"
"uchar __ovld __cnfn max(uchar x, uchar y);\n"
"char2 __ovld __cnfn max(char2 x, char y);\n"
"uchar2 __ovld __cnfn max(uchar2 x, uchar y);\n"
"char3 __ovld __cnfn max(char3 x, char y);\n"
"uchar3 __ovld __cnfn max(uchar3 x, uchar y);\n"
"char4 __ovld __cnfn max(char4 x, char y);\n"
"uchar4 __ovld __cnfn max(uchar4 x, uchar y);\n"
"char8 __ovld __cnfn max(char8 x, char y);\n"
"uchar8 __ovld __cnfn max(uchar8 x, uchar y);\n"
"char16 __ovld __cnfn max(char16 x, char y);\n"
"uchar16 __ovld __cnfn max(uchar16 x, uchar y);\n"
"short __ovld __cnfn max(short x, short y);\n"
"ushort __ovld __cnfn max(ushort x, ushort y);\n"
"short2 __ovld __cnfn max(short2 x, short y);\n"
"ushort2 __ovld __cnfn max(ushort2 x, ushort y);\n"
"short3 __ovld __cnfn max(short3 x, short y);\n"
"ushort3 __ovld __cnfn max(ushort3 x, ushort y);\n"
"short4 __ovld __cnfn max(short4 x, short y);\n"
"ushort4 __ovld __cnfn max(ushort4 x, ushort y);\n"
"short8 __ovld __cnfn max(short8 x, short y);\n"
"ushort8 __ovld __cnfn max(ushort8 x, ushort y);\n"
"short16 __ovld __cnfn max(short16 x, short y);\n"
"ushort16 __ovld __cnfn max(ushort16 x, ushort y);\n"
"int __ovld __cnfn max(int x, int y);\n"
"uint __ovld __cnfn max(uint x, uint y);\n"
"int2 __ovld __cnfn max(int2 x, int y);\n"
"uint2 __ovld __cnfn max(uint2 x, uint y);\n"
"int3 __ovld __cnfn max(int3 x, int y);\n"
"uint3 __ovld __cnfn max(uint3 x, uint y);\n"
"int4 __ovld __cnfn max(int4 x, int y);\n"
"uint4 __ovld __cnfn max(uint4 x, uint y);\n"
"int8 __ovld __cnfn max(int8 x, int y);\n"
"uint8 __ovld __cnfn max(uint8 x, uint y);\n"
"int16 __ovld __cnfn max(int16 x, int y);\n"
"uint16 __ovld __cnfn max(uint16 x, uint y);\n"
"long __ovld __cnfn max(long x, long y);\n"
"ulong __ovld __cnfn max(ulong x, ulong y);\n"
"long2 __ovld __cnfn max(long2 x, long y);\n"
"ulong2 __ovld __cnfn max(ulong2 x, ulong y);\n"
"long3 __ovld __cnfn max(long3 x, long y);\n"
"ulong3 __ovld __cnfn max(ulong3 x, ulong y);\n"
"long4 __ovld __cnfn max(long4 x, long y);\n"
"ulong4 __ovld __cnfn max(ulong4 x, ulong y);\n"
"long8 __ovld __cnfn max(long8 x, long y);\n"
"ulong8 __ovld __cnfn max(ulong8 x, ulong y);\n"
"long16 __ovld __cnfn max(long16 x, long y);\n"
"ulong16 __ovld __cnfn max(ulong16 x, ulong y);\n"
"\n"
"/**\n"
" * Returns y if y < x, otherwise it returns x.\n"
" */\n"
"char __ovld __cnfn min(char x, char y);\n"
"uchar __ovld __cnfn min(uchar x, uchar y);\n"
"char2 __ovld __cnfn min(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn min(uchar2 x, uchar2 y);\n"
"char3 __ovld __cnfn min(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn min(uchar3 x, uchar3 y);\n"
"char4 __ovld __cnfn min(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn min(uchar4 x, uchar4 y);\n"
"char8 __ovld __cnfn min(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn min(uchar8 x, uchar8 y);\n"
"char16 __ovld __cnfn min(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn min(uchar16 x, uchar16 y);\n"
"short __ovld __cnfn min(short x, short y);\n"
"ushort __ovld __cnfn min(ushort x, ushort y);\n"
"short2 __ovld __cnfn min(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn min(ushort2 x, ushort2 y);\n"
"short3 __ovld __cnfn min(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn min(ushort3 x, ushort3 y);\n"
"short4 __ovld __cnfn min(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn min(ushort4 x, ushort4 y);\n"
"short8 __ovld __cnfn min(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn min(ushort8 x, ushort8 y);\n"
"short16 __ovld __cnfn min(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn min(ushort16 x, ushort16 y);\n"
"int __ovld __cnfn min(int x, int y);\n"
"uint __ovld __cnfn min(uint x, uint y);\n"
"int2 __ovld __cnfn min(int2 x, int2 y);\n"
"uint2 __ovld __cnfn min(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn min(int3 x, int3 y);\n"
"uint3 __ovld __cnfn min(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn min(int4 x, int4 y);\n"
"uint4 __ovld __cnfn min(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn min(int8 x, int8 y);\n"
"uint8 __ovld __cnfn min(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn min(int16 x, int16 y);\n"
"uint16 __ovld __cnfn min(uint16 x, uint16 y);\n"
"long __ovld __cnfn min(long x, long y);\n"
"ulong __ovld __cnfn min(ulong x, ulong y);\n"
"long2 __ovld __cnfn min(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn min(ulong2 x, ulong2 y);\n"
"long3 __ovld __cnfn min(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn min(ulong3 x, ulong3 y);\n"
"long4 __ovld __cnfn min(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn min(ulong4 x, ulong4 y);\n"
"long8 __ovld __cnfn min(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn min(ulong8 x, ulong8 y);\n"
"long16 __ovld __cnfn min(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn min(ulong16 x, ulong16 y);\n"
"char __ovld __cnfn min(char x, char y);\n"
"uchar __ovld __cnfn min(uchar x, uchar y);\n"
"char2 __ovld __cnfn min(char2 x, char y);\n"
"uchar2 __ovld __cnfn min(uchar2 x, uchar y);\n"
"char3 __ovld __cnfn min(char3 x, char y);\n"
"uchar3 __ovld __cnfn min(uchar3 x, uchar y);\n"
"char4 __ovld __cnfn min(char4 x, char y);\n"
"uchar4 __ovld __cnfn min(uchar4 x, uchar y);\n"
"char8 __ovld __cnfn min(char8 x, char y);\n"
"uchar8 __ovld __cnfn min(uchar8 x, uchar y);\n"
"char16 __ovld __cnfn min(char16 x, char y);\n"
"uchar16 __ovld __cnfn min(uchar16 x, uchar y);\n"
"short __ovld __cnfn min(short x, short y);\n"
"ushort __ovld __cnfn min(ushort x, ushort y);\n"
"short2 __ovld __cnfn min(short2 x, short y);\n"
"ushort2 __ovld __cnfn min(ushort2 x, ushort y);\n"
"short3 __ovld __cnfn min(short3 x, short y);\n"
"ushort3 __ovld __cnfn min(ushort3 x, ushort y);\n"
"short4 __ovld __cnfn min(short4 x, short y);\n"
"ushort4 __ovld __cnfn min(ushort4 x, ushort y);\n"
"short8 __ovld __cnfn min(short8 x, short y);\n"
"ushort8 __ovld __cnfn min(ushort8 x, ushort y);\n"
"short16 __ovld __cnfn min(short16 x, short y);\n"
"ushort16 __ovld __cnfn min(ushort16 x, ushort y);\n"
"int __ovld __cnfn min(int x, int y);\n"
"uint __ovld __cnfn min(uint x, uint y);\n"
"int2 __ovld __cnfn min(int2 x, int y);\n"
"uint2 __ovld __cnfn min(uint2 x, uint y);\n"
"int3 __ovld __cnfn min(int3 x, int y);\n"
"uint3 __ovld __cnfn min(uint3 x, uint y);\n"
"int4 __ovld __cnfn min(int4 x, int y);\n"
"uint4 __ovld __cnfn min(uint4 x, uint y);\n"
"int8 __ovld __cnfn min(int8 x, int y);\n"
"uint8 __ovld __cnfn min(uint8 x, uint y);\n"
"int16 __ovld __cnfn min(int16 x, int y);\n"
"uint16 __ovld __cnfn min(uint16 x, uint y);\n"
"long __ovld __cnfn min(long x, long y);\n"
"ulong __ovld __cnfn min(ulong x, ulong y);\n"
"long2 __ovld __cnfn min(long2 x, long y);\n"
"ulong2 __ovld __cnfn min(ulong2 x, ulong y);\n"
"long3 __ovld __cnfn min(long3 x, long y);\n"
"ulong3 __ovld __cnfn min(ulong3 x, ulong y);\n"
"long4 __ovld __cnfn min(long4 x, long y);\n"
"ulong4 __ovld __cnfn min(ulong4 x, ulong y);\n"
"long8 __ovld __cnfn min(long8 x, long y);\n"
"ulong8 __ovld __cnfn min(ulong8 x, ulong y);\n"
"long16 __ovld __cnfn min(long16 x, long y);\n"
"ulong16 __ovld __cnfn min(ulong16 x, ulong y);\n"
"\n"
"/**\n"
" * Computes x * y and returns the high half of the\n"
" * product of x and y.\n"
" */\n"
"char __ovld __cnfn mul_hi(char x, char y);\n"
"uchar __ovld __cnfn mul_hi(uchar x, uchar y);\n"
"char2 __ovld __cnfn mul_hi(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn mul_hi(uchar2 x, uchar2 y);\n"
"char3 __ovld __cnfn mul_hi(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn mul_hi(uchar3 x, uchar3 y);\n"
"char4 __ovld __cnfn mul_hi(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn mul_hi(uchar4 x, uchar4 y);\n"
"char8 __ovld __cnfn mul_hi(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn mul_hi(uchar8 x, uchar8 y);\n"
"char16 __ovld __cnfn mul_hi(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn mul_hi(uchar16 x, uchar16 y);\n"
"short __ovld __cnfn mul_hi(short x, short y);\n"
"ushort __ovld __cnfn mul_hi(ushort x, ushort y);\n"
"short2 __ovld __cnfn mul_hi(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn mul_hi(ushort2 x, ushort2 y);\n"
"short3 __ovld __cnfn mul_hi(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn mul_hi(ushort3 x, ushort3 y);\n"
"short4 __ovld __cnfn mul_hi(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn mul_hi(ushort4 x, ushort4 y);\n"
"short8 __ovld __cnfn mul_hi(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn mul_hi(ushort8 x, ushort8 y);\n"
"short16 __ovld __cnfn mul_hi(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn mul_hi(ushort16 x, ushort16 y);\n"
"int __ovld __cnfn mul_hi(int x, int y);\n"
"uint __ovld __cnfn mul_hi(uint x, uint y);\n"
"int2 __ovld __cnfn mul_hi(int2 x, int2 y);\n"
"uint2 __ovld __cnfn mul_hi(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn mul_hi(int3 x, int3 y);\n"
"uint3 __ovld __cnfn mul_hi(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn mul_hi(int4 x, int4 y);\n"
"uint4 __ovld __cnfn mul_hi(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn mul_hi(int8 x, int8 y);\n"
"uint8 __ovld __cnfn mul_hi(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn mul_hi(int16 x, int16 y);\n"
"uint16 __ovld __cnfn mul_hi(uint16 x, uint16 y);\n"
"long __ovld __cnfn mul_hi(long x, long y);\n"
"ulong __ovld __cnfn mul_hi(ulong x, ulong y);\n"
"long2 __ovld __cnfn mul_hi(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn mul_hi(ulong2 x, ulong2 y);\n"
"long3 __ovld __cnfn mul_hi(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn mul_hi(ulong3 x, ulong3 y);\n"
"long4 __ovld __cnfn mul_hi(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn mul_hi(ulong4 x, ulong4 y);\n"
"long8 __ovld __cnfn mul_hi(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn mul_hi(ulong8 x, ulong8 y);\n"
"long16 __ovld __cnfn mul_hi(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn mul_hi(ulong16 x, ulong16 y);\n"
"\n"
"/**\n"
" * For each element in v, the bits are shifted left by\n"
" * the number of bits given by the corresponding\n"
" * element in i (subject to usual shift modulo rules\n"
" * described in section 6.3). Bits shifted off the left\n"
" * side of the element are shifted back in from the\n"
" * right.\n"
" */\n"
"char __ovld __cnfn rotate(char v, char i);\n"
"uchar __ovld __cnfn rotate(uchar v, uchar i);\n"
"char2 __ovld __cnfn rotate(char2 v, char2 i);\n"
"uchar2 __ovld __cnfn rotate(uchar2 v, uchar2 i);\n"
"char3 __ovld __cnfn rotate(char3 v, char3 i);\n"
"uchar3 __ovld __cnfn rotate(uchar3 v, uchar3 i);\n"
"char4 __ovld __cnfn rotate(char4 v, char4 i);\n"
"uchar4 __ovld __cnfn rotate(uchar4 v, uchar4 i);\n"
"char8 __ovld __cnfn rotate(char8 v, char8 i);\n"
"uchar8 __ovld __cnfn rotate(uchar8 v, uchar8 i);\n"
"char16 __ovld __cnfn rotate(char16 v, char16 i);\n"
"uchar16 __ovld __cnfn rotate(uchar16 v, uchar16 i);\n"
"short __ovld __cnfn rotate(short v, short i);\n"
"ushort __ovld __cnfn rotate(ushort v, ushort i);\n"
"short2 __ovld __cnfn rotate(short2 v, short2 i);\n"
"ushort2 __ovld __cnfn rotate(ushort2 v, ushort2 i);\n"
"short3 __ovld __cnfn rotate(short3 v, short3 i);\n"
"ushort3 __ovld __cnfn rotate(ushort3 v, ushort3 i);\n"
"short4 __ovld __cnfn rotate(short4 v, short4 i);\n"
"ushort4 __ovld __cnfn rotate(ushort4 v, ushort4 i);\n"
"short8 __ovld __cnfn rotate(short8 v, short8 i);\n"
"ushort8 __ovld __cnfn rotate(ushort8 v, ushort8 i);\n"
"short16 __ovld __cnfn rotate(short16 v, short16 i);\n"
"ushort16 __ovld __cnfn rotate(ushort16 v, ushort16 i);\n"
"int __ovld __cnfn rotate(int v, int i);\n"
"uint __ovld __cnfn rotate(uint v, uint i);\n"
"int2 __ovld __cnfn rotate(int2 v, int2 i);\n"
"uint2 __ovld __cnfn rotate(uint2 v, uint2 i);\n"
"int3 __ovld __cnfn rotate(int3 v, int3 i);\n"
"uint3 __ovld __cnfn rotate(uint3 v, uint3 i);\n"
"int4 __ovld __cnfn rotate(int4 v, int4 i);\n"
"uint4 __ovld __cnfn rotate(uint4 v, uint4 i);\n"
"int8 __ovld __cnfn rotate(int8 v, int8 i);\n"
"uint8 __ovld __cnfn rotate(uint8 v, uint8 i);\n"
"int16 __ovld __cnfn rotate(int16 v, int16 i);\n"
"uint16 __ovld __cnfn rotate(uint16 v, uint16 i);\n"
"long __ovld __cnfn rotate(long v, long i);\n"
"ulong __ovld __cnfn rotate(ulong v, ulong i);\n"
"long2 __ovld __cnfn rotate(long2 v, long2 i);\n"
"ulong2 __ovld __cnfn rotate(ulong2 v, ulong2 i);\n"
"long3 __ovld __cnfn rotate(long3 v, long3 i);\n"
"ulong3 __ovld __cnfn rotate(ulong3 v, ulong3 i);\n"
"long4 __ovld __cnfn rotate(long4 v, long4 i);\n"
"ulong4 __ovld __cnfn rotate(ulong4 v, ulong4 i);\n"
"long8 __ovld __cnfn rotate(long8 v, long8 i);\n"
"ulong8 __ovld __cnfn rotate(ulong8 v, ulong8 i);\n"
"long16 __ovld __cnfn rotate(long16 v, long16 i);\n"
"ulong16 __ovld __cnfn rotate(ulong16 v, ulong16 i);\n"
"\n"
"/**\n"
" * Returns x - y and saturates the result.\n"
" */\n"
"char __ovld __cnfn sub_sat(char x, char y);\n"
"uchar __ovld __cnfn sub_sat(uchar x, uchar y);\n"
"char2 __ovld __cnfn sub_sat(char2 x, char2 y);\n"
"uchar2 __ovld __cnfn sub_sat(uchar2 x, uchar2 y);\n"
"char3 __ovld __cnfn sub_sat(char3 x, char3 y);\n"
"uchar3 __ovld __cnfn sub_sat(uchar3 x, uchar3 y);\n"
"char4 __ovld __cnfn sub_sat(char4 x, char4 y);\n"
"uchar4 __ovld __cnfn sub_sat(uchar4 x, uchar4 y);\n"
"char8 __ovld __cnfn sub_sat(char8 x, char8 y);\n"
"uchar8 __ovld __cnfn sub_sat(uchar8 x, uchar8 y);\n"
"char16 __ovld __cnfn sub_sat(char16 x, char16 y);\n"
"uchar16 __ovld __cnfn sub_sat(uchar16 x, uchar16 y);\n"
"short __ovld __cnfn sub_sat(short x, short y);\n"
"ushort __ovld __cnfn sub_sat(ushort x, ushort y);\n"
"short2 __ovld __cnfn sub_sat(short2 x, short2 y);\n"
"ushort2 __ovld __cnfn sub_sat(ushort2 x, ushort2 y);\n"
"short3 __ovld __cnfn sub_sat(short3 x, short3 y);\n"
"ushort3 __ovld __cnfn sub_sat(ushort3 x, ushort3 y);\n"
"short4 __ovld __cnfn sub_sat(short4 x, short4 y);\n"
"ushort4 __ovld __cnfn sub_sat(ushort4 x, ushort4 y);\n"
"short8 __ovld __cnfn sub_sat(short8 x, short8 y);\n"
"ushort8 __ovld __cnfn sub_sat(ushort8 x, ushort8 y);\n"
"short16 __ovld __cnfn sub_sat(short16 x, short16 y);\n"
"ushort16 __ovld __cnfn sub_sat(ushort16 x, ushort16 y);\n"
"int __ovld __cnfn sub_sat(int x, int y);\n"
"uint __ovld __cnfn sub_sat(uint x, uint y);\n"
"int2 __ovld __cnfn sub_sat(int2 x, int2 y);\n"
"uint2 __ovld __cnfn sub_sat(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn sub_sat(int3 x, int3 y);\n"
"uint3 __ovld __cnfn sub_sat(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn sub_sat(int4 x, int4 y);\n"
"uint4 __ovld __cnfn sub_sat(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn sub_sat(int8 x, int8 y);\n"
"uint8 __ovld __cnfn sub_sat(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn sub_sat(int16 x, int16 y);\n"
"uint16 __ovld __cnfn sub_sat(uint16 x, uint16 y);\n"
"long __ovld __cnfn sub_sat(long x, long y);\n"
"ulong __ovld __cnfn sub_sat(ulong x, ulong y);\n"
"long2 __ovld __cnfn sub_sat(long2 x, long2 y);\n"
"ulong2 __ovld __cnfn sub_sat(ulong2 x, ulong2 y);\n"
"long3 __ovld __cnfn sub_sat(long3 x, long3 y);\n"
"ulong3 __ovld __cnfn sub_sat(ulong3 x, ulong3 y);\n"
"long4 __ovld __cnfn sub_sat(long4 x, long4 y);\n"
"ulong4 __ovld __cnfn sub_sat(ulong4 x, ulong4 y);\n"
"long8 __ovld __cnfn sub_sat(long8 x, long8 y);\n"
"ulong8 __ovld __cnfn sub_sat(ulong8 x, ulong8 y);\n"
"long16 __ovld __cnfn sub_sat(long16 x, long16 y);\n"
"ulong16 __ovld __cnfn sub_sat(ulong16 x, ulong16 y);\n"
"\n"
"/**\n"
" * result[i] = ((short)hi[i] << 8) | lo[i]\n"
" * result[i] = ((ushort)hi[i] << 8) | lo[i]\n"
" */\n"
"short __ovld __cnfn upsample(char hi, uchar lo);\n"
"ushort __ovld __cnfn upsample(uchar hi, uchar lo);\n"
"short2 __ovld __cnfn upsample(char2 hi, uchar2 lo);\n"
"short3 __ovld __cnfn upsample(char3 hi, uchar3 lo);\n"
"short4 __ovld __cnfn upsample(char4 hi, uchar4 lo);\n"
"short8 __ovld __cnfn upsample(char8 hi, uchar8 lo);\n"
"short16 __ovld __cnfn upsample(char16 hi, uchar16 lo);\n"
"ushort2 __ovld __cnfn upsample(uchar2 hi, uchar2 lo);\n"
"ushort3 __ovld __cnfn upsample(uchar3 hi, uchar3 lo);\n"
"ushort4 __ovld __cnfn upsample(uchar4 hi, uchar4 lo);\n"
"ushort8 __ovld __cnfn upsample(uchar8 hi, uchar8 lo);\n"
"ushort16 __ovld __cnfn upsample(uchar16 hi, uchar16 lo);\n"
"\n"
"/**\n"
" * result[i] = ((int)hi[i] << 16) | lo[i]\n"
" * result[i] = ((uint)hi[i] << 16) | lo[i]\n"
" */\n"
"int __ovld __cnfn upsample(short hi, ushort lo);\n"
"uint __ovld __cnfn upsample(ushort hi, ushort lo);\n"
"int2 __ovld __cnfn upsample(short2 hi, ushort2 lo);\n"
"int3 __ovld __cnfn upsample(short3 hi, ushort3 lo);\n"
"int4 __ovld __cnfn upsample(short4 hi, ushort4 lo);\n"
"int8 __ovld __cnfn upsample(short8 hi, ushort8 lo);\n"
"int16 __ovld __cnfn upsample(short16 hi, ushort16 lo);\n"
"uint2 __ovld __cnfn upsample(ushort2 hi, ushort2 lo);\n"
"uint3 __ovld __cnfn upsample(ushort3 hi, ushort3 lo);\n"
"uint4 __ovld __cnfn upsample(ushort4 hi, ushort4 lo);\n"
"uint8 __ovld __cnfn upsample(ushort8 hi, ushort8 lo);\n"
"uint16 __ovld __cnfn upsample(ushort16 hi, ushort16 lo);\n"
"/**\n"
" * result[i] = ((long)hi[i] << 32) | lo[i]\n"
" * result[i] = ((ulong)hi[i] << 32) | lo[i]\n"
" */\n"
"long __ovld __cnfn upsample(int hi, uint lo);\n"
"ulong __ovld __cnfn upsample(uint hi, uint lo);\n"
"long2 __ovld __cnfn upsample(int2 hi, uint2 lo);\n"
"long3 __ovld __cnfn upsample(int3 hi, uint3 lo);\n"
"long4 __ovld __cnfn upsample(int4 hi, uint4 lo);\n"
"long8 __ovld __cnfn upsample(int8 hi, uint8 lo);\n"
"long16 __ovld __cnfn upsample(int16 hi, uint16 lo);\n"
"ulong2 __ovld __cnfn upsample(uint2 hi, uint2 lo);\n"
"ulong3 __ovld __cnfn upsample(uint3 hi, uint3 lo);\n"
"ulong4 __ovld __cnfn upsample(uint4 hi, uint4 lo);\n"
"ulong8 __ovld __cnfn upsample(uint8 hi, uint8 lo);\n"
"ulong16 __ovld __cnfn upsample(uint16 hi, uint16 lo);\n"
"\n"
"/*\n"
" * popcount(x): returns the number of set bit in x\n"
" */\n"
"char __ovld __cnfn popcount(char x);\n"
"uchar __ovld __cnfn popcount(uchar x);\n"
"char2 __ovld __cnfn popcount(char2 x);\n"
"uchar2 __ovld __cnfn popcount(uchar2 x);\n"
"char3 __ovld __cnfn popcount(char3 x);\n"
"uchar3 __ovld __cnfn popcount(uchar3 x);\n"
"char4 __ovld __cnfn popcount(char4 x);\n"
"uchar4 __ovld __cnfn popcount(uchar4 x);\n"
"char8 __ovld __cnfn popcount(char8 x);\n"
"uchar8 __ovld __cnfn popcount(uchar8 x);\n"
"char16 __ovld __cnfn popcount(char16 x);\n"
"uchar16 __ovld __cnfn popcount(uchar16 x);\n"
"short __ovld __cnfn popcount(short x);\n"
"ushort __ovld __cnfn popcount(ushort x);\n"
"short2 __ovld __cnfn popcount(short2 x);\n"
"ushort2 __ovld __cnfn popcount(ushort2 x);\n"
"short3 __ovld __cnfn popcount(short3 x);\n"
"ushort3 __ovld __cnfn popcount(ushort3 x);\n"
"short4 __ovld __cnfn popcount(short4 x);\n"
"ushort4 __ovld __cnfn popcount(ushort4 x);\n"
"short8 __ovld __cnfn popcount(short8 x);\n"
"ushort8 __ovld __cnfn popcount(ushort8 x);\n"
"short16 __ovld __cnfn popcount(short16 x);\n"
"ushort16 __ovld __cnfn popcount(ushort16 x);\n"
"int __ovld __cnfn popcount(int x);\n"
"uint __ovld __cnfn popcount(uint x);\n"
"int2 __ovld __cnfn popcount(int2 x);\n"
"uint2 __ovld __cnfn popcount(uint2 x);\n"
"int3 __ovld __cnfn popcount(int3 x);\n"
"uint3 __ovld __cnfn popcount(uint3 x);\n"
"int4 __ovld __cnfn popcount(int4 x);\n"
"uint4 __ovld __cnfn popcount(uint4 x);\n"
"int8 __ovld __cnfn popcount(int8 x);\n"
"uint8 __ovld __cnfn popcount(uint8 x);\n"
"int16 __ovld __cnfn popcount(int16 x);\n"
"uint16 __ovld __cnfn popcount(uint16 x);\n"
"long __ovld __cnfn popcount(long x);\n"
"ulong __ovld __cnfn popcount(ulong x);\n"
"long2 __ovld __cnfn popcount(long2 x);\n"
"ulong2 __ovld __cnfn popcount(ulong2 x);\n"
"long3 __ovld __cnfn popcount(long3 x);\n"
"ulong3 __ovld __cnfn popcount(ulong3 x);\n"
"long4 __ovld __cnfn popcount(long4 x);\n"
"ulong4 __ovld __cnfn popcount(ulong4 x);\n"
"long8 __ovld __cnfn popcount(long8 x);\n"
"ulong8 __ovld __cnfn popcount(ulong8 x);\n"
"long16 __ovld __cnfn popcount(long16 x);\n"
"ulong16 __ovld __cnfn popcount(ulong16 x);\n"
"\n"
"/**\n"
" * Multiply two 24-bit integer values x and y and add\n"
" * the 32-bit integer result to the 32-bit integer z.\n"
" * Refer to definition of mul24 to see how the 24-bit\n"
" * integer multiplication is performed.\n"
" */\n"
"int __ovld __cnfn mad24(int x, int y, int z);\n"
"uint __ovld __cnfn mad24(uint x, uint y, uint z);\n"
"int2 __ovld __cnfn mad24(int2 x, int2 y, int2 z);\n"
"uint2 __ovld __cnfn mad24(uint2 x, uint2 y, uint2 z);\n"
"int3 __ovld __cnfn mad24(int3 x, int3 y, int3 z);\n"
"uint3 __ovld __cnfn mad24(uint3 x, uint3 y, uint3 z);\n"
"int4 __ovld __cnfn mad24(int4 x, int4 y, int4 z);\n"
"uint4 __ovld __cnfn mad24(uint4 x, uint4 y, uint4 z);\n"
"int8 __ovld __cnfn mad24(int8 x, int8 y, int8 z);\n"
"uint8 __ovld __cnfn mad24(uint8 x, uint8 y, uint8 z);\n"
"int16 __ovld __cnfn mad24(int16 x, int16 y, int16 z);\n"
"uint16 __ovld __cnfn mad24(uint16 x, uint16 y, uint16 z);\n"
"\n"
"/**\n"
" * Multiply two 24-bit integer values x and y. x and y\n"
" * are 32-bit integers but only the low 24-bits are used\n"
" * to perform the multiplication. mul24 should only\n"
" * be used when values in x and y are in the range [-\n"
" * 2^23, 2^23-1] if x and y are signed integers and in the\n"
" * range [0, 2^24-1] if x and y are unsigned integers. If\n"
" * x and y are not in this range, the multiplication\n"
" * result is implementation-defined.\n"
" */\n"
"int __ovld __cnfn mul24(int x, int y);\n"
"uint __ovld __cnfn mul24(uint x, uint y);\n"
"int2 __ovld __cnfn mul24(int2 x, int2 y);\n"
"uint2 __ovld __cnfn mul24(uint2 x, uint2 y);\n"
"int3 __ovld __cnfn mul24(int3 x, int3 y);\n"
"uint3 __ovld __cnfn mul24(uint3 x, uint3 y);\n"
"int4 __ovld __cnfn mul24(int4 x, int4 y);\n"
"uint4 __ovld __cnfn mul24(uint4 x, uint4 y);\n"
"int8 __ovld __cnfn mul24(int8 x, int8 y);\n"
"uint8 __ovld __cnfn mul24(uint8 x, uint8 y);\n"
"int16 __ovld __cnfn mul24(int16 x, int16 y);\n"
"uint16 __ovld __cnfn mul24(uint16 x, uint16 y);\n"
"\n"
"// OpenCL v1.1 s6.11.4, v1.2 s6.12.4, v2.0 s6.13.4 - Common Functions\n"
"\n"
"/**\n"
" * Returns fmin(fmax(x, minval), maxval).\n"
" * Results are undefined if minval > maxval.\n"
" */\n"
"float __ovld __cnfn clamp(float x, float minval, float maxval);\n"
"float2 __ovld __cnfn clamp(float2 x, float2 minval, float2 maxval);\n"
"float3 __ovld __cnfn clamp(float3 x, float3 minval, float3 maxval);\n"
"float4 __ovld __cnfn clamp(float4 x, float4 minval, float4 maxval);\n"
"float8 __ovld __cnfn clamp(float8 x, float8 minval, float8 maxval);\n"
"float16 __ovld __cnfn clamp(float16 x, float16 minval, float16 maxval);\n"
"float2 __ovld __cnfn clamp(float2 x, float minval, float maxval);\n"
"float3 __ovld __cnfn clamp(float3 x, float minval, float maxval);\n"
"float4 __ovld __cnfn clamp(float4 x, float minval, float maxval);\n"
"float8 __ovld __cnfn clamp(float8 x, float minval, float maxval);\n"
"float16 __ovld __cnfn clamp(float16 x, float minval, float maxval);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn clamp(double x, double minval, double maxval);\n"
"double2 __ovld __cnfn clamp(double2 x, double2 minval, double2 maxval);\n"
"double3 __ovld __cnfn clamp(double3 x, double3 minval, double3 maxval);\n"
"double4 __ovld __cnfn clamp(double4 x, double4 minval, double4 maxval);\n"
"double8 __ovld __cnfn clamp(double8 x, double8 minval, double8 maxval);\n"
"double16 __ovld __cnfn clamp(double16 x, double16 minval, double16 maxval);\n"
"double2 __ovld __cnfn clamp(double2 x, double minval, double maxval);\n"
"double3 __ovld __cnfn clamp(double3 x, double minval, double maxval);\n"
"double4 __ovld __cnfn clamp(double4 x, double minval, double maxval);\n"
"double8 __ovld __cnfn clamp(double8 x, double minval, double maxval);\n"
"double16 __ovld __cnfn clamp(double16 x, double minval, double maxval);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn clamp(half x, half minval, half maxval);\n"
"half2 __ovld __cnfn clamp(half2 x, half2 minval, half2 maxval);\n"
"half3 __ovld __cnfn clamp(half3 x, half3 minval, half3 maxval);\n"
"half4 __ovld __cnfn clamp(half4 x, half4 minval, half4 maxval);\n"
"half8 __ovld __cnfn clamp(half8 x, half8 minval, half8 maxval);\n"
"half16 __ovld __cnfn clamp(half16 x, half16 minval, half16 maxval);\n"
"half2 __ovld __cnfn clamp(half2 x, half minval, half maxval);\n"
"half3 __ovld __cnfn clamp(half3 x, half minval, half maxval);\n"
"half4 __ovld __cnfn clamp(half4 x, half minval, half maxval);\n"
"half8 __ovld __cnfn clamp(half8 x, half minval, half maxval);\n"
"half16 __ovld __cnfn clamp(half16 x, half minval, half maxval);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Converts radians to degrees, i.e. (180 / PI) *\n"
" * radians.\n"
" */\n"
"float __ovld __cnfn degrees(float radians);\n"
"float2 __ovld __cnfn degrees(float2 radians);\n"
"float3 __ovld __cnfn degrees(float3 radians);\n"
"float4 __ovld __cnfn degrees(float4 radians);\n"
"float8 __ovld __cnfn degrees(float8 radians);\n"
"float16 __ovld __cnfn degrees(float16 radians);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn degrees(double radians);\n"
"double2 __ovld __cnfn degrees(double2 radians);\n"
"double3 __ovld __cnfn degrees(double3 radians);\n"
"double4 __ovld __cnfn degrees(double4 radians);\n"
"double8 __ovld __cnfn degrees(double8 radians);\n"
"double16 __ovld __cnfn degrees(double16 radians);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn degrees(half radians);\n"
"half2 __ovld __cnfn degrees(half2 radians);\n"
"half3 __ovld __cnfn degrees(half3 radians);\n"
"half4 __ovld __cnfn degrees(half4 radians);\n"
"half8 __ovld __cnfn degrees(half8 radians);\n"
"half16 __ovld __cnfn degrees(half16 radians);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns y if x < y, otherwise it returns x. If x and y\n"
" * are infinite or NaN, the return values are undefined.\n"
" */\n"
"float __ovld __cnfn max(float x, float y);\n"
"float2 __ovld __cnfn max(float2 x, float2 y);\n"
"float3 __ovld __cnfn max(float3 x, float3 y);\n"
"float4 __ovld __cnfn max(float4 x, float4 y);\n"
"float8 __ovld __cnfn max(float8 x, float8 y);\n"
"float16 __ovld __cnfn max(float16 x, float16 y);\n"
"float2 __ovld __cnfn max(float2 x, float y);\n"
"float3 __ovld __cnfn max(float3 x, float y);\n"
"float4 __ovld __cnfn max(float4 x, float y);\n"
"float8 __ovld __cnfn max(float8 x, float y);\n"
"float16 __ovld __cnfn max(float16 x, float y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn max(double x, double y);\n"
"double2 __ovld __cnfn max(double2 x, double2 y);\n"
"double3 __ovld __cnfn max(double3 x, double3 y);\n"
"double4 __ovld __cnfn max(double4 x, double4 y);\n"
"double8 __ovld __cnfn max(double8 x, double8 y);\n"
"double16 __ovld __cnfn max(double16 x, double16 y);\n"
"double2 __ovld __cnfn max(double2 x, double y);\n"
"double3 __ovld __cnfn max(double3 x, double y);\n"
"double4 __ovld __cnfn max(double4 x, double y);\n"
"double8 __ovld __cnfn max(double8 x, double y);\n"
"double16 __ovld __cnfn max(double16 x, double y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn max(half x, half y);\n"
"half2 __ovld __cnfn max(half2 x, half2 y);\n"
"half3 __ovld __cnfn max(half3 x, half3 y);\n"
"half4 __ovld __cnfn max(half4 x, half4 y);\n"
"half8 __ovld __cnfn max(half8 x, half8 y);\n"
"half16 __ovld __cnfn max(half16 x, half16 y);\n"
"half2 __ovld __cnfn max(half2 x, half y);\n"
"half3 __ovld __cnfn max(half3 x, half y);\n"
"half4 __ovld __cnfn max(half4 x, half y);\n"
"half8 __ovld __cnfn max(half8 x, half y);\n"
"half16 __ovld __cnfn max(half16 x, half y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns y if y < x, otherwise it returns x. If x and y\n"
" * are infinite or NaN, the return values are undefined.\n"
" */\n"
"float __ovld __cnfn min(float x, float y);\n"
"float2 __ovld __cnfn min(float2 x, float2 y);\n"
"float3 __ovld __cnfn min(float3 x, float3 y);\n"
"float4 __ovld __cnfn min(float4 x, float4 y);\n"
"float8 __ovld __cnfn min(float8 x, float8 y);\n"
"float16 __ovld __cnfn min(float16 x, float16 y);\n"
"float2 __ovld __cnfn min(float2 x, float y);\n"
"float3 __ovld __cnfn min(float3 x, float y);\n"
"float4 __ovld __cnfn min(float4 x, float y);\n"
"float8 __ovld __cnfn min(float8 x, float y);\n"
"float16 __ovld __cnfn min(float16 x, float y);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn min(double x, double y);\n"
"double2 __ovld __cnfn min(double2 x, double2 y);\n"
"double3 __ovld __cnfn min(double3 x, double3 y);\n"
"double4 __ovld __cnfn min(double4 x, double4 y);\n"
"double8 __ovld __cnfn min(double8 x, double8 y);\n"
"double16 __ovld __cnfn min(double16 x, double16 y);\n"
"double2 __ovld __cnfn min(double2 x, double y);\n"
"double3 __ovld __cnfn min(double3 x, double y);\n"
"double4 __ovld __cnfn min(double4 x, double y);\n"
"double8 __ovld __cnfn min(double8 x, double y);\n"
"double16 __ovld __cnfn min(double16 x, double y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn min(half x, half y);\n"
"half2 __ovld __cnfn min(half2 x, half2 y);\n"
"half3 __ovld __cnfn min(half3 x, half3 y);\n"
"half4 __ovld __cnfn min(half4 x, half4 y);\n"
"half8 __ovld __cnfn min(half8 x, half8 y);\n"
"half16 __ovld __cnfn min(half16 x, half16 y);\n"
"half2 __ovld __cnfn min(half2 x, half y);\n"
"half3 __ovld __cnfn min(half3 x, half y);\n"
"half4 __ovld __cnfn min(half4 x, half y);\n"
"half8 __ovld __cnfn min(half8 x, half y);\n"
"half16 __ovld __cnfn min(half16 x, half y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the linear blend of x & y implemented as:\n"
" * x + (y - x) * a\n"
" * a must be a value in the range 0.0 ... 1.0. If a is not\n"
" * in the range 0.0 ... 1.0, the return values are\n"
" * undefined.\n"
" */\n"
"float __ovld __cnfn mix(float x, float y, float a);\n"
"float2 __ovld __cnfn mix(float2 x, float2 y, float2 a);\n"
"float3 __ovld __cnfn mix(float3 x, float3 y, float3 a);\n"
"float4 __ovld __cnfn mix(float4 x, float4 y, float4 a);\n"
"float8 __ovld __cnfn mix(float8 x, float8 y, float8 a);\n"
"float16 __ovld __cnfn mix(float16 x, float16 y, float16 a);\n"
"float2 __ovld __cnfn mix(float2 x, float2 y, float a);\n"
"float3 __ovld __cnfn mix(float3 x, float3 y, float a);\n"
"float4 __ovld __cnfn mix(float4 x, float4 y, float a);\n"
"float8 __ovld __cnfn mix(float8 x, float8 y, float a);\n"
"float16 __ovld __cnfn mix(float16 x, float16 y, float a);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn mix(double x, double y, double a);\n"
"double2 __ovld __cnfn mix(double2 x, double2 y, double2 a);\n"
"double3 __ovld __cnfn mix(double3 x, double3 y, double3 a);\n"
"double4 __ovld __cnfn mix(double4 x, double4 y, double4 a);\n"
"double8 __ovld __cnfn mix(double8 x, double8 y, double8 a);\n"
"double16 __ovld __cnfn mix(double16 x, double16 y, double16 a);\n"
"double2 __ovld __cnfn mix(double2 x, double2 y, double a);\n"
"double3 __ovld __cnfn mix(double3 x, double3 y, double a);\n"
"double4 __ovld __cnfn mix(double4 x, double4 y, double a);\n"
"double8 __ovld __cnfn mix(double8 x, double8 y, double a);\n"
"double16 __ovld __cnfn mix(double16 x, double16 y, double a);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn mix(half x, half y, half a);\n"
"half2 __ovld __cnfn mix(half2 x, half2 y, half2 a);\n"
"half3 __ovld __cnfn mix(half3 x, half3 y, half3 a);\n"
"half4 __ovld __cnfn mix(half4 x, half4 y, half4 a);\n"
"half8 __ovld __cnfn mix(half8 x, half8 y, half8 a);\n"
"half16 __ovld __cnfn mix(half16 x, half16 y, half16 a);\n"
"half2 __ovld __cnfn mix(half2 x, half2 y, half a);\n"
"half3 __ovld __cnfn mix(half3 x, half3 y, half a);\n"
"half4 __ovld __cnfn mix(half4 x, half4 y, half a);\n"
"half8 __ovld __cnfn mix(half8 x, half8 y, half a);\n"
"half16 __ovld __cnfn mix(half16 x, half16 y, half a);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Converts degrees to radians, i.e. (PI / 180) *\n"
" * degrees.\n"
" */\n"
"float __ovld __cnfn radians(float degrees);\n"
"float2 __ovld __cnfn radians(float2 degrees);\n"
"float3 __ovld __cnfn radians(float3 degrees);\n"
"float4 __ovld __cnfn radians(float4 degrees);\n"
"float8 __ovld __cnfn radians(float8 degrees);\n"
"float16 __ovld __cnfn radians(float16 degrees);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn radians(double degrees);\n"
"double2 __ovld __cnfn radians(double2 degrees);\n"
"double3 __ovld __cnfn radians(double3 degrees);\n"
"double4 __ovld __cnfn radians(double4 degrees);\n"
"double8 __ovld __cnfn radians(double8 degrees);\n"
"double16 __ovld __cnfn radians(double16 degrees);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn radians(half degrees);\n"
"half2 __ovld __cnfn radians(half2 degrees);\n"
"half3 __ovld __cnfn radians(half3 degrees);\n"
"half4 __ovld __cnfn radians(half4 degrees);\n"
"half8 __ovld __cnfn radians(half8 degrees);\n"
"half16 __ovld __cnfn radians(half16 degrees);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns 0.0 if x < edge, otherwise it returns 1.0.\n"
" */\n"
"float __ovld __cnfn step(float edge, float x);\n"
"float2 __ovld __cnfn step(float2 edge, float2 x);\n"
"float3 __ovld __cnfn step(float3 edge, float3 x);\n"
"float4 __ovld __cnfn step(float4 edge, float4 x);\n"
"float8 __ovld __cnfn step(float8 edge, float8 x);\n"
"float16 __ovld __cnfn step(float16 edge, float16 x);\n"
"float2 __ovld __cnfn step(float edge, float2 x);\n"
"float3 __ovld __cnfn step(float edge, float3 x);\n"
"float4 __ovld __cnfn step(float edge, float4 x);\n"
"float8 __ovld __cnfn step(float edge, float8 x);\n"
"float16 __ovld __cnfn step(float edge, float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn step(double edge, double x);\n"
"double2 __ovld __cnfn step(double2 edge, double2 x);\n"
"double3 __ovld __cnfn step(double3 edge, double3 x);\n"
"double4 __ovld __cnfn step(double4 edge, double4 x);\n"
"double8 __ovld __cnfn step(double8 edge, double8 x);\n"
"double16 __ovld __cnfn step(double16 edge, double16 x);\n"
"double2 __ovld __cnfn step(double edge, double2 x);\n"
"double3 __ovld __cnfn step(double edge, double3 x);\n"
"double4 __ovld __cnfn step(double edge, double4 x);\n"
"double8 __ovld __cnfn step(double edge, double8 x);\n"
"double16 __ovld __cnfn step(double edge, double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn step(half edge, half x);\n"
"half2 __ovld __cnfn step(half2 edge, half2 x);\n"
"half3 __ovld __cnfn step(half3 edge, half3 x);\n"
"half4 __ovld __cnfn step(half4 edge, half4 x);\n"
"half8 __ovld __cnfn step(half8 edge, half8 x);\n"
"half16 __ovld __cnfn step(half16 edge, half16 x);\n"
"half __ovld __cnfn step(half edge, half x);\n"
"half2 __ovld __cnfn step(half edge, half2 x);\n"
"half3 __ovld __cnfn step(half edge, half3 x);\n"
"half4 __ovld __cnfn step(half edge, half4 x);\n"
"half8 __ovld __cnfn step(half edge, half8 x);\n"
"half16 __ovld __cnfn step(half edge, half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns 0.0 if x <= edge0 and 1.0 if x >= edge1 and\n"
" * performs smooth Hermite interpolation between 0\n"
" * and 1when edge0 < x < edge1. This is useful in\n"
" * cases where you would want a threshold function\n"
" * with a smooth transition.\n"
" * This is equivalent to:\n"
" * gentype t;\n"
" * t = clamp ((x - edge0) / (edge1 - edge0), 0, 1);\n"
" * return t * t * (3 - 2 * t);\n"
" * Results are undefined if edge0 >= edge1 or if x,\n"
" * edge0 or edge1 is a NaN.\n"
" */\n"
"float __ovld __cnfn smoothstep(float edge0, float edge1, float x);\n"
"float2 __ovld __cnfn smoothstep(float2 edge0, float2 edge1, float2 x);\n"
"float3 __ovld __cnfn smoothstep(float3 edge0, float3 edge1, float3 x);\n"
"float4 __ovld __cnfn smoothstep(float4 edge0, float4 edge1, float4 x);\n"
"float8 __ovld __cnfn smoothstep(float8 edge0, float8 edge1, float8 x);\n"
"float16 __ovld __cnfn smoothstep(float16 edge0, float16 edge1, float16 x);\n"
"float2 __ovld __cnfn smoothstep(float edge0, float edge1, float2 x);\n"
"float3 __ovld __cnfn smoothstep(float edge0, float edge1, float3 x);\n"
"float4 __ovld __cnfn smoothstep(float edge0, float edge1, float4 x);\n"
"float8 __ovld __cnfn smoothstep(float edge0, float edge1, float8 x);\n"
"float16 __ovld __cnfn smoothstep(float edge0, float edge1, float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn smoothstep(double edge0, double edge1, double x);\n"
"double2 __ovld __cnfn smoothstep(double2 edge0, double2 edge1, double2 x);\n"
"double3 __ovld __cnfn smoothstep(double3 edge0, double3 edge1, double3 x);\n"
"double4 __ovld __cnfn smoothstep(double4 edge0, double4 edge1, double4 x);\n"
"double8 __ovld __cnfn smoothstep(double8 edge0, double8 edge1, double8 x);\n"
"double16 __ovld __cnfn smoothstep(double16 edge0, double16 edge1, double16 x);\n"
"double2 __ovld __cnfn smoothstep(double edge0, double edge1, double2 x);\n"
"double3 __ovld __cnfn smoothstep(double edge0, double edge1, double3 x);\n"
"double4 __ovld __cnfn smoothstep(double edge0, double edge1, double4 x);\n"
"double8 __ovld __cnfn smoothstep(double edge0, double edge1, double8 x);\n"
"double16 __ovld __cnfn smoothstep(double edge0, double edge1, double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn smoothstep(half edge0, half edge1, half x);\n"
"half2 __ovld __cnfn smoothstep(half2 edge0, half2 edge1, half2 x);\n"
"half3 __ovld __cnfn smoothstep(half3 edge0, half3 edge1, half3 x);\n"
"half4 __ovld __cnfn smoothstep(half4 edge0, half4 edge1, half4 x);\n"
"half8 __ovld __cnfn smoothstep(half8 edge0, half8 edge1, half8 x);\n"
"half16 __ovld __cnfn smoothstep(half16 edge0, half16 edge1, half16 x);\n"
"half __ovld __cnfn smoothstep(half edge0, half edge1, half x);\n"
"half2 __ovld __cnfn smoothstep(half edge0, half edge1, half2 x);\n"
"half3 __ovld __cnfn smoothstep(half edge0, half edge1, half3 x);\n"
"half4 __ovld __cnfn smoothstep(half edge0, half edge1, half4 x);\n"
"half8 __ovld __cnfn smoothstep(half edge0, half edge1, half8 x);\n"
"half16 __ovld __cnfn smoothstep(half edge0, half edge1, half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns 1.0 if x > 0, -0.0 if x = -0.0, +0.0 if x =\n"
" * +0.0, or -1.0 if x < 0. Returns 0.0 if x is a NaN.\n"
" */\n"
"float __ovld __cnfn sign(float x);\n"
"float2 __ovld __cnfn sign(float2 x);\n"
"float3 __ovld __cnfn sign(float3 x);\n"
"float4 __ovld __cnfn sign(float4 x);\n"
"float8 __ovld __cnfn sign(float8 x);\n"
"float16 __ovld __cnfn sign(float16 x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn sign(double x);\n"
"double2 __ovld __cnfn sign(double2 x);\n"
"double3 __ovld __cnfn sign(double3 x);\n"
"double4 __ovld __cnfn sign(double4 x);\n"
"double8 __ovld __cnfn sign(double8 x);\n"
"double16 __ovld __cnfn sign(double16 x);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn sign(half x);\n"
"half2 __ovld __cnfn sign(half2 x);\n"
"half3 __ovld __cnfn sign(half3 x);\n"
"half4 __ovld __cnfn sign(half4 x);\n"
"half8 __ovld __cnfn sign(half8 x);\n"
"half16 __ovld __cnfn sign(half16 x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"// OpenCL v1.1 s6.11.5, v1.2 s6.12.5, v2.0 s6.13.5 - Geometric Functions\n"
"\n"
"/**\n"
" * Returns the cross product of p0.xyz and p1.xyz. The\n"
" * w component of float4 result returned will be 0.0.\n"
" */\n"
"float4 __ovld __cnfn cross(float4 p0, float4 p1);\n"
"float3 __ovld __cnfn cross(float3 p0, float3 p1);\n"
"#ifdef cl_khr_fp64\n"
"double4 __ovld __cnfn cross(double4 p0, double4 p1);\n"
"double3 __ovld __cnfn cross(double3 p0, double3 p1);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half4 __ovld __cnfn cross(half4 p0, half4 p1);\n"
"half3 __ovld __cnfn cross(half3 p0, half3 p1);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Compute dot product.\n"
" */\n"
"float __ovld __cnfn dot(float p0, float p1);\n"
"float __ovld __cnfn dot(float2 p0, float2 p1);\n"
"float __ovld __cnfn dot(float3 p0, float3 p1);\n"
"float __ovld __cnfn dot(float4 p0, float4 p1);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn dot(double p0, double p1);\n"
"double __ovld __cnfn dot(double2 p0, double2 p1);\n"
"double __ovld __cnfn dot(double3 p0, double3 p1);\n"
"double __ovld __cnfn dot(double4 p0, double4 p1);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn dot(half p0, half p1);\n"
"half __ovld __cnfn dot(half2 p0, half2 p1);\n"
"half __ovld __cnfn dot(half3 p0, half3 p1);\n"
"half __ovld __cnfn dot(half4 p0, half4 p1);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the distance between p0 and p1. This is\n"
" * calculated as length(p0 - p1).\n"
" */\n"
"float __ovld __cnfn distance(float p0, float p1);\n"
"float __ovld __cnfn distance(float2 p0, float2 p1);\n"
"float __ovld __cnfn distance(float3 p0, float3 p1);\n"
"float __ovld __cnfn distance(float4 p0, float4 p1);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn distance(double p0, double p1);\n"
"double __ovld __cnfn distance(double2 p0, double2 p1);\n"
"double __ovld __cnfn distance(double3 p0, double3 p1);\n"
"double __ovld __cnfn distance(double4 p0, double4 p1);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn distance(half p0, half p1);\n"
"half __ovld __cnfn distance(half2 p0, half2 p1);\n"
"half __ovld __cnfn distance(half3 p0, half3 p1);\n"
"half __ovld __cnfn distance(half4 p0, half4 p1);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Return the length of vector p, i.e.,\n"
" * sqrt(p.x2 + p.y 2 + ...)\n"
" */\n"
"float __ovld __cnfn length(float p);\n"
"float __ovld __cnfn length(float2 p);\n"
"float __ovld __cnfn length(float3 p);\n"
"float __ovld __cnfn length(float4 p);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn length(double p);\n"
"double __ovld __cnfn length(double2 p);\n"
"double __ovld __cnfn length(double3 p);\n"
"double __ovld __cnfn length(double4 p);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn length(half p);\n"
"half __ovld __cnfn length(half2 p);\n"
"half __ovld __cnfn length(half3 p);\n"
"half __ovld __cnfn length(half4 p);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns a vector in the same direction as p but with a\n"
" * length of 1.\n"
" */\n"
"float __ovld __cnfn normalize(float p);\n"
"float2 __ovld __cnfn normalize(float2 p);\n"
"float3 __ovld __cnfn normalize(float3 p);\n"
"float4 __ovld __cnfn normalize(float4 p);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn normalize(double p);\n"
"double2 __ovld __cnfn normalize(double2 p);\n"
"double3 __ovld __cnfn normalize(double3 p);\n"
"double4 __ovld __cnfn normalize(double4 p);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn normalize(half p);\n"
"half2 __ovld __cnfn normalize(half2 p);\n"
"half3 __ovld __cnfn normalize(half3 p);\n"
"half4 __ovld __cnfn normalize(half4 p);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns fast_length(p0 - p1).\n"
" */\n"
"float __ovld __cnfn fast_distance(float p0, float p1);\n"
"float __ovld __cnfn fast_distance(float2 p0, float2 p1);\n"
"float __ovld __cnfn fast_distance(float3 p0, float3 p1);\n"
"float __ovld __cnfn fast_distance(float4 p0, float4 p1);\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fast_distance(half p0, half p1);\n"
"half __ovld __cnfn fast_distance(half2 p0, half2 p1);\n"
"half __ovld __cnfn fast_distance(half3 p0, half3 p1);\n"
"half __ovld __cnfn fast_distance(half4 p0, half4 p1);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the length of vector p computed as:\n"
" * half_sqrt(p.x2 + p.y2 + ...)\n"
" */\n"
"float __ovld __cnfn fast_length(float p);\n"
"float __ovld __cnfn fast_length(float2 p);\n"
"float __ovld __cnfn fast_length(float3 p);\n"
"float __ovld __cnfn fast_length(float4 p);\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fast_length(half p);\n"
"half __ovld __cnfn fast_length(half2 p);\n"
"half __ovld __cnfn fast_length(half3 p);\n"
"half __ovld __cnfn fast_length(half4 p);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns a vector in the same direction as p but with a\n"
" * length of 1. fast_normalize is computed as:\n"
" * p * half_rsqrt (p.x^2 + p.y^2 + ... )\n"
" * The result shall be within 8192 ulps error from the\n"
" * infinitely precise result of\n"
" * if (all(p == 0.0f))\n"
" * result = p;\n"
" * else\n"
" * result = p / sqrt (p.x^2 + p.y^2 + ...);\n"
" * with the following exceptions:\n"
" * 1) If the sum of squares is greater than FLT_MAX\n"
" * then the value of the floating-point values in the\n"
" * result vector are undefined.\n"
" * 2) If the sum of squares is less than FLT_MIN then\n"
" * the implementation may return back p.\n"
" * 3) If the device is in \"denorms are flushed to zero\"\n"
" * mode, individual operand elements with magnitude\n"
" * less than sqrt(FLT_MIN) may be flushed to zero\n"
" * before proceeding with the calculation.\n"
" */\n"
"float __ovld __cnfn fast_normalize(float p);\n"
"float2 __ovld __cnfn fast_normalize(float2 p);\n"
"float3 __ovld __cnfn fast_normalize(float3 p);\n"
"float4 __ovld __cnfn fast_normalize(float4 p);\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn fast_normalize(half p);\n"
"half2 __ovld __cnfn fast_normalize(half2 p);\n"
"half3 __ovld __cnfn fast_normalize(half3 p);\n"
"half4 __ovld __cnfn fast_normalize(half4 p);\n"
"#endif //cl_khr_fp16\n"
"\n"
"// OpenCL v1.1 s6.11.6, v1.2 s6.12.6, v2.0 s6.13.6 - Relational Functions\n"
"\n"
"/**\n"
" * intn isequal (floatn x, floatn y)\n"
" * Returns the component-wise compare of x == y.\n"
" */\n"
"int __ovld __cnfn isequal(float x, float y);\n"
"int2 __ovld __cnfn isequal(float2 x, float2 y);\n"
"int3 __ovld __cnfn isequal(float3 x, float3 y);\n"
"int4 __ovld __cnfn isequal(float4 x, float4 y);\n"
"int8 __ovld __cnfn isequal(float8 x, float8 y);\n"
"int16 __ovld __cnfn isequal(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isequal(double x, double y);\n"
"long2 __ovld __cnfn isequal(double2 x, double2 y);\n"
"long3 __ovld __cnfn isequal(double3 x, double3 y);\n"
"long4 __ovld __cnfn isequal(double4 x, double4 y);\n"
"long8 __ovld __cnfn isequal(double8 x, double8 y);\n"
"long16 __ovld __cnfn isequal(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isequal(half x, half y);\n"
"short2 __ovld __cnfn isequal(half2 x, half2 y);\n"
"short3 __ovld __cnfn isequal(half3 x, half3 y);\n"
"short4 __ovld __cnfn isequal(half4 x, half4 y);\n"
"short8 __ovld __cnfn isequal(half8 x, half8 y);\n"
"short16 __ovld __cnfn isequal(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the component-wise compare of x != y.\n"
" */\n"
"int __ovld __cnfn isnotequal(float x, float y);\n"
"int2 __ovld __cnfn isnotequal(float2 x, float2 y);\n"
"int3 __ovld __cnfn isnotequal(float3 x, float3 y);\n"
"int4 __ovld __cnfn isnotequal(float4 x, float4 y);\n"
"int8 __ovld __cnfn isnotequal(float8 x, float8 y);\n"
"int16 __ovld __cnfn isnotequal(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isnotequal(double x, double y);\n"
"long2 __ovld __cnfn isnotequal(double2 x, double2 y);\n"
"long3 __ovld __cnfn isnotequal(double3 x, double3 y);\n"
"long4 __ovld __cnfn isnotequal(double4 x, double4 y);\n"
"long8 __ovld __cnfn isnotequal(double8 x, double8 y);\n"
"long16 __ovld __cnfn isnotequal(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isnotequal(half x, half y);\n"
"short2 __ovld __cnfn isnotequal(half2 x, half2 y);\n"
"short3 __ovld __cnfn isnotequal(half3 x, half3 y);\n"
"short4 __ovld __cnfn isnotequal(half4 x, half4 y);\n"
"short8 __ovld __cnfn isnotequal(half8 x, half8 y);\n"
"short16 __ovld __cnfn isnotequal(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the component-wise compare of x > y.\n"
" */\n"
"int __ovld __cnfn isgreater(float x, float y);\n"
"int2 __ovld __cnfn isgreater(float2 x, float2 y);\n"
"int3 __ovld __cnfn isgreater(float3 x, float3 y);\n"
"int4 __ovld __cnfn isgreater(float4 x, float4 y);\n"
"int8 __ovld __cnfn isgreater(float8 x, float8 y);\n"
"int16 __ovld __cnfn isgreater(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isgreater(double x, double y);\n"
"long2 __ovld __cnfn isgreater(double2 x, double2 y);\n"
"long3 __ovld __cnfn isgreater(double3 x, double3 y);\n"
"long4 __ovld __cnfn isgreater(double4 x, double4 y);\n"
"long8 __ovld __cnfn isgreater(double8 x, double8 y);\n"
"long16 __ovld __cnfn isgreater(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isgreater(half x, half y);\n"
"short2 __ovld __cnfn isgreater(half2 x, half2 y);\n"
"short3 __ovld __cnfn isgreater(half3 x, half3 y);\n"
"short4 __ovld __cnfn isgreater(half4 x, half4 y);\n"
"short8 __ovld __cnfn isgreater(half8 x, half8 y);\n"
"short16 __ovld __cnfn isgreater(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the component-wise compare of x >= y.\n"
" */\n"
"int __ovld __cnfn isgreaterequal(float x, float y);\n"
"int2 __ovld __cnfn isgreaterequal(float2 x, float2 y);\n"
"int3 __ovld __cnfn isgreaterequal(float3 x, float3 y);\n"
"int4 __ovld __cnfn isgreaterequal(float4 x, float4 y);\n"
"int8 __ovld __cnfn isgreaterequal(float8 x, float8 y);\n"
"int16 __ovld __cnfn isgreaterequal(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isgreaterequal(double x, double y);\n"
"long2 __ovld __cnfn isgreaterequal(double2 x, double2 y);\n"
"long3 __ovld __cnfn isgreaterequal(double3 x, double3 y);\n"
"long4 __ovld __cnfn isgreaterequal(double4 x, double4 y);\n"
"long8 __ovld __cnfn isgreaterequal(double8 x, double8 y);\n"
"long16 __ovld __cnfn isgreaterequal(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isgreaterequal(half x, half y);\n"
"short2 __ovld __cnfn isgreaterequal(half2 x, half2 y);\n"
"short3 __ovld __cnfn isgreaterequal(half3 x, half3 y);\n"
"short4 __ovld __cnfn isgreaterequal(half4 x, half4 y);\n"
"short8 __ovld __cnfn isgreaterequal(half8 x, half8 y);\n"
"short16 __ovld __cnfn isgreaterequal(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the component-wise compare of x < y.\n"
" */\n"
"int __ovld __cnfn isless(float x, float y);\n"
"int2 __ovld __cnfn isless(float2 x, float2 y);\n"
"int3 __ovld __cnfn isless(float3 x, float3 y);\n"
"int4 __ovld __cnfn isless(float4 x, float4 y);\n"
"int8 __ovld __cnfn isless(float8 x, float8 y);\n"
"int16 __ovld __cnfn isless(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isless(double x, double y);\n"
"long2 __ovld __cnfn isless(double2 x, double2 y);\n"
"long3 __ovld __cnfn isless(double3 x, double3 y);\n"
"long4 __ovld __cnfn isless(double4 x, double4 y);\n"
"long8 __ovld __cnfn isless(double8 x, double8 y);\n"
"long16 __ovld __cnfn isless(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isless(half x, half y);\n"
"short2 __ovld __cnfn isless(half2 x, half2 y);\n"
"short3 __ovld __cnfn isless(half3 x, half3 y);\n"
"short4 __ovld __cnfn isless(half4 x, half4 y);\n"
"short8 __ovld __cnfn isless(half8 x, half8 y);\n"
"short16 __ovld __cnfn isless(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the component-wise compare of x <= y.\n"
" */\n"
"int __ovld __cnfn islessequal(float x, float y);\n"
"int2 __ovld __cnfn islessequal(float2 x, float2 y);\n"
"int3 __ovld __cnfn islessequal(float3 x, float3 y);\n"
"int4 __ovld __cnfn islessequal(float4 x, float4 y);\n"
"int8 __ovld __cnfn islessequal(float8 x, float8 y);\n"
"int16 __ovld __cnfn islessequal(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn islessequal(double x, double y);\n"
"long2 __ovld __cnfn islessequal(double2 x, double2 y);\n"
"long3 __ovld __cnfn islessequal(double3 x, double3 y);\n"
"long4 __ovld __cnfn islessequal(double4 x, double4 y);\n"
"long8 __ovld __cnfn islessequal(double8 x, double8 y);\n"
"long16 __ovld __cnfn islessequal(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn islessequal(half x, half y);\n"
"short2 __ovld __cnfn islessequal(half2 x, half2 y);\n"
"short3 __ovld __cnfn islessequal(half3 x, half3 y);\n"
"short4 __ovld __cnfn islessequal(half4 x, half4 y);\n"
"short8 __ovld __cnfn islessequal(half8 x, half8 y);\n"
"short16 __ovld __cnfn islessequal(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns the component-wise compare of\n"
" * (x < y) || (x > y) .\n"
" */\n"
"int __ovld __cnfn islessgreater(float x, float y);\n"
"int2 __ovld __cnfn islessgreater(float2 x, float2 y);\n"
"int3 __ovld __cnfn islessgreater(float3 x, float3 y);\n"
"int4 __ovld __cnfn islessgreater(float4 x, float4 y);\n"
"int8 __ovld __cnfn islessgreater(float8 x, float8 y);\n"
"int16 __ovld __cnfn islessgreater(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn islessgreater(double x, double y);\n"
"long2 __ovld __cnfn islessgreater(double2 x, double2 y);\n"
"long3 __ovld __cnfn islessgreater(double3 x, double3 y);\n"
"long4 __ovld __cnfn islessgreater(double4 x, double4 y);\n"
"long8 __ovld __cnfn islessgreater(double8 x, double8 y);\n"
"long16 __ovld __cnfn islessgreater(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn islessgreater(half x, half y);\n"
"short2 __ovld __cnfn islessgreater(half2 x, half2 y);\n"
"short3 __ovld __cnfn islessgreater(half3 x, half3 y);\n"
"short4 __ovld __cnfn islessgreater(half4 x, half4 y);\n"
"short8 __ovld __cnfn islessgreater(half8 x, half8 y);\n"
"short16 __ovld __cnfn islessgreater(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Test for finite value.\n"
" */\n"
"int __ovld __cnfn isfinite(float);\n"
"int2 __ovld __cnfn isfinite(float2);\n"
"int3 __ovld __cnfn isfinite(float3);\n"
"int4 __ovld __cnfn isfinite(float4);\n"
"int8 __ovld __cnfn isfinite(float8);\n"
"int16 __ovld __cnfn isfinite(float16);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isfinite(double);\n"
"long2 __ovld __cnfn isfinite(double2);\n"
"long3 __ovld __cnfn isfinite(double3);\n"
"long4 __ovld __cnfn isfinite(double4);\n"
"long8 __ovld __cnfn isfinite(double8);\n"
"long16 __ovld __cnfn isfinite(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isfinite(half);\n"
"short2 __ovld __cnfn isfinite(half2);\n"
"short3 __ovld __cnfn isfinite(half3);\n"
"short4 __ovld __cnfn isfinite(half4);\n"
"short8 __ovld __cnfn isfinite(half8);\n"
"short16 __ovld __cnfn isfinite(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Test for infinity value (+ve or -ve) .\n"
" */\n"
"int __ovld __cnfn isinf(float);\n"
"int2 __ovld __cnfn isinf(float2);\n"
"int3 __ovld __cnfn isinf(float3);\n"
"int4 __ovld __cnfn isinf(float4);\n"
"int8 __ovld __cnfn isinf(float8);\n"
"int16 __ovld __cnfn isinf(float16);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isinf(double);\n"
"long2 __ovld __cnfn isinf(double2);\n"
"long3 __ovld __cnfn isinf(double3);\n"
"long4 __ovld __cnfn isinf(double4);\n"
"long8 __ovld __cnfn isinf(double8);\n"
"long16 __ovld __cnfn isinf(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isinf(half);\n"
"short2 __ovld __cnfn isinf(half2);\n"
"short3 __ovld __cnfn isinf(half3);\n"
"short4 __ovld __cnfn isinf(half4);\n"
"short8 __ovld __cnfn isinf(half8);\n"
"short16 __ovld __cnfn isinf(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Test for a NaN.\n"
" */\n"
"int __ovld __cnfn isnan(float);\n"
"int2 __ovld __cnfn isnan(float2);\n"
"int3 __ovld __cnfn isnan(float3);\n"
"int4 __ovld __cnfn isnan(float4);\n"
"int8 __ovld __cnfn isnan(float8);\n"
"int16 __ovld __cnfn isnan(float16);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isnan(double);\n"
"long2 __ovld __cnfn isnan(double2);\n"
"long3 __ovld __cnfn isnan(double3);\n"
"long4 __ovld __cnfn isnan(double4);\n"
"long8 __ovld __cnfn isnan(double8);\n"
"long16 __ovld __cnfn isnan(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isnan(half);\n"
"short2 __ovld __cnfn isnan(half2);\n"
"short3 __ovld __cnfn isnan(half3);\n"
"short4 __ovld __cnfn isnan(half4);\n"
"short8 __ovld __cnfn isnan(half8);\n"
"short16 __ovld __cnfn isnan(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Test for a normal value.\n"
" */\n"
"int __ovld __cnfn isnormal(float);\n"
"int2 __ovld __cnfn isnormal(float2);\n"
"int3 __ovld __cnfn isnormal(float3);\n"
"int4 __ovld __cnfn isnormal(float4);\n"
"int8 __ovld __cnfn isnormal(float8);\n"
"int16 __ovld __cnfn isnormal(float16);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isnormal(double);\n"
"long2 __ovld __cnfn isnormal(double2);\n"
"long3 __ovld __cnfn isnormal(double3);\n"
"long4 __ovld __cnfn isnormal(double4);\n"
"long8 __ovld __cnfn isnormal(double8);\n"
"long16 __ovld __cnfn isnormal(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isnormal(half);\n"
"short2 __ovld __cnfn isnormal(half2);\n"
"short3 __ovld __cnfn isnormal(half3);\n"
"short4 __ovld __cnfn isnormal(half4);\n"
"short8 __ovld __cnfn isnormal(half8);\n"
"short16 __ovld __cnfn isnormal(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Test if arguments are ordered. isordered() takes\n"
" * arguments x and y, and returns the result\n"
" * isequal(x, x) && isequal(y, y).\n"
" */\n"
"int __ovld __cnfn isordered(float x, float y);\n"
"int2 __ovld __cnfn isordered(float2 x, float2 y);\n"
"int3 __ovld __cnfn isordered(float3 x, float3 y);\n"
"int4 __ovld __cnfn isordered(float4 x, float4 y);\n"
"int8 __ovld __cnfn isordered(float8 x, float8 y);\n"
"int16 __ovld __cnfn isordered(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isordered(double x, double y);\n"
"long2 __ovld __cnfn isordered(double2 x, double2 y);\n"
"long3 __ovld __cnfn isordered(double3 x, double3 y);\n"
"long4 __ovld __cnfn isordered(double4 x, double4 y);\n"
"long8 __ovld __cnfn isordered(double8 x, double8 y);\n"
"long16 __ovld __cnfn isordered(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isordered(half x, half y);\n"
"short2 __ovld __cnfn isordered(half2 x, half2 y);\n"
"short3 __ovld __cnfn isordered(half3 x, half3 y);\n"
"short4 __ovld __cnfn isordered(half4 x, half4 y);\n"
"short8 __ovld __cnfn isordered(half8 x, half8 y);\n"
"short16 __ovld __cnfn isordered(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Test if arguments are unordered. isunordered()\n"
" * takes arguments x and y, returning non-zero if x or y\n"
" * is NaN, and zero otherwise.\n"
" */\n"
"int __ovld __cnfn isunordered(float x, float y);\n"
"int2 __ovld __cnfn isunordered(float2 x, float2 y);\n"
"int3 __ovld __cnfn isunordered(float3 x, float3 y);\n"
"int4 __ovld __cnfn isunordered(float4 x, float4 y);\n"
"int8 __ovld __cnfn isunordered(float8 x, float8 y);\n"
"int16 __ovld __cnfn isunordered(float16 x, float16 y);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn isunordered(double x, double y);\n"
"long2 __ovld __cnfn isunordered(double2 x, double2 y);\n"
"long3 __ovld __cnfn isunordered(double3 x, double3 y);\n"
"long4 __ovld __cnfn isunordered(double4 x, double4 y);\n"
"long8 __ovld __cnfn isunordered(double8 x, double8 y);\n"
"long16 __ovld __cnfn isunordered(double16 x, double16 y);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn isunordered(half x, half y);\n"
"short2 __ovld __cnfn isunordered(half2 x, half2 y);\n"
"short3 __ovld __cnfn isunordered(half3 x, half3 y);\n"
"short4 __ovld __cnfn isunordered(half4 x, half4 y);\n"
"short8 __ovld __cnfn isunordered(half8 x, half8 y);\n"
"short16 __ovld __cnfn isunordered(half16 x, half16 y);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Test for sign bit. The scalar version of the function\n"
" * returns a 1 if the sign bit in the float is set else returns\n"
" * 0. The vector version of the function returns the\n"
" * following for each component in floatn: a -1 if the\n"
" * sign bit in the float is set else returns 0.\n"
" */\n"
"int __ovld __cnfn signbit(float);\n"
"int2 __ovld __cnfn signbit(float2);\n"
"int3 __ovld __cnfn signbit(float3);\n"
"int4 __ovld __cnfn signbit(float4);\n"
"int8 __ovld __cnfn signbit(float8);\n"
"int16 __ovld __cnfn signbit(float16);\n"
"#ifdef cl_khr_fp64\n"
"int __ovld __cnfn signbit(double);\n"
"long2 __ovld __cnfn signbit(double2);\n"
"long3 __ovld __cnfn signbit(double3);\n"
"long4 __ovld __cnfn signbit(double4);\n"
"long8 __ovld __cnfn signbit(double8);\n"
"long16 __ovld __cnfn signbit(double16);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"int __ovld __cnfn signbit(half);\n"
"short2 __ovld __cnfn signbit(half2);\n"
"short3 __ovld __cnfn signbit(half3);\n"
"short4 __ovld __cnfn signbit(half4);\n"
"short8 __ovld __cnfn signbit(half8);\n"
"short16 __ovld __cnfn signbit(half16);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Returns 1 if the most significant bit in any component\n"
" * of x is set; otherwise returns 0.\n"
" */\n"
"int __ovld __cnfn any(char x);\n"
"int __ovld __cnfn any(char2 x);\n"
"int __ovld __cnfn any(char3 x);\n"
"int __ovld __cnfn any(char4 x);\n"
"int __ovld __cnfn any(char8 x);\n"
"int __ovld __cnfn any(char16 x);\n"
"int __ovld __cnfn any(short x);\n"
"int __ovld __cnfn any(short2 x);\n"
"int __ovld __cnfn any(short3 x);\n"
"int __ovld __cnfn any(short4 x);\n"
"int __ovld __cnfn any(short8 x);\n"
"int __ovld __cnfn any(short16 x);\n"
"int __ovld __cnfn any(int x);\n"
"int __ovld __cnfn any(int2 x);\n"
"int __ovld __cnfn any(int3 x);\n"
"int __ovld __cnfn any(int4 x);\n"
"int __ovld __cnfn any(int8 x);\n"
"int __ovld __cnfn any(int16 x);\n"
"int __ovld __cnfn any(long x);\n"
"int __ovld __cnfn any(long2 x);\n"
"int __ovld __cnfn any(long3 x);\n"
"int __ovld __cnfn any(long4 x);\n"
"int __ovld __cnfn any(long8 x);\n"
"int __ovld __cnfn any(long16 x);\n"
"\n"
"/**\n"
" * Returns 1 if the most significant bit in all components\n"
" * of x is set; otherwise returns 0.\n"
" */\n"
"int __ovld __cnfn all(char x);\n"
"int __ovld __cnfn all(char2 x);\n"
"int __ovld __cnfn all(char3 x);\n"
"int __ovld __cnfn all(char4 x);\n"
"int __ovld __cnfn all(char8 x);\n"
"int __ovld __cnfn all(char16 x);\n"
"int __ovld __cnfn all(short x);\n"
"int __ovld __cnfn all(short2 x);\n"
"int __ovld __cnfn all(short3 x);\n"
"int __ovld __cnfn all(short4 x);\n"
"int __ovld __cnfn all(short8 x);\n"
"int __ovld __cnfn all(short16 x);\n"
"int __ovld __cnfn all(int x);\n"
"int __ovld __cnfn all(int2 x);\n"
"int __ovld __cnfn all(int3 x);\n"
"int __ovld __cnfn all(int4 x);\n"
"int __ovld __cnfn all(int8 x);\n"
"int __ovld __cnfn all(int16 x);\n"
"int __ovld __cnfn all(long x);\n"
"int __ovld __cnfn all(long2 x);\n"
"int __ovld __cnfn all(long3 x);\n"
"int __ovld __cnfn all(long4 x);\n"
"int __ovld __cnfn all(long8 x);\n"
"int __ovld __cnfn all(long16 x);\n"
"\n"
"/**\n"
" * Each bit of the result is the corresponding bit of a if\n"
" * the corresponding bit of c is 0. Otherwise it is the\n"
" * corresponding bit of b.\n"
" */\n"
"char __ovld __cnfn bitselect(char a, char b, char c);\n"
"uchar __ovld __cnfn bitselect(uchar a, uchar b, uchar c);\n"
"char2 __ovld __cnfn bitselect(char2 a, char2 b, char2 c);\n"
"uchar2 __ovld __cnfn bitselect(uchar2 a, uchar2 b, uchar2 c);\n"
"char3 __ovld __cnfn bitselect(char3 a, char3 b, char3 c);\n"
"uchar3 __ovld __cnfn bitselect(uchar3 a, uchar3 b, uchar3 c);\n"
"char4 __ovld __cnfn bitselect(char4 a, char4 b, char4 c);\n"
"uchar4 __ovld __cnfn bitselect(uchar4 a, uchar4 b, uchar4 c);\n"
"char8 __ovld __cnfn bitselect(char8 a, char8 b, char8 c);\n"
"uchar8 __ovld __cnfn bitselect(uchar8 a, uchar8 b, uchar8 c);\n"
"char16 __ovld __cnfn bitselect(char16 a, char16 b, char16 c);\n"
"uchar16 __ovld __cnfn bitselect(uchar16 a, uchar16 b, uchar16 c);\n"
"short __ovld __cnfn bitselect(short a, short b, short c);\n"
"ushort __ovld __cnfn bitselect(ushort a, ushort b, ushort c);\n"
"short2 __ovld __cnfn bitselect(short2 a, short2 b, short2 c);\n"
"ushort2 __ovld __cnfn bitselect(ushort2 a, ushort2 b, ushort2 c);\n"
"short3 __ovld __cnfn bitselect(short3 a, short3 b, short3 c);\n"
"ushort3 __ovld __cnfn bitselect(ushort3 a, ushort3 b, ushort3 c);\n"
"short4 __ovld __cnfn bitselect(short4 a, short4 b, short4 c);\n"
"ushort4 __ovld __cnfn bitselect(ushort4 a, ushort4 b, ushort4 c);\n"
"short8 __ovld __cnfn bitselect(short8 a, short8 b, short8 c);\n"
"ushort8 __ovld __cnfn bitselect(ushort8 a, ushort8 b, ushort8 c);\n"
"short16 __ovld __cnfn bitselect(short16 a, short16 b, short16 c);\n"
"ushort16 __ovld __cnfn bitselect(ushort16 a, ushort16 b, ushort16 c);\n"
"int __ovld __cnfn bitselect(int a, int b, int c);\n"
"uint __ovld __cnfn bitselect(uint a, uint b, uint c);\n"
"int2 __ovld __cnfn bitselect(int2 a, int2 b, int2 c);\n"
"uint2 __ovld __cnfn bitselect(uint2 a, uint2 b, uint2 c);\n"
"int3 __ovld __cnfn bitselect(int3 a, int3 b, int3 c);\n"
"uint3 __ovld __cnfn bitselect(uint3 a, uint3 b, uint3 c);\n"
"int4 __ovld __cnfn bitselect(int4 a, int4 b, int4 c);\n"
"uint4 __ovld __cnfn bitselect(uint4 a, uint4 b, uint4 c);\n"
"int8 __ovld __cnfn bitselect(int8 a, int8 b, int8 c);\n"
"uint8 __ovld __cnfn bitselect(uint8 a, uint8 b, uint8 c);\n"
"int16 __ovld __cnfn bitselect(int16 a, int16 b, int16 c);\n"
"uint16 __ovld __cnfn bitselect(uint16 a, uint16 b, uint16 c);\n"
"long __ovld __cnfn bitselect(long a, long b, long c);\n"
"ulong __ovld __cnfn bitselect(ulong a, ulong b, ulong c);\n"
"long2 __ovld __cnfn bitselect(long2 a, long2 b, long2 c);\n"
"ulong2 __ovld __cnfn bitselect(ulong2 a, ulong2 b, ulong2 c);\n"
"long3 __ovld __cnfn bitselect(long3 a, long3 b, long3 c);\n"
"ulong3 __ovld __cnfn bitselect(ulong3 a, ulong3 b, ulong3 c);\n"
"long4 __ovld __cnfn bitselect(long4 a, long4 b, long4 c);\n"
"ulong4 __ovld __cnfn bitselect(ulong4 a, ulong4 b, ulong4 c);\n"
"long8 __ovld __cnfn bitselect(long8 a, long8 b, long8 c);\n"
"ulong8 __ovld __cnfn bitselect(ulong8 a, ulong8 b, ulong8 c);\n"
"long16 __ovld __cnfn bitselect(long16 a, long16 b, long16 c);\n"
"ulong16 __ovld __cnfn bitselect(ulong16 a, ulong16 b, ulong16 c);\n"
"float __ovld __cnfn bitselect(float a, float b, float c);\n"
"float2 __ovld __cnfn bitselect(float2 a, float2 b, float2 c);\n"
"float3 __ovld __cnfn bitselect(float3 a, float3 b, float3 c);\n"
"float4 __ovld __cnfn bitselect(float4 a, float4 b, float4 c);\n"
"float8 __ovld __cnfn bitselect(float8 a, float8 b, float8 c);\n"
"float16 __ovld __cnfn bitselect(float16 a, float16 b, float16 c);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn bitselect(double a, double b, double c);\n"
"double2 __ovld __cnfn bitselect(double2 a, double2 b, double2 c);\n"
"double3 __ovld __cnfn bitselect(double3 a, double3 b, double3 c);\n"
"double4 __ovld __cnfn bitselect(double4 a, double4 b, double4 c);\n"
"double8 __ovld __cnfn bitselect(double8 a, double8 b, double8 c);\n"
"double16 __ovld __cnfn bitselect(double16 a, double16 b, double16 c);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn bitselect(half a, half b, half c);\n"
"half2 __ovld __cnfn bitselect(half2 a, half2 b, half2 c);\n"
"half3 __ovld __cnfn bitselect(half3 a, half3 b, half3 c);\n"
"half4 __ovld __cnfn bitselect(half4 a, half4 b, half4 c);\n"
"half8 __ovld __cnfn bitselect(half8 a, half8 b, half8 c);\n"
"half16 __ovld __cnfn bitselect(half16 a, half16 b, half16 c);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * For each component of a vector type,\n"
" * result[i] = if MSB of c[i] is set ? b[i] : a[i].\n"
" * For a scalar type, result = c ? b : a.\n"
" * b and a must have the same type.\n"
" * c must have the same number of elements and bits as a.\n"
" */\n"
"char __ovld __cnfn select(char a, char b, char c);\n"
"uchar __ovld __cnfn select(uchar a, uchar b, char c);\n"
"char2 __ovld __cnfn select(char2 a, char2 b, char2 c);\n"
"uchar2 __ovld __cnfn select(uchar2 a, uchar2 b, char2 c);\n"
"char3 __ovld __cnfn select(char3 a, char3 b, char3 c);\n"
"uchar3 __ovld __cnfn select(uchar3 a, uchar3 b, char3 c);\n"
"char4 __ovld __cnfn select(char4 a, char4 b, char4 c);\n"
"uchar4 __ovld __cnfn select(uchar4 a, uchar4 b, char4 c);\n"
"char8 __ovld __cnfn select(char8 a, char8 b, char8 c);\n"
"uchar8 __ovld __cnfn select(uchar8 a, uchar8 b, char8 c);\n"
"char16 __ovld __cnfn select(char16 a, char16 b, char16 c);\n"
"uchar16 __ovld __cnfn select(uchar16 a, uchar16 b, char16 c);\n"
"\n"
"short __ovld __cnfn select(short a, short b, short c);\n"
"ushort __ovld __cnfn select(ushort a, ushort b, short c);\n"
"short2 __ovld __cnfn select(short2 a, short2 b, short2 c);\n"
"ushort2 __ovld __cnfn select(ushort2 a, ushort2 b, short2 c);\n"
"short3 __ovld __cnfn select(short3 a, short3 b, short3 c);\n"
"ushort3 __ovld __cnfn select(ushort3 a, ushort3 b, short3 c);\n"
"short4 __ovld __cnfn select(short4 a, short4 b, short4 c);\n"
"ushort4 __ovld __cnfn select(ushort4 a, ushort4 b, short4 c);\n"
"short8 __ovld __cnfn select(short8 a, short8 b, short8 c);\n"
"ushort8 __ovld __cnfn select(ushort8 a, ushort8 b, short8 c);\n"
"short16 __ovld __cnfn select(short16 a, short16 b, short16 c);\n"
"ushort16 __ovld __cnfn select(ushort16 a, ushort16 b, short16 c);\n"
"\n"
"int __ovld __cnfn select(int a, int b, int c);\n"
"uint __ovld __cnfn select(uint a, uint b, int c);\n"
"int2 __ovld __cnfn select(int2 a, int2 b, int2 c);\n"
"uint2 __ovld __cnfn select(uint2 a, uint2 b, int2 c);\n"
"int3 __ovld __cnfn select(int3 a, int3 b, int3 c);\n"
"uint3 __ovld __cnfn select(uint3 a, uint3 b, int3 c);\n"
"int4 __ovld __cnfn select(int4 a, int4 b, int4 c);\n"
"uint4 __ovld __cnfn select(uint4 a, uint4 b, int4 c);\n"
"int8 __ovld __cnfn select(int8 a, int8 b, int8 c);\n"
"uint8 __ovld __cnfn select(uint8 a, uint8 b, int8 c);\n"
"int16 __ovld __cnfn select(int16 a, int16 b, int16 c);\n"
"uint16 __ovld __cnfn select(uint16 a, uint16 b, int16 c);\n"
"float __ovld __cnfn select(float a, float b, int c);\n"
"float2 __ovld __cnfn select(float2 a, float2 b, int2 c);\n"
"float3 __ovld __cnfn select(float3 a, float3 b, int3 c);\n"
"float4 __ovld __cnfn select(float4 a, float4 b, int4 c);\n"
"float8 __ovld __cnfn select(float8 a, float8 b, int8 c);\n"
"float16 __ovld __cnfn select(float16 a, float16 b, int16 c);\n"
"\n"
"long __ovld __cnfn select(long a, long b, long c);\n"
"ulong __ovld __cnfn select(ulong a, ulong b, long c);\n"
"long2 __ovld __cnfn select(long2 a, long2 b, long2 c);\n"
"ulong2 __ovld __cnfn select(ulong2 a, ulong2 b, long2 c);\n"
"long3 __ovld __cnfn select(long3 a, long3 b, long3 c);\n"
"ulong3 __ovld __cnfn select(ulong3 a, ulong3 b, long3 c);\n"
"long4 __ovld __cnfn select(long4 a, long4 b, long4 c);\n"
"ulong4 __ovld __cnfn select(ulong4 a, ulong4 b, long4 c);\n"
"long8 __ovld __cnfn select(long8 a, long8 b, long8 c);\n"
"ulong8 __ovld __cnfn select(ulong8 a, ulong8 b, long8 c);\n"
"long16 __ovld __cnfn select(long16 a, long16 b, long16 c);\n"
"ulong16 __ovld __cnfn select(ulong16 a, ulong16 b, long16 c);\n"
"\n"
"char __ovld __cnfn select(char a, char b, uchar c);\n"
"uchar __ovld __cnfn select(uchar a, uchar b, uchar c);\n"
"char2 __ovld __cnfn select(char2 a, char2 b, uchar2 c);\n"
"uchar2 __ovld __cnfn select(uchar2 a, uchar2 b, uchar2 c);\n"
"char3 __ovld __cnfn select(char3 a, char3 b, uchar3 c);\n"
"uchar3 __ovld __cnfn select(uchar3 a, uchar3 b, uchar3 c);\n"
"char4 __ovld __cnfn select(char4 a, char4 b, uchar4 c);\n"
"uchar4 __ovld __cnfn select(uchar4 a, uchar4 b, uchar4 c);\n"
"char8 __ovld __cnfn select(char8 a, char8 b, uchar8 c);\n"
"uchar8 __ovld __cnfn select(uchar8 a, uchar8 b, uchar8 c);\n"
"char16 __ovld __cnfn select(char16 a, char16 b, uchar16 c);\n"
"uchar16 __ovld __cnfn select(uchar16 a, uchar16 b, uchar16 c);\n"
"\n"
"short __ovld __cnfn select(short a, short b, ushort c);\n"
"ushort __ovld __cnfn select(ushort a, ushort b, ushort c);\n"
"short2 __ovld __cnfn select(short2 a, short2 b, ushort2 c);\n"
"ushort2 __ovld __cnfn select(ushort2 a, ushort2 b, ushort2 c);\n"
"short3 __ovld __cnfn select(short3 a, short3 b, ushort3 c);\n"
"ushort3 __ovld __cnfn select(ushort3 a, ushort3 b, ushort3 c);\n"
"short4 __ovld __cnfn select(short4 a, short4 b, ushort4 c);\n"
"ushort4 __ovld __cnfn select(ushort4 a, ushort4 b, ushort4 c);\n"
"short8 __ovld __cnfn select(short8 a, short8 b, ushort8 c);\n"
"ushort8 __ovld __cnfn select(ushort8 a, ushort8 b, ushort8 c);\n"
"short16 __ovld __cnfn select(short16 a, short16 b, ushort16 c);\n"
"ushort16 __ovld __cnfn select(ushort16 a, ushort16 b, ushort16 c);\n"
"\n"
"int __ovld __cnfn select(int a, int b, uint c);\n"
"uint __ovld __cnfn select(uint a, uint b, uint c);\n"
"int2 __ovld __cnfn select(int2 a, int2 b, uint2 c);\n"
"uint2 __ovld __cnfn select(uint2 a, uint2 b, uint2 c);\n"
"int3 __ovld __cnfn select(int3 a, int3 b, uint3 c);\n"
"uint3 __ovld __cnfn select(uint3 a, uint3 b, uint3 c);\n"
"int4 __ovld __cnfn select(int4 a, int4 b, uint4 c);\n"
"uint4 __ovld __cnfn select(uint4 a, uint4 b, uint4 c);\n"
"int8 __ovld __cnfn select(int8 a, int8 b, uint8 c);\n"
"uint8 __ovld __cnfn select(uint8 a, uint8 b, uint8 c);\n"
"int16 __ovld __cnfn select(int16 a, int16 b, uint16 c);\n"
"uint16 __ovld __cnfn select(uint16 a, uint16 b, uint16 c);\n"
"float __ovld __cnfn select(float a, float b, uint c);\n"
"float2 __ovld __cnfn select(float2 a, float2 b, uint2 c);\n"
"float3 __ovld __cnfn select(float3 a, float3 b, uint3 c);\n"
"float4 __ovld __cnfn select(float4 a, float4 b, uint4 c);\n"
"float8 __ovld __cnfn select(float8 a, float8 b, uint8 c);\n"
"float16 __ovld __cnfn select(float16 a, float16 b, uint16 c);\n"
"\n"
"long __ovld __cnfn select(long a, long b, ulong c);\n"
"ulong __ovld __cnfn select(ulong a, ulong b, ulong c);\n"
"long2 __ovld __cnfn select(long2 a, long2 b, ulong2 c);\n"
"ulong2 __ovld __cnfn select(ulong2 a, ulong2 b, ulong2 c);\n"
"long3 __ovld __cnfn select(long3 a, long3 b, ulong3 c);\n"
"ulong3 __ovld __cnfn select(ulong3 a, ulong3 b, ulong3 c);\n"
"long4 __ovld __cnfn select(long4 a, long4 b, ulong4 c);\n"
"ulong4 __ovld __cnfn select(ulong4 a, ulong4 b, ulong4 c);\n"
"long8 __ovld __cnfn select(long8 a, long8 b, ulong8 c);\n"
"ulong8 __ovld __cnfn select(ulong8 a, ulong8 b, ulong8 c);\n"
"long16 __ovld __cnfn select(long16 a, long16 b, ulong16 c);\n"
"ulong16 __ovld __cnfn select(ulong16 a, ulong16 b, ulong16 c);\n"
"\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __cnfn select(double a, double b, long c);\n"
"double2 __ovld __cnfn select(double2 a, double2 b, long2 c);\n"
"double3 __ovld __cnfn select(double3 a, double3 b, long3 c);\n"
"double4 __ovld __cnfn select(double4 a, double4 b, long4 c);\n"
"double8 __ovld __cnfn select(double8 a, double8 b, long8 c);\n"
"double16 __ovld __cnfn select(double16 a, double16 b, long16 c);\n"
"double __ovld __cnfn select(double a, double b, ulong c);\n"
"double2 __ovld __cnfn select(double2 a, double2 b, ulong2 c);\n"
"double3 __ovld __cnfn select(double3 a, double3 b, ulong3 c);\n"
"double4 __ovld __cnfn select(double4 a, double4 b, ulong4 c);\n"
"double8 __ovld __cnfn select(double8 a, double8 b, ulong8 c);\n"
"double16 __ovld __cnfn select(double16 a, double16 b, ulong16 c);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __cnfn select(half a, half b, short c);\n"
"half2 __ovld __cnfn select(half2 a, half2 b, short2 c);\n"
"half3 __ovld __cnfn select(half3 a, half3 b, short3 c);\n"
"half4 __ovld __cnfn select(half4 a, half4 b, short4 c);\n"
"half8 __ovld __cnfn select(half8 a, half8 b, short8 c);\n"
"half16 __ovld __cnfn select(half16 a, half16 b, short16 c);\n"
"half __ovld __cnfn select(half a, half b, ushort c);\n"
"half2 __ovld __cnfn select(half2 a, half2 b, ushort2 c);\n"
"half3 __ovld __cnfn select(half3 a, half3 b, ushort3 c);\n"
"half4 __ovld __cnfn select(half4 a, half4 b, ushort4 c);\n"
"half8 __ovld __cnfn select(half8 a, half8 b, ushort8 c);\n"
"half16 __ovld __cnfn select(half16 a, half16 b, ushort16 c);\n"
"#endif //cl_khr_fp16\n"
"\n"
"// OpenCL v1.1 s6.11.7, v1.2 s6.12.7, v2.0 s6.13.7 - Vector Data Load and Store Functions\n"
"// OpenCL extensions v1.1 s9.6.6, v1.2 s9.5.6, v2.0 s9.4.6 - Vector Data Load and Store Functions for Half Type\n"
"/**\n"
" * Use generic type gentype to indicate the built-in data types\n"
" * char, uchar, short, ushort, int, uint, long, ulong, float,\n"
" * double or half.\n"
" *\n"
" * vloadn return sizeof (gentypen) bytes of data read from address (p + (offset * n)).\n"
" *\n"
" * vstoren write sizeof (gentypen) bytes given by data to address (p + (offset * n)).\n"
" *\n"
" * The address computed as (p + (offset * n)) must be\n"
" * 8-bit aligned if gentype is char, uchar;\n"
" * 16-bit aligned if gentype is short, ushort, half;\n"
" * 32-bit aligned if gentype is int, uint, float;\n"
" * 64-bit aligned if gentype is long, ulong, double.\n"
" */\n"
"\n"
"char2 __ovld vload2(size_t offset, const __constant char *p);\n"
"uchar2 __ovld vload2(size_t offset, const __constant uchar *p);\n"
"short2 __ovld vload2(size_t offset, const __constant short *p);\n"
"ushort2 __ovld vload2(size_t offset, const __constant ushort *p);\n"
"int2 __ovld vload2(size_t offset, const __constant int *p);\n"
"uint2 __ovld vload2(size_t offset, const __constant uint *p);\n"
"long2 __ovld vload2(size_t offset, const __constant long *p);\n"
"ulong2 __ovld vload2(size_t offset, const __constant ulong *p);\n"
"float2 __ovld vload2(size_t offset, const __constant float *p);\n"
"char3 __ovld vload3(size_t offset, const __constant char *p);\n"
"uchar3 __ovld vload3(size_t offset, const __constant uchar *p);\n"
"short3 __ovld vload3(size_t offset, const __constant short *p);\n"
"ushort3 __ovld vload3(size_t offset, const __constant ushort *p);\n"
"int3 __ovld vload3(size_t offset, const __constant int *p);\n"
"uint3 __ovld vload3(size_t offset, const __constant uint *p);\n"
"long3 __ovld vload3(size_t offset, const __constant long *p);\n"
"ulong3 __ovld vload3(size_t offset, const __constant ulong *p);\n"
"float3 __ovld vload3(size_t offset, const __constant float *p);\n"
"char4 __ovld vload4(size_t offset, const __constant char *p);\n"
"uchar4 __ovld vload4(size_t offset, const __constant uchar *p);\n"
"short4 __ovld vload4(size_t offset, const __constant short *p);\n"
"ushort4 __ovld vload4(size_t offset, const __constant ushort *p);\n"
"int4 __ovld vload4(size_t offset, const __constant int *p);\n"
"uint4 __ovld vload4(size_t offset, const __constant uint *p);\n"
"long4 __ovld vload4(size_t offset, const __constant long *p);\n"
"ulong4 __ovld vload4(size_t offset, const __constant ulong *p);\n"
"float4 __ovld vload4(size_t offset, const __constant float *p);\n"
"char8 __ovld vload8(size_t offset, const __constant char *p);\n"
"uchar8 __ovld vload8(size_t offset, const __constant uchar *p);\n"
"short8 __ovld vload8(size_t offset, const __constant short *p);\n"
"ushort8 __ovld vload8(size_t offset, const __constant ushort *p);\n"
"int8 __ovld vload8(size_t offset, const __constant int *p);\n"
"uint8 __ovld vload8(size_t offset, const __constant uint *p);\n"
"long8 __ovld vload8(size_t offset, const __constant long *p);\n"
"ulong8 __ovld vload8(size_t offset, const __constant ulong *p);\n"
"float8 __ovld vload8(size_t offset, const __constant float *p);\n"
"char16 __ovld vload16(size_t offset, const __constant char *p);\n"
"uchar16 __ovld vload16(size_t offset, const __constant uchar *p);\n"
"short16 __ovld vload16(size_t offset, const __constant short *p);\n"
"ushort16 __ovld vload16(size_t offset, const __constant ushort *p);\n"
"int16 __ovld vload16(size_t offset, const __constant int *p);\n"
"uint16 __ovld vload16(size_t offset, const __constant uint *p);\n"
"long16 __ovld vload16(size_t offset, const __constant long *p);\n"
"ulong16 __ovld vload16(size_t offset, const __constant ulong *p);\n"
"float16 __ovld vload16(size_t offset, const __constant float *p);\n"
"#ifdef cl_khr_fp64\n"
"double2 __ovld vload2(size_t offset, const __constant double *p);\n"
"double3 __ovld vload3(size_t offset, const __constant double *p);\n"
"double4 __ovld vload4(size_t offset, const __constant double *p);\n"
"double8 __ovld vload8(size_t offset, const __constant double *p);\n"
"double16 __ovld vload16(size_t offset, const __constant double *p);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half __ovld vload(size_t offset, const __constant half *p);\n"
"half2 __ovld vload2(size_t offset, const __constant half *p);\n"
"half3 __ovld vload3(size_t offset, const __constant half *p);\n"
"half4 __ovld vload4(size_t offset, const __constant half *p);\n"
"half8 __ovld vload8(size_t offset, const __constant half *p);\n"
"half16 __ovld vload16(size_t offset, const __constant half *p);\n"
"#endif //cl_khr_fp16\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"char2 __ovld vload2(size_t offset, const char *p);\n"
"uchar2 __ovld vload2(size_t offset, const uchar *p);\n"
"short2 __ovld vload2(size_t offset, const short *p);\n"
"ushort2 __ovld vload2(size_t offset, const ushort *p);\n"
"int2 __ovld vload2(size_t offset, const int *p);\n"
"uint2 __ovld vload2(size_t offset, const uint *p);\n"
"long2 __ovld vload2(size_t offset, const long *p);\n"
"ulong2 __ovld vload2(size_t offset, const ulong *p);\n"
"float2 __ovld vload2(size_t offset, const float *p);\n"
"char3 __ovld vload3(size_t offset, const char *p);\n"
"uchar3 __ovld vload3(size_t offset, const uchar *p);\n"
"short3 __ovld vload3(size_t offset, const short *p);\n"
"ushort3 __ovld vload3(size_t offset, const ushort *p);\n"
"int3 __ovld vload3(size_t offset, const int *p);\n"
"uint3 __ovld vload3(size_t offset, const uint *p);\n"
"long3 __ovld vload3(size_t offset, const long *p);\n"
"ulong3 __ovld vload3(size_t offset, const ulong *p);\n"
"float3 __ovld vload3(size_t offset, const float *p);\n"
"char4 __ovld vload4(size_t offset, const char *p);\n"
"uchar4 __ovld vload4(size_t offset, const uchar *p);\n"
"short4 __ovld vload4(size_t offset, const short *p);\n"
"ushort4 __ovld vload4(size_t offset, const ushort *p);\n"
"int4 __ovld vload4(size_t offset, const int *p);\n"
"uint4 __ovld vload4(size_t offset, const uint *p);\n"
"long4 __ovld vload4(size_t offset, const long *p);\n"
"ulong4 __ovld vload4(size_t offset, const ulong *p);\n"
"float4 __ovld vload4(size_t offset, const float *p);\n"
"char8 __ovld vload8(size_t offset, const char *p);\n"
"uchar8 __ovld vload8(size_t offset, const uchar *p);\n"
"short8 __ovld vload8(size_t offset, const short *p);\n"
"ushort8 __ovld vload8(size_t offset, const ushort *p);\n"
"int8 __ovld vload8(size_t offset, const int *p);\n"
"uint8 __ovld vload8(size_t offset, const uint *p);\n"
"long8 __ovld vload8(size_t offset, const long *p);\n"
"ulong8 __ovld vload8(size_t offset, const ulong *p);\n"
"float8 __ovld vload8(size_t offset, const float *p);\n"
"char16 __ovld vload16(size_t offset, const char *p);\n"
"uchar16 __ovld vload16(size_t offset, const uchar *p);\n"
"short16 __ovld vload16(size_t offset, const short *p);\n"
"ushort16 __ovld vload16(size_t offset, const ushort *p);\n"
"int16 __ovld vload16(size_t offset, const int *p);\n"
"uint16 __ovld vload16(size_t offset, const uint *p);\n"
"long16 __ovld vload16(size_t offset, const long *p);\n"
"ulong16 __ovld vload16(size_t offset, const ulong *p);\n"
"float16 __ovld vload16(size_t offset, const float *p);\n"
"\n"
"#ifdef cl_khr_fp64\n"
"double2 __ovld vload2(size_t offset, const double *p);\n"
"double3 __ovld vload3(size_t offset, const double *p);\n"
"double4 __ovld vload4(size_t offset, const double *p);\n"
"double8 __ovld vload8(size_t offset, const double *p);\n"
"double16 __ovld vload16(size_t offset, const double *p);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half __ovld vload(size_t offset, const half *p);\n"
"half2 __ovld vload2(size_t offset, const half *p);\n"
"half3 __ovld vload3(size_t offset, const half *p);\n"
"half4 __ovld vload4(size_t offset, const half *p);\n"
"half8 __ovld vload8(size_t offset, const half *p);\n"
"half16 __ovld vload16(size_t offset, const half *p);\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"char2 __ovld vload2(size_t offset, const __global char *p);\n"
"uchar2 __ovld vload2(size_t offset, const __global uchar *p);\n"
"short2 __ovld vload2(size_t offset, const __global short *p);\n"
"ushort2 __ovld vload2(size_t offset, const __global ushort *p);\n"
"int2 __ovld vload2(size_t offset, const __global int *p);\n"
"uint2 __ovld vload2(size_t offset, const __global uint *p);\n"
"long2 __ovld vload2(size_t offset, const __global long *p);\n"
"ulong2 __ovld vload2(size_t offset, const __global ulong *p);\n"
"float2 __ovld vload2(size_t offset, const __global float *p);\n"
"char3 __ovld vload3(size_t offset, const __global char *p);\n"
"uchar3 __ovld vload3(size_t offset, const __global uchar *p);\n"
"short3 __ovld vload3(size_t offset, const __global short *p);\n"
"ushort3 __ovld vload3(size_t offset, const __global ushort *p);\n"
"int3 __ovld vload3(size_t offset, const __global int *p);\n"
"uint3 __ovld vload3(size_t offset, const __global uint *p);\n"
"long3 __ovld vload3(size_t offset, const __global long *p);\n"
"ulong3 __ovld vload3(size_t offset, const __global ulong *p);\n"
"float3 __ovld vload3(size_t offset, const __global float *p);\n"
"char4 __ovld vload4(size_t offset, const __global char *p);\n"
"uchar4 __ovld vload4(size_t offset, const __global uchar *p);\n"
"short4 __ovld vload4(size_t offset, const __global short *p);\n"
"ushort4 __ovld vload4(size_t offset, const __global ushort *p);\n"
"int4 __ovld vload4(size_t offset, const __global int *p);\n"
"uint4 __ovld vload4(size_t offset, const __global uint *p);\n"
"long4 __ovld vload4(size_t offset, const __global long *p);\n"
"ulong4 __ovld vload4(size_t offset, const __global ulong *p);\n"
"float4 __ovld vload4(size_t offset, const __global float *p);\n"
"char8 __ovld vload8(size_t offset, const __global char *p);\n"
"uchar8 __ovld vload8(size_t offset, const __global uchar *p);\n"
"short8 __ovld vload8(size_t offset, const __global short *p);\n"
"ushort8 __ovld vload8(size_t offset, const __global ushort *p);\n"
"int8 __ovld vload8(size_t offset, const __global int *p);\n"
"uint8 __ovld vload8(size_t offset, const __global uint *p);\n"
"long8 __ovld vload8(size_t offset, const __global long *p);\n"
"ulong8 __ovld vload8(size_t offset, const __global ulong *p);\n"
"float8 __ovld vload8(size_t offset, const __global float *p);\n"
"char16 __ovld vload16(size_t offset, const __global char *p);\n"
"uchar16 __ovld vload16(size_t offset, const __global uchar *p);\n"
"short16 __ovld vload16(size_t offset, const __global short *p);\n"
"ushort16 __ovld vload16(size_t offset, const __global ushort *p);\n"
"int16 __ovld vload16(size_t offset, const __global int *p);\n"
"uint16 __ovld vload16(size_t offset, const __global uint *p);\n"
"long16 __ovld vload16(size_t offset, const __global long *p);\n"
"ulong16 __ovld vload16(size_t offset, const __global ulong *p);\n"
"float16 __ovld vload16(size_t offset, const __global float *p);\n"
"char2 __ovld vload2(size_t offset, const __local char *p);\n"
"uchar2 __ovld vload2(size_t offset, const __local uchar *p);\n"
"short2 __ovld vload2(size_t offset, const __local short *p);\n"
"ushort2 __ovld vload2(size_t offset, const __local ushort *p);\n"
"int2 __ovld vload2(size_t offset, const __local int *p);\n"
"uint2 __ovld vload2(size_t offset, const __local uint *p);\n"
"long2 __ovld vload2(size_t offset, const __local long *p);\n"
"ulong2 __ovld vload2(size_t offset, const __local ulong *p);\n"
"float2 __ovld vload2(size_t offset, const __local float *p);\n"
"char3 __ovld vload3(size_t offset, const __local char *p);\n"
"uchar3 __ovld vload3(size_t offset, const __local uchar *p);\n"
"short3 __ovld vload3(size_t offset, const __local short *p);\n"
"ushort3 __ovld vload3(size_t offset, const __local ushort *p);\n"
"int3 __ovld vload3(size_t offset, const __local int *p);\n"
"uint3 __ovld vload3(size_t offset, const __local uint *p);\n"
"long3 __ovld vload3(size_t offset, const __local long *p);\n"
"ulong3 __ovld vload3(size_t offset, const __local ulong *p);\n"
"float3 __ovld vload3(size_t offset, const __local float *p);\n"
"char4 __ovld vload4(size_t offset, const __local char *p);\n"
"uchar4 __ovld vload4(size_t offset, const __local uchar *p);\n"
"short4 __ovld vload4(size_t offset, const __local short *p);\n"
"ushort4 __ovld vload4(size_t offset, const __local ushort *p);\n"
"int4 __ovld vload4(size_t offset, const __local int *p);\n"
"uint4 __ovld vload4(size_t offset, const __local uint *p);\n"
"long4 __ovld vload4(size_t offset, const __local long *p);\n"
"ulong4 __ovld vload4(size_t offset, const __local ulong *p);\n"
"float4 __ovld vload4(size_t offset, const __local float *p);\n"
"char8 __ovld vload8(size_t offset, const __local char *p);\n"
"uchar8 __ovld vload8(size_t offset, const __local uchar *p);\n"
"short8 __ovld vload8(size_t offset, const __local short *p);\n"
"ushort8 __ovld vload8(size_t offset, const __local ushort *p);\n"
"int8 __ovld vload8(size_t offset, const __local int *p);\n"
"uint8 __ovld vload8(size_t offset, const __local uint *p);\n"
"long8 __ovld vload8(size_t offset, const __local long *p);\n"
"ulong8 __ovld vload8(size_t offset, const __local ulong *p);\n"
"float8 __ovld vload8(size_t offset, const __local float *p);\n"
"char16 __ovld vload16(size_t offset, const __local char *p);\n"
"uchar16 __ovld vload16(size_t offset, const __local uchar *p);\n"
"short16 __ovld vload16(size_t offset, const __local short *p);\n"
"ushort16 __ovld vload16(size_t offset, const __local ushort *p);\n"
"int16 __ovld vload16(size_t offset, const __local int *p);\n"
"uint16 __ovld vload16(size_t offset, const __local uint *p);\n"
"long16 __ovld vload16(size_t offset, const __local long *p);\n"
"ulong16 __ovld vload16(size_t offset, const __local ulong *p);\n"
"float16 __ovld vload16(size_t offset, const __local float *p);\n"
"char2 __ovld vload2(size_t offset, const __private char *p);\n"
"uchar2 __ovld vload2(size_t offset, const __private uchar *p);\n"
"short2 __ovld vload2(size_t offset, const __private short *p);\n"
"ushort2 __ovld vload2(size_t offset, const __private ushort *p);\n"
"int2 __ovld vload2(size_t offset, const __private int *p);\n"
"uint2 __ovld vload2(size_t offset, const __private uint *p);\n"
"long2 __ovld vload2(size_t offset, const __private long *p);\n"
"ulong2 __ovld vload2(size_t offset, const __private ulong *p);\n"
"float2 __ovld vload2(size_t offset, const __private float *p);\n"
"char3 __ovld vload3(size_t offset, const __private char *p);\n"
"uchar3 __ovld vload3(size_t offset, const __private uchar *p);\n"
"short3 __ovld vload3(size_t offset, const __private short *p);\n"
"ushort3 __ovld vload3(size_t offset, const __private ushort *p);\n"
"int3 __ovld vload3(size_t offset, const __private int *p);\n"
"uint3 __ovld vload3(size_t offset, const __private uint *p);\n"
"long3 __ovld vload3(size_t offset, const __private long *p);\n"
"ulong3 __ovld vload3(size_t offset, const __private ulong *p);\n"
"float3 __ovld vload3(size_t offset, const __private float *p);\n"
"char4 __ovld vload4(size_t offset, const __private char *p);\n"
"uchar4 __ovld vload4(size_t offset, const __private uchar *p);\n"
"short4 __ovld vload4(size_t offset, const __private short *p);\n"
"ushort4 __ovld vload4(size_t offset, const __private ushort *p);\n"
"int4 __ovld vload4(size_t offset, const __private int *p);\n"
"uint4 __ovld vload4(size_t offset, const __private uint *p);\n"
"long4 __ovld vload4(size_t offset, const __private long *p);\n"
"ulong4 __ovld vload4(size_t offset, const __private ulong *p);\n"
"float4 __ovld vload4(size_t offset, const __private float *p);\n"
"char8 __ovld vload8(size_t offset, const __private char *p);\n"
"uchar8 __ovld vload8(size_t offset, const __private uchar *p);\n"
"short8 __ovld vload8(size_t offset, const __private short *p);\n"
"ushort8 __ovld vload8(size_t offset, const __private ushort *p);\n"
"int8 __ovld vload8(size_t offset, const __private int *p);\n"
"uint8 __ovld vload8(size_t offset, const __private uint *p);\n"
"long8 __ovld vload8(size_t offset, const __private long *p);\n"
"ulong8 __ovld vload8(size_t offset, const __private ulong *p);\n"
"float8 __ovld vload8(size_t offset, const __private float *p);\n"
"char16 __ovld vload16(size_t offset, const __private char *p);\n"
"uchar16 __ovld vload16(size_t offset, const __private uchar *p);\n"
"short16 __ovld vload16(size_t offset, const __private short *p);\n"
"ushort16 __ovld vload16(size_t offset, const __private ushort *p);\n"
"int16 __ovld vload16(size_t offset, const __private int *p);\n"
"uint16 __ovld vload16(size_t offset, const __private uint *p);\n"
"long16 __ovld vload16(size_t offset, const __private long *p);\n"
"ulong16 __ovld vload16(size_t offset, const __private ulong *p);\n"
"float16 __ovld vload16(size_t offset, const __private float *p);\n"
"\n"
"#ifdef cl_khr_fp64\n"
"double2 __ovld vload2(size_t offset, const __global double *p);\n"
"double3 __ovld vload3(size_t offset, const __global double *p);\n"
"double4 __ovld vload4(size_t offset, const __global double *p);\n"
"double8 __ovld vload8(size_t offset, const __global double *p);\n"
"double16 __ovld vload16(size_t offset, const __global double *p);\n"
"double2 __ovld vload2(size_t offset, const __local double *p);\n"
"double3 __ovld vload3(size_t offset, const __local double *p);\n"
"double4 __ovld vload4(size_t offset, const __local double *p);\n"
"double8 __ovld vload8(size_t offset, const __local double *p);\n"
"double16 __ovld vload16(size_t offset, const __local double *p);\n"
"double2 __ovld vload2(size_t offset, const __private double *p);\n"
"double3 __ovld vload3(size_t offset, const __private double *p);\n"
"double4 __ovld vload4(size_t offset, const __private double *p);\n"
"double8 __ovld vload8(size_t offset, const __private double *p);\n"
"double16 __ovld vload16(size_t offset, const __private double *p);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half __ovld vload(size_t offset, const __global half *p);\n"
"half2 __ovld vload2(size_t offset, const __global half *p);\n"
"half3 __ovld vload3(size_t offset, const __global half *p);\n"
"half4 __ovld vload4(size_t offset, const __global half *p);\n"
"half8 __ovld vload8(size_t offset, const __global half *p);\n"
"half16 __ovld vload16(size_t offset, const __global half *p);\n"
"half __ovld vload(size_t offset, const __local half *p);\n"
"half2 __ovld vload2(size_t offset, const __local half *p);\n"
"half3 __ovld vload3(size_t offset, const __local half *p);\n"
"half4 __ovld vload4(size_t offset, const __local half *p);\n"
"half8 __ovld vload8(size_t offset, const __local half *p);\n"
"half16 __ovld vload16(size_t offset, const __local half *p);\n"
"half __ovld vload(size_t offset, const __private half *p);\n"
"half2 __ovld vload2(size_t offset, const __private half *p);\n"
"half3 __ovld vload3(size_t offset, const __private half *p);\n"
"half4 __ovld vload4(size_t offset, const __private half *p);\n"
"half8 __ovld vload8(size_t offset, const __private half *p);\n"
"half16 __ovld vload16(size_t offset, const __private half *p);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"void __ovld vstore2(char2 data, size_t offset, char *p);\n"
"void __ovld vstore2(uchar2 data, size_t offset, uchar *p);\n"
"void __ovld vstore2(short2 data, size_t offset, short *p);\n"
"void __ovld vstore2(ushort2 data, size_t offset, ushort *p);\n"
"void __ovld vstore2(int2 data, size_t offset, int *p);\n"
"void __ovld vstore2(uint2 data, size_t offset, uint *p);\n"
"void __ovld vstore2(long2 data, size_t offset, long *p);\n"
"void __ovld vstore2(ulong2 data, size_t offset, ulong *p);\n"
"void __ovld vstore2(float2 data, size_t offset, float *p);\n"
"void __ovld vstore3(char3 data, size_t offset, char *p);\n"
"void __ovld vstore3(uchar3 data, size_t offset, uchar *p);\n"
"void __ovld vstore3(short3 data, size_t offset, short *p);\n"
"void __ovld vstore3(ushort3 data, size_t offset, ushort *p);\n"
"void __ovld vstore3(int3 data, size_t offset, int *p);\n"
"void __ovld vstore3(uint3 data, size_t offset, uint *p);\n"
"void __ovld vstore3(long3 data, size_t offset, long *p);\n"
"void __ovld vstore3(ulong3 data, size_t offset, ulong *p);\n"
"void __ovld vstore3(float3 data, size_t offset, float *p);\n"
"void __ovld vstore4(char4 data, size_t offset, char *p);\n"
"void __ovld vstore4(uchar4 data, size_t offset, uchar *p);\n"
"void __ovld vstore4(short4 data, size_t offset, short *p);\n"
"void __ovld vstore4(ushort4 data, size_t offset, ushort *p);\n"
"void __ovld vstore4(int4 data, size_t offset, int *p);\n"
"void __ovld vstore4(uint4 data, size_t offset, uint *p);\n"
"void __ovld vstore4(long4 data, size_t offset, long *p);\n"
"void __ovld vstore4(ulong4 data, size_t offset, ulong *p);\n"
"void __ovld vstore4(float4 data, size_t offset, float *p);\n"
"void __ovld vstore8(char8 data, size_t offset, char *p);\n"
"void __ovld vstore8(uchar8 data, size_t offset, uchar *p);\n"
"void __ovld vstore8(short8 data, size_t offset, short *p);\n"
"void __ovld vstore8(ushort8 data, size_t offset, ushort *p);\n"
"void __ovld vstore8(int8 data, size_t offset, int *p);\n"
"void __ovld vstore8(uint8 data, size_t offset, uint *p);\n"
"void __ovld vstore8(long8 data, size_t offset, long *p);\n"
"void __ovld vstore8(ulong8 data, size_t offset, ulong *p);\n"
"void __ovld vstore8(float8 data, size_t offset, float *p);\n"
"void __ovld vstore16(char16 data, size_t offset, char *p);\n"
"void __ovld vstore16(uchar16 data, size_t offset, uchar *p);\n"
"void __ovld vstore16(short16 data, size_t offset, short *p);\n"
"void __ovld vstore16(ushort16 data, size_t offset, ushort *p);\n"
"void __ovld vstore16(int16 data, size_t offset, int *p);\n"
"void __ovld vstore16(uint16 data, size_t offset, uint *p);\n"
"void __ovld vstore16(long16 data, size_t offset, long *p);\n"
"void __ovld vstore16(ulong16 data, size_t offset, ulong *p);\n"
"void __ovld vstore16(float16 data, size_t offset, float *p);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstore2(double2 data, size_t offset, double *p);\n"
"void __ovld vstore3(double3 data, size_t offset, double *p);\n"
"void __ovld vstore4(double4 data, size_t offset, double *p);\n"
"void __ovld vstore8(double8 data, size_t offset, double *p);\n"
"void __ovld vstore16(double16 data, size_t offset, double *p);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"void __ovld vstore(half data, size_t offset, half *p);\n"
"void __ovld vstore2(half2 data, size_t offset, half *p);\n"
"void __ovld vstore3(half3 data, size_t offset, half *p);\n"
"void __ovld vstore4(half4 data, size_t offset, half *p);\n"
"void __ovld vstore8(half8 data, size_t offset, half *p);\n"
"void __ovld vstore16(half16 data, size_t offset, half *p);\n"
"#endif //cl_khr_fp16\n"
"#else\n"
"void __ovld vstore2(char2 data, size_t offset, __global char *p);\n"
"void __ovld vstore2(uchar2 data, size_t offset, __global uchar *p);\n"
"void __ovld vstore2(short2 data, size_t offset, __global short *p);\n"
"void __ovld vstore2(ushort2 data, size_t offset, __global ushort *p);\n"
"void __ovld vstore2(int2 data, size_t offset, __global int *p);\n"
"void __ovld vstore2(uint2 data, size_t offset, __global uint *p);\n"
"void __ovld vstore2(long2 data, size_t offset, __global long *p);\n"
"void __ovld vstore2(ulong2 data, size_t offset, __global ulong *p);\n"
"void __ovld vstore2(float2 data, size_t offset, __global float *p);\n"
"void __ovld vstore3(char3 data, size_t offset, __global char *p);\n"
"void __ovld vstore3(uchar3 data, size_t offset, __global uchar *p);\n"
"void __ovld vstore3(short3 data, size_t offset, __global short *p);\n"
"void __ovld vstore3(ushort3 data, size_t offset, __global ushort *p);\n"
"void __ovld vstore3(int3 data, size_t offset, __global int *p);\n"
"void __ovld vstore3(uint3 data, size_t offset, __global uint *p);\n"
"void __ovld vstore3(long3 data, size_t offset, __global long *p);\n"
"void __ovld vstore3(ulong3 data, size_t offset, __global ulong *p);\n"
"void __ovld vstore3(float3 data, size_t offset, __global float *p);\n"
"void __ovld vstore4(char4 data, size_t offset, __global char *p);\n"
"void __ovld vstore4(uchar4 data, size_t offset, __global uchar *p);\n"
"void __ovld vstore4(short4 data, size_t offset, __global short *p);\n"
"void __ovld vstore4(ushort4 data, size_t offset, __global ushort *p);\n"
"void __ovld vstore4(int4 data, size_t offset, __global int *p);\n"
"void __ovld vstore4(uint4 data, size_t offset, __global uint *p);\n"
"void __ovld vstore4(long4 data, size_t offset, __global long *p);\n"
"void __ovld vstore4(ulong4 data, size_t offset, __global ulong *p);\n"
"void __ovld vstore4(float4 data, size_t offset, __global float *p);\n"
"void __ovld vstore8(char8 data, size_t offset, __global char *p);\n"
"void __ovld vstore8(uchar8 data, size_t offset, __global uchar *p);\n"
"void __ovld vstore8(short8 data, size_t offset, __global short *p);\n"
"void __ovld vstore8(ushort8 data, size_t offset, __global ushort *p);\n"
"void __ovld vstore8(int8 data, size_t offset, __global int *p);\n"
"void __ovld vstore8(uint8 data, size_t offset, __global uint *p);\n"
"void __ovld vstore8(long8 data, size_t offset, __global long *p);\n"
"void __ovld vstore8(ulong8 data, size_t offset, __global ulong *p);\n"
"void __ovld vstore8(float8 data, size_t offset, __global float *p);\n"
"void __ovld vstore16(char16 data, size_t offset, __global char *p);\n"
"void __ovld vstore16(uchar16 data, size_t offset, __global uchar *p);\n"
"void __ovld vstore16(short16 data, size_t offset, __global short *p);\n"
"void __ovld vstore16(ushort16 data, size_t offset, __global ushort *p);\n"
"void __ovld vstore16(int16 data, size_t offset, __global int *p);\n"
"void __ovld vstore16(uint16 data, size_t offset, __global uint *p);\n"
"void __ovld vstore16(long16 data, size_t offset, __global long *p);\n"
"void __ovld vstore16(ulong16 data, size_t offset, __global ulong *p);\n"
"void __ovld vstore16(float16 data, size_t offset, __global float *p);\n"
"void __ovld vstore2(char2 data, size_t offset, __local char *p);\n"
"void __ovld vstore2(uchar2 data, size_t offset, __local uchar *p);\n"
"void __ovld vstore2(short2 data, size_t offset, __local short *p);\n"
"void __ovld vstore2(ushort2 data, size_t offset, __local ushort *p);\n"
"void __ovld vstore2(int2 data, size_t offset, __local int *p);\n"
"void __ovld vstore2(uint2 data, size_t offset, __local uint *p);\n"
"void __ovld vstore2(long2 data, size_t offset, __local long *p);\n"
"void __ovld vstore2(ulong2 data, size_t offset, __local ulong *p);\n"
"void __ovld vstore2(float2 data, size_t offset, __local float *p);\n"
"void __ovld vstore3(char3 data, size_t offset, __local char *p);\n"
"void __ovld vstore3(uchar3 data, size_t offset, __local uchar *p);\n"
"void __ovld vstore3(short3 data, size_t offset, __local short *p);\n"
"void __ovld vstore3(ushort3 data, size_t offset, __local ushort *p);\n"
"void __ovld vstore3(int3 data, size_t offset, __local int *p);\n"
"void __ovld vstore3(uint3 data, size_t offset, __local uint *p);\n"
"void __ovld vstore3(long3 data, size_t offset, __local long *p);\n"
"void __ovld vstore3(ulong3 data, size_t offset, __local ulong *p);\n"
"void __ovld vstore3(float3 data, size_t offset, __local float *p);\n"
"void __ovld vstore4(char4 data, size_t offset, __local char *p);\n"
"void __ovld vstore4(uchar4 data, size_t offset, __local uchar *p);\n"
"void __ovld vstore4(short4 data, size_t offset, __local short *p);\n"
"void __ovld vstore4(ushort4 data, size_t offset, __local ushort *p);\n"
"void __ovld vstore4(int4 data, size_t offset, __local int *p);\n"
"void __ovld vstore4(uint4 data, size_t offset, __local uint *p);\n"
"void __ovld vstore4(long4 data, size_t offset, __local long *p);\n"
"void __ovld vstore4(ulong4 data, size_t offset, __local ulong *p);\n"
"void __ovld vstore4(float4 data, size_t offset, __local float *p);\n"
"void __ovld vstore8(char8 data, size_t offset, __local char *p);\n"
"void __ovld vstore8(uchar8 data, size_t offset, __local uchar *p);\n"
"void __ovld vstore8(short8 data, size_t offset, __local short *p);\n"
"void __ovld vstore8(ushort8 data, size_t offset, __local ushort *p);\n"
"void __ovld vstore8(int8 data, size_t offset, __local int *p);\n"
"void __ovld vstore8(uint8 data, size_t offset, __local uint *p);\n"
"void __ovld vstore8(long8 data, size_t offset, __local long *p);\n"
"void __ovld vstore8(ulong8 data, size_t offset, __local ulong *p);\n"
"void __ovld vstore8(float8 data, size_t offset, __local float *p);\n"
"void __ovld vstore16(char16 data, size_t offset, __local char *p);\n"
"void __ovld vstore16(uchar16 data, size_t offset, __local uchar *p);\n"
"void __ovld vstore16(short16 data, size_t offset, __local short *p);\n"
"void __ovld vstore16(ushort16 data, size_t offset, __local ushort *p);\n"
"void __ovld vstore16(int16 data, size_t offset, __local int *p);\n"
"void __ovld vstore16(uint16 data, size_t offset, __local uint *p);\n"
"void __ovld vstore16(long16 data, size_t offset, __local long *p);\n"
"void __ovld vstore16(ulong16 data, size_t offset, __local ulong *p);\n"
"void __ovld vstore16(float16 data, size_t offset, __local float *p);\n"
"void __ovld vstore2(char2 data, size_t offset, __private char *p);\n"
"void __ovld vstore2(uchar2 data, size_t offset, __private uchar *p);\n"
"void __ovld vstore2(short2 data, size_t offset, __private short *p);\n"
"void __ovld vstore2(ushort2 data, size_t offset, __private ushort *p);\n"
"void __ovld vstore2(int2 data, size_t offset, __private int *p);\n"
"void __ovld vstore2(uint2 data, size_t offset, __private uint *p);\n"
"void __ovld vstore2(long2 data, size_t offset, __private long *p);\n"
"void __ovld vstore2(ulong2 data, size_t offset, __private ulong *p);\n"
"void __ovld vstore2(float2 data, size_t offset, __private float *p);\n"
"void __ovld vstore3(char3 data, size_t offset, __private char *p);\n"
"void __ovld vstore3(uchar3 data, size_t offset, __private uchar *p);\n"
"void __ovld vstore3(short3 data, size_t offset, __private short *p);\n"
"void __ovld vstore3(ushort3 data, size_t offset, __private ushort *p);\n"
"void __ovld vstore3(int3 data, size_t offset, __private int *p);\n"
"void __ovld vstore3(uint3 data, size_t offset, __private uint *p);\n"
"void __ovld vstore3(long3 data, size_t offset, __private long *p);\n"
"void __ovld vstore3(ulong3 data, size_t offset, __private ulong *p);\n"
"void __ovld vstore3(float3 data, size_t offset, __private float *p);\n"
"void __ovld vstore4(char4 data, size_t offset, __private char *p);\n"
"void __ovld vstore4(uchar4 data, size_t offset, __private uchar *p);\n"
"void __ovld vstore4(short4 data, size_t offset, __private short *p);\n"
"void __ovld vstore4(ushort4 data, size_t offset, __private ushort *p);\n"
"void __ovld vstore4(int4 data, size_t offset, __private int *p);\n"
"void __ovld vstore4(uint4 data, size_t offset, __private uint *p);\n"
"void __ovld vstore4(long4 data, size_t offset, __private long *p);\n"
"void __ovld vstore4(ulong4 data, size_t offset, __private ulong *p);\n"
"void __ovld vstore4(float4 data, size_t offset, __private float *p);\n"
"void __ovld vstore8(char8 data, size_t offset, __private char *p);\n"
"void __ovld vstore8(uchar8 data, size_t offset, __private uchar *p);\n"
"void __ovld vstore8(short8 data, size_t offset, __private short *p);\n"
"void __ovld vstore8(ushort8 data, size_t offset, __private ushort *p);\n"
"void __ovld vstore8(int8 data, size_t offset, __private int *p);\n"
"void __ovld vstore8(uint8 data, size_t offset, __private uint *p);\n"
"void __ovld vstore8(long8 data, size_t offset, __private long *p);\n"
"void __ovld vstore8(ulong8 data, size_t offset, __private ulong *p);\n"
"void __ovld vstore8(float8 data, size_t offset, __private float *p);\n"
"void __ovld vstore16(char16 data, size_t offset, __private char *p);\n"
"void __ovld vstore16(uchar16 data, size_t offset, __private uchar *p);\n"
"void __ovld vstore16(short16 data, size_t offset, __private short *p);\n"
"void __ovld vstore16(ushort16 data, size_t offset, __private ushort *p);\n"
"void __ovld vstore16(int16 data, size_t offset, __private int *p);\n"
"void __ovld vstore16(uint16 data, size_t offset, __private uint *p);\n"
"void __ovld vstore16(long16 data, size_t offset, __private long *p);\n"
"void __ovld vstore16(ulong16 data, size_t offset, __private ulong *p);\n"
"void __ovld vstore16(float16 data, size_t offset, __private float *p);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstore2(double2 data, size_t offset, __global double *p);\n"
"void __ovld vstore3(double3 data, size_t offset, __global double *p);\n"
"void __ovld vstore4(double4 data, size_t offset, __global double *p);\n"
"void __ovld vstore8(double8 data, size_t offset, __global double *p);\n"
"void __ovld vstore16(double16 data, size_t offset, __global double *p);\n"
"void __ovld vstore2(double2 data, size_t offset, __local double *p);\n"
"void __ovld vstore3(double3 data, size_t offset, __local double *p);\n"
"void __ovld vstore4(double4 data, size_t offset, __local double *p);\n"
"void __ovld vstore8(double8 data, size_t offset, __local double *p);\n"
"void __ovld vstore16(double16 data, size_t offset, __local double *p);\n"
"void __ovld vstore2(double2 data, size_t offset, __private double *p);\n"
"void __ovld vstore3(double3 data, size_t offset, __private double *p);\n"
"void __ovld vstore4(double4 data, size_t offset, __private double *p);\n"
"void __ovld vstore8(double8 data, size_t offset, __private double *p);\n"
"void __ovld vstore16(double16 data, size_t offset, __private double *p);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"void __ovld vstore(half data, size_t offset, __global half *p);\n"
"void __ovld vstore2(half2 data, size_t offset, __global half *p);\n"
"void __ovld vstore3(half3 data, size_t offset, __global half *p);\n"
"void __ovld vstore4(half4 data, size_t offset, __global half *p);\n"
"void __ovld vstore8(half8 data, size_t offset, __global half *p);\n"
"void __ovld vstore16(half16 data, size_t offset, __global half *p);\n"
"void __ovld vstore(half data, size_t offset, __local half *p);\n"
"void __ovld vstore2(half2 data, size_t offset, __local half *p);\n"
"void __ovld vstore3(half3 data, size_t offset, __local half *p);\n"
"void __ovld vstore4(half4 data, size_t offset, __local half *p);\n"
"void __ovld vstore8(half8 data, size_t offset, __local half *p);\n"
"void __ovld vstore16(half16 data, size_t offset, __local half *p);\n"
"void __ovld vstore(half data, size_t offset, __private half *p);\n"
"void __ovld vstore2(half2 data, size_t offset, __private half *p);\n"
"void __ovld vstore3(half3 data, size_t offset, __private half *p);\n"
"void __ovld vstore4(half4 data, size_t offset, __private half *p);\n"
"void __ovld vstore8(half8 data, size_t offset, __private half *p);\n"
"void __ovld vstore16(half16 data, size_t offset, __private half *p);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Read sizeof (half) bytes of data from address\n"
" * (p + offset). The data read is interpreted as a\n"
" * half value. The half value is converted to a\n"
" * float value and the float value is returned.\n"
" * The read address computed as (p + offset)\n"
" * must be 16-bit aligned.\n"
" */\n"
"float __ovld vload_half(size_t offset, const __constant half *p);\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld vload_half(size_t offset, const half *p);\n"
"#else\n"
"float __ovld vload_half(size_t offset, const __global half *p);\n"
"float __ovld vload_half(size_t offset, const __local half *p);\n"
"float __ovld vload_half(size_t offset, const __private half *p);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Read sizeof (halfn) bytes of data from address\n"
" * (p + (offset * n)). The data read is interpreted\n"
" * as a halfn value. The halfn value read is\n"
" * converted to a floatn value and the floatn\n"
" * value is returned. The read address computed\n"
" * as (p + (offset * n)) must be 16-bit aligned.\n"
" */\n"
"float2 __ovld vload_half2(size_t offset, const __constant half *p);\n"
"float3 __ovld vload_half3(size_t offset, const __constant half *p);\n"
"float4 __ovld vload_half4(size_t offset, const __constant half *p);\n"
"float8 __ovld vload_half8(size_t offset, const __constant half *p);\n"
"float16 __ovld vload_half16(size_t offset, const __constant half *p);\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float2 __ovld vload_half2(size_t offset, const half *p);\n"
"float3 __ovld vload_half3(size_t offset, const half *p);\n"
"float4 __ovld vload_half4(size_t offset, const half *p);\n"
"float8 __ovld vload_half8(size_t offset, const half *p);\n"
"float16 __ovld vload_half16(size_t offset, const half *p);\n"
"#else\n"
"float2 __ovld vload_half2(size_t offset, const __global half *p);\n"
"float3 __ovld vload_half3(size_t offset, const __global half *p);\n"
"float4 __ovld vload_half4(size_t offset, const __global half *p);\n"
"float8 __ovld vload_half8(size_t offset, const __global half *p);\n"
"float16 __ovld vload_half16(size_t offset, const __global half *p);\n"
"float2 __ovld vload_half2(size_t offset, const __local half *p);\n"
"float3 __ovld vload_half3(size_t offset, const __local half *p);\n"
"float4 __ovld vload_half4(size_t offset, const __local half *p);\n"
"float8 __ovld vload_half8(size_t offset, const __local half *p);\n"
"float16 __ovld vload_half16(size_t offset, const __local half *p);\n"
"float2 __ovld vload_half2(size_t offset, const __private half *p);\n"
"float3 __ovld vload_half3(size_t offset, const __private half *p);\n"
"float4 __ovld vload_half4(size_t offset, const __private half *p);\n"
"float8 __ovld vload_half8(size_t offset, const __private half *p);\n"
"float16 __ovld vload_half16(size_t offset, const __private half *p);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * The float value given by data is first\n"
" * converted to a half value using the appropriate\n"
" * rounding mode. The half value is then written\n"
" * to address computed as (p + offset). The\n"
" * address computed as (p + offset) must be 16-\n"
" * bit aligned.\n"
" * vstore_half use the current rounding mode.\n"
" * The default current rounding mode is round to\n"
" * nearest even.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"void __ovld vstore_half(float data, size_t offset, half *p);\n"
"void __ovld vstore_half_rte(float data, size_t offset, half *p);\n"
"void __ovld vstore_half_rtz(float data, size_t offset, half *p);\n"
"void __ovld vstore_half_rtp(float data, size_t offset, half *p);\n"
"void __ovld vstore_half_rtn(float data, size_t offset, half *p);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstore_half(double data, size_t offset, half *p);\n"
"void __ovld vstore_half_rte(double data, size_t offset, half *p);\n"
"void __ovld vstore_half_rtz(double data, size_t offset, half *p);\n"
"void __ovld vstore_half_rtp(double data, size_t offset, half *p);\n"
"void __ovld vstore_half_rtn(double data, size_t offset, half *p);\n"
"#endif //cl_khr_fp64\n"
"#else\n"
"void __ovld vstore_half(float data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rte(float data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rtz(float data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rtp(float data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rtn(float data, size_t offset, __global half *p);\n"
"void __ovld vstore_half(float data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rte(float data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rtz(float data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rtp(float data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rtn(float data, size_t offset, __local half *p);\n"
"void __ovld vstore_half(float data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rte(float data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rtz(float data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rtp(float data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rtn(float data, size_t offset, __private half *p);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstore_half(double data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rte(double data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rtz(double data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rtp(double data, size_t offset, __global half *p);\n"
"void __ovld vstore_half_rtn(double data, size_t offset, __global half *p);\n"
"void __ovld vstore_half(double data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rte(double data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rtz(double data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rtp(double data, size_t offset, __local half *p);\n"
"void __ovld vstore_half_rtn(double data, size_t offset, __local half *p);\n"
"void __ovld vstore_half(double data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rte(double data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rtz(double data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rtp(double data, size_t offset, __private half *p);\n"
"void __ovld vstore_half_rtn(double data, size_t offset, __private half *p);\n"
"#endif //cl_khr_fp64\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * The floatn value given by data is converted to\n"
" * a halfn value using the appropriate rounding\n"
" * mode. The halfn value is then written to\n"
" * address computed as (p + (offset * n)). The\n"
" * address computed as (p + (offset * n)) must be\n"
" * 16-bit aligned.\n"
" * vstore_halfn uses the current rounding mode.\n"
" * The default current rounding mode is round to\n"
" * nearest even.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"void __ovld vstore_half2(float2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3(float3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4(float4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8(float8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16(float16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rte(float2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rte(float3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rte(float4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rte(float8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rte(float16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rtz(float2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rtz(float3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rtz(float4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rtz(float8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rtz(float16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rtp(float2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rtp(float3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rtp(float4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rtp(float8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rtp(float16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rtn(float2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rtn(float3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rtn(float4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rtn(float8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rtn(float16 data, size_t offset, half *p);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstore_half2(double2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3(double3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4(double4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8(double8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16(double16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rte(double2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rte(double3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rte(double4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rte(double8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rte(double16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rtz(double2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rtz(double3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rtz(double4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rtz(double8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rtz(double16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rtp(double2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rtp(double3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rtp(double4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rtp(double8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rtp(double16 data, size_t offset, half *p);\n"
"void __ovld vstore_half2_rtn(double2 data, size_t offset, half *p);\n"
"void __ovld vstore_half3_rtn(double3 data, size_t offset, half *p);\n"
"void __ovld vstore_half4_rtn(double4 data, size_t offset, half *p);\n"
"void __ovld vstore_half8_rtn(double8 data, size_t offset, half *p);\n"
"void __ovld vstore_half16_rtn(double16 data, size_t offset, half *p);\n"
"#endif //cl_khr_fp64\n"
"#else\n"
"void __ovld vstore_half2(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16(float16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rte(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rte(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rte(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rte(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rte(float16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rtz(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rtz(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rtz(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rtz(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rtz(float16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rtp(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rtp(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rtp(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rtp(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rtp(float16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rtn(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rtn(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rtn(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rtn(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rtn(float16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16(float16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rte(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rte(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rte(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rte(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rte(float16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rtz(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rtz(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rtz(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rtz(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rtz(float16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rtp(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rtp(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rtp(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rtp(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rtp(float16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rtn(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rtn(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rtn(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rtn(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rtn(float16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16(float16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rte(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rte(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rte(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rte(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rte(float16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rtz(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rtz(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rtz(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rtz(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rtz(float16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rtp(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rtp(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rtp(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rtp(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rtp(float16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rtn(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rtn(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rtn(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rtn(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rtn(float16 data, size_t offset, __private half *p);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstore_half2(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16(double16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rte(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rte(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rte(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rte(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rte(double16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rtz(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rtz(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rtz(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rtz(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rtz(double16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rtp(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rtp(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rtp(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rtp(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rtp(double16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2_rtn(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half3_rtn(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half4_rtn(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half8_rtn(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half16_rtn(double16 data, size_t offset, __global half *p);\n"
"void __ovld vstore_half2(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16(double16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rte(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rte(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rte(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rte(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rte(double16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rtz(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rtz(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rtz(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rtz(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rtz(double16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rtp(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rtp(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rtp(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rtp(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rtp(double16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2_rtn(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half3_rtn(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half4_rtn(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half8_rtn(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half16_rtn(double16 data, size_t offset, __local half *p);\n"
"void __ovld vstore_half2(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16(double16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rte(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rte(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rte(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rte(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rte(double16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rtz(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rtz(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rtz(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rtz(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rtz(double16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rtp(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rtp(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rtp(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rtp(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rtp(double16 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half2_rtn(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half3_rtn(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half4_rtn(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half8_rtn(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstore_half16_rtn(double16 data, size_t offset, __private half *p);\n"
"#endif //cl_khr_fp64\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * For n = 1, 2, 4, 8 and 16 read sizeof (halfn)\n"
" * bytes of data from address (p + (offset * n)).\n"
" * The data read is interpreted as a halfn value.\n"
" * The halfn value read is converted to a floatn\n"
" * value and the floatn value is returned.\n"
" * The address computed as (p + (offset * n))\n"
" * must be aligned to sizeof (halfn) bytes.\n"
" * For n = 3, vloada_half3 reads a half3 from\n"
" * address (p + (offset * 4)) and returns a float3.\n"
" * The address computed as (p + (offset * 4))\n"
" * must be aligned to sizeof (half) * 4 bytes.\n"
" */\n"
"float __ovld vloada_half(size_t offset, const __constant half *p);\n"
"float2 __ovld vloada_half2(size_t offset, const __constant half *p);\n"
"float3 __ovld vloada_half3(size_t offset, const __constant half *p);\n"
"float4 __ovld vloada_half4(size_t offset, const __constant half *p);\n"
"float8 __ovld vloada_half8(size_t offset, const __constant half *p);\n"
"float16 __ovld vloada_half16(size_t offset, const __constant half *p);\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float __ovld vloada_half(size_t offset, const half *p);\n"
"float2 __ovld vloada_half2(size_t offset, const half *p);\n"
"float3 __ovld vloada_half3(size_t offset, const half *p);\n"
"float4 __ovld vloada_half4(size_t offset, const half *p);\n"
"float8 __ovld vloada_half8(size_t offset, const half *p);\n"
"float16 __ovld vloada_half16(size_t offset, const half *p);\n"
"#else\n"
"float __ovld vloada_half(size_t offset, const __global half *p);\n"
"float2 __ovld vloada_half2(size_t offset, const __global half *p);\n"
"float3 __ovld vloada_half3(size_t offset, const __global half *p);\n"
"float4 __ovld vloada_half4(size_t offset, const __global half *p);\n"
"float8 __ovld vloada_half8(size_t offset, const __global half *p);\n"
"float16 __ovld vloada_half16(size_t offset, const __global half *p);\n"
"float __ovld vloada_half(size_t offset, const __local half *p);\n"
"float2 __ovld vloada_half2(size_t offset, const __local half *p);\n"
"float3 __ovld vloada_half3(size_t offset, const __local half *p);\n"
"float4 __ovld vloada_half4(size_t offset, const __local half *p);\n"
"float8 __ovld vloada_half8(size_t offset, const __local half *p);\n"
"float16 __ovld vloada_half16(size_t offset, const __local half *p);\n"
"float __ovld vloada_half(size_t offset, const __private half *p);\n"
"float2 __ovld vloada_half2(size_t offset, const __private half *p);\n"
"float3 __ovld vloada_half3(size_t offset, const __private half *p);\n"
"float4 __ovld vloada_half4(size_t offset, const __private half *p);\n"
"float8 __ovld vloada_half8(size_t offset, const __private half *p);\n"
"float16 __ovld vloada_half16(size_t offset, const __private half *p);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * The floatn value given by data is converted to\n"
" * a halfn value using the appropriate rounding\n"
" * mode.\n"
" * For n = 1, 2, 4, 8 and 16, the halfn value is\n"
" * written to the address computed as (p + (offset\n"
" * * n)). The address computed as (p + (offset *\n"
" * n)) must be aligned to sizeof (halfn) bytes.\n"
" * For n = 3, the half3 value is written to the\n"
" * address computed as (p + (offset * 4)). The\n"
" * address computed as (p + (offset * 4)) must be\n"
" * aligned to sizeof (half) * 4 bytes.\n"
" * vstorea_halfn uses the current rounding\n"
" * mode. The default current rounding mode is\n"
" * round to nearest even.\n"
" */\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"void __ovld vstorea_half(float data, size_t offset, half *p);\n"
"void __ovld vstorea_half2(float2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3(float3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4(float4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8(float8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16(float16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rte(float data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rte(float2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rte(float3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rte(float4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rte(float8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rte(float16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(float data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rtz(float2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rtz(float3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rtz(float4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rtz(float8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rtz(float16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(float data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rtp(float2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rtp(float3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rtp(float4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rtp(float8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rtp(float16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(float data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rtn(float2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rtn(float3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rtn(float4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rtn(float8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rtn(float16 data, size_t offset, half *p);\n"
"\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstorea_half(double data, size_t offset, half *p);\n"
"void __ovld vstorea_half2(double2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3(double3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4(double4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8(double8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16(double16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rte(double data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rte(double2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rte(double3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rte(double4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rte(double8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rte(double16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(double data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rtz(double2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rtz(double3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rtz(double4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rtz(double8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rtz(double16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(double data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rtp(double2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rtp(double3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rtp(double4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rtp(double8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rtp(double16 data, size_t offset, half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(double data, size_t offset, half *p);\n"
"void __ovld vstorea_half2_rtn(double2 data, size_t offset, half *p);\n"
"void __ovld vstorea_half3_rtn(double3 data, size_t offset, half *p);\n"
"void __ovld vstorea_half4_rtn(double4 data, size_t offset, half *p);\n"
"void __ovld vstorea_half8_rtn(double8 data, size_t offset, half *p);\n"
"void __ovld vstorea_half16_rtn(double16 data, size_t offset, half *p);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#else\n"
"void __ovld vstorea_half(float data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16(float16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rte(float data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rte(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rte(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rte(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rte(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rte(float16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(float data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rtz(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rtz(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rtz(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rtz(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rtz(float16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(float data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rtp(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rtp(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rtp(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rtp(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rtp(float16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(float data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rtn(float2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rtn(float3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rtn(float4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rtn(float8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rtn(float16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half(float data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16(float16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rte(float data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rte(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rte(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rte(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rte(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rte(float16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(float data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rtz(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rtz(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rtz(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rtz(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rtz(float16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(float data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rtp(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rtp(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rtp(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rtp(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rtp(float16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(float data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rtn(float2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rtn(float3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rtn(float4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rtn(float8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rtn(float16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half(float data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16(float16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rte(float data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rte(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rte(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rte(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rte(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rte(float16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(float data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rtz(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rtz(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rtz(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rtz(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rtz(float16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(float data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rtp(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rtp(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rtp(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rtp(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rtp(float16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(float data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rtn(float2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rtn(float3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rtn(float4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rtn(float8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rtn(float16 data, size_t offset, __private half *p);\n"
"\n"
"#ifdef cl_khr_fp64\n"
"void __ovld vstorea_half(double data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16(double16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rte(double data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rte(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rte(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rte(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rte(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rte(double16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(double data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rtz(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rtz(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rtz(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rtz(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rtz(double16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(double data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rtp(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rtp(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rtp(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rtp(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rtp(double16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(double data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half2_rtn(double2 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half3_rtn(double3 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half4_rtn(double4 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half8_rtn(double8 data, size_t offset, __global half *p);\n"
"void __ovld vstorea_half16_rtn(double16 data, size_t offset, __global half *p);\n"
"\n"
"void __ovld vstorea_half(double data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16(double16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rte(double data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rte(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rte(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rte(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rte(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rte(double16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(double data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rtz(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rtz(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rtz(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rtz(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rtz(double16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(double data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rtp(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rtp(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rtp(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rtp(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rtp(double16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(double data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half2_rtn(double2 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half3_rtn(double3 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half4_rtn(double4 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half8_rtn(double8 data, size_t offset, __local half *p);\n"
"void __ovld vstorea_half16_rtn(double16 data, size_t offset, __local half *p);\n"
"\n"
"void __ovld vstorea_half(double data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16(double16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rte(double data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rte(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rte(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rte(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rte(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rte(double16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rtz(double data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rtz(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rtz(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rtz(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rtz(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rtz(double16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rtp(double data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rtp(double2 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rtp(double3 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rtp(double4 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rtp(double8 data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rtp(double16 data, size_t offset, __private half *p);\n"
"\n"
"void __ovld vstorea_half_rtn(double data, size_t offset, __private half *p);\n"
"void __ovld vstorea_half2_rtn(double2 data,size_t offset, __private half *p);\n"
"void __ovld vstorea_half3_rtn(double3 data,size_t offset, __private half *p);\n"
"void __ovld vstorea_half4_rtn(double4 data,size_t offset, __private half *p);\n"
"void __ovld vstorea_half8_rtn(double8 data,size_t offset, __private half *p);\n"
"void __ovld vstorea_half16_rtn(double16 data,size_t offset, __private half *p);\n"
"#endif //cl_khr_fp64\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL v1.1 s6.11.8, v1.2 s6.12.8, v2.0 s6.13.8 - Synchronization Functions\n"
"\n"
"// Flag type and values for barrier, mem_fence, read_mem_fence, write_mem_fence\n"
"typedef uint cl_mem_fence_flags;\n"
"\n"
"/**\n"
" * Queue a memory fence to ensure correct\n"
" * ordering of memory operations to local memory\n"
" */\n"
"#define CLK_LOCAL_MEM_FENCE    0x01\n"
"\n"
"/**\n"
" * Queue a memory fence to ensure correct\n"
" * ordering of memory operations to global memory\n"
" */\n"
"#define CLK_GLOBAL_MEM_FENCE   0x02\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"/**\n"
" * Queue a memory fence to ensure correct ordering of memory\n"
" * operations between work-items of a work-group to\n"
" * image memory.\n"
" */\n"
"#define CLK_IMAGE_MEM_FENCE  0x04\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * All work-items in a work-group executing the kernel\n"
" * on a processor must execute this function before any\n"
" * are allowed to continue execution beyond the barrier.\n"
" * This function must be encountered by all work-items in\n"
" * a work-group executing the kernel.\n"
" * If barrier is inside a conditional statement, then all\n"
" * work-items must enter the conditional if any work-item\n"
" * enters the conditional statement and executes the\n"
" * barrier.\n"
" * If barrer is inside a loop, all work-items must execute\n"
" * the barrier for each iteration of the loop before any are\n"
" * allowed to continue execution beyond the barrier.\n"
" * The barrier function also queues a memory fence\n"
" * (reads and writes) to ensure correct ordering of\n"
" * memory operations to local or global memory.\n"
" * The flags argument specifies the memory address space\n"
" * and can be set to a combination of the following literal\n"
" * values.\n"
" * CLK_LOCAL_MEM_FENCE - The barrier function\n"
" * will either flush any variables stored in local memory\n"
" * or queue a memory fence to ensure correct ordering of\n"
" * memory operations to local memory.\n"
" * CLK_GLOBAL_MEM_FENCE - The barrier function\n"
" * will queue a memory fence to ensure correct ordering\n"
" * of memory operations to global memory. This can be\n"
" * useful when work-items, for example, write to buffer or\n"
" * image objects and then want to read the updated data.\n"
" */\n"
"\n"
"void __ovld __conv barrier(cl_mem_fence_flags flags);\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"typedef enum memory_scope {\n"
"  memory_scope_work_item = __OPENCL_MEMORY_SCOPE_WORK_ITEM,\n"
"  memory_scope_work_group = __OPENCL_MEMORY_SCOPE_WORK_GROUP,\n"
"  memory_scope_device = __OPENCL_MEMORY_SCOPE_DEVICE,\n"
"  memory_scope_all_svm_devices = __OPENCL_MEMORY_SCOPE_ALL_SVM_DEVICES,\n"
"#if defined(cl_intel_subgroups) || defined(cl_khr_subgroups)\n"
"  memory_scope_sub_group = __OPENCL_MEMORY_SCOPE_SUB_GROUP\n"
"#endif\n"
"} memory_scope;\n"
"\n"
"void __ovld __conv work_group_barrier(cl_mem_fence_flags flags, memory_scope scope);\n"
"void __ovld __conv work_group_barrier(cl_mem_fence_flags flags);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL v1.1 s6.11.9, v1.2 s6.12.9 - Explicit Memory Fence Functions\n"
"\n"
"/**\n"
" * Orders loads and stores of a work-item\n"
" * executing a kernel. This means that loads\n"
" * and stores preceding the mem_fence will\n"
" * be committed to memory before any loads\n"
" * and stores following the mem_fence.\n"
" * The flags argument specifies the memory\n"
" * address space and can be set to a\n"
" * combination of the following literal\n"
" * values:\n"
" * CLK_LOCAL_MEM_FENCE\n"
" * CLK_GLOBAL_MEM_FENCE.\n"
" */\n"
"void __ovld mem_fence(cl_mem_fence_flags flags);\n"
"\n"
"/**\n"
" * Read memory barrier that orders only\n"
" * loads.\n"
" * The flags argument specifies the memory\n"
" * address space and can be set to a\n"
" * combination of the following literal\n"
" * values:\n"
" * CLK_LOCAL_MEM_FENCE\n"
" * CLK_GLOBAL_MEM_FENCE.\n"
" */\n"
"void __ovld read_mem_fence(cl_mem_fence_flags flags);\n"
"\n"
"/**\n"
" * Write memory barrier that orders only\n"
" * stores.\n"
" * The flags argument specifies the memory\n"
" * address space and can be set to a\n"
" * combination of the following literal\n"
" * values:\n"
" * CLK_LOCAL_MEM_FENCE\n"
" * CLK_GLOBAL_MEM_FENCE.\n"
" */\n"
"void __ovld write_mem_fence(cl_mem_fence_flags flags);\n"
"\n"
"// OpenCL v2.0 s6.13.9 - Address Space Qualifier Functions\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"cl_mem_fence_flags __ovld get_fence(const void *ptr);\n"
"cl_mem_fence_flags __ovld get_fence(void *ptr);\n"
"\n"
"/**\n"
" * Builtin functions to_global, to_local, and to_private need to be declared as Clang builtin functions\n"
" * and checked in Sema since they should be declared as\n"
" *   addr gentype* to_addr (gentype*);\n"
" * where gentype is builtin type or user defined type.\n"
" */\n"
"\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL v1.1 s6.11.10, v1.2 s6.12.10, v2.0 s6.13.10 - Async Copies from Global to Local Memory, Local to Global Memory, and Prefetch\n"
"\n"
"/**\n"
" * event_t async_work_group_copy (\n"
" * __global gentype *dst,\n"
" * const __local gentype *src,\n"
" * size_t num_elements,\n"
" * event_t event)\n"
" * Perform an async copy of num_elements\n"
" * gentype elements from src to dst. The async\n"
" * copy is performed by all work-items in a workgroup\n"
" * and this built-in function must therefore\n"
" * be encountered by all work-items in a workgroup\n"
" * executing the kernel with the same\n"
" * argument values; otherwise the results are\n"
" * undefined.\n"
" * Returns an event object that can be used by\n"
" * wait_group_events to wait for the async copy\n"
" * to finish. The event argument can also be used\n"
" * to associate the async_work_group_copy with\n"
" * a previous async copy allowing an event to be\n"
" * shared by multiple async copies; otherwise event\n"
" * should be zero.\n"
" * If event argument is non-zero, the event object\n"
" * supplied in event argument will be returned.\n"
" * This function does not perform any implicit\n"
" * synchronization of source data such as using a\n"
" * barrier before performing the copy.\n"
" */\n"
"event_t __ovld async_work_group_copy(__local char *dst, const __global char *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uchar *dst, const __global uchar *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local short *dst, const __global short *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ushort *dst, const __global ushort *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local int *dst, const __global int *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uint *dst, const __global uint *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local long *dst, const __global long *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ulong *dst, const __global ulong *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local float *dst, const __global float *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local char2 *dst, const __global char2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uchar2 *dst, const __global uchar2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local short2 *dst, const __global short2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ushort2 *dst, const __global ushort2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local int2 *dst, const __global int2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uint2 *dst, const __global uint2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local long2 *dst, const __global long2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ulong2 *dst, const __global ulong2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local float2 *dst, const __global float2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local char3 *dst, const __global char3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uchar3 *dst, const __global uchar3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local short3 *dst, const __global short3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ushort3 *dst, const __global ushort3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local int3 *dst, const __global int3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uint3 *dst, const __global uint3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local long3 *dst, const __global long3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ulong3 *dst, const __global ulong3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local float3 *dst, const __global float3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local char4 *dst, const __global char4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uchar4 *dst, const __global uchar4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local short4 *dst, const __global short4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ushort4 *dst, const __global ushort4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local int4 *dst, const __global int4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uint4 *dst, const __global uint4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local long4 *dst, const __global long4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ulong4 *dst, const __global ulong4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local float4 *dst, const __global float4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local char8 *dst, const __global char8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uchar8 *dst, const __global uchar8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local short8 *dst, const __global short8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ushort8 *dst, const __global ushort8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local int8 *dst, const __global int8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uint8 *dst, const __global uint8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local long8 *dst, const __global long8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ulong8 *dst, const __global ulong8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local float8 *dst, const __global float8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local char16 *dst, const __global char16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uchar16 *dst, const __global uchar16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local short16 *dst, const __global short16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ushort16 *dst, const __global ushort16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local int16 *dst, const __global int16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local uint16 *dst, const __global uint16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local long16 *dst, const __global long16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local ulong16 *dst, const __global ulong16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local float16 *dst, const __global float16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global char *dst, const __local char *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uchar *dst, const __local uchar *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global short *dst, const __local short *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ushort *dst, const __local ushort *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global int *dst, const __local int *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uint *dst, const __local uint *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global long *dst, const __local long *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ulong *dst, const __local ulong *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global float *dst, const __local float *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global char2 *dst, const __local char2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uchar2 *dst, const __local uchar2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global short2 *dst, const __local short2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ushort2 *dst, const __local ushort2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global int2 *dst, const __local int2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uint2 *dst, const __local uint2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global long2 *dst, const __local long2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ulong2 *dst, const __local ulong2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global float2 *dst, const __local float2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global char3 *dst, const __local char3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uchar3 *dst, const __local uchar3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global short3 *dst, const __local short3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ushort3 *dst, const __local ushort3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global int3 *dst, const __local int3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uint3 *dst, const __local uint3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global long3 *dst, const __local long3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ulong3 *dst, const __local ulong3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global float3 *dst, const __local float3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global char4 *dst, const __local char4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uchar4 *dst, const __local uchar4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global short4 *dst, const __local short4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ushort4 *dst, const __local ushort4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global int4 *dst, const __local int4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uint4 *dst, const __local uint4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global long4 *dst, const __local long4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ulong4 *dst, const __local ulong4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global float4 *dst, const __local float4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global char8 *dst, const __local char8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uchar8 *dst, const __local uchar8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global short8 *dst, const __local short8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ushort8 *dst, const __local ushort8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global int8 *dst, const __local int8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uint8 *dst, const __local uint8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global long8 *dst, const __local long8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ulong8 *dst, const __local ulong8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global float8 *dst, const __local float8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global char16 *dst, const __local char16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uchar16 *dst, const __local uchar16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global short16 *dst, const __local short16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ushort16 *dst, const __local ushort16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global int16 *dst, const __local int16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global uint16 *dst, const __local uint16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global long16 *dst, const __local long16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global ulong16 *dst, const __local ulong16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global float16 *dst, const __local float16 *src, size_t num_elements, event_t event);\n"
"#ifdef cl_khr_fp64\n"
"event_t __ovld async_work_group_copy(__local double *dst, const __global double *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local double2 *dst, const __global double2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local double3 *dst, const __global double3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local double4 *dst, const __global double4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local double8 *dst, const __global double8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local double16 *dst, const __global double16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global double *dst, const __local double *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global double2 *dst, const __local double2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global double3 *dst, const __local double3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global double4 *dst, const __local double4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global double8 *dst, const __local double8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global double16 *dst, const __local double16 *src, size_t num_elements, event_t event);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"event_t __ovld async_work_group_copy(__local half *dst, const __global half *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local half2 *dst, const __global half2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local half3 *dst, const __global half3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local half4 *dst, const __global half4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local half8 *dst, const __global half8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__local half16 *dst, const __global half16 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global half *dst, const __local half *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global half2 *dst, const __local half2 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global half3 *dst, const __local half3 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global half4 *dst, const __local half4 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global half8 *dst, const __local half8 *src, size_t num_elements, event_t event);\n"
"event_t __ovld async_work_group_copy(__global half16 *dst, const __local half16 *src, size_t num_elements, event_t event);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Perform an async gather of num_elements\n"
" * gentype elements from src to dst. The\n"
" * src_stride is the stride in elements for each\n"
" * gentype element read from src. The dst_stride\n"
" * is the stride in elements for each gentype\n"
" * element written to dst. The async gather is\n"
" * performed by all work-items in a work-group.\n"
" * This built-in function must therefore be\n"
" * encountered by all work-items in a work-group\n"
" * executing the kernel with the same argument\n"
" * values; otherwise the results are undefined.\n"
" * Returns an event object that can be used by\n"
" * wait_group_events to wait for the async copy\n"
" * to finish. The event argument can also be used\n"
" * to associate the\n"
" * async_work_group_strided_copy with a\n"
" * previous async copy allowing an event to be\n"
" * shared by multiple async copies; otherwise event\n"
" * should be zero.\n"
" * If event argument is non-zero, the event object\n"
" * supplied in event argument will be returned.\n"
" * This function does not perform any implicit\n"
" * synchronization of source data such as using a\n"
" * barrier before performing the copy.\n"
" */\n"
"event_t __ovld async_work_group_strided_copy(__local char *dst, const __global char *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uchar *dst, const __global uchar *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local short *dst, const __global short *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ushort *dst, const __global ushort *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local int *dst, const __global int *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uint *dst, const __global uint *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local long *dst, const __global long *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ulong *dst, const __global ulong *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local float *dst, const __global float *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local char2 *dst, const __global char2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uchar2 *dst, const __global uchar2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local short2 *dst, const __global short2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ushort2 *dst, const __global ushort2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local int2 *dst, const __global int2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uint2 *dst, const __global uint2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local long2 *dst, const __global long2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ulong2 *dst, const __global ulong2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local float2 *dst, const __global float2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local char3 *dst, const __global char3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uchar3 *dst, const __global uchar3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local short3 *dst, const __global short3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ushort3 *dst, const __global ushort3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local int3 *dst, const __global int3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uint3 *dst, const __global uint3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local long3 *dst, const __global long3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ulong3 *dst, const __global ulong3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local float3 *dst, const __global float3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local char4 *dst, const __global char4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uchar4 *dst, const __global uchar4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local short4 *dst, const __global short4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ushort4 *dst, const __global ushort4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local int4 *dst, const __global int4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uint4 *dst, const __global uint4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local long4 *dst, const __global long4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ulong4 *dst, const __global ulong4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local float4 *dst, const __global float4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local char8 *dst, const __global char8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uchar8 *dst, const __global uchar8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local short8 *dst, const __global short8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ushort8 *dst, const __global ushort8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local int8 *dst, const __global int8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uint8 *dst, const __global uint8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local long8 *dst, const __global long8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ulong8 *dst, const __global ulong8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local float8 *dst, const __global float8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local char16 *dst, const __global char16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uchar16 *dst, const __global uchar16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local short16 *dst, const __global short16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ushort16 *dst, const __global ushort16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local int16 *dst, const __global int16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local uint16 *dst, const __global uint16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local long16 *dst, const __global long16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local ulong16 *dst, const __global ulong16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local float16 *dst, const __global float16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global char *dst, const __local char *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uchar *dst, const __local uchar *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global short *dst, const __local short *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ushort *dst, const __local ushort *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global int *dst, const __local int *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uint *dst, const __local uint *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global long *dst, const __local long *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ulong *dst, const __local ulong *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global float *dst, const __local float *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global char2 *dst, const __local char2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uchar2 *dst, const __local uchar2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global short2 *dst, const __local short2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ushort2 *dst, const __local ushort2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global int2 *dst, const __local int2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uint2 *dst, const __local uint2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global long2 *dst, const __local long2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ulong2 *dst, const __local ulong2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global float2 *dst, const __local float2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global char3 *dst, const __local char3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uchar3 *dst, const __local uchar3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global short3 *dst, const __local short3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ushort3 *dst, const __local ushort3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global int3 *dst, const __local int3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uint3 *dst, const __local uint3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global long3 *dst, const __local long3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ulong3 *dst, const __local ulong3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global float3 *dst, const __local float3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global char4 *dst, const __local char4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uchar4 *dst, const __local uchar4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global short4 *dst, const __local short4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ushort4 *dst, const __local ushort4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global int4 *dst, const __local int4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uint4 *dst, const __local uint4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global long4 *dst, const __local long4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ulong4 *dst, const __local ulong4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global float4 *dst, const __local float4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global char8 *dst, const __local char8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uchar8 *dst, const __local uchar8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global short8 *dst, const __local short8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ushort8 *dst, const __local ushort8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global int8 *dst, const __local int8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uint8 *dst, const __local uint8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global long8 *dst, const __local long8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ulong8 *dst, const __local ulong8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global float8 *dst, const __local float8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global char16 *dst, const __local char16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uchar16 *dst, const __local uchar16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global short16 *dst, const __local short16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ushort16 *dst, const __local ushort16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global int16 *dst, const __local int16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global uint16 *dst, const __local uint16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global long16 *dst, const __local long16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global ulong16 *dst, const __local ulong16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global float16 *dst, const __local float16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"#ifdef cl_khr_fp64\n"
"event_t __ovld async_work_group_strided_copy(__local double *dst, const __global double *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local double2 *dst, const __global double2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local double3 *dst, const __global double3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local double4 *dst, const __global double4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local double8 *dst, const __global double8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local double16 *dst, const __global double16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global double *dst, const __local double *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global double2 *dst, const __local double2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global double3 *dst, const __local double3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global double4 *dst, const __local double4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global double8 *dst, const __local double8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global double16 *dst, const __local double16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"event_t __ovld async_work_group_strided_copy(__local half *dst, const __global half *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local half2 *dst, const __global half2 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local half3 *dst, const __global half3 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local half4 *dst, const __global half4 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local half8 *dst, const __global half8 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__local half16 *dst, const __global half16 *src, size_t num_elements, size_t src_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global half *dst, const __local half *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global half2 *dst, const __local half2 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global half3 *dst, const __local half3 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global half4 *dst, const __local half4 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global half8 *dst, const __local half8 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"event_t __ovld async_work_group_strided_copy(__global half16 *dst, const __local half16 *src, size_t num_elements, size_t dst_stride, event_t event);\n"
"#endif //cl_khr_fp16\n"
"\n"
"/**\n"
" * Wait for events that identify the\n"
" * async_work_group_copy operations to\n"
" * complete. The event objects specified in\n"
" * event_list will be released after the wait is\n"
" * performed.\n"
" * This function must be encountered by all workitems\n"
" * in a work-group executing the kernel with\n"
" * the same num_events and event objects specified\n"
" * in event_list; otherwise the results are undefined.\n"
" */\n"
"void __ovld wait_group_events(int num_events, event_t *event_list);\n"
"\n"
"/**\n"
" * Prefetch num_elements * sizeof(gentype)\n"
" * bytes into the global cache. The prefetch\n"
" * instruction is applied to a work-item in a workgroup\n"
" * and does not affect the functional\n"
" * behavior of the kernel.\n"
" */\n"
"void __ovld prefetch(const __global char *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uchar *p, size_t num_elements);\n"
"void __ovld prefetch(const __global short *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ushort *p, size_t num_elements);\n"
"void __ovld prefetch(const __global int *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uint *p, size_t num_elements);\n"
"void __ovld prefetch(const __global long *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ulong *p, size_t num_elements);\n"
"void __ovld prefetch(const __global float *p, size_t num_elements);\n"
"void __ovld prefetch(const __global char2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uchar2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global short2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ushort2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global int2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uint2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global long2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ulong2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global float2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global char3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uchar3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global short3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ushort3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global int3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uint3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global long3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ulong3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global float3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global char4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uchar4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global short4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ushort4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global int4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uint4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global long4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ulong4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global float4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global char8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uchar8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global short8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ushort8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global int8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uint8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global long8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ulong8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global float8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global char16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uchar16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global short16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ushort16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global int16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global uint16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global long16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global ulong16 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global float16 *p, size_t num_elements);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld prefetch(const __global double *p, size_t num_elements);\n"
"void __ovld prefetch(const __global double2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global double3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global double4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global double8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global double16 *p, size_t num_elements);\n"
"#endif //cl_khr_fp64\n"
"#ifdef cl_khr_fp16\n"
"void __ovld prefetch(const __global half *p, size_t num_elements);\n"
"void __ovld prefetch(const __global half2 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global half3 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global half4 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global half8 *p, size_t num_elements);\n"
"void __ovld prefetch(const __global half16 *p, size_t num_elements);\n"
"#endif // cl_khr_fp16\n"
"\n"
"// OpenCL v1.1 s6.11.1, v1.2 s6.12.11 - Atomic Functions\n"
"\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable\n"
"#pragma OPENCL EXTENSION cl_khr_int64_extended_atomics : enable\n"
"#endif\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * (old + val) and store result at location\n"
" * pointed by p. The function returns old.\n"
" */\n"
"int __ovld atomic_add(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_add(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_add(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_add(volatile __local unsigned int *p, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_base_atomics)\n"
"int __ovld atom_add(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_add(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_base_atomics)\n"
"int __ovld atom_add(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_add(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_base_atomics)\n"
"long __ovld atom_add(volatile __global long *p, long val);\n"
"unsigned long __ovld atom_add(volatile __global unsigned long *p, unsigned long val);\n"
"long __ovld atom_add(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_add(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old) stored at location pointed by p.\n"
" * Compute (old - val) and store result at location pointed by p. The function\n"
" * returns old.\n"
" */\n"
"int __ovld atomic_sub(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_sub(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_sub(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_sub(volatile __local unsigned int *p, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_base_atomics)\n"
"int __ovld atom_sub(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_sub(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_base_atomics)\n"
"int __ovld atom_sub(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_sub(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_base_atomics)\n"
"long __ovld atom_sub(volatile __global long *p, long val);\n"
"unsigned long __ovld atom_sub(volatile __global unsigned long *p, unsigned long val);\n"
"long __ovld atom_sub(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_sub(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Swaps the old value stored at location p\n"
" * with new value given by val. Returns old\n"
" * value.\n"
" */\n"
"int __ovld atomic_xchg(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_xchg(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_xchg(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_xchg(volatile __local unsigned int *p, unsigned int val);\n"
"float __ovld atomic_xchg(volatile __global float *p, float val);\n"
"float __ovld atomic_xchg(volatile __local float *p, float val);\n"
"\n"
"#if defined(cl_khr_global_int32_base_atomics)\n"
"int __ovld atom_xchg(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_xchg(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_base_atomics)\n"
"int __ovld atom_xchg(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_xchg(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_base_atomics)\n"
"long __ovld atom_xchg(volatile __global long *p, long val);\n"
"long __ovld atom_xchg(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_xchg(volatile __global unsigned long *p, unsigned long val);\n"
"unsigned long __ovld atom_xchg(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * (old + 1) and store result at location\n"
" * pointed by p. The function returns old.\n"
" */\n"
"int __ovld atomic_inc(volatile __global int *p);\n"
"unsigned int __ovld atomic_inc(volatile __global unsigned int *p);\n"
"int __ovld atomic_inc(volatile __local int *p);\n"
"unsigned int __ovld atomic_inc(volatile __local unsigned int *p);\n"
"\n"
"#if defined(cl_khr_global_int32_base_atomics)\n"
"int __ovld atom_inc(volatile __global int *p);\n"
"unsigned int __ovld atom_inc(volatile __global unsigned int *p);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_base_atomics)\n"
"int __ovld atom_inc(volatile __local int *p);\n"
"unsigned int __ovld atom_inc(volatile __local unsigned int *p);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_base_atomics)\n"
"long __ovld atom_inc(volatile __global long *p);\n"
"unsigned long __ovld atom_inc(volatile __global unsigned long *p);\n"
"long __ovld atom_inc(volatile __local long *p);\n"
"unsigned long __ovld atom_inc(volatile __local unsigned long *p);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * (old - 1) and store result at location\n"
" * pointed by p. The function returns old.\n"
" */\n"
"int __ovld atomic_dec(volatile __global int *p);\n"
"unsigned int __ovld atomic_dec(volatile __global unsigned int *p);\n"
"int __ovld atomic_dec(volatile __local int *p);\n"
"unsigned int __ovld atomic_dec(volatile __local unsigned int *p);\n"
"\n"
"#if defined(cl_khr_global_int32_base_atomics)\n"
"int __ovld atom_dec(volatile __global int *p);\n"
"unsigned int __ovld atom_dec(volatile __global unsigned int *p);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_base_atomics)\n"
"int __ovld atom_dec(volatile __local int *p);\n"
"unsigned int __ovld atom_dec(volatile __local unsigned int *p);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_base_atomics)\n"
"long __ovld atom_dec(volatile __global long *p);\n"
"unsigned long __ovld atom_dec(volatile __global unsigned long *p);\n"
"long __ovld atom_dec(volatile __local long *p);\n"
"unsigned long __ovld atom_dec(volatile __local unsigned long *p);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * (old == cmp) ? val : old and store result at\n"
" * location pointed by p. The function\n"
" * returns old.\n"
" */\n"
"int __ovld atomic_cmpxchg(volatile __global int *p, int cmp, int val);\n"
"unsigned int __ovld atomic_cmpxchg(volatile __global unsigned int *p, unsigned int cmp, unsigned int val);\n"
"int __ovld atomic_cmpxchg(volatile __local int *p, int cmp, int val);\n"
"unsigned int __ovld atomic_cmpxchg(volatile __local unsigned int *p, unsigned int cmp, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_base_atomics)\n"
"int __ovld atom_cmpxchg(volatile __global int *p, int cmp, int val);\n"
"unsigned int __ovld atom_cmpxchg(volatile __global unsigned int *p, unsigned int cmp, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_base_atomics)\n"
"int __ovld atom_cmpxchg(volatile __local int *p, int cmp, int val);\n"
"unsigned int __ovld atom_cmpxchg(volatile __local unsigned int *p, unsigned int cmp, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_base_atomics)\n"
"long __ovld atom_cmpxchg(volatile __global long *p, long cmp, long val);\n"
"unsigned long __ovld atom_cmpxchg(volatile __global unsigned long *p, unsigned long cmp, unsigned long val);\n"
"long __ovld atom_cmpxchg(volatile __local long *p, long cmp, long val);\n"
"unsigned long __ovld atom_cmpxchg(volatile __local unsigned long *p, unsigned long cmp, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * min(old, val) and store minimum value at\n"
" * location pointed by p. The function\n"
" * returns old.\n"
" */\n"
"int __ovld atomic_min(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_min(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_min(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_min(volatile __local unsigned int *p, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_extended_atomics)\n"
"int __ovld atom_min(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_min(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_extended_atomics)\n"
"int __ovld atom_min(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_min(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_extended_atomics)\n"
"long __ovld atom_min(volatile __global long *p, long val);\n"
"unsigned long __ovld atom_min(volatile __global unsigned long *p, unsigned long val);\n"
"long __ovld atom_min(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_min(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * max(old, val) and store maximum value at\n"
" * location pointed by p. The function\n"
" * returns old.\n"
" */\n"
"int __ovld atomic_max(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_max(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_max(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_max(volatile __local unsigned int *p, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_extended_atomics)\n"
"int __ovld atom_max(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_max(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_extended_atomics)\n"
"int __ovld atom_max(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_max(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_extended_atomics)\n"
"long __ovld atom_max(volatile __global long *p, long val);\n"
"unsigned long __ovld atom_max(volatile __global unsigned long *p, unsigned long val);\n"
"long __ovld atom_max(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_max(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * (old & val) and store result at location\n"
" * pointed by p. The function returns old.\n"
" */\n"
"int __ovld atomic_and(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_and(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_and(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_and(volatile __local unsigned int *p, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_extended_atomics)\n"
"int __ovld atom_and(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_and(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_extended_atomics)\n"
"int __ovld atom_and(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_and(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_extended_atomics)\n"
"long __ovld atom_and(volatile __global long *p, long val);\n"
"unsigned long __ovld atom_and(volatile __global unsigned long *p, unsigned long val);\n"
"long __ovld atom_and(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_and(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * (old | val) and store result at location\n"
" * pointed by p. The function returns old.\n"
" */\n"
"int __ovld atomic_or(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_or(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_or(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_or(volatile __local unsigned int *p, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_extended_atomics)\n"
"int __ovld atom_or(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_or(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_extended_atomics)\n"
"int __ovld atom_or(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_or(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_extended_atomics)\n"
"long __ovld atom_or(volatile __global long *p, long val);\n"
"unsigned long __ovld atom_or(volatile __global unsigned long *p, unsigned long val);\n"
"long __ovld atom_or(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_or(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"/**\n"
" * Read the 32-bit value (referred to as old)\n"
" * stored at location pointed by p. Compute\n"
" * (old ^ val) and store result at location\n"
" * pointed by p. The function returns old.\n"
" */\n"
"int __ovld atomic_xor(volatile __global int *p, int val);\n"
"unsigned int __ovld atomic_xor(volatile __global unsigned int *p, unsigned int val);\n"
"int __ovld atomic_xor(volatile __local int *p, int val);\n"
"unsigned int __ovld atomic_xor(volatile __local unsigned int *p, unsigned int val);\n"
"\n"
"#if defined(cl_khr_global_int32_extended_atomics)\n"
"int __ovld atom_xor(volatile __global int *p, int val);\n"
"unsigned int __ovld atom_xor(volatile __global unsigned int *p, unsigned int val);\n"
"#endif\n"
"#if defined(cl_khr_local_int32_extended_atomics)\n"
"int __ovld atom_xor(volatile __local int *p, int val);\n"
"unsigned int __ovld atom_xor(volatile __local unsigned int *p, unsigned int val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_extended_atomics)\n"
"long __ovld atom_xor(volatile __global long *p, long val);\n"
"unsigned long __ovld atom_xor(volatile __global unsigned long *p, unsigned long val);\n"
"long __ovld atom_xor(volatile __local long *p, long val);\n"
"unsigned long __ovld atom_xor(volatile __local unsigned long *p, unsigned long val);\n"
"#endif\n"
"\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : disable\n"
"#pragma OPENCL EXTENSION cl_khr_int64_extended_atomics : disable\n"
"#endif\n"
"\n"
"// OpenCL v2.0 s6.13.11 - Atomics Functions\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#ifndef ATOMIC_VAR_INIT\n"
"#define ATOMIC_VAR_INIT(x) (x)\n"
"#endif //ATOMIC_VAR_INIT\n"
"#define ATOMIC_FLAG_INIT 0\n"
"\n"
"// enum values aligned with what clang uses in EmitAtomicExpr()\n"
"typedef enum memory_order\n"
"{\n"
"  memory_order_relaxed = __ATOMIC_RELAXED,\n"
"  memory_order_acquire = __ATOMIC_ACQUIRE,\n"
"  memory_order_release = __ATOMIC_RELEASE,\n"
"  memory_order_acq_rel = __ATOMIC_ACQ_REL,\n"
"  memory_order_seq_cst = __ATOMIC_SEQ_CST\n"
"} memory_order;\n"
"\n"
"// double atomics support requires extensions cl_khr_int64_base_atomics and cl_khr_int64_extended_atomics\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable\n"
"#pragma OPENCL EXTENSION cl_khr_int64_extended_atomics : enable\n"
"#endif\n"
"\n"
"// atomic_init()\n"
"void __ovld atomic_init(volatile atomic_int *object, int value);\n"
"void __ovld atomic_init(volatile atomic_uint *object, uint value);\n"
"void __ovld atomic_init(volatile atomic_float *object, float value);\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"void __ovld atomic_init(volatile atomic_long *object, long value);\n"
"void __ovld atomic_init(volatile atomic_ulong *object, ulong value);\n"
"#ifdef cl_khr_fp64\n"
"void __ovld atomic_init(volatile atomic_double *object, double value);\n"
"#endif //cl_khr_fp64\n"
"#endif\n"
"\n"
"// atomic_work_item_fence()\n"
"void __ovld atomic_work_item_fence(cl_mem_fence_flags flags, memory_order order, memory_scope scope);\n"
"\n"
"// atomic_fetch()\n"
"\n"
"int __ovld atomic_fetch_add(volatile atomic_int *object, int operand);\n"
"int __ovld atomic_fetch_add_explicit(volatile atomic_int *object, int operand, memory_order order);\n"
"int __ovld atomic_fetch_add_explicit(volatile atomic_int *object, int operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_add(volatile atomic_uint *object, uint operand);\n"
"uint __ovld atomic_fetch_add_explicit(volatile atomic_uint *object, uint operand, memory_order order);\n"
"uint __ovld atomic_fetch_add_explicit(volatile atomic_uint *object, uint operand, memory_order order, memory_scope scope);\n"
"int __ovld atomic_fetch_sub(volatile atomic_int *object, int operand);\n"
"int __ovld atomic_fetch_sub_explicit(volatile atomic_int *object, int operand, memory_order order);\n"
"int __ovld atomic_fetch_sub_explicit(volatile atomic_int *object, int operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_sub(volatile atomic_uint *object, uint operand);\n"
"uint __ovld atomic_fetch_sub_explicit(volatile atomic_uint *object, uint operand, memory_order order);\n"
"uint __ovld atomic_fetch_sub_explicit(volatile atomic_uint *object, uint operand, memory_order order, memory_scope scope);\n"
"int __ovld atomic_fetch_or(volatile atomic_int *object, int operand);\n"
"int __ovld atomic_fetch_or_explicit(volatile atomic_int *object, int operand, memory_order order);\n"
"int __ovld atomic_fetch_or_explicit(volatile atomic_int *object, int operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_or(volatile atomic_uint *object, uint operand);\n"
"uint __ovld atomic_fetch_or_explicit(volatile atomic_uint *object, uint operand, memory_order order);\n"
"uint __ovld atomic_fetch_or_explicit(volatile atomic_uint *object, uint operand, memory_order order, memory_scope scope);\n"
"int __ovld atomic_fetch_xor(volatile atomic_int *object, int operand);\n"
"int __ovld atomic_fetch_xor_explicit(volatile atomic_int *object, int operand, memory_order order);\n"
"int __ovld atomic_fetch_xor_explicit(volatile atomic_int *object, int operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_xor(volatile atomic_uint *object, uint operand);\n"
"uint __ovld atomic_fetch_xor_explicit(volatile atomic_uint *object, uint operand, memory_order order);\n"
"uint __ovld atomic_fetch_xor_explicit(volatile atomic_uint *object, uint operand, memory_order order, memory_scope scope);\n"
"int __ovld atomic_fetch_and(volatile atomic_int *object, int operand);\n"
"int __ovld atomic_fetch_and_explicit(volatile atomic_int *object, int operand, memory_order order);\n"
"int __ovld atomic_fetch_and_explicit(volatile atomic_int *object, int operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_and(volatile atomic_uint *object, uint operand);\n"
"uint __ovld atomic_fetch_and_explicit(volatile atomic_uint *object, uint operand, memory_order order);\n"
"uint __ovld atomic_fetch_and_explicit(volatile atomic_uint *object, uint operand, memory_order order, memory_scope scope);\n"
"int __ovld atomic_fetch_min(volatile atomic_int *object, int operand);\n"
"int __ovld atomic_fetch_min_explicit(volatile atomic_int *object, int operand, memory_order order);\n"
"int __ovld atomic_fetch_min_explicit(volatile atomic_int *object, int operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_min(volatile atomic_uint *object, uint operand);\n"
"uint __ovld atomic_fetch_min_explicit(volatile atomic_uint *object, uint operand, memory_order order);\n"
"uint __ovld atomic_fetch_min_explicit(volatile atomic_uint *object, uint operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_min(volatile atomic_uint *object, int operand);\n"
"uint __ovld atomic_fetch_min_explicit(volatile atomic_uint *object, int operand, memory_order order);\n"
"uint __ovld atomic_fetch_min_explicit(volatile atomic_uint *object, int operand, memory_order order, memory_scope scope);\n"
"int __ovld atomic_fetch_max(volatile atomic_int *object, int operand);\n"
"int __ovld atomic_fetch_max_explicit(volatile atomic_int *object, int operand, memory_order order);\n"
"int __ovld atomic_fetch_max_explicit(volatile atomic_int *object, int operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_max(volatile atomic_uint *object, uint operand);\n"
"uint __ovld atomic_fetch_max_explicit(volatile atomic_uint *object, uint operand, memory_order order);\n"
"uint __ovld atomic_fetch_max_explicit(volatile atomic_uint *object, uint operand, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_fetch_max(volatile atomic_uint *object, int operand);\n"
"uint __ovld atomic_fetch_max_explicit(volatile atomic_uint *object, int operand, memory_order order);\n"
"uint __ovld atomic_fetch_max_explicit(volatile atomic_uint *object, int operand, memory_order order, memory_scope scope);\n"
"\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"long __ovld atomic_fetch_add(volatile atomic_long *object, long operand);\n"
"long __ovld atomic_fetch_add_explicit(volatile atomic_long *object, long operand, memory_order order);\n"
"long __ovld atomic_fetch_add_explicit(volatile atomic_long *object, long operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_add(volatile atomic_ulong *object, ulong operand);\n"
"ulong __ovld atomic_fetch_add_explicit(volatile atomic_ulong *object, ulong operand, memory_order order);\n"
"ulong __ovld atomic_fetch_add_explicit(volatile atomic_ulong *object, ulong operand, memory_order order, memory_scope scope);\n"
"long __ovld atomic_fetch_sub(volatile atomic_long *object, long operand);\n"
"long __ovld atomic_fetch_sub_explicit(volatile atomic_long *object, long operand, memory_order order);\n"
"long __ovld atomic_fetch_sub_explicit(volatile atomic_long *object, long operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_sub(volatile atomic_ulong *object, ulong operand);\n"
"ulong __ovld atomic_fetch_sub_explicit(volatile atomic_ulong *object, ulong operand, memory_order order);\n"
"ulong __ovld atomic_fetch_sub_explicit(volatile atomic_ulong *object, ulong operand, memory_order order, memory_scope scope);\n"
"long __ovld atomic_fetch_or(volatile atomic_long *object, long operand);\n"
"long __ovld atomic_fetch_or_explicit(volatile atomic_long *object, long operand, memory_order order);\n"
"long __ovld atomic_fetch_or_explicit(volatile atomic_long *object, long operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_or(volatile atomic_ulong *object, ulong operand);\n"
"ulong __ovld atomic_fetch_or_explicit(volatile atomic_ulong *object, ulong operand, memory_order order);\n"
"ulong __ovld atomic_fetch_or_explicit(volatile atomic_ulong *object, ulong operand, memory_order order, memory_scope scope);\n"
"long __ovld atomic_fetch_xor(volatile atomic_long *object, long operand);\n"
"long __ovld atomic_fetch_xor_explicit(volatile atomic_long *object, long operand, memory_order order);\n"
"long __ovld atomic_fetch_xor_explicit(volatile atomic_long *object, long operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_xor(volatile atomic_ulong *object, ulong operand);\n"
"ulong __ovld atomic_fetch_xor_explicit(volatile atomic_ulong *object, ulong operand, memory_order order);\n"
"ulong __ovld atomic_fetch_xor_explicit(volatile atomic_ulong *object, ulong operand, memory_order order, memory_scope scope);\n"
"long __ovld atomic_fetch_and(volatile atomic_long *object, long operand);\n"
"long __ovld atomic_fetch_and_explicit(volatile atomic_long *object, long operand, memory_order order);\n"
"long __ovld atomic_fetch_and_explicit(volatile atomic_long *object, long operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_and(volatile atomic_ulong *object, ulong operand);\n"
"ulong __ovld atomic_fetch_and_explicit(volatile atomic_ulong *object, ulong operand, memory_order order);\n"
"ulong __ovld atomic_fetch_and_explicit(volatile atomic_ulong *object, ulong operand, memory_order order, memory_scope scope);\n"
"long __ovld atomic_fetch_min(volatile atomic_long *object, long operand);\n"
"long __ovld atomic_fetch_min_explicit(volatile atomic_long *object, long operand, memory_order order);\n"
"long __ovld atomic_fetch_min_explicit(volatile atomic_long *object, long operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_min(volatile atomic_ulong *object, ulong operand);\n"
"ulong __ovld atomic_fetch_min_explicit(volatile atomic_ulong *object, ulong operand, memory_order order);\n"
"ulong __ovld atomic_fetch_min_explicit(volatile atomic_ulong *object, ulong operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_min(volatile atomic_ulong *object, long operand);\n"
"ulong __ovld atomic_fetch_min_explicit(volatile atomic_ulong *object, long operand, memory_order order);\n"
"ulong __ovld atomic_fetch_min_explicit(volatile atomic_ulong *object, long operand, memory_order order, memory_scope scope);\n"
"long __ovld atomic_fetch_max(volatile atomic_long *object, long operand);\n"
"long __ovld atomic_fetch_max_explicit(volatile atomic_long *object, long operand, memory_order order);\n"
"long __ovld atomic_fetch_max_explicit(volatile atomic_long *object, long operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_max(volatile atomic_ulong *object, ulong operand);\n"
"ulong __ovld atomic_fetch_max_explicit(volatile atomic_ulong *object, ulong operand, memory_order order);\n"
"ulong __ovld atomic_fetch_max_explicit(volatile atomic_ulong *object, ulong operand, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_fetch_max(volatile atomic_ulong *object, long operand);\n"
"ulong __ovld atomic_fetch_max_explicit(volatile atomic_ulong *object, long operand, memory_order order);\n"
"ulong __ovld atomic_fetch_max_explicit(volatile atomic_ulong *object, long operand, memory_order order, memory_scope scope);\n"
"#endif //defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"\n"
"// OpenCL v2.0 s6.13.11.7.5:\n"
"// add/sub: atomic type argument can be uintptr_t/intptr_t, value type argument can be ptrdiff_t.\n"
"// or/xor/and/min/max: atomic type argument can be intptr_t/uintptr_t, value type argument can be intptr_t/uintptr_t.\n"
"\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"uintptr_t __ovld atomic_fetch_add(volatile atomic_uintptr_t *object, ptrdiff_t operand);\n"
"uintptr_t __ovld atomic_fetch_add_explicit(volatile atomic_uintptr_t *object, ptrdiff_t operand, memory_order order);\n"
"uintptr_t __ovld atomic_fetch_add_explicit(volatile atomic_uintptr_t *object, ptrdiff_t operand, memory_order order, memory_scope scope);\n"
"uintptr_t __ovld atomic_fetch_sub(volatile atomic_uintptr_t *object, ptrdiff_t operand);\n"
"uintptr_t __ovld atomic_fetch_sub_explicit(volatile atomic_uintptr_t *object, ptrdiff_t operand, memory_order order);\n"
"uintptr_t __ovld atomic_fetch_sub_explicit(volatile atomic_uintptr_t *object, ptrdiff_t operand, memory_order order, memory_scope scope);\n"
"\n"
"uintptr_t __ovld atomic_fetch_or(volatile atomic_uintptr_t *object, intptr_t operand);\n"
"uintptr_t __ovld atomic_fetch_or_explicit(volatile atomic_uintptr_t *object, intptr_t operand, memory_order order);\n"
"uintptr_t __ovld atomic_fetch_or_explicit(volatile atomic_uintptr_t *object, intptr_t operand, memory_order order, memory_scope scope);\n"
"uintptr_t __ovld atomic_fetch_xor(volatile atomic_uintptr_t *object, intptr_t operand);\n"
"uintptr_t __ovld atomic_fetch_xor_explicit(volatile atomic_uintptr_t *object, intptr_t operand, memory_order order);\n"
"uintptr_t __ovld atomic_fetch_xor_explicit(volatile atomic_uintptr_t *object, intptr_t operand, memory_order order, memory_scope scope);\n"
"uintptr_t __ovld atomic_fetch_and(volatile atomic_uintptr_t *object, intptr_t operand);\n"
"uintptr_t __ovld atomic_fetch_and_explicit(volatile atomic_uintptr_t *object, intptr_t operand, memory_order order);\n"
"uintptr_t __ovld atomic_fetch_and_explicit(volatile atomic_uintptr_t *object, intptr_t operand, memory_order order, memory_scope scope);\n"
"uintptr_t __ovld atomic_fetch_min(volatile atomic_uintptr_t *object, intptr_t opermax);\n"
"uintptr_t __ovld atomic_fetch_min_explicit(volatile atomic_uintptr_t *object, intptr_t opermax, memory_order minder);\n"
"uintptr_t __ovld atomic_fetch_min_explicit(volatile atomic_uintptr_t *object, intptr_t opermax, memory_order minder, memory_scope scope);\n"
"uintptr_t __ovld atomic_fetch_max(volatile atomic_uintptr_t *object, intptr_t opermax);\n"
"uintptr_t __ovld atomic_fetch_max_explicit(volatile atomic_uintptr_t *object, intptr_t opermax, memory_order minder);\n"
"uintptr_t __ovld atomic_fetch_max_explicit(volatile atomic_uintptr_t *object, intptr_t opermax, memory_order minder, memory_scope scope);\n"
"\n"
"intptr_t __ovld atomic_fetch_or(volatile atomic_intptr_t *object, uintptr_t operand);\n"
"intptr_t __ovld atomic_fetch_or_explicit(volatile atomic_intptr_t *object, uintptr_t operand, memory_order order);\n"
"intptr_t __ovld atomic_fetch_or_explicit(volatile atomic_intptr_t *object, uintptr_t operand, memory_order order, memory_scope scope);\n"
"intptr_t __ovld atomic_fetch_xor(volatile atomic_intptr_t *object, uintptr_t operand);\n"
"intptr_t __ovld atomic_fetch_xor_explicit(volatile atomic_intptr_t *object, uintptr_t operand, memory_order order);\n"
"intptr_t __ovld atomic_fetch_xor_explicit(volatile atomic_intptr_t *object, uintptr_t operand, memory_order order, memory_scope scope);\n"
"intptr_t __ovld atomic_fetch_and(volatile atomic_intptr_t *object, uintptr_t operand);\n"
"intptr_t __ovld atomic_fetch_and_explicit(volatile atomic_intptr_t *object, uintptr_t operand, memory_order order);\n"
"intptr_t __ovld atomic_fetch_and_explicit(volatile atomic_intptr_t *object, uintptr_t operand, memory_order order, memory_scope scope);\n"
"intptr_t __ovld atomic_fetch_min(volatile atomic_intptr_t *object, uintptr_t opermax);\n"
"intptr_t __ovld atomic_fetch_min_explicit(volatile atomic_intptr_t *object, uintptr_t opermax, memory_order minder);\n"
"intptr_t __ovld atomic_fetch_min_explicit(volatile atomic_intptr_t *object, uintptr_t opermax, memory_order minder, memory_scope scope);\n"
"intptr_t __ovld atomic_fetch_max(volatile atomic_intptr_t *object, uintptr_t opermax);\n"
"intptr_t __ovld atomic_fetch_max_explicit(volatile atomic_intptr_t *object, uintptr_t opermax, memory_order minder);\n"
"intptr_t __ovld atomic_fetch_max_explicit(volatile atomic_intptr_t *object, uintptr_t opermax, memory_order minder, memory_scope scope);\n"
"#endif\n"
"\n"
"// atomic_store()\n"
"\n"
"void __ovld atomic_store(volatile atomic_int *object, int desired);\n"
"void __ovld atomic_store_explicit(volatile atomic_int *object, int desired, memory_order order);\n"
"void __ovld atomic_store_explicit(volatile atomic_int *object, int desired, memory_order order, memory_scope scope);\n"
"void __ovld atomic_store(volatile atomic_uint *object, uint desired);\n"
"void __ovld atomic_store_explicit(volatile atomic_uint *object, uint desired, memory_order order);\n"
"void __ovld atomic_store_explicit(volatile atomic_uint *object, uint desired, memory_order order, memory_scope scope);\n"
"void __ovld atomic_store(volatile atomic_float *object, float desired);\n"
"void __ovld atomic_store_explicit(volatile atomic_float *object, float desired, memory_order order);\n"
"void __ovld atomic_store_explicit(volatile atomic_float *object, float desired, memory_order order, memory_scope scope);\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"#ifdef cl_khr_fp64\n"
"void __ovld atomic_store(volatile atomic_double *object, double desired);\n"
"void __ovld atomic_store_explicit(volatile atomic_double *object, double desired, memory_order order);\n"
"void __ovld atomic_store_explicit(volatile atomic_double *object, double desired, memory_order order, memory_scope scope);\n"
"#endif //cl_khr_fp64\n"
"void __ovld atomic_store(volatile atomic_long *object, long desired);\n"
"void __ovld atomic_store_explicit(volatile atomic_long *object, long desired, memory_order order);\n"
"void __ovld atomic_store_explicit(volatile atomic_long *object, long desired, memory_order order, memory_scope scope);\n"
"void __ovld atomic_store(volatile atomic_ulong *object, ulong desired);\n"
"void __ovld atomic_store_explicit(volatile atomic_ulong *object, ulong desired, memory_order order);\n"
"void __ovld atomic_store_explicit(volatile atomic_ulong *object, ulong desired, memory_order order, memory_scope scope);\n"
"#endif\n"
"\n"
"// atomic_load()\n"
"\n"
"int __ovld atomic_load(volatile atomic_int *object);\n"
"int __ovld atomic_load_explicit(volatile atomic_int *object, memory_order order);\n"
"int __ovld atomic_load_explicit(volatile atomic_int *object, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_load(volatile atomic_uint *object);\n"
"uint __ovld atomic_load_explicit(volatile atomic_uint *object, memory_order order);\n"
"uint __ovld atomic_load_explicit(volatile atomic_uint *object, memory_order order, memory_scope scope);\n"
"float __ovld atomic_load(volatile atomic_float *object);\n"
"float __ovld atomic_load_explicit(volatile atomic_float *object, memory_order order);\n"
"float __ovld atomic_load_explicit(volatile atomic_float *object, memory_order order, memory_scope scope);\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"#ifdef cl_khr_fp64\n"
"double __ovld atomic_load(volatile atomic_double *object);\n"
"double __ovld atomic_load_explicit(volatile atomic_double *object, memory_order order);\n"
"double __ovld atomic_load_explicit(volatile atomic_double *object, memory_order order, memory_scope scope);\n"
"#endif //cl_khr_fp64\n"
"long __ovld atomic_load(volatile atomic_long *object);\n"
"long __ovld atomic_load_explicit(volatile atomic_long *object, memory_order order);\n"
"long __ovld atomic_load_explicit(volatile atomic_long *object, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_load(volatile atomic_ulong *object);\n"
"ulong __ovld atomic_load_explicit(volatile atomic_ulong *object, memory_order order);\n"
"ulong __ovld atomic_load_explicit(volatile atomic_ulong *object, memory_order order, memory_scope scope);\n"
"#endif\n"
"\n"
"// atomic_exchange()\n"
"\n"
"int __ovld atomic_exchange(volatile atomic_int *object, int desired);\n"
"int __ovld atomic_exchange_explicit(volatile atomic_int *object, int desired, memory_order order);\n"
"int __ovld atomic_exchange_explicit(volatile atomic_int *object, int desired, memory_order order, memory_scope scope);\n"
"uint __ovld atomic_exchange(volatile atomic_uint *object, uint desired);\n"
"uint __ovld atomic_exchange_explicit(volatile atomic_uint *object, uint desired, memory_order order);\n"
"uint __ovld atomic_exchange_explicit(volatile atomic_uint *object, uint desired, memory_order order, memory_scope scope);\n"
"float __ovld atomic_exchange(volatile atomic_float *object, float desired);\n"
"float __ovld atomic_exchange_explicit(volatile atomic_float *object, float desired, memory_order order);\n"
"float __ovld atomic_exchange_explicit(volatile atomic_float *object, float desired, memory_order order, memory_scope scope);\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"#ifdef cl_khr_fp64\n"
"double __ovld atomic_exchange(volatile atomic_double *object, double desired);\n"
"double __ovld atomic_exchange_explicit(volatile atomic_double *object, double desired, memory_order order);\n"
"double __ovld atomic_exchange_explicit(volatile atomic_double *object, double desired, memory_order order, memory_scope scope);\n"
"#endif //cl_khr_fp64\n"
"long __ovld atomic_exchange(volatile atomic_long *object, long desired);\n"
"long __ovld atomic_exchange_explicit(volatile atomic_long *object, long desired, memory_order order);\n"
"long __ovld atomic_exchange_explicit(volatile atomic_long *object, long desired, memory_order order, memory_scope scope);\n"
"ulong __ovld atomic_exchange(volatile atomic_ulong *object, ulong desired);\n"
"ulong __ovld atomic_exchange_explicit(volatile atomic_ulong *object, ulong desired, memory_order order);\n"
"ulong __ovld atomic_exchange_explicit(volatile atomic_ulong *object, ulong desired, memory_order order, memory_scope scope);\n"
"#endif\n"
"\n"
"// atomic_compare_exchange_strong() and atomic_compare_exchange_weak()\n"
"\n"
"bool __ovld atomic_compare_exchange_strong(volatile atomic_int *object, int *expected, int desired);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_int *object, int *expected,\n"
"                                                                                 int desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_int *object, int *expected,\n"
"                                                                                 int desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_strong(volatile atomic_uint *object, uint *expected, uint desired);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_uint *object, uint *expected,\n"
"                                                                                 uint desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_uint *object, uint *expected,\n"
"                                                                                 uint desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_weak(volatile atomic_int *object, int *expected, int desired);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_int *object, int *expected,\n"
"                                                                                 int desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_int *object, int *expected,\n"
"                                                                                 int desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_weak(volatile atomic_uint *object, uint *expected, uint desired);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_uint *object, uint *expected,\n"
"                                                                                 uint desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_uint *object, uint *expected,\n"
"                                                                                 uint desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_strong(volatile atomic_float *object, float *expected, float desired);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_float *object, float *expected,\n"
"                                                                                 float desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_float *object, float *expected,\n"
"                                                                                 float desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_weak(volatile atomic_float *object, float *expected, float desired);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_float *object, float *expected,\n"
"                                                                                 float desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_float *object, float *expected,\n"
"                                                                                 float desired, memory_order success, memory_order failure, memory_scope scope);\n"
"#if defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)\n"
"#ifdef cl_khr_fp64\n"
"bool __ovld atomic_compare_exchange_strong(volatile atomic_double *object, double *expected, double desired);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_double *object, double *expected,\n"
"                                                                                 double desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_double *object, double *expected,\n"
"                                                                                 double desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_weak(volatile atomic_double *object, double *expected, double desired);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_double *object, double *expected,\n"
"                                                                                 double desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_double *object, double *expected,\n"
"                                                                                 double desired, memory_order success, memory_order failure, memory_scope scope);\n"
"#endif //cl_khr_fp64\n"
"bool __ovld atomic_compare_exchange_strong(volatile atomic_long *object, long *expected, long desired);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_long *object, long *expected,\n"
"                                                                                 long desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_long *object, long *expected,\n"
"                                                                                 long desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_weak(volatile atomic_long *object, long *expected, long desired);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_long *object, long *expected,\n"
"                                                                                 long desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_long *object, long *expected,\n"
"                                                                                 long desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_strong(volatile atomic_ulong *object, ulong *expected, ulong desired);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_ulong *object, ulong *expected,\n"
"                                                                                 ulong desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_strong_explicit(volatile atomic_ulong *object, ulong *expected,\n"
"                                                                                 ulong desired, memory_order success, memory_order failure, memory_scope scope);\n"
"bool __ovld atomic_compare_exchange_weak(volatile atomic_ulong *object, ulong *expected, ulong desired);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_ulong *object, ulong *expected,\n"
"                                                                                 ulong desired, memory_order success, memory_order failure);\n"
"bool __ovld atomic_compare_exchange_weak_explicit(volatile atomic_ulong *object, ulong *expected,\n"
"                                                                                 ulong desired, memory_order success, memory_order failure, memory_scope scope);\n"
"#endif\n"
"\n"
"// atomic_flag_test_and_set() and atomic_flag_clear()\n"
"\n"
"bool __ovld atomic_flag_test_and_set(volatile atomic_flag *object);\n"
"bool __ovld atomic_flag_test_and_set_explicit(volatile atomic_flag *object, memory_order order);\n"
"bool __ovld atomic_flag_test_and_set_explicit(volatile atomic_flag *object, memory_order order, memory_scope scope);\n"
"void __ovld atomic_flag_clear(volatile atomic_flag *object);\n"
"void __ovld atomic_flag_clear_explicit(volatile atomic_flag *object, memory_order order);\n"
"void __ovld atomic_flag_clear_explicit(volatile atomic_flag *object, memory_order order, memory_scope scope);\n"
"\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL v1.1 s6.11.12, v1.2 s6.12.12, v2.0 s6.13.12 - Miscellaneous Vector Functions\n"
"\n"
"/**\n"
" * The shuffle and shuffle2 built-in functions construct\n"
" * a permutation of elements from one or two input\n"
" * vectors respectively that are of the same type,\n"
" * returning a vector with the same element type as the\n"
" * input and length that is the same as the shuffle mask.\n"
" * The size of each element in the mask must match the\n"
" * size of each element in the result. For shuffle, only\n"
" * the ilogb(2m-1) least significant bits of each mask\n"
" * element are considered. For shuffle2, only the\n"
" * ilogb(2m-1)+1 least significant bits of each mask\n"
" * element are considered. Other bits in the mask shall\n"
" * be ignored.\n"
" * The elements of the input vectors are numbered from\n"
" * left to right across one or both of the vectors. For this\n"
" * purpose, the number of elements in a vector is given\n"
" * by vec_step(gentypem). The shuffle mask operand\n"
" * specifies, for each element of the result vector, which\n"
" * element of the one or two input vectors the result\n"
" * element gets.\n"
" * Examples:\n"
" * uint4 mask = (uint4)(3, 2,\n"
" * 1, 0);\n"
" * float4 a;\n"
" * float4 r = shuffle(a, mask);\n"
" * // r.s0123 = a.wzyx\n"
" * uint8 mask = (uint8)(0, 1, 2, 3,\n"
" * 4, 5, 6, 7);\n"
" * float4 a, b;\n"
" * float8 r = shuffle2(a, b, mask);\n"
" * // r.s0123 = a.xyzw\n"
" * // r.s4567 = b.xyzw\n"
" * uint4 mask;\n"
" * float8 a;\n"
" * float4 b;\n"
" * b = shuffle(a, mask);\n"
" * Examples that are not valid are:\n"
" * uint8 mask;\n"
" * short16 a;\n"
" * short8 b;\n"
" * b = shuffle(a, mask); <- not valid\n"
" */\n"
"char2 __ovld __cnfn shuffle(char2 x, uchar2 mask);\n"
"char2 __ovld __cnfn shuffle(char4 x, uchar2 mask);\n"
"char2 __ovld __cnfn shuffle(char8 x, uchar2 mask);\n"
"char2 __ovld __cnfn shuffle(char16 x, uchar2 mask);\n"
"\n"
"uchar2 __ovld __cnfn shuffle(uchar2 x, uchar2 mask);\n"
"uchar2 __ovld __cnfn shuffle(uchar4 x, uchar2 mask);\n"
"uchar2 __ovld __cnfn shuffle(uchar8 x, uchar2 mask);\n"
"uchar2 __ovld __cnfn shuffle(uchar16 x, uchar2 mask);\n"
"\n"
"short2 __ovld __cnfn shuffle(short2 x, ushort2 mask);\n"
"short2 __ovld __cnfn shuffle(short4 x, ushort2 mask);\n"
"short2 __ovld __cnfn shuffle(short8 x, ushort2 mask);\n"
"short2 __ovld __cnfn shuffle(short16 x, ushort2 mask);\n"
"\n"
"ushort2 __ovld __cnfn shuffle(ushort2 x, ushort2 mask);\n"
"ushort2 __ovld __cnfn shuffle(ushort4 x, ushort2 mask);\n"
"ushort2 __ovld __cnfn shuffle(ushort8 x, ushort2 mask);\n"
"ushort2 __ovld __cnfn shuffle(ushort16 x, ushort2 mask);\n"
"\n"
"int2 __ovld __cnfn shuffle(int2 x, uint2 mask);\n"
"int2 __ovld __cnfn shuffle(int4 x, uint2 mask);\n"
"int2 __ovld __cnfn shuffle(int8 x, uint2 mask);\n"
"int2 __ovld __cnfn shuffle(int16 x, uint2 mask);\n"
"\n"
"uint2 __ovld __cnfn shuffle(uint2 x, uint2 mask);\n"
"uint2 __ovld __cnfn shuffle(uint4 x, uint2 mask);\n"
"uint2 __ovld __cnfn shuffle(uint8 x, uint2 mask);\n"
"uint2 __ovld __cnfn shuffle(uint16 x, uint2 mask);\n"
"\n"
"long2 __ovld __cnfn shuffle(long2 x, ulong2 mask);\n"
"long2 __ovld __cnfn shuffle(long4 x, ulong2 mask);\n"
"long2 __ovld __cnfn shuffle(long8 x, ulong2 mask);\n"
"long2 __ovld __cnfn shuffle(long16 x, ulong2 mask);\n"
"\n"
"ulong2 __ovld __cnfn shuffle(ulong2 x, ulong2 mask);\n"
"ulong2 __ovld __cnfn shuffle(ulong4 x, ulong2 mask);\n"
"ulong2 __ovld __cnfn shuffle(ulong8 x, ulong2 mask);\n"
"ulong2 __ovld __cnfn shuffle(ulong16 x, ulong2 mask);\n"
"\n"
"float2 __ovld __cnfn shuffle(float2 x, uint2 mask);\n"
"float2 __ovld __cnfn shuffle(float4 x, uint2 mask);\n"
"float2 __ovld __cnfn shuffle(float8 x, uint2 mask);\n"
"float2 __ovld __cnfn shuffle(float16 x, uint2 mask);\n"
"\n"
"char4 __ovld __cnfn shuffle(char2 x, uchar4 mask);\n"
"char4 __ovld __cnfn shuffle(char4 x, uchar4 mask);\n"
"char4 __ovld __cnfn shuffle(char8 x, uchar4 mask);\n"
"char4 __ovld __cnfn shuffle(char16 x, uchar4 mask);\n"
"\n"
"uchar4 __ovld __cnfn shuffle(uchar2 x, uchar4 mask);\n"
"uchar4 __ovld __cnfn shuffle(uchar4 x, uchar4 mask);\n"
"uchar4 __ovld __cnfn shuffle(uchar8 x, uchar4 mask);\n"
"uchar4 __ovld __cnfn shuffle(uchar16 x, uchar4 mask);\n"
"\n"
"short4 __ovld __cnfn shuffle(short2 x, ushort4 mask);\n"
"short4 __ovld __cnfn shuffle(short4 x, ushort4 mask);\n"
"short4 __ovld __cnfn shuffle(short8 x, ushort4 mask);\n"
"short4 __ovld __cnfn shuffle(short16 x, ushort4 mask);\n"
"\n"
"ushort4 __ovld __cnfn shuffle(ushort2 x, ushort4 mask);\n"
"ushort4 __ovld __cnfn shuffle(ushort4 x, ushort4 mask);\n"
"ushort4 __ovld __cnfn shuffle(ushort8 x, ushort4 mask);\n"
"ushort4 __ovld __cnfn shuffle(ushort16 x, ushort4 mask);\n"
"\n"
"int4 __ovld __cnfn shuffle(int2 x, uint4 mask);\n"
"int4 __ovld __cnfn shuffle(int4 x, uint4 mask);\n"
"int4 __ovld __cnfn shuffle(int8 x, uint4 mask);\n"
"int4 __ovld __cnfn shuffle(int16 x, uint4 mask);\n"
"\n"
"uint4 __ovld __cnfn shuffle(uint2 x, uint4 mask);\n"
"uint4 __ovld __cnfn shuffle(uint4 x, uint4 mask);\n"
"uint4 __ovld __cnfn shuffle(uint8 x, uint4 mask);\n"
"uint4 __ovld __cnfn shuffle(uint16 x, uint4 mask);\n"
"\n"
"long4 __ovld __cnfn shuffle(long2 x, ulong4 mask);\n"
"long4 __ovld __cnfn shuffle(long4 x, ulong4 mask);\n"
"long4 __ovld __cnfn shuffle(long8 x, ulong4 mask);\n"
"long4 __ovld __cnfn shuffle(long16 x, ulong4 mask);\n"
"\n"
"ulong4 __ovld __cnfn shuffle(ulong2 x, ulong4 mask);\n"
"ulong4 __ovld __cnfn shuffle(ulong4 x, ulong4 mask);\n"
"ulong4 __ovld __cnfn shuffle(ulong8 x, ulong4 mask);\n"
"ulong4 __ovld __cnfn shuffle(ulong16 x, ulong4 mask);\n"
"\n"
"float4 __ovld __cnfn shuffle(float2 x, uint4 mask);\n"
"float4 __ovld __cnfn shuffle(float4 x, uint4 mask);\n"
"float4 __ovld __cnfn shuffle(float8 x, uint4 mask);\n"
"float4 __ovld __cnfn shuffle(float16 x, uint4 mask);\n"
"\n"
"char8 __ovld __cnfn shuffle(char2 x, uchar8 mask);\n"
"char8 __ovld __cnfn shuffle(char4 x, uchar8 mask);\n"
"char8 __ovld __cnfn shuffle(char8 x, uchar8 mask);\n"
"char8 __ovld __cnfn shuffle(char16 x, uchar8 mask);\n"
"\n"
"uchar8 __ovld __cnfn shuffle(uchar2 x, uchar8 mask);\n"
"uchar8 __ovld __cnfn shuffle(uchar4 x, uchar8 mask);\n"
"uchar8 __ovld __cnfn shuffle(uchar8 x, uchar8 mask);\n"
"uchar8 __ovld __cnfn shuffle(uchar16 x, uchar8 mask);\n"
"\n"
"short8 __ovld __cnfn shuffle(short2 x, ushort8 mask);\n"
"short8 __ovld __cnfn shuffle(short4 x, ushort8 mask);\n"
"short8 __ovld __cnfn shuffle(short8 x, ushort8 mask);\n"
"short8 __ovld __cnfn shuffle(short16 x, ushort8 mask);\n"
"\n"
"ushort8 __ovld __cnfn shuffle(ushort2 x, ushort8 mask);\n"
"ushort8 __ovld __cnfn shuffle(ushort4 x, ushort8 mask);\n"
"ushort8 __ovld __cnfn shuffle(ushort8 x, ushort8 mask);\n"
"ushort8 __ovld __cnfn shuffle(ushort16 x, ushort8 mask);\n"
"\n"
"int8 __ovld __cnfn shuffle(int2 x, uint8 mask);\n"
"int8 __ovld __cnfn shuffle(int4 x, uint8 mask);\n"
"int8 __ovld __cnfn shuffle(int8 x, uint8 mask);\n"
"int8 __ovld __cnfn shuffle(int16 x, uint8 mask);\n"
"\n"
"uint8 __ovld __cnfn shuffle(uint2 x, uint8 mask);\n"
"uint8 __ovld __cnfn shuffle(uint4 x, uint8 mask);\n"
"uint8 __ovld __cnfn shuffle(uint8 x, uint8 mask);\n"
"uint8 __ovld __cnfn shuffle(uint16 x, uint8 mask);\n"
"\n"
"long8 __ovld __cnfn shuffle(long2 x, ulong8 mask);\n"
"long8 __ovld __cnfn shuffle(long4 x, ulong8 mask);\n"
"long8 __ovld __cnfn shuffle(long8 x, ulong8 mask);\n"
"long8 __ovld __cnfn shuffle(long16 x, ulong8 mask);\n"
"\n"
"ulong8 __ovld __cnfn shuffle(ulong2 x, ulong8 mask);\n"
"ulong8 __ovld __cnfn shuffle(ulong4 x, ulong8 mask);\n"
"ulong8 __ovld __cnfn shuffle(ulong8 x, ulong8 mask);\n"
"ulong8 __ovld __cnfn shuffle(ulong16 x, ulong8 mask);\n"
"\n"
"float8 __ovld __cnfn shuffle(float2 x, uint8 mask);\n"
"float8 __ovld __cnfn shuffle(float4 x, uint8 mask);\n"
"float8 __ovld __cnfn shuffle(float8 x, uint8 mask);\n"
"float8 __ovld __cnfn shuffle(float16 x, uint8 mask);\n"
"\n"
"char16 __ovld __cnfn shuffle(char2 x, uchar16 mask);\n"
"char16 __ovld __cnfn shuffle(char4 x, uchar16 mask);\n"
"char16 __ovld __cnfn shuffle(char8 x, uchar16 mask);\n"
"char16 __ovld __cnfn shuffle(char16 x, uchar16 mask);\n"
"\n"
"uchar16 __ovld __cnfn shuffle(uchar2 x, uchar16 mask);\n"
"uchar16 __ovld __cnfn shuffle(uchar4 x, uchar16 mask);\n"
"uchar16 __ovld __cnfn shuffle(uchar8 x, uchar16 mask);\n"
"uchar16 __ovld __cnfn shuffle(uchar16 x, uchar16 mask);\n"
"\n"
"short16 __ovld __cnfn shuffle(short2 x, ushort16 mask);\n"
"short16 __ovld __cnfn shuffle(short4 x, ushort16 mask);\n"
"short16 __ovld __cnfn shuffle(short8 x, ushort16 mask);\n"
"short16 __ovld __cnfn shuffle(short16 x, ushort16 mask);\n"
"\n"
"ushort16 __ovld __cnfn shuffle(ushort2 x, ushort16 mask);\n"
"ushort16 __ovld __cnfn shuffle(ushort4 x, ushort16 mask);\n"
"ushort16 __ovld __cnfn shuffle(ushort8 x, ushort16 mask);\n"
"ushort16 __ovld __cnfn shuffle(ushort16 x, ushort16 mask);\n"
"\n"
"int16 __ovld __cnfn shuffle(int2 x, uint16 mask);\n"
"int16 __ovld __cnfn shuffle(int4 x, uint16 mask);\n"
"int16 __ovld __cnfn shuffle(int8 x, uint16 mask);\n"
"int16 __ovld __cnfn shuffle(int16 x, uint16 mask);\n"
"\n"
"uint16 __ovld __cnfn shuffle(uint2 x, uint16 mask);\n"
"uint16 __ovld __cnfn shuffle(uint4 x, uint16 mask);\n"
"uint16 __ovld __cnfn shuffle(uint8 x, uint16 mask);\n"
"uint16 __ovld __cnfn shuffle(uint16 x, uint16 mask);\n"
"\n"
"long16 __ovld __cnfn shuffle(long2 x, ulong16 mask);\n"
"long16 __ovld __cnfn shuffle(long4 x, ulong16 mask);\n"
"long16 __ovld __cnfn shuffle(long8 x, ulong16 mask);\n"
"long16 __ovld __cnfn shuffle(long16 x, ulong16 mask);\n"
"\n"
"ulong16 __ovld __cnfn shuffle(ulong2 x, ulong16 mask);\n"
"ulong16 __ovld __cnfn shuffle(ulong4 x, ulong16 mask);\n"
"ulong16 __ovld __cnfn shuffle(ulong8 x, ulong16 mask);\n"
"ulong16 __ovld __cnfn shuffle(ulong16 x, ulong16 mask);\n"
"\n"
"float16 __ovld __cnfn shuffle(float2 x, uint16 mask);\n"
"float16 __ovld __cnfn shuffle(float4 x, uint16 mask);\n"
"float16 __ovld __cnfn shuffle(float8 x, uint16 mask);\n"
"float16 __ovld __cnfn shuffle(float16 x, uint16 mask);\n"
"\n"
"#ifdef cl_khr_fp64\n"
"double2 __ovld __cnfn shuffle(double2 x, ulong2 mask);\n"
"double2 __ovld __cnfn shuffle(double4 x, ulong2 mask);\n"
"double2 __ovld __cnfn shuffle(double8 x, ulong2 mask);\n"
"double2 __ovld __cnfn shuffle(double16 x, ulong2 mask);\n"
"\n"
"double4 __ovld __cnfn shuffle(double2 x, ulong4 mask);\n"
"double4 __ovld __cnfn shuffle(double4 x, ulong4 mask);\n"
"double4 __ovld __cnfn shuffle(double8 x, ulong4 mask);\n"
"double4 __ovld __cnfn shuffle(double16 x, ulong4 mask);\n"
"\n"
"double8 __ovld __cnfn shuffle(double2 x, ulong8 mask);\n"
"double8 __ovld __cnfn shuffle(double4 x, ulong8 mask);\n"
"double8 __ovld __cnfn shuffle(double8 x, ulong8 mask);\n"
"double8 __ovld __cnfn shuffle(double16 x, ulong8 mask);\n"
"\n"
"double16 __ovld __cnfn shuffle(double2 x, ulong16 mask);\n"
"double16 __ovld __cnfn shuffle(double4 x, ulong16 mask);\n"
"double16 __ovld __cnfn shuffle(double8 x, ulong16 mask);\n"
"double16 __ovld __cnfn shuffle(double16 x, ulong16 mask);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half2 __ovld __cnfn shuffle(half2 x, ushort2 mask);\n"
"half2 __ovld __cnfn shuffle(half4 x, ushort2 mask);\n"
"half2 __ovld __cnfn shuffle(half8 x, ushort2 mask);\n"
"half2 __ovld __cnfn shuffle(half16 x, ushort2 mask);\n"
"\n"
"half4 __ovld __cnfn shuffle(half2 x, ushort4 mask);\n"
"half4 __ovld __cnfn shuffle(half4 x, ushort4 mask);\n"
"half4 __ovld __cnfn shuffle(half8 x, ushort4 mask);\n"
"half4 __ovld __cnfn shuffle(half16 x, ushort4 mask);\n"
"\n"
"half8 __ovld __cnfn shuffle(half2 x, ushort8 mask);\n"
"half8 __ovld __cnfn shuffle(half4 x, ushort8 mask);\n"
"half8 __ovld __cnfn shuffle(half8 x, ushort8 mask);\n"
"half8 __ovld __cnfn shuffle(half16 x, ushort8 mask);\n"
"\n"
"half16 __ovld __cnfn shuffle(half2 x, ushort16 mask);\n"
"half16 __ovld __cnfn shuffle(half4 x, ushort16 mask);\n"
"half16 __ovld __cnfn shuffle(half8 x, ushort16 mask);\n"
"half16 __ovld __cnfn shuffle(half16 x, ushort16 mask);\n"
"#endif //cl_khr_fp16\n"
"\n"
"char2 __ovld __cnfn shuffle2(char2 x, char2 y, uchar2 mask);\n"
"char2 __ovld __cnfn shuffle2(char4 x, char4 y, uchar2 mask);\n"
"char2 __ovld __cnfn shuffle2(char8 x, char8 y, uchar2 mask);\n"
"char2 __ovld __cnfn shuffle2(char16 x, char16 y, uchar2 mask);\n"
"\n"
"uchar2 __ovld __cnfn shuffle2(uchar2 x, uchar2 y, uchar2 mask);\n"
"uchar2 __ovld __cnfn shuffle2(uchar4 x, uchar4 y, uchar2 mask);\n"
"uchar2 __ovld __cnfn shuffle2(uchar8 x, uchar8 y, uchar2 mask);\n"
"uchar2 __ovld __cnfn shuffle2(uchar16 x, uchar16 y, uchar2 mask);\n"
"\n"
"short2 __ovld __cnfn shuffle2(short2 x, short2 y, ushort2 mask);\n"
"short2 __ovld __cnfn shuffle2(short4 x, short4 y, ushort2 mask);\n"
"short2 __ovld __cnfn shuffle2(short8 x, short8 y, ushort2 mask);\n"
"short2 __ovld __cnfn shuffle2(short16 x, short16 y, ushort2 mask);\n"
"\n"
"ushort2 __ovld __cnfn shuffle2(ushort2 x, ushort2 y, ushort2 mask);\n"
"ushort2 __ovld __cnfn shuffle2(ushort4 x, ushort4 y, ushort2 mask);\n"
"ushort2 __ovld __cnfn shuffle2(ushort8 x, ushort8 y, ushort2 mask);\n"
"ushort2 __ovld __cnfn shuffle2(ushort16 x, ushort16 y, ushort2 mask);\n"
"\n"
"int2 __ovld __cnfn shuffle2(int2 x, int2 y, uint2 mask);\n"
"int2 __ovld __cnfn shuffle2(int4 x, int4 y, uint2 mask);\n"
"int2 __ovld __cnfn shuffle2(int8 x, int8 y, uint2 mask);\n"
"int2 __ovld __cnfn shuffle2(int16 x, int16 y, uint2 mask);\n"
"\n"
"uint2 __ovld __cnfn shuffle2(uint2 x, uint2 y, uint2 mask);\n"
"uint2 __ovld __cnfn shuffle2(uint4 x, uint4 y, uint2 mask);\n"
"uint2 __ovld __cnfn shuffle2(uint8 x, uint8 y, uint2 mask);\n"
"uint2 __ovld __cnfn shuffle2(uint16 x, uint16 y, uint2 mask);\n"
"\n"
"long2 __ovld __cnfn shuffle2(long2 x, long2 y, ulong2 mask);\n"
"long2 __ovld __cnfn shuffle2(long4 x, long4 y, ulong2 mask);\n"
"long2 __ovld __cnfn shuffle2(long8 x, long8 y, ulong2 mask);\n"
"long2 __ovld __cnfn shuffle2(long16 x, long16 y, ulong2 mask);\n"
"\n"
"ulong2 __ovld __cnfn shuffle2(ulong2 x, ulong2 y, ulong2 mask);\n"
"ulong2 __ovld __cnfn shuffle2(ulong4 x, ulong4 y, ulong2 mask);\n"
"ulong2 __ovld __cnfn shuffle2(ulong8 x, ulong8 y, ulong2 mask);\n"
"ulong2 __ovld __cnfn shuffle2(ulong16 x, ulong16 y, ulong2 mask);\n"
"\n"
"float2 __ovld __cnfn shuffle2(float2 x, float2 y, uint2 mask);\n"
"float2 __ovld __cnfn shuffle2(float4 x, float4 y, uint2 mask);\n"
"float2 __ovld __cnfn shuffle2(float8 x, float8 y, uint2 mask);\n"
"float2 __ovld __cnfn shuffle2(float16 x, float16 y, uint2 mask);\n"
"\n"
"char4 __ovld __cnfn shuffle2(char2 x, char2 y, uchar4 mask);\n"
"char4 __ovld __cnfn shuffle2(char4 x, char4 y, uchar4 mask);\n"
"char4 __ovld __cnfn shuffle2(char8 x, char8 y, uchar4 mask);\n"
"char4 __ovld __cnfn shuffle2(char16 x, char16 y, uchar4 mask);\n"
"\n"
"uchar4 __ovld __cnfn shuffle2(uchar2 x, uchar2 y, uchar4 mask);\n"
"uchar4 __ovld __cnfn shuffle2(uchar4 x, uchar4 y, uchar4 mask);\n"
"uchar4 __ovld __cnfn shuffle2(uchar8 x, uchar8 y, uchar4 mask);\n"
"uchar4 __ovld __cnfn shuffle2(uchar16 x, uchar16 y, uchar4 mask);\n"
"\n"
"short4 __ovld __cnfn shuffle2(short2 x, short2 y, ushort4 mask);\n"
"short4 __ovld __cnfn shuffle2(short4 x, short4 y, ushort4 mask);\n"
"short4 __ovld __cnfn shuffle2(short8 x, short8 y, ushort4 mask);\n"
"short4 __ovld __cnfn shuffle2(short16 x, short16 y, ushort4 mask);\n"
"\n"
"ushort4 __ovld __cnfn shuffle2(ushort2 x, ushort2 y, ushort4 mask);\n"
"ushort4 __ovld __cnfn shuffle2(ushort4 x, ushort4 y, ushort4 mask);\n"
"ushort4 __ovld __cnfn shuffle2(ushort8 x, ushort8 y, ushort4 mask);\n"
"ushort4 __ovld __cnfn shuffle2(ushort16 x, ushort16 y, ushort4 mask);\n"
"\n"
"int4 __ovld __cnfn shuffle2(int2 x, int2 y, uint4 mask);\n"
"int4 __ovld __cnfn shuffle2(int4 x, int4 y, uint4 mask);\n"
"int4 __ovld __cnfn shuffle2(int8 x, int8 y, uint4 mask);\n"
"int4 __ovld __cnfn shuffle2(int16 x, int16 y, uint4 mask);\n"
"\n"
"uint4 __ovld __cnfn shuffle2(uint2 x, uint2 y, uint4 mask);\n"
"uint4 __ovld __cnfn shuffle2(uint4 x, uint4 y, uint4 mask);\n"
"uint4 __ovld __cnfn shuffle2(uint8 x, uint8 y, uint4 mask);\n"
"uint4 __ovld __cnfn shuffle2(uint16 x, uint16 y, uint4 mask);\n"
"\n"
"long4 __ovld __cnfn shuffle2(long2 x, long2 y, ulong4 mask);\n"
"long4 __ovld __cnfn shuffle2(long4 x, long4 y, ulong4 mask);\n"
"long4 __ovld __cnfn shuffle2(long8 x, long8 y, ulong4 mask);\n"
"long4 __ovld __cnfn shuffle2(long16 x, long16 y, ulong4 mask);\n"
"\n"
"ulong4 __ovld __cnfn shuffle2(ulong2 x, ulong2 y, ulong4 mask);\n"
"ulong4 __ovld __cnfn shuffle2(ulong4 x, ulong4 y, ulong4 mask);\n"
"ulong4 __ovld __cnfn shuffle2(ulong8 x, ulong8 y, ulong4 mask);\n"
"ulong4 __ovld __cnfn shuffle2(ulong16 x, ulong16 y, ulong4 mask);\n"
"\n"
"float4 __ovld __cnfn shuffle2(float2 x, float2 y, uint4 mask);\n"
"float4 __ovld __cnfn shuffle2(float4 x, float4 y, uint4 mask);\n"
"float4 __ovld __cnfn shuffle2(float8 x, float8 y, uint4 mask);\n"
"float4 __ovld __cnfn shuffle2(float16 x, float16 y, uint4 mask);\n"
"\n"
"char8 __ovld __cnfn shuffle2(char2 x, char2 y, uchar8 mask);\n"
"char8 __ovld __cnfn shuffle2(char4 x, char4 y, uchar8 mask);\n"
"char8 __ovld __cnfn shuffle2(char8 x, char8 y, uchar8 mask);\n"
"char8 __ovld __cnfn shuffle2(char16 x, char16 y, uchar8 mask);\n"
"\n"
"uchar8 __ovld __cnfn shuffle2(uchar2 x, uchar2 y, uchar8 mask);\n"
"uchar8 __ovld __cnfn shuffle2(uchar4 x, uchar4 y, uchar8 mask);\n"
"uchar8 __ovld __cnfn shuffle2(uchar8 x, uchar8 y, uchar8 mask);\n"
"uchar8 __ovld __cnfn shuffle2(uchar16 x, uchar16 y, uchar8 mask);\n"
"\n"
"short8 __ovld __cnfn shuffle2(short2 x, short2 y, ushort8 mask);\n"
"short8 __ovld __cnfn shuffle2(short4 x, short4 y, ushort8 mask);\n"
"short8 __ovld __cnfn shuffle2(short8 x, short8 y, ushort8 mask);\n"
"short8 __ovld __cnfn shuffle2(short16 x, short16 y, ushort8 mask);\n"
"\n"
"ushort8 __ovld __cnfn shuffle2(ushort2 x, ushort2 y, ushort8 mask);\n"
"ushort8 __ovld __cnfn shuffle2(ushort4 x, ushort4 y, ushort8 mask);\n"
"ushort8 __ovld __cnfn shuffle2(ushort8 x, ushort8 y, ushort8 mask);\n"
"ushort8 __ovld __cnfn shuffle2(ushort16 x, ushort16 y, ushort8 mask);\n"
"\n"
"int8 __ovld __cnfn shuffle2(int2 x, int2 y, uint8 mask);\n"
"int8 __ovld __cnfn shuffle2(int4 x, int4 y, uint8 mask);\n"
"int8 __ovld __cnfn shuffle2(int8 x, int8 y, uint8 mask);\n"
"int8 __ovld __cnfn shuffle2(int16 x, int16 y, uint8 mask);\n"
"\n"
"uint8 __ovld __cnfn shuffle2(uint2 x, uint2 y, uint8 mask);\n"
"uint8 __ovld __cnfn shuffle2(uint4 x, uint4 y, uint8 mask);\n"
"uint8 __ovld __cnfn shuffle2(uint8 x, uint8 y, uint8 mask);\n"
"uint8 __ovld __cnfn shuffle2(uint16 x, uint16 y, uint8 mask);\n"
"\n"
"long8 __ovld __cnfn shuffle2(long2 x, long2 y, ulong8 mask);\n"
"long8 __ovld __cnfn shuffle2(long4 x, long4 y, ulong8 mask);\n"
"long8 __ovld __cnfn shuffle2(long8 x, long8 y, ulong8 mask);\n"
"long8 __ovld __cnfn shuffle2(long16 x, long16 y, ulong8 mask);\n"
"\n"
"ulong8 __ovld __cnfn shuffle2(ulong2 x, ulong2 y, ulong8 mask);\n"
"ulong8 __ovld __cnfn shuffle2(ulong4 x, ulong4 y, ulong8 mask);\n"
"ulong8 __ovld __cnfn shuffle2(ulong8 x, ulong8 y, ulong8 mask);\n"
"ulong8 __ovld __cnfn shuffle2(ulong16 x, ulong16 y, ulong8 mask);\n"
"\n"
"float8 __ovld __cnfn shuffle2(float2 x, float2 y, uint8 mask);\n"
"float8 __ovld __cnfn shuffle2(float4 x, float4 y, uint8 mask);\n"
"float8 __ovld __cnfn shuffle2(float8 x, float8 y, uint8 mask);\n"
"float8 __ovld __cnfn shuffle2(float16 x, float16 y, uint8 mask);\n"
"\n"
"char16 __ovld __cnfn shuffle2(char2 x, char2 y, uchar16 mask);\n"
"char16 __ovld __cnfn shuffle2(char4 x, char4 y, uchar16 mask);\n"
"char16 __ovld __cnfn shuffle2(char8 x, char8 y, uchar16 mask);\n"
"char16 __ovld __cnfn shuffle2(char16 x, char16 y, uchar16 mask);\n"
"\n"
"uchar16 __ovld __cnfn shuffle2(uchar2 x, uchar2 y, uchar16 mask);\n"
"uchar16 __ovld __cnfn shuffle2(uchar4 x, uchar4 y, uchar16 mask);\n"
"uchar16 __ovld __cnfn shuffle2(uchar8 x, uchar8 y, uchar16 mask);\n"
"uchar16 __ovld __cnfn shuffle2(uchar16 x, uchar16 y, uchar16 mask);\n"
"\n"
"short16 __ovld __cnfn shuffle2(short2 x, short2 y, ushort16 mask);\n"
"short16 __ovld __cnfn shuffle2(short4 x, short4 y, ushort16 mask);\n"
"short16 __ovld __cnfn shuffle2(short8 x, short8 y, ushort16 mask);\n"
"short16 __ovld __cnfn shuffle2(short16 x, short16 y, ushort16 mask);\n"
"\n"
"ushort16 __ovld __cnfn shuffle2(ushort2 x, ushort2 y, ushort16 mask);\n"
"ushort16 __ovld __cnfn shuffle2(ushort4 x, ushort4 y, ushort16 mask);\n"
"ushort16 __ovld __cnfn shuffle2(ushort8 x, ushort8 y, ushort16 mask);\n"
"ushort16 __ovld __cnfn shuffle2(ushort16 x, ushort16 y, ushort16 mask);\n"
"\n"
"int16 __ovld __cnfn shuffle2(int2 x, int2 y, uint16 mask);\n"
"int16 __ovld __cnfn shuffle2(int4 x, int4 y, uint16 mask);\n"
"int16 __ovld __cnfn shuffle2(int8 x, int8 y, uint16 mask);\n"
"int16 __ovld __cnfn shuffle2(int16 x, int16 y, uint16 mask);\n"
"\n"
"uint16 __ovld __cnfn shuffle2(uint2 x, uint2 y, uint16 mask);\n"
"uint16 __ovld __cnfn shuffle2(uint4 x, uint4 y, uint16 mask);\n"
"uint16 __ovld __cnfn shuffle2(uint8 x, uint8 y, uint16 mask);\n"
"uint16 __ovld __cnfn shuffle2(uint16 x, uint16 y, uint16 mask);\n"
"\n"
"long16 __ovld __cnfn shuffle2(long2 x, long2 y, ulong16 mask);\n"
"long16 __ovld __cnfn shuffle2(long4 x, long4 y, ulong16 mask);\n"
"long16 __ovld __cnfn shuffle2(long8 x, long8 y, ulong16 mask);\n"
"long16 __ovld __cnfn shuffle2(long16 x, long16 y, ulong16 mask);\n"
"\n"
"ulong16 __ovld __cnfn shuffle2(ulong2 x, ulong2 y, ulong16 mask);\n"
"ulong16 __ovld __cnfn shuffle2(ulong4 x, ulong4 y, ulong16 mask);\n"
"ulong16 __ovld __cnfn shuffle2(ulong8 x, ulong8 y, ulong16 mask);\n"
"ulong16 __ovld __cnfn shuffle2(ulong16 x, ulong16 y, ulong16 mask);\n"
"\n"
"float16 __ovld __cnfn shuffle2(float2 x, float2 y, uint16 mask);\n"
"float16 __ovld __cnfn shuffle2(float4 x, float4 y, uint16 mask);\n"
"float16 __ovld __cnfn shuffle2(float8 x, float8 y, uint16 mask);\n"
"float16 __ovld __cnfn shuffle2(float16 x, float16 y, uint16 mask);\n"
"\n"
"#ifdef cl_khr_fp64\n"
"double2 __ovld __cnfn shuffle2(double2 x, double2 y, ulong2 mask);\n"
"double2 __ovld __cnfn shuffle2(double4 x, double4 y, ulong2 mask);\n"
"double2 __ovld __cnfn shuffle2(double8 x, double8 y, ulong2 mask);\n"
"double2 __ovld __cnfn shuffle2(double16 x, double16 y, ulong2 mask);\n"
"\n"
"double4 __ovld __cnfn shuffle2(double2 x, double2 y, ulong4 mask);\n"
"double4 __ovld __cnfn shuffle2(double4 x, double4 y, ulong4 mask);\n"
"double4 __ovld __cnfn shuffle2(double8 x, double8 y, ulong4 mask);\n"
"double4 __ovld __cnfn shuffle2(double16 x, double16 y, ulong4 mask);\n"
"\n"
"double8 __ovld __cnfn shuffle2(double2 x, double2 y, ulong8 mask);\n"
"double8 __ovld __cnfn shuffle2(double4 x, double4 y, ulong8 mask);\n"
"double8 __ovld __cnfn shuffle2(double8 x, double8 y, ulong8 mask);\n"
"double8 __ovld __cnfn shuffle2(double16 x, double16 y, ulong8 mask);\n"
"\n"
"double16 __ovld __cnfn shuffle2(double2 x, double2 y, ulong16 mask);\n"
"double16 __ovld __cnfn shuffle2(double4 x, double4 y, ulong16 mask);\n"
"double16 __ovld __cnfn shuffle2(double8 x, double8 y, ulong16 mask);\n"
"double16 __ovld __cnfn shuffle2(double16 x, double16 y, ulong16 mask);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half2 __ovld __cnfn shuffle2(half2 x, half2 y, ushort2 mask);\n"
"half2 __ovld __cnfn shuffle2(half4 x, half4 y, ushort2 mask);\n"
"half2 __ovld __cnfn shuffle2(half8 x, half8 y, ushort2 mask);\n"
"half2 __ovld __cnfn shuffle2(half16 x, half16 y, ushort2 mask);\n"
"\n"
"half4 __ovld __cnfn shuffle2(half2 x, half2 y, ushort4 mask);\n"
"half4 __ovld __cnfn shuffle2(half4 x, half4 y, ushort4 mask);\n"
"half4 __ovld __cnfn shuffle2(half8 x, half8 y, ushort4 mask);\n"
"half4 __ovld __cnfn shuffle2(half16 x, half16 y, ushort4 mask);\n"
"\n"
"half8 __ovld __cnfn shuffle2(half2 x, half2 y, ushort8 mask);\n"
"half8 __ovld __cnfn shuffle2(half4 x, half4 y, ushort8 mask);\n"
"half8 __ovld __cnfn shuffle2(half8 x, half8 y, ushort8 mask);\n"
"half8 __ovld __cnfn shuffle2(half16 x, half16 y, ushort8 mask);\n"
"\n"
"half16 __ovld __cnfn shuffle2(half2 x, half2 y, ushort16 mask);\n"
"half16 __ovld __cnfn shuffle2(half4 x, half4 y, ushort16 mask);\n"
"half16 __ovld __cnfn shuffle2(half8 x, half8 y, ushort16 mask);\n"
"half16 __ovld __cnfn shuffle2(half16 x, half16 y, ushort16 mask);\n"
"#endif //cl_khr_fp16\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"// OpenCL v1.2 s6.12.13, v2.0 s6.13.13 - printf\n"
"\n"
"int printf(__constant const char* st, ...);\n"
"#endif\n"
"\n"
"// OpenCL v1.1 s6.11.3, v1.2 s6.12.14, v2.0 s6.13.14 - Image Read and Write Functions\n"
"\n"
"// These values need to match the runtime equivalent\n"
"//\n"
"// Addressing Mode.\n"
"//\n"
"#define CLK_ADDRESS_NONE                0\n"
"#define CLK_ADDRESS_CLAMP_TO_EDGE       2\n"
"#define CLK_ADDRESS_CLAMP               4\n"
"#define CLK_ADDRESS_REPEAT              6\n"
"#define CLK_ADDRESS_MIRRORED_REPEAT     8\n"
"\n"
"//\n"
"// Coordination Normalization\n"
"//\n"
"#define CLK_NORMALIZED_COORDS_FALSE     0\n"
"#define CLK_NORMALIZED_COORDS_TRUE      1\n"
"\n"
"//\n"
"// Filtering Mode.\n"
"//\n"
"#define CLK_FILTER_NEAREST              0x10\n"
"#define CLK_FILTER_LINEAR               0x20\n"
"\n"
"#ifdef cl_khr_gl_msaa_sharing\n"
"#pragma OPENCL EXTENSION cl_khr_gl_msaa_sharing : enable\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"/**\n"
" * Use the coordinate (coord.xy) to do an element lookup in\n"
" * the 2D image object specified by image.\n"
" *\n"
" * Use the coordinate (coord.x, coord.y, coord.z) to do\n"
" * an element lookup in the 3D image object specified\n"
" * by image. coord.w is ignored.\n"
" *\n"
" * Use the coordinate (coord.z) to index into the\n"
" * 2D image array object specified by image_array\n"
" * and (coord.x, coord.y) to do an element lookup in\n"
" * the 2D image object specified by image.\n"
" *\n"
" * Use the coordinate (x) to do an element lookup in\n"
" * the 1D image object specified by image.\n"
" *\n"
" * Use the coordinate (coord.y) to index into the\n"
" * 1D image array object specified by image_array\n"
" * and (coord.x) to do an element lookup in\n"
" * the 1D image object specified by image.\n"
" *\n"
" * Use the coordinate (cood.xy) and sample to do an\n"
" * element lookup in the 2D multi-sample image specified\n"
" * by image.\n"
" *\n"
" * Use coord.xy and sample to do an element\n"
" * lookup in the 2D multi-sample image layer\n"
" * identified by index coord.z in the 2D multi-sample\n"
" * image array specified by image.\n"
" *\n"
" * For mipmap images, use the mip-level specified by\n"
" * the Level-of-Detail (lod) or use gradients for LOD\n"
" * computation.\n"
" *\n"
" * read_imagef returns floating-point values in the\n"
" * range [0.0 ... 1.0] for image objects created with\n"
" * image_channel_data_type set to one of the predefined\n"
" * packed formats or CL_UNORM_INT8, or\n"
" * CL_UNORM_INT16.\n"
" *\n"
" * read_imagef returns floating-point values in the\n"
" * range [-1.0 ... 1.0] for image objects created with\n"
" * image_channel_data_type set to CL_SNORM_INT8,\n"
" * or CL_SNORM_INT16.\n"
" *\n"
" * read_imagef returns floating-point values for image\n"
" * objects created with image_channel_data_type set to\n"
" * CL_HALF_FLOAT or CL_FLOAT.\n"
" *\n"
" * read_imagei and read_imageui return\n"
" * unnormalized signed integer and unsigned integer\n"
" * values respectively. Each channel will be stored in a\n"
" * 32-bit integer.\n"
" *\n"
" * read_imagei can only be used with image objects\n"
" * created with image_channel_data_type set to one of\n"
" * the following values:\n"
" * CL_SIGNED_INT8,\n"
" * CL_SIGNED_INT16 and\n"
" * CL_SIGNED_INT32.\n"
" * If the image_channel_data_type is not one of the\n"
" * above values, the values returned by read_imagei\n"
" * are undefined.\n"
" *\n"
" * read_imageui can only be used with image objects\n"
" * created with image_channel_data_type set to one of\n"
" * the following values:\n"
" * CL_UNSIGNED_INT8,\n"
" * CL_UNSIGNED_INT16 and\n"
" * CL_UNSIGNED_INT32.\n"
" * If the image_channel_data_type is not one of the\n"
" * above values, the values returned by read_imageui\n"
" * are undefined.\n"
" *\n"
" * The read_image{i|ui} calls support a nearest filter\n"
" * only. The filter_mode specified in sampler\n"
" * must be set to CLK_FILTER_NEAREST; otherwise\n"
" * the values returned are undefined.\n"
"\n"
" * The read_image{f|i|ui} calls that take\n"
" * integer coordinates must use a sampler with\n"
" * normalized coordinates set to\n"
" * CLK_NORMALIZED_COORDS_FALSE and\n"
" * addressing mode set to\n"
" * CLK_ADDRESS_CLAMP_TO_EDGE,\n"
" * CLK_ADDRESS_CLAMP or CLK_ADDRESS_NONE;\n"
" * otherwise the values returned are undefined.\n"
" *\n"
" * Values returned by read_imagef for image objects\n"
" * with image_channel_data_type values not specified\n"
" * in the description above are undefined.\n"
" */\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_t image, sampler_t sampler, int2 coord);\n"
"float4 __purefn __ovld read_imagef(read_only image2d_t image, sampler_t sampler, float2 coord);\n"
"\n"
"int4 __purefn __ovld read_imagei(read_only image2d_t image, sampler_t sampler, int2 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_t image, sampler_t sampler, float2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_t image, sampler_t sampler, int2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_t image, sampler_t sampler, float2 coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image3d_t image, sampler_t sampler, int4 coord);\n"
"float4 __purefn __ovld read_imagef(read_only image3d_t image, sampler_t sampler, float4 coord);\n"
"\n"
"int4 __purefn __ovld read_imagei(read_only image3d_t image, sampler_t sampler, int4 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image3d_t image, sampler_t sampler, float4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image3d_t image, sampler_t sampler, int4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image3d_t image, sampler_t sampler, float4 coord);\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"float4 __purefn __ovld read_imagef(read_only image2d_array_t image_array, sampler_t sampler, int4 coord);\n"
"float4 __purefn __ovld read_imagef(read_only image2d_array_t image_array, sampler_t sampler, float4 coord);\n"
"\n"
"int4 __purefn __ovld read_imagei(read_only image2d_array_t image_array, sampler_t sampler, int4 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_array_t image_array, sampler_t sampler, float4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_array_t image_array, sampler_t sampler, int4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_array_t image_array, sampler_t sampler, float4 coord);\n"
"#endif // __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_t image, sampler_t sampler, int coord);\n"
"float4 __purefn __ovld read_imagef(read_only image1d_t image, sampler_t sampler, float coord);\n"
"\n"
"int4 __purefn __ovld read_imagei(read_only image1d_t image, sampler_t sampler, int coord);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_t image, sampler_t sampler, float coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_t image, sampler_t sampler, int coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_t image, sampler_t sampler, float coord);\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"float4 __purefn __ovld read_imagef(read_only image1d_array_t image_array, sampler_t sampler, int2 coord);\n"
"float4 __purefn __ovld read_imagef(read_only image1d_array_t image_array, sampler_t sampler, float2 coord);\n"
"\n"
"int4 __purefn __ovld read_imagei(read_only image1d_array_t image_array, sampler_t sampler, int2 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_array_t image_array, sampler_t sampler, float2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_array_t image_array, sampler_t sampler, int2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_array_t image_array, sampler_t sampler, float2 coord);\n"
"#endif // __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"\n"
"#ifdef cl_khr_depth_images\n"
"float __purefn __ovld read_imagef(read_only image2d_depth_t image, sampler_t sampler, float2 coord);\n"
"float __purefn __ovld read_imagef(read_only image2d_depth_t image, sampler_t sampler, int2 coord);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_array_depth_t image, sampler_t sampler, float4 coord);\n"
"float __purefn __ovld read_imagef(read_only image2d_array_depth_t image, sampler_t sampler, int4 coord);\n"
"#endif //cl_khr_depth_images\n"
"\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"float4 __purefn __ovld read_imagef(read_only image2d_msaa_t image, int2 coord, int sample);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_msaa_t image, int2 coord, int sample);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_msaa_t image, int2 coord, int sample);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_msaa_depth_t image, int2 coord, int sample);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_array_msaa_t image, int4 coord, int sample);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_array_msaa_t image, int4 coord, int sample);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_array_msaa_t image, int4 coord, int sample);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_array_msaa_depth_t image, int4 coord, int sample);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"// OpenCL Extension v2.0 s9.18 - Mipmaps\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#ifdef cl_khr_mipmap_image\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_t image, sampler_t sampler, float coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_t image, sampler_t sampler, float coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_t image, sampler_t sampler, float coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_depth_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_array_depth_t image, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_t image, sampler_t sampler, float coord, float gradientX, float gradientY);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_t image, sampler_t sampler, float coord, float gradientX, float gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_t image, sampler_t sampler, float coord, float gradientX, float gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float gradientX, float gradientY);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float gradientX, float gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float gradientX, float gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_depth_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_array_depth_t image, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image3d_t image, sampler_t sampler, float4 coord, float4 gradientX, float4 gradientY);\n"
"int4 __purefn __ovld read_imagei(read_only image3d_t image, sampler_t sampler, float4 coord, float4 gradientX, float4 gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_only image3d_t image, sampler_t sampler, float4 coord, float4 gradientX, float4 gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_t image, sampler_t sampler, float coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_t image, sampler_t sampler, float coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_t image, sampler_t sampler, float coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_depth_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_only image2d_array_depth_t image, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_only image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_only image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"#endif //cl_khr_mipmap_image\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"\n"
"/**\n"
"* Sampler-less Image Access\n"
"*/\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_t image, int coord);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_t image, int coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_t image, int coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_buffer_t image, int coord);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_buffer_t image, int coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_buffer_t image, int coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image1d_array_t image, int2 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image1d_array_t image, int2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image1d_array_t image, int2 coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_t image, int2 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_t image, int2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_t image, int2 coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image2d_array_t image, int4 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image2d_array_t image, int4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image2d_array_t image, int4 coord);\n"
"\n"
"#ifdef cl_khr_depth_images\n"
"float __purefn __ovld read_imagef(read_only image2d_depth_t image, int2 coord);\n"
"float __purefn __ovld read_imagef(read_only image2d_array_depth_t image, int4 coord);\n"
"#endif //cl_khr_depth_images\n"
"\n"
"float4 __purefn __ovld read_imagef(read_only image3d_t image, int4 coord);\n"
"int4 __purefn __ovld read_imagei(read_only image3d_t image, int4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_only image3d_t image, int4 coord);\n"
"\n"
"#endif // __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"\n"
"// Image read functions returning half4 type\n"
"#ifdef cl_khr_fp16\n"
"half4 __purefn __ovld read_imageh(read_only image1d_t image, sampler_t sampler, int coord);\n"
"half4 __purefn __ovld read_imageh(read_only image1d_t image, sampler_t sampler, float coord);\n"
"half4 __purefn __ovld read_imageh(read_only image2d_t image, sampler_t sampler, int2 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image2d_t image, sampler_t sampler, float2 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image3d_t image, sampler_t sampler, int4 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image3d_t image, sampler_t sampler, float4 coord);\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"half4 __purefn __ovld read_imageh(read_only image1d_array_t image, sampler_t sampler, int2 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image1d_array_t image, sampler_t sampler, float2 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image2d_array_t image, sampler_t sampler, int4 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image2d_array_t image, sampler_t sampler, float4 coord);\n"
"/**\n"
" * Sampler-less Image Access\n"
" */\n"
"half4 __purefn __ovld read_imageh(read_only image1d_t image, int coord);\n"
"half4 __purefn __ovld read_imageh(read_only image2d_t image, int2 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image3d_t image, int4 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image1d_array_t image, int2 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image2d_array_t image, int4 coord);\n"
"half4 __purefn __ovld read_imageh(read_only image1d_buffer_t image, int coord);\n"
"#endif // __OPENCL_C_VERSION__ >= CL_VERSION_1_2\n"
"#endif //cl_khr_fp16\n"
"\n"
"// Image read functions for read_write images\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"float4 __purefn __ovld read_imagef(read_write image1d_t image, int coord);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_t image, int coord);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_t image, int coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image1d_buffer_t image, int coord);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_buffer_t image, int coord);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_buffer_t image, int coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image1d_array_t image, int2 coord);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_array_t image, int2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_array_t image, int2 coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_t image, int2 coord);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_t image, int2 coord);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_t image, int2 coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_array_t image, int4 coord);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_array_t image, int4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_array_t image, int4 coord);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image3d_t image, int4 coord);\n"
"int4 __purefn __ovld read_imagei(read_write image3d_t image, int4 coord);\n"
"uint4 __purefn __ovld read_imageui(read_write image3d_t image, int4 coord);\n"
"\n"
"#ifdef cl_khr_depth_images\n"
"float __purefn __ovld read_imagef(read_write image2d_depth_t image, int2 coord);\n"
"float __purefn __ovld read_imagef(read_write image2d_array_depth_t image, int4 coord);\n"
"#endif //cl_khr_depth_images\n"
"\n"
"#if cl_khr_gl_msaa_sharing\n"
"float4 __purefn __ovld read_imagef(read_write image2d_msaa_t image, int2 coord, int sample);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_msaa_t image, int2 coord, int sample);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_msaa_t image, int2 coord, int sample);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_array_msaa_t image, int4 coord, int sample);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_array_msaa_t image, int4 coord, int sample);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_array_msaa_t image, int4 coord, int sample);\n"
"\n"
"float __purefn __ovld read_imagef(read_write image2d_msaa_depth_t image, int2 coord, int sample);\n"
"float __purefn __ovld read_imagef(read_write image2d_array_msaa_depth_t image, int4 coord, int sample);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#ifdef cl_khr_mipmap_image\n"
"float4 __purefn __ovld read_imagef(read_write image1d_t image, sampler_t sampler, float coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_t image, sampler_t sampler, float coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_t image, sampler_t sampler, float coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_write image2d_depth_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_write image2d_array_depth_t image, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image1d_t image, sampler_t sampler, float coord, float gradientX, float gradientY);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_t image, sampler_t sampler, float coord, float gradientX, float gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_t image, sampler_t sampler, float coord, float gradientX, float gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float gradientX, float gradientY);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float gradientX, float gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float gradientX, float gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float __purefn __ovld read_imagef(read_write image2d_depth_t image, sampler_t sampler, float2 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float __purefn __ovld read_imagef(read_write image2d_array_depth_t image, sampler_t sampler, float4 coord, float2 gradientX, float2 gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image3d_t image, sampler_t sampler, float4 coord, float4 gradientX, float4 gradientY);\n"
"int4 __purefn __ovld read_imagei(read_write image3d_t image, sampler_t sampler, float4 coord, float4 gradientX, float4 gradientY);\n"
"uint4 __purefn __ovld read_imageui(read_write image3d_t image, sampler_t sampler, float4 coord, float4 gradientX, float4 gradientY);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image1d_t image, sampler_t sampler, float coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_t image, sampler_t sampler, float coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_t image, sampler_t sampler, float coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image1d_array_t image_array, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_write image2d_depth_t image, sampler_t sampler, float2 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image2d_array_t image_array, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float __purefn __ovld read_imagef(read_write image2d_array_depth_t image, sampler_t sampler, float4 coord, float lod);\n"
"\n"
"float4 __purefn __ovld read_imagef(read_write image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"int4 __purefn __ovld read_imagei(read_write image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"uint4 __purefn __ovld read_imageui(read_write image3d_t image, sampler_t sampler, float4 coord, float lod);\n"
"#endif //cl_khr_mipmap_image\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// Image read functions returning half4 type\n"
"#ifdef cl_khr_fp16\n"
"half4 __purefn __ovld read_imageh(read_write image1d_t image, int coord);\n"
"half4 __purefn __ovld read_imageh(read_write image2d_t image, int2 coord);\n"
"half4 __purefn __ovld read_imageh(read_write image3d_t image, int4 coord);\n"
"half4 __purefn __ovld read_imageh(read_write image1d_array_t image, int2 coord);\n"
"half4 __purefn __ovld read_imageh(read_write image2d_array_t image, int4 coord);\n"
"half4 __purefn __ovld read_imageh(read_write image1d_buffer_t image, int coord);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Write color value to location specified by coordinate\n"
" * (coord.x, coord.y) in the 2D image object specified by image.\n"
" * (coord.x, coord.y) are considered to be unnormalized coordinates\n"
" * and must be in the range 0 ... image width - 1, and 0\n"
" * ... image height - 1.\n"
"\n"
" * Write color value to location specified by coordinate\n"
" * (coord.x, coord.y) in the 2D image object specified by index\n"
" * (coord.z) of the 2D image array object image_array.\n"
" * (coord.x, coord.y) are considered to be unnormalized\n"
" * coordinates and must be in the range 0 ... image width\n"
" * - 1.\n"
" *\n"
" * Write color value to location specified by coordinate\n"
" * (coord) in the 1D image (buffer) object specified by image.\n"
" * coord is considered to be unnormalized coordinates\n"
" * and must be in the range 0 ... image width - 1.\n"
" *\n"
" * Write color value to location specified by coordinate\n"
" * (coord.x) in the 1D image object specified by index\n"
" * (coord.y) of the 1D image array object image_array.\n"
" * x is considered to be unnormalized coordinates\n"
" * and must be in the range 0 ... image width - 1.\n"
" *\n"
" * Write color value to location specified by coordinate\n"
" * (coord.x, coord.y, coord.z) in the 3D image object specified by image.\n"
" * coord.x & coord.y are considered to be unnormalized coordinates\n"
" * and must be in the range 0 ... image width - 1, and 0\n"
" * ... image height - 1.\n"
" *\n"
" * For mipmap images, use mip-level specified by lod.\n"
" *\n"
" * Appropriate data format conversion to the specified\n"
" * image format is done before writing the color value.\n"
" *\n"
" * write_imagef can only be used with image objects\n"
" * created with image_channel_data_type set to one of\n"
" * the pre-defined packed formats or set to\n"
" * CL_SNORM_INT8, CL_UNORM_INT8,\n"
" * CL_SNORM_INT16, CL_UNORM_INT16,\n"
" * CL_HALF_FLOAT or CL_FLOAT. Appropriate data\n"
" * format conversion will be done to convert channel\n"
" * data from a floating-point value to actual data format\n"
" * in which the channels are stored.\n"
" *\n"
" * write_imagei can only be used with image objects\n"
" * created with image_channel_data_type set to one of\n"
" * the following values:\n"
" * CL_SIGNED_INT8,\n"
" * CL_SIGNED_INT16 and\n"
" * CL_SIGNED_INT32.\n"
" *\n"
" * write_imageui can only be used with image objects\n"
" * created with image_channel_data_type set to one of\n"
" * the following values:\n"
" * CL_UNSIGNED_INT8,\n"
" * CL_UNSIGNED_INT16 and\n"
" * CL_UNSIGNED_INT32.\n"
" *\n"
" * The behavior of write_imagef, write_imagei and\n"
" * write_imageui for image objects created with\n"
" * image_channel_data_type values not specified in\n"
" * the description above or with (x, y) coordinate\n"
" * values that are not in the range (0 ... image width -1,\n"
" * 0 ... image height - 1), respectively, is undefined.\n"
" */\n"
"void __ovld write_imagef(write_only image2d_t image, int2 coord, float4 color);\n"
"void __ovld write_imagei(write_only image2d_t image, int2 coord, int4 color);\n"
"void __ovld write_imageui(write_only image2d_t image, int2 coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image2d_array_t image_array, int4 coord, float4 color);\n"
"void __ovld write_imagei(write_only image2d_array_t image_array, int4 coord, int4 color);\n"
"void __ovld write_imageui(write_only image2d_array_t image_array, int4 coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image1d_t image, int coord, float4 color);\n"
"void __ovld write_imagei(write_only image1d_t image, int coord, int4 color);\n"
"void __ovld write_imageui(write_only image1d_t image, int coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image1d_buffer_t image, int coord, float4 color);\n"
"void __ovld write_imagei(write_only image1d_buffer_t image, int coord, int4 color);\n"
"void __ovld write_imageui(write_only image1d_buffer_t image, int coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image1d_array_t image_array, int2 coord, float4 color);\n"
"void __ovld write_imagei(write_only image1d_array_t image_array, int2 coord, int4 color);\n"
"void __ovld write_imageui(write_only image1d_array_t image_array, int2 coord, uint4 color);\n"
"\n"
"#ifdef cl_khr_3d_image_writes\n"
"void __ovld write_imagef(write_only image3d_t image, int4 coord, float4 color);\n"
"void __ovld write_imagei(write_only image3d_t image, int4 coord, int4 color);\n"
"void __ovld write_imageui(write_only image3d_t image, int4 coord, uint4 color);\n"
"#endif\n"
"\n"
"#ifdef cl_khr_depth_images\n"
"void __ovld write_imagef(write_only image2d_depth_t image, int2 coord, float color);\n"
"void __ovld write_imagef(write_only image2d_array_depth_t image, int4 coord, float color);\n"
"#endif //cl_khr_depth_images\n"
"\n"
"// OpenCL Extension v2.0 s9.18 - Mipmaps\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#ifdef cl_khr_mipmap_image\n"
"void __ovld write_imagef(write_only image1d_t image, int coord, int lod, float4 color);\n"
"void __ovld write_imagei(write_only image1d_t image, int coord, int lod, int4 color);\n"
"void __ovld write_imageui(write_only image1d_t image, int coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image1d_array_t image_array, int2 coord, int lod, float4 color);\n"
"void __ovld write_imagei(write_only image1d_array_t image_array, int2 coord, int lod, int4 color);\n"
"void __ovld write_imageui(write_only image1d_array_t image_array, int2 coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image2d_t image, int2 coord, int lod, float4 color);\n"
"void __ovld write_imagei(write_only image2d_t image, int2 coord, int lod, int4 color);\n"
"void __ovld write_imageui(write_only image2d_t image, int2 coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image2d_array_t image_array, int4 coord, int lod, float4 color);\n"
"void __ovld write_imagei(write_only image2d_array_t image_array, int4 coord, int lod, int4 color);\n"
"void __ovld write_imageui(write_only image2d_array_t image_array, int4 coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(write_only image2d_depth_t image, int2 coord, int lod, float color);\n"
"void __ovld write_imagef(write_only image2d_array_depth_t image, int4 coord, int lod, float color);\n"
"\n"
"#ifdef cl_khr_3d_image_writes\n"
"void __ovld write_imagef(write_only image3d_t image, int4 coord, int lod, float4 color);\n"
"void __ovld write_imagei(write_only image3d_t image, int4 coord, int lod, int4 color);\n"
"void __ovld write_imageui(write_only image3d_t image, int4 coord, int lod, uint4 color);\n"
"#endif\n"
"#endif //cl_khr_mipmap_image\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// Image write functions for half4 type\n"
"#ifdef cl_khr_fp16\n"
"void __ovld write_imageh(write_only image1d_t image, int coord, half4 color);\n"
"void __ovld write_imageh(write_only image2d_t image, int2 coord, half4 color);\n"
"#ifdef cl_khr_3d_image_writes\n"
"void __ovld write_imageh(write_only image3d_t image, int4 coord, half4 color);\n"
"#endif\n"
"void __ovld write_imageh(write_only image1d_array_t image, int2 coord, half4 color);\n"
"void __ovld write_imageh(write_only image2d_array_t image, int4 coord, half4 color);\n"
"void __ovld write_imageh(write_only image1d_buffer_t image, int coord, half4 color);\n"
"#endif //cl_khr_fp16\n"
"\n"
"// Image write functions for read_write images\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"void __ovld write_imagef(read_write image2d_t image, int2 coord, float4 color);\n"
"void __ovld write_imagei(read_write image2d_t image, int2 coord, int4 color);\n"
"void __ovld write_imageui(read_write image2d_t image, int2 coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image2d_array_t image_array, int4 coord, float4 color);\n"
"void __ovld write_imagei(read_write image2d_array_t image_array, int4 coord, int4 color);\n"
"void __ovld write_imageui(read_write image2d_array_t image_array, int4 coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image1d_t image, int coord, float4 color);\n"
"void __ovld write_imagei(read_write image1d_t image, int coord, int4 color);\n"
"void __ovld write_imageui(read_write image1d_t image, int coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image1d_buffer_t image, int coord, float4 color);\n"
"void __ovld write_imagei(read_write image1d_buffer_t image, int coord, int4 color);\n"
"void __ovld write_imageui(read_write image1d_buffer_t image, int coord, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image1d_array_t image_array, int2 coord, float4 color);\n"
"void __ovld write_imagei(read_write image1d_array_t image_array, int2 coord, int4 color);\n"
"void __ovld write_imageui(read_write image1d_array_t image_array, int2 coord, uint4 color);\n"
"\n"
"#ifdef cl_khr_3d_image_writes\n"
"void __ovld write_imagef(read_write image3d_t image, int4 coord, float4 color);\n"
"void __ovld write_imagei(read_write image3d_t image, int4 coord, int4 color);\n"
"void __ovld write_imageui(read_write image3d_t image, int4 coord, uint4 color);\n"
"#endif\n"
"\n"
"#ifdef cl_khr_depth_images\n"
"void __ovld write_imagef(read_write image2d_depth_t image, int2 coord, float color);\n"
"void __ovld write_imagef(read_write image2d_array_depth_t image, int4 coord, float color);\n"
"#endif //cl_khr_depth_images\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#ifdef cl_khr_mipmap_image\n"
"void __ovld write_imagef(read_write image1d_t image, int coord, int lod, float4 color);\n"
"void __ovld write_imagei(read_write image1d_t image, int coord, int lod, int4 color);\n"
"void __ovld write_imageui(read_write image1d_t image, int coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image1d_array_t image_array, int2 coord, int lod, float4 color);\n"
"void __ovld write_imagei(read_write image1d_array_t image_array, int2 coord, int lod, int4 color);\n"
"void __ovld write_imageui(read_write image1d_array_t image_array, int2 coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image2d_t image, int2 coord, int lod, float4 color);\n"
"void __ovld write_imagei(read_write image2d_t image, int2 coord, int lod, int4 color);\n"
"void __ovld write_imageui(read_write image2d_t image, int2 coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image2d_array_t image_array, int4 coord, int lod, float4 color);\n"
"void __ovld write_imagei(read_write image2d_array_t image_array, int4 coord, int lod, int4 color);\n"
"void __ovld write_imageui(read_write image2d_array_t image_array, int4 coord, int lod, uint4 color);\n"
"\n"
"void __ovld write_imagef(read_write image2d_depth_t image, int2 coord, int lod, float color);\n"
"void __ovld write_imagef(read_write image2d_array_depth_t image, int4 coord, int lod, float color);\n"
"\n"
"#ifdef cl_khr_3d_image_writes\n"
"void __ovld write_imagef(read_write image3d_t image, int4 coord, int lod, float4 color);\n"
"void __ovld write_imagei(read_write image3d_t image, int4 coord, int lod, int4 color);\n"
"void __ovld write_imageui(read_write image3d_t image, int4 coord, int lod, uint4 color);\n"
"#endif\n"
"#endif //cl_khr_mipmap_image\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// Image write functions for half4 type\n"
"#ifdef cl_khr_fp16\n"
"void __ovld write_imageh(read_write image1d_t image, int coord, half4 color);\n"
"void __ovld write_imageh(read_write image2d_t image, int2 coord, half4 color);\n"
"#ifdef cl_khr_3d_image_writes\n"
"void __ovld write_imageh(read_write image3d_t image, int4 coord, half4 color);\n"
"#endif\n"
"void __ovld write_imageh(read_write image1d_array_t image, int2 coord, half4 color);\n"
"void __ovld write_imageh(read_write image2d_array_t image, int4 coord, half4 color);\n"
"void __ovld write_imageh(read_write image1d_buffer_t image, int coord, half4 color);\n"
"#endif //cl_khr_fp16\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// Note: In OpenCL v1.0/1.1/1.2, image argument of image query builtin functions does not have\n"
"// access qualifier, which by default assume read_only access qualifier. Image query builtin\n"
"// functions with write_only image argument should also be declared.\n"
"\n"
"/**\n"
" * Return the image width in pixels.\n"
" *\n"
"  */\n"
"int __ovld __cnfn get_image_width(read_only image1d_t image);\n"
"int __ovld __cnfn get_image_width(read_only image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_width(read_only image2d_t image);\n"
"#ifdef cl_khr_3d_image_writes\n"
"int __ovld __cnfn get_image_width(read_only image3d_t image);\n"
"#endif\n"
"int __ovld __cnfn get_image_width(read_only image1d_array_t image);\n"
"int __ovld __cnfn get_image_width(read_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_width(read_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_width(read_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_width(read_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_width(read_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_width(read_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_width(read_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"int __ovld __cnfn get_image_width(write_only image1d_t image);\n"
"int __ovld __cnfn get_image_width(write_only image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_width(write_only image2d_t image);\n"
"#ifdef cl_khr_3d_image_writes\n"
"int __ovld __cnfn get_image_width(write_only image3d_t image);\n"
"#endif\n"
"int __ovld __cnfn get_image_width(write_only image1d_array_t image);\n"
"int __ovld __cnfn get_image_width(write_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_width(write_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_width(write_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_width(write_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_width(write_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_width(write_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_width(write_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int __ovld __cnfn get_image_width(read_write image1d_t image);\n"
"int __ovld __cnfn get_image_width(read_write image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_width(read_write image2d_t image);\n"
"int __ovld __cnfn get_image_width(read_write image3d_t image);\n"
"int __ovld __cnfn get_image_width(read_write image1d_array_t image);\n"
"int __ovld __cnfn get_image_width(read_write image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_width(read_write image2d_depth_t image);\n"
"int __ovld __cnfn get_image_width(read_write image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_width(read_write image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_width(read_write image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_width(read_write image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_width(read_write image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Return the image height in pixels.\n"
" */\n"
"int __ovld __cnfn get_image_height(read_only image2d_t image);\n"
"int __ovld __cnfn get_image_height(read_only image3d_t image);\n"
"int __ovld __cnfn get_image_height(read_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_height(read_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_height(read_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_height(read_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_height(read_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_height(read_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_height(read_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"int __ovld __cnfn get_image_height(write_only image2d_t image);\n"
"#ifdef cl_khr_3d_image_writes\n"
"int __ovld __cnfn get_image_height(write_only image3d_t image);\n"
"#endif\n"
"int __ovld __cnfn get_image_height(write_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_height(write_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_height(write_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_height(write_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_height(write_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_height(write_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_height(write_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int __ovld __cnfn get_image_height(read_write image2d_t image);\n"
"int __ovld __cnfn get_image_height(read_write image3d_t image);\n"
"int __ovld __cnfn get_image_height(read_write image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_height(read_write image2d_depth_t image);\n"
"int __ovld __cnfn get_image_height(read_write image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_height(read_write image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_height(read_write image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_height(read_write image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_height(read_write image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Return the image depth in pixels.\n"
" */\n"
"int __ovld __cnfn get_image_depth(read_only image3d_t image);\n"
"\n"
"#ifdef cl_khr_3d_image_writes\n"
"int __ovld __cnfn get_image_depth(write_only image3d_t image);\n"
"#endif\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int __ovld __cnfn get_image_depth(read_write image3d_t image);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL Extension v2.0 s9.18 - Mipmaps\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#ifdef cl_khr_mipmap_image\n"
"/**\n"
" * Return the image miplevels.\n"
" */\n"
"\n"
"int __ovld get_image_num_mip_levels(read_only image1d_t image);\n"
"int __ovld get_image_num_mip_levels(read_only image2d_t image);\n"
"int __ovld get_image_num_mip_levels(read_only image3d_t image);\n"
"\n"
"int __ovld get_image_num_mip_levels(write_only image1d_t image);\n"
"int __ovld get_image_num_mip_levels(write_only image2d_t image);\n"
"#ifdef cl_khr_3d_image_writes\n"
"int __ovld get_image_num_mip_levels(write_only image3d_t image);\n"
"#endif\n"
"\n"
"int __ovld get_image_num_mip_levels(read_write image1d_t image);\n"
"int __ovld get_image_num_mip_levels(read_write image2d_t image);\n"
"int __ovld get_image_num_mip_levels(read_write image3d_t image);\n"
"\n"
"int __ovld get_image_num_mip_levels(read_only image1d_array_t image);\n"
"int __ovld get_image_num_mip_levels(read_only image2d_array_t image);\n"
"int __ovld get_image_num_mip_levels(read_only image2d_array_depth_t image);\n"
"int __ovld get_image_num_mip_levels(read_only image2d_depth_t image);\n"
"\n"
"int __ovld get_image_num_mip_levels(write_only image1d_array_t image);\n"
"int __ovld get_image_num_mip_levels(write_only image2d_array_t image);\n"
"int __ovld get_image_num_mip_levels(write_only image2d_array_depth_t image);\n"
"int __ovld get_image_num_mip_levels(write_only image2d_depth_t image);\n"
"\n"
"int __ovld get_image_num_mip_levels(read_write image1d_array_t image);\n"
"int __ovld get_image_num_mip_levels(read_write image2d_array_t image);\n"
"int __ovld get_image_num_mip_levels(read_write image2d_array_depth_t image);\n"
"int __ovld get_image_num_mip_levels(read_write image2d_depth_t image);\n"
"\n"
"#endif //cl_khr_mipmap_image\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Return the channel data type. Valid values are:\n"
" * CLK_SNORM_INT8\n"
" * CLK_SNORM_INT16\n"
" * CLK_UNORM_INT8\n"
" * CLK_UNORM_INT16\n"
" * CLK_UNORM_SHORT_565\n"
" * CLK_UNORM_SHORT_555\n"
" * CLK_UNORM_SHORT_101010\n"
" * CLK_SIGNED_INT8\n"
" * CLK_SIGNED_INT16\n"
" * CLK_SIGNED_INT32\n"
" * CLK_UNSIGNED_INT8\n"
" * CLK_UNSIGNED_INT16\n"
" * CLK_UNSIGNED_INT32\n"
" * CLK_HALF_FLOAT\n"
" * CLK_FLOAT\n"
" */\n"
"\n"
"//\n"
"// Channel Datatype.\n"
"//\n"
"#define CLK_SNORM_INT8        0x10D0\n"
"#define CLK_SNORM_INT16       0x10D1\n"
"#define CLK_UNORM_INT8        0x10D2\n"
"#define CLK_UNORM_INT16       0x10D3\n"
"#define CLK_UNORM_SHORT_565   0x10D4\n"
"#define CLK_UNORM_SHORT_555   0x10D5\n"
"#define CLK_UNORM_INT_101010  0x10D6\n"
"#define CLK_SIGNED_INT8       0x10D7\n"
"#define CLK_SIGNED_INT16      0x10D8\n"
"#define CLK_SIGNED_INT32      0x10D9\n"
"#define CLK_UNSIGNED_INT8     0x10DA\n"
"#define CLK_UNSIGNED_INT16    0x10DB\n"
"#define CLK_UNSIGNED_INT32    0x10DC\n"
"#define CLK_HALF_FLOAT        0x10DD\n"
"#define CLK_FLOAT             0x10DE\n"
"#define CLK_UNORM_INT24       0x10DF\n"
"\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image1d_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image3d_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image1d_array_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image1d_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_t image);\n"
"#ifdef cl_khr_3d_image_writes\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image3d_t image);\n"
"#endif\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image1d_array_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(write_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image1d_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image3d_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image1d_array_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_depth_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_data_type(read_write image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Return the image channel order. Valid values are:\n"
" * CLK_A\n"
" * CLK_R\n"
" * CLK_Rx\n"
" * CLK_RG\n"
" * CLK_RGx\n"
" * CLK_RA\n"
" * CLK_RGB\n"
" * CLK_RGBx\n"
" * CLK_RGBA\n"
" * CLK_ARGB\n"
" * CLK_BGRA\n"
" * CLK_INTENSITY\n"
" * CLK_LUMINANCE\n"
" */\n"
"// Channel order, numbering must be aligned with cl_channel_order in cl.h\n"
"//\n"
"#define CLK_R         0x10B0\n"
"#define CLK_A         0x10B1\n"
"#define CLK_RG        0x10B2\n"
"#define CLK_RA        0x10B3\n"
"#define CLK_RGB       0x10B4\n"
"#define CLK_RGBA      0x10B5\n"
"#define CLK_BGRA      0x10B6\n"
"#define CLK_ARGB      0x10B7\n"
"#define CLK_INTENSITY 0x10B8\n"
"#define CLK_LUMINANCE 0x10B9\n"
"#define CLK_Rx                0x10BA\n"
"#define CLK_RGx               0x10BB\n"
"#define CLK_RGBx              0x10BC\n"
"#define CLK_DEPTH             0x10BD\n"
"#define CLK_DEPTH_STENCIL     0x10BE\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#define CLK_sRGB              0x10BF\n"
"#define CLK_sRGBx             0x10C0\n"
"#define CLK_sRGBA             0x10C1\n"
"#define CLK_sBGRA             0x10C2\n"
"#define CLK_ABGR              0x10C3\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"int __ovld __cnfn get_image_channel_order(read_only image1d_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image3d_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image1d_array_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"int __ovld __cnfn get_image_channel_order(write_only image1d_t image);\n"
"int __ovld __cnfn get_image_channel_order(write_only image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_t image);\n"
"#ifdef cl_khr_3d_image_writes\n"
"int __ovld __cnfn get_image_channel_order(write_only image3d_t image);\n"
"#endif\n"
"int __ovld __cnfn get_image_channel_order(write_only image1d_array_t image);\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_depth_t image);\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_order(write_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int __ovld __cnfn get_image_channel_order(read_write image1d_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image1d_buffer_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image3d_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image1d_array_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_depth_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_array_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_msaa_depth_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_array_msaa_t image);\n"
"int __ovld __cnfn get_image_channel_order(read_write image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Return the 2D image width and height as an int2\n"
" * type. The width is returned in the x component, and\n"
" * the height in the y component.\n"
" */\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_array_depth_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_msaa_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_msaa_depth_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_array_msaa_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_t image);\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_array_depth_t image);\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_msaa_t image);\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_msaa_depth_t image);\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_array_msaa_t image);\n"
"int2 __ovld __cnfn get_image_dim(write_only image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_array_t image);\n"
"#ifdef cl_khr_depth_images\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_array_depth_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_depth_t image);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_msaa_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_msaa_depth_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_array_msaa_t image);\n"
"int2 __ovld __cnfn get_image_dim(read_write image2d_array_msaa_depth_t image);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Return the 3D image width, height, and depth as an\n"
" * int4 type. The width is returned in the x\n"
" * component, height in the y component, depth in the z\n"
" * component and the w component is 0.\n"
" */\n"
"int4 __ovld __cnfn get_image_dim(read_only image3d_t image);\n"
"#ifdef cl_khr_3d_image_writes\n"
"int4 __ovld __cnfn get_image_dim(write_only image3d_t image);\n"
"#endif\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int4 __ovld __cnfn get_image_dim(read_write image3d_t image);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
" * Return the image array size.\n"
" */\n"
"\n"
"size_t __ovld __cnfn get_image_array_size(read_only image1d_array_t image_array);\n"
"size_t __ovld __cnfn get_image_array_size(read_only image2d_array_t image_array);\n"
"#ifdef cl_khr_depth_images\n"
"size_t __ovld __cnfn get_image_array_size(read_only image2d_array_depth_t image_array);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"size_t __ovld __cnfn get_image_array_size(read_only image2d_array_msaa_t image_array);\n"
"size_t __ovld __cnfn get_image_array_size(read_only image2d_array_msaa_depth_t image_array);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"size_t __ovld __cnfn get_image_array_size(write_only image1d_array_t image_array);\n"
"size_t __ovld __cnfn get_image_array_size(write_only image2d_array_t image_array);\n"
"#ifdef cl_khr_depth_images\n"
"size_t __ovld __cnfn get_image_array_size(write_only image2d_array_depth_t image_array);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"size_t __ovld __cnfn get_image_array_size(write_only image2d_array_msaa_t image_array);\n"
"size_t __ovld __cnfn get_image_array_size(write_only image2d_array_msaa_depth_t image_array);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"size_t __ovld __cnfn get_image_array_size(read_write image1d_array_t image_array);\n"
"size_t __ovld __cnfn get_image_array_size(read_write image2d_array_t image_array);\n"
"#ifdef cl_khr_depth_images\n"
"size_t __ovld __cnfn get_image_array_size(read_write image2d_array_depth_t image_array);\n"
"#endif //cl_khr_depth_images\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"size_t __ovld __cnfn get_image_array_size(read_write image2d_array_msaa_t image_array);\n"
"size_t __ovld __cnfn get_image_array_size(read_write image2d_array_msaa_depth_t image_array);\n"
"#endif //cl_khr_gl_msaa_sharing\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"/**\n"
"* Return the number of samples associated with image\n"
"*/\n"
"#if defined(cl_khr_gl_msaa_sharing)\n"
"int __ovld get_image_num_samples(read_only image2d_msaa_t image);\n"
"int __ovld get_image_num_samples(read_only image2d_msaa_depth_t image);\n"
"int __ovld get_image_num_samples(read_only image2d_array_msaa_depth_t image);\n"
"int __ovld get_image_num_samples(read_only image2d_array_msaa_t image);\n"
"int __ovld get_image_num_samples(read_only image2d_array_msaa_depth_t image);\n"
"\n"
"int __ovld get_image_num_samples(write_only image2d_msaa_t image);\n"
"int __ovld get_image_num_samples(write_only image2d_msaa_depth_t image);\n"
"int __ovld get_image_num_samples(write_only image2d_array_msaa_depth_t image);\n"
"int __ovld get_image_num_samples(write_only image2d_array_msaa_t image);\n"
"int __ovld get_image_num_samples(write_only image2d_array_msaa_depth_t image);\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int __ovld get_image_num_samples(read_write image2d_msaa_t image);\n"
"int __ovld get_image_num_samples(read_write image2d_msaa_depth_t image);\n"
"int __ovld get_image_num_samples(read_write image2d_array_msaa_depth_t image);\n"
"int __ovld get_image_num_samples(read_write image2d_array_msaa_t image);\n"
"int __ovld get_image_num_samples(read_write image2d_array_msaa_depth_t image);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#endif\n"
"\n"
"// OpenCL v2.0 s6.13.15 - Work-group Functions\n"
"\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"int __ovld __conv work_group_all(int predicate);\n"
"int __ovld __conv work_group_any(int predicate);\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __conv work_group_broadcast(half a, size_t local_id);\n"
"half __ovld __conv work_group_broadcast(half a, size_t x, size_t y);\n"
"half __ovld __conv work_group_broadcast(half a, size_t x, size_t y, size_t z);\n"
"#endif\n"
"int __ovld __conv work_group_broadcast(int a, size_t local_id);\n"
"int __ovld __conv work_group_broadcast(int a, size_t x, size_t y);\n"
"int __ovld __conv work_group_broadcast(int a, size_t x, size_t y, size_t z);\n"
"uint __ovld __conv work_group_broadcast(uint a, size_t local_id);\n"
"uint __ovld __conv work_group_broadcast(uint a, size_t x, size_t y);\n"
"uint __ovld __conv work_group_broadcast(uint a, size_t x, size_t y, size_t z);\n"
"long __ovld __conv work_group_broadcast(long a, size_t local_id);\n"
"long __ovld __conv work_group_broadcast(long a, size_t x, size_t y);\n"
"long __ovld __conv work_group_broadcast(long a, size_t x, size_t y, size_t z);\n"
"ulong __ovld __conv work_group_broadcast(ulong a, size_t local_id);\n"
"ulong __ovld __conv work_group_broadcast(ulong a, size_t x, size_t y);\n"
"ulong __ovld __conv work_group_broadcast(ulong a, size_t x, size_t y, size_t z);\n"
"float __ovld __conv work_group_broadcast(float a, size_t local_id);\n"
"float __ovld __conv work_group_broadcast(float a, size_t x, size_t y);\n"
"float __ovld __conv work_group_broadcast(float a, size_t x, size_t y, size_t z);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __conv work_group_broadcast(double a, size_t local_id);\n"
"double __ovld __conv work_group_broadcast(double a, size_t x, size_t y);\n"
"double __ovld __conv work_group_broadcast(double a, size_t x, size_t y, size_t z);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half __ovld __conv work_group_reduce_add(half x);\n"
"half __ovld __conv work_group_reduce_min(half x);\n"
"half __ovld __conv work_group_reduce_max(half x);\n"
"half __ovld __conv work_group_scan_exclusive_add(half x);\n"
"half __ovld __conv work_group_scan_exclusive_min(half x);\n"
"half __ovld __conv work_group_scan_exclusive_max(half x);\n"
"half __ovld __conv work_group_scan_inclusive_add(half x);\n"
"half __ovld __conv work_group_scan_inclusive_min(half x);\n"
"half __ovld __conv work_group_scan_inclusive_max(half x);\n"
"#endif\n"
"int __ovld __conv work_group_reduce_add(int x);\n"
"int __ovld __conv work_group_reduce_min(int x);\n"
"int __ovld __conv work_group_reduce_max(int x);\n"
"int __ovld __conv work_group_scan_exclusive_add(int x);\n"
"int __ovld __conv work_group_scan_exclusive_min(int x);\n"
"int __ovld __conv work_group_scan_exclusive_max(int x);\n"
"int __ovld __conv work_group_scan_inclusive_add(int x);\n"
"int __ovld __conv work_group_scan_inclusive_min(int x);\n"
"int __ovld __conv work_group_scan_inclusive_max(int x);\n"
"uint __ovld __conv work_group_reduce_add(uint x);\n"
"uint __ovld __conv work_group_reduce_min(uint x);\n"
"uint __ovld __conv work_group_reduce_max(uint x);\n"
"uint __ovld __conv work_group_scan_exclusive_add(uint x);\n"
"uint __ovld __conv work_group_scan_exclusive_min(uint x);\n"
"uint __ovld __conv work_group_scan_exclusive_max(uint x);\n"
"uint __ovld __conv work_group_scan_inclusive_add(uint x);\n"
"uint __ovld __conv work_group_scan_inclusive_min(uint x);\n"
"uint __ovld __conv work_group_scan_inclusive_max(uint x);\n"
"long __ovld __conv work_group_reduce_add(long x);\n"
"long __ovld __conv work_group_reduce_min(long x);\n"
"long __ovld __conv work_group_reduce_max(long x);\n"
"long __ovld __conv work_group_scan_exclusive_add(long x);\n"
"long __ovld __conv work_group_scan_exclusive_min(long x);\n"
"long __ovld __conv work_group_scan_exclusive_max(long x);\n"
"long __ovld __conv work_group_scan_inclusive_add(long x);\n"
"long __ovld __conv work_group_scan_inclusive_min(long x);\n"
"long __ovld __conv work_group_scan_inclusive_max(long x);\n"
"ulong __ovld __conv work_group_reduce_add(ulong x);\n"
"ulong __ovld __conv work_group_reduce_min(ulong x);\n"
"ulong __ovld __conv work_group_reduce_max(ulong x);\n"
"ulong __ovld __conv work_group_scan_exclusive_add(ulong x);\n"
"ulong __ovld __conv work_group_scan_exclusive_min(ulong x);\n"
"ulong __ovld __conv work_group_scan_exclusive_max(ulong x);\n"
"ulong __ovld __conv work_group_scan_inclusive_add(ulong x);\n"
"ulong __ovld __conv work_group_scan_inclusive_min(ulong x);\n"
"ulong __ovld __conv work_group_scan_inclusive_max(ulong x);\n"
"float __ovld __conv work_group_reduce_add(float x);\n"
"float __ovld __conv work_group_reduce_min(float x);\n"
"float __ovld __conv work_group_reduce_max(float x);\n"
"float __ovld __conv work_group_scan_exclusive_add(float x);\n"
"float __ovld __conv work_group_scan_exclusive_min(float x);\n"
"float __ovld __conv work_group_scan_exclusive_max(float x);\n"
"float __ovld __conv work_group_scan_inclusive_add(float x);\n"
"float __ovld __conv work_group_scan_inclusive_min(float x);\n"
"float __ovld __conv work_group_scan_inclusive_max(float x);\n"
"#ifdef cl_khr_fp64\n"
"double __ovld __conv work_group_reduce_add(double x);\n"
"double __ovld __conv work_group_reduce_min(double x);\n"
"double __ovld __conv work_group_reduce_max(double x);\n"
"double __ovld __conv work_group_scan_exclusive_add(double x);\n"
"double __ovld __conv work_group_scan_exclusive_min(double x);\n"
"double __ovld __conv work_group_scan_exclusive_max(double x);\n"
"double __ovld __conv work_group_scan_inclusive_add(double x);\n"
"double __ovld __conv work_group_scan_inclusive_min(double x);\n"
"double __ovld __conv work_group_scan_inclusive_max(double x);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL v2.0 s6.13.16 - Pipe Functions\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"#define CLK_NULL_RESERVE_ID (__builtin_astype(((void*)(__SIZE_MAX__)), reserve_id_t))\n"
"bool __ovld is_valid_reserve_id(reserve_id_t reserve_id);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"\n"
"// OpenCL v2.0 s6.13.17 - Enqueue Kernels\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"#define CL_COMPLETE                                 0x0\n"
"#define CL_RUNNING                                  0x1\n"
"#define CL_SUBMITTED                                0x2\n"
"#define CL_QUEUED                                   0x3\n"
"\n"
"#define CLK_SUCCESS                                 0\n"
"#define CLK_ENQUEUE_FAILURE                         -101\n"
"#define CLK_INVALID_QUEUE                           -102\n"
"#define CLK_INVALID_NDRANGE                         -160\n"
"#define CLK_INVALID_EVENT_WAIT_LIST                 -57\n"
"#define CLK_DEVICE_QUEUE_FULL                       -161\n"
"#define CLK_INVALID_ARG_SIZE                        -51\n"
"#define CLK_EVENT_ALLOCATION_FAILURE                -100\n"
"#define CLK_OUT_OF_RESOURCES                        -5\n"
"\n"
"#define CLK_NULL_QUEUE                              0\n"
"#define CLK_NULL_EVENT (__builtin_astype(((void*)(__SIZE_MAX__)), clk_event_t))\n"
"\n"
"// execution model related definitions\n"
"#define CLK_ENQUEUE_FLAGS_NO_WAIT                   0x0\n"
"#define CLK_ENQUEUE_FLAGS_WAIT_KERNEL               0x1\n"
"#define CLK_ENQUEUE_FLAGS_WAIT_WORK_GROUP           0x2\n"
"\n"
"typedef int kernel_enqueue_flags_t;\n"
"typedef int clk_profiling_info;\n"
"\n"
"// Profiling info name (see capture_event_profiling_info)\n"
"#define CLK_PROFILING_COMMAND_EXEC_TIME 0x1\n"
"\n"
"#define MAX_WORK_DIM        3\n"
"\n"
"typedef struct {\n"
"    unsigned int workDimension;\n"
"    size_t globalWorkOffset[MAX_WORK_DIM];\n"
"    size_t globalWorkSize[MAX_WORK_DIM];\n"
"    size_t localWorkSize[MAX_WORK_DIM];\n"
"} ndrange_t;\n"
"\n"
"ndrange_t __ovld ndrange_1D(size_t);\n"
"ndrange_t __ovld ndrange_1D(size_t, size_t);\n"
"ndrange_t __ovld ndrange_1D(size_t, size_t, size_t);\n"
"\n"
"ndrange_t __ovld ndrange_2D(const size_t[2]);\n"
"ndrange_t __ovld ndrange_2D(const size_t[2], const size_t[2]);\n"
"ndrange_t __ovld ndrange_2D(const size_t[2], const size_t[2], const size_t[2]);\n"
"\n"
"ndrange_t __ovld ndrange_3D(const size_t[3]);\n"
"ndrange_t __ovld ndrange_3D(const size_t[3], const size_t[3]);\n"
"ndrange_t __ovld ndrange_3D(const size_t[3], const size_t[3], const size_t[3]);\n"
"\n"
"int __ovld enqueue_marker(queue_t, uint, const __private clk_event_t*, __private clk_event_t*);\n"
"\n"
"void __ovld retain_event(clk_event_t);\n"
"\n"
"void __ovld release_event(clk_event_t);\n"
"\n"
"clk_event_t __ovld create_user_event(void);\n"
"\n"
"void __ovld set_user_event_status(clk_event_t e, int state);\n"
"\n"
"bool __ovld is_valid_event (clk_event_t event);\n"
"\n"
"void __ovld capture_event_profiling_info(clk_event_t, clk_profiling_info, __global void* value);\n"
"\n"
"queue_t __ovld get_default_queue(void);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"// OpenCL Extension v2.0 s9.17 - Sub-groups\n"
"\n"
"#if defined(cl_intel_subgroups) || defined(cl_khr_subgroups)\n"
"// Shared Sub Group Functions\n"
"uint    __ovld get_sub_group_size(void);\n"
"uint    __ovld get_max_sub_group_size(void);\n"
"uint    __ovld get_num_sub_groups(void);\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"uint    __ovld get_enqueued_num_sub_groups(void);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"uint    __ovld get_sub_group_id(void);\n"
"uint    __ovld get_sub_group_local_id(void);\n"
"\n"
"void    __ovld __conv sub_group_barrier(cl_mem_fence_flags flags);\n"
"#if __OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"void    __ovld __conv sub_group_barrier(cl_mem_fence_flags flags, memory_scope scope);\n"
"#endif //__OPENCL_C_VERSION__ >= CL_VERSION_2_0\n"
"\n"
"int     __ovld __conv sub_group_all(int predicate);\n"
"int     __ovld __conv sub_group_any(int predicate);\n"
"\n"
"int     __ovld __conv sub_group_broadcast(int   x, uint sub_group_local_id);\n"
"uint    __ovld __conv sub_group_broadcast(uint  x, uint sub_group_local_id);\n"
"long    __ovld __conv sub_group_broadcast(long  x, uint sub_group_local_id);\n"
"ulong   __ovld __conv sub_group_broadcast(ulong x, uint sub_group_local_id);\n"
"float   __ovld __conv sub_group_broadcast(float x, uint sub_group_local_id);\n"
"\n"
"int     __ovld __conv sub_group_reduce_add(int   x);\n"
"uint    __ovld __conv sub_group_reduce_add(uint  x);\n"
"long    __ovld __conv sub_group_reduce_add(long  x);\n"
"ulong   __ovld __conv sub_group_reduce_add(ulong x);\n"
"float   __ovld __conv sub_group_reduce_add(float x);\n"
"int     __ovld __conv sub_group_reduce_min(int   x);\n"
"uint    __ovld __conv sub_group_reduce_min(uint  x);\n"
"long    __ovld __conv sub_group_reduce_min(long  x);\n"
"ulong   __ovld __conv sub_group_reduce_min(ulong x);\n"
"float   __ovld __conv sub_group_reduce_min(float x);\n"
"int     __ovld __conv sub_group_reduce_max(int   x);\n"
"uint    __ovld __conv sub_group_reduce_max(uint  x);\n"
"long    __ovld __conv sub_group_reduce_max(long  x);\n"
"ulong   __ovld __conv sub_group_reduce_max(ulong x);\n"
"float   __ovld __conv sub_group_reduce_max(float x);\n"
"\n"
"int     __ovld __conv sub_group_scan_exclusive_add(int   x);\n"
"uint    __ovld __conv sub_group_scan_exclusive_add(uint  x);\n"
"long    __ovld __conv sub_group_scan_exclusive_add(long  x);\n"
"ulong   __ovld __conv sub_group_scan_exclusive_add(ulong x);\n"
"float   __ovld __conv sub_group_scan_exclusive_add(float x);\n"
"int     __ovld __conv sub_group_scan_exclusive_min(int   x);\n"
"uint    __ovld __conv sub_group_scan_exclusive_min(uint  x);\n"
"long    __ovld __conv sub_group_scan_exclusive_min(long  x);\n"
"ulong   __ovld __conv sub_group_scan_exclusive_min(ulong x);\n"
"float   __ovld __conv sub_group_scan_exclusive_min(float x);\n"
"int     __ovld __conv sub_group_scan_exclusive_max(int   x);\n"
"uint    __ovld __conv sub_group_scan_exclusive_max(uint  x);\n"
"long    __ovld __conv sub_group_scan_exclusive_max(long  x);\n"
"ulong   __ovld __conv sub_group_scan_exclusive_max(ulong x);\n"
"float   __ovld __conv sub_group_scan_exclusive_max(float x);\n"
"\n"
"int     __ovld __conv sub_group_scan_inclusive_add(int   x);\n"
"uint    __ovld __conv sub_group_scan_inclusive_add(uint  x);\n"
"long    __ovld __conv sub_group_scan_inclusive_add(long  x);\n"
"ulong   __ovld __conv sub_group_scan_inclusive_add(ulong x);\n"
"float   __ovld __conv sub_group_scan_inclusive_add(float x);\n"
"int     __ovld __conv sub_group_scan_inclusive_min(int   x);\n"
"uint    __ovld __conv sub_group_scan_inclusive_min(uint  x);\n"
"long    __ovld __conv sub_group_scan_inclusive_min(long  x);\n"
"ulong   __ovld __conv sub_group_scan_inclusive_min(ulong x);\n"
"float   __ovld __conv sub_group_scan_inclusive_min(float x);\n"
"int     __ovld __conv sub_group_scan_inclusive_max(int   x);\n"
"uint    __ovld __conv sub_group_scan_inclusive_max(uint  x);\n"
"long    __ovld __conv sub_group_scan_inclusive_max(long  x);\n"
"ulong   __ovld __conv sub_group_scan_inclusive_max(ulong x);\n"
"float   __ovld __conv sub_group_scan_inclusive_max(float x);\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half    __ovld __conv sub_group_broadcast(half x, uint sub_group_local_id);\n"
"half    __ovld __conv sub_group_reduce_add(half x);\n"
"half    __ovld __conv sub_group_reduce_min(half x);\n"
"half    __ovld __conv sub_group_reduce_max(half x);\n"
"half    __ovld __conv sub_group_scan_exclusive_add(half x);\n"
"half    __ovld __conv sub_group_scan_exclusive_min(half x);\n"
"half    __ovld __conv sub_group_scan_exclusive_max(half x);\n"
"half    __ovld __conv sub_group_scan_inclusive_add(half x);\n"
"half    __ovld __conv sub_group_scan_inclusive_min(half x);\n"
"half    __ovld __conv sub_group_scan_inclusive_max(half x);\n"
"#endif //cl_khr_fp16\n"
"\n"
"#ifdef cl_khr_fp64\n"
"double  __ovld __conv sub_group_broadcast(double x, uint sub_group_local_id);\n"
"double  __ovld __conv sub_group_reduce_add(double x);\n"
"double  __ovld __conv sub_group_reduce_min(double x);\n"
"double  __ovld __conv sub_group_reduce_max(double x);\n"
"double  __ovld __conv sub_group_scan_exclusive_add(double x);\n"
"double  __ovld __conv sub_group_scan_exclusive_min(double x);\n"
"double  __ovld __conv sub_group_scan_exclusive_max(double x);\n"
"double  __ovld __conv sub_group_scan_inclusive_add(double x);\n"
"double  __ovld __conv sub_group_scan_inclusive_min(double x);\n"
"double  __ovld __conv sub_group_scan_inclusive_max(double x);\n"
"#endif //cl_khr_fp64\n"
"\n"
"#endif //cl_khr_subgroups cl_intel_subgroups\n"
"\n"
"#if defined(cl_intel_subgroups)\n"
"// Intel-Specific Sub Group Functions\n"
"float   __ovld __conv intel_sub_group_shuffle( float  x, uint c );\n"
"float2  __ovld __conv intel_sub_group_shuffle( float2 x, uint c );\n"
"float3  __ovld __conv intel_sub_group_shuffle( float3 x, uint c );\n"
"float4  __ovld __conv intel_sub_group_shuffle( float4 x, uint c );\n"
"float8  __ovld __conv intel_sub_group_shuffle( float8 x, uint c );\n"
"float16 __ovld __conv intel_sub_group_shuffle( float16 x, uint c );\n"
"\n"
"int     __ovld __conv intel_sub_group_shuffle( int  x, uint c );\n"
"int2    __ovld __conv intel_sub_group_shuffle( int2 x, uint c );\n"
"int3    __ovld __conv intel_sub_group_shuffle( int3 x, uint c );\n"
"int4    __ovld __conv intel_sub_group_shuffle( int4 x, uint c );\n"
"int8    __ovld __conv intel_sub_group_shuffle( int8 x, uint c );\n"
"int16   __ovld __conv intel_sub_group_shuffle( int16 x, uint c );\n"
"\n"
"uint    __ovld __conv intel_sub_group_shuffle( uint  x, uint c );\n"
"uint2   __ovld __conv intel_sub_group_shuffle( uint2 x, uint c );\n"
"uint3   __ovld __conv intel_sub_group_shuffle( uint3 x, uint c );\n"
"uint4   __ovld __conv intel_sub_group_shuffle( uint4 x, uint c );\n"
"uint8   __ovld __conv intel_sub_group_shuffle( uint8 x, uint c );\n"
"uint16  __ovld __conv intel_sub_group_shuffle( uint16 x, uint c );\n"
"\n"
"long    __ovld __conv intel_sub_group_shuffle( long x, uint c );\n"
"ulong   __ovld __conv intel_sub_group_shuffle( ulong x, uint c );\n"
"\n"
"float   __ovld __conv intel_sub_group_shuffle_down( float  cur, float  next, uint c );\n"
"float2  __ovld __conv intel_sub_group_shuffle_down( float2 cur, float2 next, uint c );\n"
"float3  __ovld __conv intel_sub_group_shuffle_down( float3 cur, float3 next, uint c );\n"
"float4  __ovld __conv intel_sub_group_shuffle_down( float4 cur, float4 next, uint c );\n"
"float8  __ovld __conv intel_sub_group_shuffle_down( float8 cur, float8 next, uint c );\n"
"float16 __ovld __conv intel_sub_group_shuffle_down( float16 cur, float16 next, uint c );\n"
"\n"
"int     __ovld __conv intel_sub_group_shuffle_down( int  cur, int  next, uint c );\n"
"int2    __ovld __conv intel_sub_group_shuffle_down( int2 cur, int2 next, uint c );\n"
"int3    __ovld __conv intel_sub_group_shuffle_down( int3 cur, int3 next, uint c );\n"
"int4    __ovld __conv intel_sub_group_shuffle_down( int4 cur, int4 next, uint c );\n"
"int8    __ovld __conv intel_sub_group_shuffle_down( int8 cur, int8 next, uint c );\n"
"int16   __ovld __conv intel_sub_group_shuffle_down( int16 cur, int16 next, uint c );\n"
"\n"
"uint    __ovld __conv intel_sub_group_shuffle_down( uint  cur, uint  next, uint c );\n"
"uint2   __ovld __conv intel_sub_group_shuffle_down( uint2 cur, uint2 next, uint c );\n"
"uint3   __ovld __conv intel_sub_group_shuffle_down( uint3 cur, uint3 next, uint c );\n"
"uint4   __ovld __conv intel_sub_group_shuffle_down( uint4 cur, uint4 next, uint c );\n"
"uint8   __ovld __conv intel_sub_group_shuffle_down( uint8 cur, uint8 next, uint c );\n"
"uint16  __ovld __conv intel_sub_group_shuffle_down( uint16 cur, uint16 next, uint c );\n"
"\n"
"long    __ovld __conv intel_sub_group_shuffle_down( long prev, long cur, uint c );\n"
"ulong   __ovld __conv intel_sub_group_shuffle_down( ulong prev, ulong cur, uint c );\n"
"\n"
"float   __ovld __conv intel_sub_group_shuffle_up( float  prev, float  cur, uint c );\n"
"float2  __ovld __conv intel_sub_group_shuffle_up( float2 prev, float2 cur, uint c );\n"
"float3  __ovld __conv intel_sub_group_shuffle_up( float3 prev, float3 cur, uint c );\n"
"float4  __ovld __conv intel_sub_group_shuffle_up( float4 prev, float4 cur, uint c );\n"
"float8  __ovld __conv intel_sub_group_shuffle_up( float8 prev, float8 cur, uint c );\n"
"float16 __ovld __conv intel_sub_group_shuffle_up( float16 prev, float16 cur, uint c );\n"
"\n"
"int     __ovld __conv intel_sub_group_shuffle_up( int  prev, int  cur, uint c );\n"
"int2    __ovld __conv intel_sub_group_shuffle_up( int2 prev, int2 cur, uint c );\n"
"int3    __ovld __conv intel_sub_group_shuffle_up( int3 prev, int3 cur, uint c );\n"
"int4    __ovld __conv intel_sub_group_shuffle_up( int4 prev, int4 cur, uint c );\n"
"int8    __ovld __conv intel_sub_group_shuffle_up( int8 prev, int8 cur, uint c );\n"
"int16   __ovld __conv intel_sub_group_shuffle_up( int16 prev, int16 cur, uint c );\n"
"\n"
"uint    __ovld __conv intel_sub_group_shuffle_up( uint  prev, uint  cur, uint c );\n"
"uint2   __ovld __conv intel_sub_group_shuffle_up( uint2 prev, uint2 cur, uint c );\n"
"uint3   __ovld __conv intel_sub_group_shuffle_up( uint3 prev, uint3 cur, uint c );\n"
"uint4   __ovld __conv intel_sub_group_shuffle_up( uint4 prev, uint4 cur, uint c );\n"
"uint8   __ovld __conv intel_sub_group_shuffle_up( uint8 prev, uint8 cur, uint c );\n"
"uint16  __ovld __conv intel_sub_group_shuffle_up( uint16 prev, uint16 cur, uint c );\n"
"\n"
"long    __ovld __conv intel_sub_group_shuffle_up( long prev, long cur, uint c );\n"
"ulong   __ovld __conv intel_sub_group_shuffle_up( ulong prev, ulong cur, uint c );\n"
"\n"
"float   __ovld __conv intel_sub_group_shuffle_xor( float  x, uint c );\n"
"float2  __ovld __conv intel_sub_group_shuffle_xor( float2 x, uint c );\n"
"float3  __ovld __conv intel_sub_group_shuffle_xor( float3 x, uint c );\n"
"float4  __ovld __conv intel_sub_group_shuffle_xor( float4 x, uint c );\n"
"float8  __ovld __conv intel_sub_group_shuffle_xor( float8 x, uint c );\n"
"float16 __ovld __conv intel_sub_group_shuffle_xor( float16 x, uint c );\n"
"\n"
"int     __ovld __conv intel_sub_group_shuffle_xor( int  x, uint c );\n"
"int2    __ovld __conv intel_sub_group_shuffle_xor( int2 x, uint c );\n"
"int3    __ovld __conv intel_sub_group_shuffle_xor( int3 x, uint c );\n"
"int4    __ovld __conv intel_sub_group_shuffle_xor( int4 x, uint c );\n"
"int8    __ovld __conv intel_sub_group_shuffle_xor( int8 x, uint c );\n"
"int16   __ovld __conv intel_sub_group_shuffle_xor( int16 x, uint c );\n"
"\n"
"uint    __ovld __conv intel_sub_group_shuffle_xor( uint  x, uint c );\n"
"uint2   __ovld __conv intel_sub_group_shuffle_xor( uint2 x, uint c );\n"
"uint3   __ovld __conv intel_sub_group_shuffle_xor( uint3 x, uint c );\n"
"uint4   __ovld __conv intel_sub_group_shuffle_xor( uint4 x, uint c );\n"
"uint8   __ovld __conv intel_sub_group_shuffle_xor( uint8 x, uint c );\n"
"uint16  __ovld __conv intel_sub_group_shuffle_xor( uint16 x, uint c );\n"
"\n"
"long    __ovld __conv intel_sub_group_shuffle_xor( long x, uint c );\n"
"ulong   __ovld __conv intel_sub_group_shuffle_xor( ulong x, uint c );\n"
"\n"
"uint    __ovld __conv intel_sub_group_block_read( read_only image2d_t image, int2 coord );\n"
"uint2   __ovld __conv intel_sub_group_block_read2( read_only image2d_t image, int2 coord );\n"
"uint4   __ovld __conv intel_sub_group_block_read4( read_only image2d_t image, int2 coord );\n"
"uint8   __ovld __conv intel_sub_group_block_read8( read_only image2d_t image, int2 coord );\n"
"\n"
"#if (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"uint    __ovld __conv intel_sub_group_block_read(read_write image2d_t image, int2 coord);\n"
"uint2   __ovld __conv intel_sub_group_block_read2(read_write image2d_t image, int2 coord);\n"
"uint4   __ovld __conv intel_sub_group_block_read4(read_write image2d_t image, int2 coord);\n"
"uint8   __ovld __conv intel_sub_group_block_read8(read_write image2d_t image, int2 coord);\n"
"#endif // (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"\n"
"uint    __ovld __conv intel_sub_group_block_read( const __global uint* p );\n"
"uint2   __ovld __conv intel_sub_group_block_read2( const __global uint* p );\n"
"uint4   __ovld __conv intel_sub_group_block_read4( const __global uint* p );\n"
"uint8   __ovld __conv intel_sub_group_block_read8( const __global uint* p );\n"
"\n"
"void    __ovld __conv intel_sub_group_block_write(write_only image2d_t image, int2 coord, uint data);\n"
"void    __ovld __conv intel_sub_group_block_write2(write_only image2d_t image, int2 coord, uint2 data);\n"
"void    __ovld __conv intel_sub_group_block_write4(write_only image2d_t image, int2 coord, uint4 data);\n"
"void    __ovld __conv intel_sub_group_block_write8(write_only image2d_t image, int2 coord, uint8 data);\n"
"\n"
"#if (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"void    __ovld __conv intel_sub_group_block_write(read_write image2d_t image, int2 coord, uint data);\n"
"void    __ovld __conv intel_sub_group_block_write2(read_write image2d_t image, int2 coord, uint2 data);\n"
"void    __ovld __conv intel_sub_group_block_write4(read_write image2d_t image, int2 coord, uint4 data);\n"
"void    __ovld __conv intel_sub_group_block_write8(read_write image2d_t image, int2 coord, uint8 data);\n"
"#endif // (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"\n"
"void    __ovld __conv intel_sub_group_block_write( __global uint* p, uint data );\n"
"void    __ovld __conv intel_sub_group_block_write2( __global uint* p, uint2 data );\n"
"void    __ovld __conv intel_sub_group_block_write4( __global uint* p, uint4 data );\n"
"void    __ovld __conv intel_sub_group_block_write8( __global uint* p, uint8 data );\n"
"\n"
"#ifdef cl_khr_fp16\n"
"half    __ovld __conv intel_sub_group_shuffle( half x, uint c );\n"
"half    __ovld __conv intel_sub_group_shuffle_down( half prev, half cur, uint c );\n"
"half    __ovld __conv intel_sub_group_shuffle_up( half prev, half cur, uint c );\n"
"half    __ovld __conv intel_sub_group_shuffle_xor( half x, uint c );\n"
"#endif\n"
"\n"
"#if defined(cl_khr_fp64)\n"
"double  __ovld __conv intel_sub_group_shuffle( double x, uint c );\n"
"double  __ovld __conv intel_sub_group_shuffle_down( double prev, double cur, uint c );\n"
"double  __ovld __conv intel_sub_group_shuffle_up( double prev, double cur, uint c );\n"
"double  __ovld __conv intel_sub_group_shuffle_xor( double x, uint c );\n"
"#endif\n"
"\n"
"#endif //cl_intel_subgroups\n"
"\n"
"#if defined(cl_intel_subgroups_short)\n"
"short       __ovld __conv intel_sub_group_broadcast( short  x, uint sub_group_local_id );\n"
"short2      __ovld __conv intel_sub_group_broadcast( short2 x, uint sub_group_local_id );\n"
"short3      __ovld __conv intel_sub_group_broadcast( short3 x, uint sub_group_local_id );\n"
"short4      __ovld __conv intel_sub_group_broadcast( short4 x, uint sub_group_local_id );\n"
"short8      __ovld __conv intel_sub_group_broadcast( short8 x, uint sub_group_local_id );\n"
"\n"
"ushort      __ovld __conv intel_sub_group_broadcast( ushort  x, uint sub_group_local_id );\n"
"ushort2     __ovld __conv intel_sub_group_broadcast( ushort2 x, uint sub_group_local_id );\n"
"ushort3     __ovld __conv intel_sub_group_broadcast( ushort3 x, uint sub_group_local_id );\n"
"ushort4     __ovld __conv intel_sub_group_broadcast( ushort4 x, uint sub_group_local_id );\n"
"ushort8     __ovld __conv intel_sub_group_broadcast( ushort8 x, uint sub_group_local_id );\n"
"\n"
"short       __ovld __conv intel_sub_group_shuffle( short   x, uint c );\n"
"short2      __ovld __conv intel_sub_group_shuffle( short2  x, uint c );\n"
"short3      __ovld __conv intel_sub_group_shuffle( short3  x, uint c );\n"
"short4      __ovld __conv intel_sub_group_shuffle( short4  x, uint c );\n"
"short8      __ovld __conv intel_sub_group_shuffle( short8  x, uint c );\n"
"short16     __ovld __conv intel_sub_group_shuffle( short16 x, uint c);\n"
"\n"
"ushort      __ovld __conv intel_sub_group_shuffle( ushort   x, uint c );\n"
"ushort2     __ovld __conv intel_sub_group_shuffle( ushort2  x, uint c );\n"
"ushort3     __ovld __conv intel_sub_group_shuffle( ushort3  x, uint c );\n"
"ushort4     __ovld __conv intel_sub_group_shuffle( ushort4  x, uint c );\n"
"ushort8     __ovld __conv intel_sub_group_shuffle( ushort8  x, uint c );\n"
"ushort16    __ovld __conv intel_sub_group_shuffle( ushort16 x, uint c );\n"
"\n"
"short       __ovld __conv intel_sub_group_shuffle_down( short   cur, short   next, uint c );\n"
"short2      __ovld __conv intel_sub_group_shuffle_down( short2  cur, short2  next, uint c );\n"
"short3      __ovld __conv intel_sub_group_shuffle_down( short3  cur, short3  next, uint c );\n"
"short4      __ovld __conv intel_sub_group_shuffle_down( short4  cur, short4  next, uint c );\n"
"short8      __ovld __conv intel_sub_group_shuffle_down( short8  cur, short8  next, uint c );\n"
"short16     __ovld __conv intel_sub_group_shuffle_down( short16 cur, short16 next, uint c );\n"
"\n"
"ushort      __ovld __conv intel_sub_group_shuffle_down( ushort   cur, ushort   next, uint c );\n"
"ushort2     __ovld __conv intel_sub_group_shuffle_down( ushort2  cur, ushort2  next, uint c );\n"
"ushort3     __ovld __conv intel_sub_group_shuffle_down( ushort3  cur, ushort3  next, uint c );\n"
"ushort4     __ovld __conv intel_sub_group_shuffle_down( ushort4  cur, ushort4  next, uint c );\n"
"ushort8     __ovld __conv intel_sub_group_shuffle_down( ushort8  cur, ushort8  next, uint c );\n"
"ushort16    __ovld __conv intel_sub_group_shuffle_down( ushort16 cur, ushort16 next, uint c );\n"
"\n"
"short       __ovld __conv intel_sub_group_shuffle_up( short   cur, short   next, uint c );\n"
"short2      __ovld __conv intel_sub_group_shuffle_up( short2  cur, short2  next, uint c );\n"
"short3      __ovld __conv intel_sub_group_shuffle_up( short3  cur, short3  next, uint c );\n"
"short4      __ovld __conv intel_sub_group_shuffle_up( short4  cur, short4  next, uint c );\n"
"short8      __ovld __conv intel_sub_group_shuffle_up( short8  cur, short8  next, uint c );\n"
"short16     __ovld __conv intel_sub_group_shuffle_up( short16 cur, short16 next, uint c );\n"
"\n"
"ushort      __ovld __conv intel_sub_group_shuffle_up( ushort   cur, ushort   next, uint c );\n"
"ushort2     __ovld __conv intel_sub_group_shuffle_up( ushort2  cur, ushort2  next, uint c );\n"
"ushort3     __ovld __conv intel_sub_group_shuffle_up( ushort3  cur, ushort3  next, uint c );\n"
"ushort4     __ovld __conv intel_sub_group_shuffle_up( ushort4  cur, ushort4  next, uint c );\n"
"ushort8     __ovld __conv intel_sub_group_shuffle_up( ushort8  cur, ushort8  next, uint c );\n"
"ushort16    __ovld __conv intel_sub_group_shuffle_up( ushort16 cur, ushort16 next, uint c );\n"
"\n"
"short       __ovld __conv intel_sub_group_shuffle_xor( short   x, uint c );\n"
"short2      __ovld __conv intel_sub_group_shuffle_xor( short2  x, uint c );\n"
"short3      __ovld __conv intel_sub_group_shuffle_xor( short3  x, uint c );\n"
"short4      __ovld __conv intel_sub_group_shuffle_xor( short4  x, uint c );\n"
"short8      __ovld __conv intel_sub_group_shuffle_xor( short8  x, uint c );\n"
"short16     __ovld __conv intel_sub_group_shuffle_xor( short16 x, uint c );\n"
"\n"
"ushort      __ovld __conv intel_sub_group_shuffle_xor( ushort   x, uint c );\n"
"ushort2     __ovld __conv intel_sub_group_shuffle_xor( ushort2  x, uint c );\n"
"ushort3     __ovld __conv intel_sub_group_shuffle_xor( ushort3  x, uint c );\n"
"ushort4     __ovld __conv intel_sub_group_shuffle_xor( ushort4  x, uint c );\n"
"ushort8     __ovld __conv intel_sub_group_shuffle_xor( ushort8  x, uint c );\n"
"ushort16    __ovld __conv intel_sub_group_shuffle_xor( ushort16 x, uint c );\n"
"\n"
"short       __ovld __conv intel_sub_group_reduce_add( short   x );\n"
"ushort      __ovld __conv intel_sub_group_reduce_add( ushort  x );\n"
"short       __ovld __conv intel_sub_group_reduce_min( short   x );\n"
"ushort      __ovld __conv intel_sub_group_reduce_min( ushort  x );\n"
"short       __ovld __conv intel_sub_group_reduce_max( short   x );\n"
"ushort      __ovld __conv intel_sub_group_reduce_max( ushort  x );\n"
"\n"
"short       __ovld __conv intel_sub_group_scan_exclusive_add( short   x );\n"
"ushort      __ovld __conv intel_sub_group_scan_exclusive_add( ushort  x );\n"
"short       __ovld __conv intel_sub_group_scan_exclusive_min( short   x );\n"
"ushort      __ovld __conv intel_sub_group_scan_exclusive_min( ushort  x );\n"
"short       __ovld __conv intel_sub_group_scan_exclusive_max( short   x );\n"
"ushort      __ovld __conv intel_sub_group_scan_exclusive_max( ushort  x );\n"
"\n"
"short       __ovld __conv intel_sub_group_scan_inclusive_add( short   x );\n"
"ushort      __ovld __conv intel_sub_group_scan_inclusive_add( ushort  x );\n"
"short       __ovld __conv intel_sub_group_scan_inclusive_min( short   x );\n"
"ushort      __ovld __conv intel_sub_group_scan_inclusive_min( ushort  x );\n"
"short       __ovld __conv intel_sub_group_scan_inclusive_max( short   x );\n"
"ushort      __ovld __conv intel_sub_group_scan_inclusive_max( ushort  x );\n"
"\n"
"uint       __ovld __conv intel_sub_group_block_read_ui( read_only image2d_t image, int2 byte_coord );\n"
"uint2      __ovld __conv intel_sub_group_block_read_ui2( read_only image2d_t image, int2 byte_coord );\n"
"uint4      __ovld __conv intel_sub_group_block_read_ui4( read_only image2d_t image, int2 byte_coord );\n"
"uint8      __ovld __conv intel_sub_group_block_read_ui8( read_only image2d_t image, int2 byte_coord );\n"
"\n"
"#if (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"uint       __ovld __conv intel_sub_group_block_read_ui( read_write image2d_t image, int2 byte_coord );\n"
"uint2      __ovld __conv intel_sub_group_block_read_ui2( read_write image2d_t image, int2 byte_coord );\n"
"uint4      __ovld __conv intel_sub_group_block_read_ui4( read_write image2d_t image, int2 byte_coord );\n"
"uint8      __ovld __conv intel_sub_group_block_read_ui8( read_write image2d_t image, int2 byte_coord );\n"
"#endif // (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"\n"
"uint       __ovld __conv intel_sub_group_block_read_ui( const __global uint* p );\n"
"uint2      __ovld __conv intel_sub_group_block_read_ui2( const __global uint* p );\n"
"uint4      __ovld __conv intel_sub_group_block_read_ui4( const __global uint* p );\n"
"uint8      __ovld __conv intel_sub_group_block_read_ui8( const __global uint* p );\n"
"\n"
"void       __ovld __conv intel_sub_group_block_write_ui( read_only image2d_t image, int2 byte_coord, uint data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui2( read_only image2d_t image, int2 byte_coord, uint2 data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui4( read_only image2d_t image, int2 byte_coord, uint4 data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui8( read_only image2d_t image, int2 byte_coord, uint8 data );\n"
"\n"
"#if (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"void       __ovld __conv intel_sub_group_block_write_ui( read_write image2d_t image, int2 byte_coord, uint data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui2( read_write image2d_t image, int2 byte_coord, uint2 data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui4( read_write image2d_t image, int2 byte_coord, uint4 data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui8( read_write image2d_t image, int2 byte_coord, uint8 data );\n"
"#endif // (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"\n"
"void       __ovld __conv intel_sub_group_block_write_ui( __global uint* p, uint data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui2( __global uint* p, uint2 data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui4( __global uint* p, uint4 data );\n"
"void       __ovld __conv intel_sub_group_block_write_ui8( __global uint* p, uint8 data );\n"
"\n"
"ushort      __ovld __conv intel_sub_group_block_read_us( read_only image2d_t image, int2 coord );\n"
"ushort2     __ovld __conv intel_sub_group_block_read_us2( read_only image2d_t image, int2 coord );\n"
"ushort4     __ovld __conv intel_sub_group_block_read_us4( read_only image2d_t image, int2 coord );\n"
"ushort8     __ovld __conv intel_sub_group_block_read_us8( read_only image2d_t image, int2 coord );\n"
"\n"
"#if (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"ushort      __ovld __conv intel_sub_group_block_read_us(read_write image2d_t image, int2 coord);\n"
"ushort2     __ovld __conv intel_sub_group_block_read_us2(read_write image2d_t image, int2 coord);\n"
"ushort4     __ovld __conv intel_sub_group_block_read_us4(read_write image2d_t image, int2 coord);\n"
"ushort8     __ovld __conv intel_sub_group_block_read_us8(read_write image2d_t image, int2 coord);\n"
"#endif // (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"\n"
"ushort      __ovld __conv intel_sub_group_block_read_us(  const __global ushort* p );\n"
"ushort2     __ovld __conv intel_sub_group_block_read_us2( const __global ushort* p );\n"
"ushort4     __ovld __conv intel_sub_group_block_read_us4( const __global ushort* p );\n"
"ushort8     __ovld __conv intel_sub_group_block_read_us8( const __global ushort* p );\n"
"\n"
"void        __ovld __conv intel_sub_group_block_write_us(write_only image2d_t image, int2 coord, ushort  data);\n"
"void        __ovld __conv intel_sub_group_block_write_us2(write_only image2d_t image, int2 coord, ushort2 data);\n"
"void        __ovld __conv intel_sub_group_block_write_us4(write_only image2d_t image, int2 coord, ushort4 data);\n"
"void        __ovld __conv intel_sub_group_block_write_us8(write_only image2d_t image, int2 coord, ushort8 data);\n"
"\n"
"#if (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"void        __ovld __conv intel_sub_group_block_write_us(read_write image2d_t image, int2 coord, ushort  data);\n"
"void        __ovld __conv intel_sub_group_block_write_us2(read_write image2d_t image, int2 coord, ushort2 data);\n"
"void        __ovld __conv intel_sub_group_block_write_us4(read_write image2d_t image, int2 coord, ushort4 data);\n"
"void        __ovld __conv intel_sub_group_block_write_us8(read_write image2d_t image, int2 coord, ushort8 data);\n"
"#endif // (__OPENCL_C_VERSION__ >= CL_VERSION_2_0)\n"
"\n"
"void        __ovld __conv intel_sub_group_block_write_us(  __global ushort* p, ushort  data );\n"
"void        __ovld __conv intel_sub_group_block_write_us2( __global ushort* p, ushort2 data );\n"
"void        __ovld __conv intel_sub_group_block_write_us4( __global ushort* p, ushort4 data );\n"
"void        __ovld __conv intel_sub_group_block_write_us8( __global ushort* p, ushort8 data );\n"
"#endif // cl_intel_subgroups_short\n"
"\n"
"#ifdef cl_intel_device_side_avc_motion_estimation\n"
"#pragma OPENCL EXTENSION cl_intel_device_side_avc_motion_estimation : begin\n"
"\n"
"#define CLK_AVC_ME_MAJOR_16x16_INTEL 0x0\n"
"#define CLK_AVC_ME_MAJOR_16x8_INTEL 0x1\n"
"#define CLK_AVC_ME_MAJOR_8x16_INTEL 0x2\n"
"#define CLK_AVC_ME_MAJOR_8x8_INTEL 0x3\n"
"\n"
"#define CLK_AVC_ME_MINOR_8x8_INTEL 0x0\n"
"#define CLK_AVC_ME_MINOR_8x4_INTEL 0x1\n"
"#define CLK_AVC_ME_MINOR_4x8_INTEL 0x2\n"
"#define CLK_AVC_ME_MINOR_4x4_INTEL 0x3\n"
"\n"
"#define CLK_AVC_ME_MAJOR_FORWARD_INTEL 0x0\n"
"#define CLK_AVC_ME_MAJOR_BACKWARD_INTEL 0x1\n"
"#define CLK_AVC_ME_MAJOR_BIDIRECTIONAL_INTEL 0x2\n"
"\n"
"#define CLK_AVC_ME_PARTITION_MASK_ALL_INTEL 0x0\n"
"#define CLK_AVC_ME_PARTITION_MASK_16x16_INTEL 0x7E\n"
"#define CLK_AVC_ME_PARTITION_MASK_16x8_INTEL 0x7D\n"
"#define CLK_AVC_ME_PARTITION_MASK_8x16_INTEL 0x7B\n"
"#define CLK_AVC_ME_PARTITION_MASK_8x8_INTEL 0x77\n"
"#define CLK_AVC_ME_PARTITION_MASK_8x4_INTEL 0x6F\n"
"#define CLK_AVC_ME_PARTITION_MASK_4x8_INTEL 0x5F\n"
"#define CLK_AVC_ME_PARTITION_MASK_4x4_INTEL 0x3F\n"
"\n"
"#define CLK_AVC_ME_SLICE_TYPE_PRED_INTEL 0x0\n"
"#define CLK_AVC_ME_SLICE_TYPE_BPRED_INTEL 0x1\n"
"#define CLK_AVC_ME_SLICE_TYPE_INTRA_INTEL 0x2\n"
"\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_EXHAUSTIVE_INTEL 0x0\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_SMALL_INTEL 0x1\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_TINY_INTEL 0x2\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_EXTRA_TINY_INTEL 0x3\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_DIAMOND_INTEL 0x4\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_LARGE_DIAMOND_INTEL 0x5\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_RESERVED0_INTEL 0x6\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_RESERVED1_INTEL 0x7\n"
"#define CLK_AVC_ME_SEARCH_WINDOW_CUSTOM_INTEL 0x8\n"
"\n"
"#define CLK_AVC_ME_SAD_ADJUST_MODE_NONE_INTEL 0x0\n"
"#define CLK_AVC_ME_SAD_ADJUST_MODE_HAAR_INTEL 0x2\n"
"\n"
"#define CLK_AVC_ME_SUBPIXEL_MODE_INTEGER_INTEL 0x0\n"
"#define CLK_AVC_ME_SUBPIXEL_MODE_HPEL_INTEL 0x1\n"
"#define CLK_AVC_ME_SUBPIXEL_MODE_QPEL_INTEL 0x3\n"
"\n"
"#define CLK_AVC_ME_COST_PRECISION_QPEL_INTEL 0x0\n"
"#define CLK_AVC_ME_COST_PRECISION_HPEL_INTEL 0x1\n"
"#define CLK_AVC_ME_COST_PRECISION_PEL_INTEL 0x2\n"
"#define CLK_AVC_ME_COST_PRECISION_DPEL_INTEL 0x3\n"
"\n"
"#define CLK_AVC_ME_BIDIR_WEIGHT_QUARTER_INTEL 0x10\n"
"#define CLK_AVC_ME_BIDIR_WEIGHT_THIRD_INTEL 0x15\n"
"#define CLK_AVC_ME_BIDIR_WEIGHT_HALF_INTEL 0x20\n"
"#define CLK_AVC_ME_BIDIR_WEIGHT_TWO_THIRD_INTEL 0x2B\n"
"#define CLK_AVC_ME_BIDIR_WEIGHT_THREE_QUARTER_INTEL 0x30\n"
"\n"
"#define CLK_AVC_ME_BORDER_REACHED_LEFT_INTEL 0x0\n"
"#define CLK_AVC_ME_BORDER_REACHED_RIGHT_INTEL 0x2\n"
"#define CLK_AVC_ME_BORDER_REACHED_TOP_INTEL 0x4\n"
"#define CLK_AVC_ME_BORDER_REACHED_BOTTOM_INTEL 0x8\n"
"\n"
"#define CLK_AVC_ME_INTRA_16x16_INTEL 0x0\n"
"#define CLK_AVC_ME_INTRA_8x8_INTEL 0x1\n"
"#define CLK_AVC_ME_INTRA_4x4_INTEL 0x2\n"
"\n"
"#define CLK_AVC_ME_SKIP_BLOCK_PARTITION_16x16_INTEL 0x0\n"
"#define CLK_AVC_ME_SKIP_BLOCK_PARTITION_8x8_INTEL 0x4000\n"
"\n"
"#define CLK_AVC_ME_SKIP_BLOCK_16x16_FORWARD_ENABLE_INTEL (0x1 << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_16x16_BACKWARD_ENABLE_INTEL (0x2 << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_16x16_DUAL_ENABLE_INTEL (0x3 << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_FORWARD_ENABLE_INTEL (0x55 << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_BACKWARD_ENABLE_INTEL (0xAA << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_DUAL_ENABLE_INTEL (0xFF << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_0_FORWARD_ENABLE_INTEL (0x1 << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_0_BACKWARD_ENABLE_INTEL (0x2 << 24)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_1_FORWARD_ENABLE_INTEL (0x1 << 26)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_1_BACKWARD_ENABLE_INTEL (0x2 << 26)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_2_FORWARD_ENABLE_INTEL (0x1 << 28)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_2_BACKWARD_ENABLE_INTEL (0x2 << 28)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_3_FORWARD_ENABLE_INTEL (0x1 << 30)\n"
"#define CLK_AVC_ME_SKIP_BLOCK_8x8_3_BACKWARD_ENABLE_INTEL (0x2 << 30)\n"
"\n"
"#define CLK_AVC_ME_BLOCK_BASED_SKIP_4x4_INTEL 0x00\n"
"#define CLK_AVC_ME_BLOCK_BASED_SKIP_8x8_INTEL 0x80\n"
"\n"
"#define CLK_AVC_ME_INTRA_LUMA_PARTITION_MASK_ALL_INTEL 0x0\n"
"#define CLK_AVC_ME_INTRA_LUMA_PARTITION_MASK_16x16_INTEL 0x6\n"
"#define CLK_AVC_ME_INTRA_LUMA_PARTITION_MASK_8x8_INTEL 0x5\n"
"#define CLK_AVC_ME_INTRA_LUMA_PARTITION_MASK_4x4_INTEL 0x3\n"
"\n"
"#define CLK_AVC_ME_INTRA_NEIGHBOR_LEFT_MASK_ENABLE_INTEL 0x60\n"
"#define CLK_AVC_ME_INTRA_NEIGHBOR_UPPER_MASK_ENABLE_INTEL 0x10\n"
"#define CLK_AVC_ME_INTRA_NEIGHBOR_UPPER_RIGHT_MASK_ENABLE_INTEL 0x8\n"
"#define CLK_AVC_ME_INTRA_NEIGHBOR_UPPER_LEFT_MASK_ENABLE_INTEL 0x4\n"
"\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_VERTICAL_INTEL 0x0\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_HORIZONTAL_INTEL 0x1\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_DC_INTEL 0x2\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_DIAGONAL_DOWN_LEFT_INTEL 0x3\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_DIAGONAL_DOWN_RIGHT_INTEL 0x4\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_PLANE_INTEL 0x4\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_VERTICAL_RIGHT_INTEL 0x5\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_HORIZONTAL_DOWN_INTEL 0x6\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_VERTICAL_LEFT_INTEL 0x7\n"
"#define CLK_AVC_ME_LUMA_PREDICTOR_MODE_HORIZONTAL_UP_INTEL 0x8\n"
"#define CLK_AVC_ME_CHROMA_PREDICTOR_MODE_DC_INTEL 0x0\n"
"#define CLK_AVC_ME_CHROMA_PREDICTOR_MODE_HORIZONTAL_INTEL 0x1\n"
"#define CLK_AVC_ME_CHROMA_PREDICTOR_MODE_VERTICAL_INTEL 0x2\n"
"#define CLK_AVC_ME_CHROMA_PREDICTOR_MODE_PLANE_INTEL 0x3\n"
"\n"
"#define CLK_AVC_ME_FRAME_FORWARD_INTEL 0x1\n"
"#define CLK_AVC_ME_FRAME_BACKWARD_INTEL 0x2\n"
"#define CLK_AVC_ME_FRAME_DUAL_INTEL 0x3\n"
"\n"
"#define CLK_AVC_ME_INTERLACED_SCAN_TOP_FIELD_INTEL 0x0\n"
"#define CLK_AVC_ME_INTERLACED_SCAN_BOTTOM_FIELD_INTEL 0x1\n"
"\n"
"#define CLK_AVC_ME_INITIALIZE_INTEL 0x0\n"
"\n"
"#define CLK_AVC_IME_PAYLOAD_INITIALIZE_INTEL 0x0\n"
"#define CLK_AVC_REF_PAYLOAD_INITIALIZE_INTEL 0x0\n"
"#define CLK_AVC_SIC_PAYLOAD_INITIALIZE_INTEL 0x0\n"
"\n"
"#define CLK_AVC_IME_RESULT_INITIALIZE_INTEL 0x0\n"
"#define CLK_AVC_REF_RESULT_INITIALIZE_INTEL 0x0\n"
"#define CLK_AVC_SIC_RESULT_INITIALIZE_INTEL 0x0\n"
"\n"
"#define CLK_AVC_IME_RESULT_SINGLE_REFERENCE_STREAMOUT_INITIALIZE_INTEL 0x0\n"
"#define CLK_AVC_IME_RESULT_SINGLE_REFERENCE_STREAMIN_INITIALIZE_INTEL 0x0\n"
"#define CLK_AVC_IME_RESULT_DUAL_REFERENCE_STREAMOUT_INITIALIZE_INTEL 0x0\n"
"#define CLK_AVC_IME_RESULT_DUAL_REFERENCE_STREAMIN_INITIALIZE_INTEL 0x0\n"
"\n"
"// MCE built-in functions\n"
"uchar __ovld\n"
"intel_sub_group_avc_mce_get_default_inter_base_multi_reference_penalty(\n"
"    uchar slice_type, uchar qp);\n"
"ulong __ovld intel_sub_group_avc_mce_get_default_inter_shape_penalty(\n"
"    uchar slice_type, uchar qp);\n"
"uchar __ovld intel_sub_group_avc_mce_get_default_inter_direction_penalty(\n"
"    uchar slice_type, uchar qp);\n"
"uint __ovld intel_sub_group_avc_mce_get_default_intra_luma_shape_penalty(\n"
"    uchar slice_type, uchar qp);\n"
"uint2 __ovld\n"
"intel_sub_group_avc_mce_get_default_inter_motion_vector_cost_table(\n"
"    uchar slice_type, uchar qp);\n"
"uchar __ovld intel_sub_group_avc_mce_get_default_intra_luma_mode_penalty(\n"
"    uchar slice_type, uchar qp);\n"
"\n"
"uint2 __ovld intel_sub_group_avc_mce_get_default_high_penalty_cost_table();\n"
"uint2 __ovld intel_sub_group_avc_mce_get_default_medium_penalty_cost_table();\n"
"uint2 __ovld intel_sub_group_avc_mce_get_default_low_penalty_cost_table();\n"
"uint __ovld intel_sub_group_avc_mce_get_default_non_dc_luma_intra_penalty();\n"
"uchar __ovld\n"
"intel_sub_group_avc_mce_get_default_intra_chroma_mode_base_penalty();\n"
"\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_inter_base_multi_reference_penalty(\n"
"    uchar reference_base_penalty, intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_inter_shape_penalty(\n"
"    ulong packed_shape_penalty, intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_inter_direction_penalty(\n"
"    uchar direction_cost, intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_motion_vector_cost_function(\n"
"    ulong packed_cost_center_delta, uint2 packed_cost_table,\n"
"    uchar cost_precision, intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_ac_only_haar(\n"
"    intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_source_interlaced_field_polarity(\n"
"    uchar src_field_polarity, intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_single_reference_interlaced_field_polarity(\n"
"    uchar ref_field_polarity, intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_mce_set_dual_reference_interlaced_field_polarities(\n"
"    uchar fwd_ref_field_polarity, uchar bwd_ref_field_polarity,\n"
"    intel_sub_group_avc_mce_payload_t payload);\n"
"\n"
"ulong __ovld intel_sub_group_avc_mce_get_motion_vectors(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"ushort __ovld intel_sub_group_avc_mce_get_inter_distortions(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"ushort __ovld intel_sub_group_avc_mce_get_best_inter_distortion(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"uchar __ovld intel_sub_group_avc_mce_get_inter_major_shape(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"uchar __ovld intel_sub_group_avc_mce_get_inter_minor_shapes(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"uchar __ovld intel_sub_group_avc_mce_get_inter_directions(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"uchar __ovld intel_sub_group_avc_mce_get_inter_motion_vector_count(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"uint __ovld intel_sub_group_avc_mce_get_inter_reference_ids(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"uchar __ovld\n"
"intel_sub_group_avc_mce_get_inter_reference_interlaced_field_polarities(\n"
"    uint packed_reference_ids, uint packed_reference_parameter_field_polarities,\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"\n"
"// IME built-in functions\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_initialize(\n"
"    ushort2 src_coord, uchar partition_mask, uchar sad_adjustment);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_single_reference(\n"
"    short2 ref_offset, uchar search_window_config,\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_dual_reference(\n"
"    short2 fwd_ref_offset, short2 bwd_ref_offset, uchar search_window_config,\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_max_motion_vector_count(\n"
"    uchar max_motion_vector_count, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_unidirectional_mix_disable(\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_early_search_termination_threshold(\n"
"    uchar threshold, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_weighted_sad(\n"
"    uint packed_sad_weights, intel_sub_group_avc_ime_payload_t payload);\n"
"\n"
"__attribute__((deprecated(\"If you use the latest Intel driver, please use \"\n"
"                          \"intel_sub_group_avc_ime_ref_window_size instead\",\n"
"                          \"intel_sub_group_avc_ime_ref_window_size\")))\n"
"ushort2 __ovld\n"
"intel_sub_group_ime_ref_window_size(uchar search_window_config, char dual_ref);\n"
"ushort2 __ovld intel_sub_group_avc_ime_ref_window_size(\n"
"    uchar search_window_config, char dual_ref);\n"
"short2 __ovld intel_sub_group_avc_ime_adjust_ref_offset(\n"
"    short2 ref_offset, ushort2 src_coord, ushort2 ref_window_size,\n"
"    ushort2 image_size);\n"
"\n"
"intel_sub_group_avc_ime_result_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_single_reference(\n"
"    read_only image2d_t src_image, read_only image2d_t ref_image,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_result_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_dual_reference(\n"
"    read_only image2d_t src_image, read_only image2d_t fwd_ref_image,\n"
"    read_only image2d_t bwd_ref_image, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_result_single_reference_streamout_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_single_reference_streamout(\n"
"    read_only image2d_t src_image, read_only image2d_t ref_image,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_result_dual_reference_streamout_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_dual_reference_streamout(\n"
"    read_only image2d_t src_image, read_only image2d_t fwd_ref_image,\n"
"    read_only image2d_t bwd_ref_image, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_result_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_single_reference_streamin(\n"
"    read_only image2d_t src_image, read_only image2d_t ref_image,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_ime_payload_t payload,\n"
"    intel_sub_group_avc_ime_single_reference_streamin_t streamin_components);\n"
"intel_sub_group_avc_ime_result_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_dual_reference_streamin(\n"
"    read_only image2d_t src_image, read_only image2d_t fwd_ref_image,\n"
"    read_only image2d_t bwd_ref_image, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_ime_payload_t payload,\n"
"    intel_sub_group_avc_ime_dual_reference_streamin_t streamin_components);\n"
"intel_sub_group_avc_ime_result_single_reference_streamout_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_single_reference_streaminout(\n"
"    read_only image2d_t src_image, read_only image2d_t ref_image,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_ime_payload_t payload,\n"
"    intel_sub_group_avc_ime_single_reference_streamin_t streamin_components);\n"
"intel_sub_group_avc_ime_result_dual_reference_streamout_t __ovld\n"
"intel_sub_group_avc_ime_evaluate_with_dual_reference_streaminout(\n"
"    read_only image2d_t src_image, read_only image2d_t fwd_ref_image,\n"
"    read_only image2d_t bwd_ref_image, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_ime_payload_t payload,\n"
"    intel_sub_group_avc_ime_dual_reference_streamin_t streamin_components);\n"
"\n"
"intel_sub_group_avc_ime_single_reference_streamin_t __ovld\n"
"intel_sub_group_avc_ime_get_single_reference_streamin(\n"
"    intel_sub_group_avc_ime_result_single_reference_streamout_t result);\n"
"intel_sub_group_avc_ime_dual_reference_streamin_t __ovld\n"
"intel_sub_group_avc_ime_get_dual_reference_streamin(\n"
"    intel_sub_group_avc_ime_result_dual_reference_streamout_t result);\n"
"intel_sub_group_avc_ime_result_t __ovld\n"
"intel_sub_group_avc_ime_strip_single_reference_streamout(\n"
"    intel_sub_group_avc_ime_result_single_reference_streamout_t result);\n"
"intel_sub_group_avc_ime_result_t __ovld\n"
"intel_sub_group_avc_ime_strip_dual_reference_streamout(\n"
"    intel_sub_group_avc_ime_result_dual_reference_streamout_t result);\n"
"\n"
"uint __ovld intel_sub_group_avc_ime_get_streamout_major_shape_motion_vectors(\n"
"    intel_sub_group_avc_ime_result_single_reference_streamout_t result,\n"
"    uchar major_shape);\n"
"ushort __ovld intel_sub_group_avc_ime_get_streamout_major_shape_distortions(\n"
"    intel_sub_group_avc_ime_result_single_reference_streamout_t result,\n"
"    uchar major_shape);\n"
"uchar __ovld intel_sub_group_avc_ime_get_streamout_major_shape_reference_ids(\n"
"    intel_sub_group_avc_ime_result_single_reference_streamout_t result,\n"
"    uchar major_shape);\n"
"uint __ovld intel_sub_group_avc_ime_get_streamout_major_shape_motion_vectors(\n"
"    intel_sub_group_avc_ime_result_dual_reference_streamout_t result,\n"
"    uchar major_shape, uchar direction);\n"
"ushort __ovld intel_sub_group_avc_ime_get_streamout_major_shape_distortions(\n"
"    intel_sub_group_avc_ime_result_dual_reference_streamout_t result,\n"
"    uchar major_shape, uchar direction);\n"
"uchar __ovld intel_sub_group_avc_ime_get_streamout_major_shape_reference_ids(\n"
"    intel_sub_group_avc_ime_result_dual_reference_streamout_t result,\n"
"    uchar major_shape, uchar direction);\n"
"\n"
"uchar __ovld intel_sub_group_avc_ime_get_border_reached(\n"
"    uchar image_select, intel_sub_group_avc_ime_result_t result);\n"
"uchar __ovld intel_sub_group_avc_ime_get_truncated_search_indication(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uchar __ovld\n"
"intel_sub_group_avc_ime_get_unidirectional_early_search_termination(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uint __ovld intel_sub_group_avc_ime_get_weighting_pattern_minimum_motion_vector(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"ushort __ovld intel_sub_group_avc_ime_get_weighting_pattern_minimum_distortion(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"\n"
"// REF built-in functions\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_fme_initialize(\n"
"    ushort2 src_coord, ulong motion_vectors, uchar major_shapes,\n"
"    uchar minor_shapes, uchar directions, uchar pixel_resolution,\n"
"    uchar sad_adjustment);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_bme_initialize(\n"
"    ushort2 src_coord, ulong motion_vectors, uchar major_shapes,\n"
"    uchar minor_shapes, uchar directions, uchar pixel_resolution,\n"
"    uchar bidirectional_weight, uchar sad_adjustment);\n"
"\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_bidirectional_mix_disable(\n"
"    intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_bilinear_filter_enable(\n"
"    intel_sub_group_avc_ref_payload_t payload);\n"
"\n"
"intel_sub_group_avc_ref_result_t __ovld\n"
"intel_sub_group_avc_ref_evaluate_with_single_reference(\n"
"    read_only image2d_t src_image, read_only image2d_t ref_image,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_ref_result_t __ovld\n"
"intel_sub_group_avc_ref_evaluate_with_dual_reference(\n"
"    read_only image2d_t src_image, read_only image2d_t fwd_ref_image,\n"
"    read_only image2d_t bwd_ref_image, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_ref_result_t __ovld\n"
"intel_sub_group_avc_ref_evaluate_with_multi_reference(\n"
"    read_only image2d_t src_image, uint packed_reference_ids,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_ref_result_t __ovld\n"
"intel_sub_group_avc_ref_evaluate_with_multi_reference(\n"
"    read_only image2d_t src_image, uint packed_reference_ids,\n"
"    uchar packed_reference_field_polarities, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_ref_payload_t payload);\n"
"\n"
"// SIC built-in functions\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_initialize(\n"
"    ushort2 src_coord);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_configure_skc(\n"
"    uint skip_block_partition_type, uint skip_motion_vector_mask,\n"
"    ulong motion_vectors, uchar bidirectional_weight, uchar skip_sad_adjustment,\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_configure_ipe(\n"
"    uchar luma_intra_partition_mask, uchar intra_neighbour_availabilty,\n"
"    uchar left_edge_luma_pixels, uchar upper_left_corner_luma_pixel,\n"
"    uchar upper_edge_luma_pixels, uchar upper_right_edge_luma_pixels,\n"
"    uchar intra_sad_adjustment, intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_configure_ipe(\n"
"    uchar luma_intra_partition_mask, uchar intra_neighbour_availabilty,\n"
"    uchar left_edge_luma_pixels, uchar upper_left_corner_luma_pixel,\n"
"    uchar upper_edge_luma_pixels, uchar upper_right_edge_luma_pixels,\n"
"    ushort left_edge_chroma_pixels, ushort upper_left_corner_chroma_pixel,\n"
"    ushort upper_edge_chroma_pixels, uchar intra_sad_adjustment,\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"uint __ovld\n"
"intel_sub_group_avc_sic_get_motion_vector_mask(\n"
"    uint skip_block_partition_type, uchar direction);\n"
"\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_intra_luma_shape_penalty(\n"
"    uint packed_shape_cost, intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_intra_luma_mode_cost_function(\n"
"    uchar luma_mode_penalty, uint luma_packed_neighbor_modes,\n"
"    uint luma_packed_non_dc_penalty, intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_intra_chroma_mode_cost_function(\n"
"    uchar chroma_mode_penalty, intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_skc_bilinear_filter_enable(\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_skc_forward_transform_enable(\n"
"    ulong packed_sad_coefficients, intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_block_based_raw_skip_sad(\n"
"    uchar block_based_skip_type,\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_sic_result_t __ovld\n"
"intel_sub_group_avc_sic_evaluate_ipe(\n"
"    read_only image2d_t src_image, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_result_t __ovld\n"
"intel_sub_group_avc_sic_evaluate_with_single_reference(\n"
"    read_only image2d_t src_image, read_only image2d_t ref_image,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_result_t __ovld\n"
"intel_sub_group_avc_sic_evaluate_with_dual_reference(\n"
"    read_only image2d_t src_image, read_only image2d_t fwd_ref_image,\n"
"    read_only image2d_t bwd_ref_image, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_result_t __ovld\n"
"intel_sub_group_avc_sic_evaluate_with_multi_reference(\n"
"    read_only image2d_t src_image, uint packed_reference_ids,\n"
"    sampler_t vme_media_sampler, intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_result_t __ovld\n"
"intel_sub_group_avc_sic_evaluate_with_multi_reference(\n"
"    read_only image2d_t src_image, uint packed_reference_ids,\n"
"    uchar packed_reference_field_polarities, sampler_t vme_media_sampler,\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"uchar __ovld intel_sub_group_avc_sic_get_ipe_luma_shape(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"ushort __ovld intel_sub_group_avc_sic_get_best_ipe_luma_distortion(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"ushort __ovld intel_sub_group_avc_sic_get_best_ipe_chroma_distortion(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"ulong __ovld intel_sub_group_avc_sic_get_packed_ipe_luma_modes(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"uchar __ovld intel_sub_group_avc_sic_get_ipe_chroma_mode(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"uint __ovld intel_sub_group_avc_sic_get_packed_skc_luma_count_threshold(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"ulong __ovld intel_sub_group_avc_sic_get_packed_skc_luma_sum_threshold(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"ushort __ovld intel_sub_group_avc_sic_get_inter_raw_sads(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"\n"
"// Wrappers\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_inter_base_multi_reference_penalty(\n"
"    uchar reference_base_penalty, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_inter_base_multi_reference_penalty(\n"
"    uchar reference_base_penalty, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_inter_base_multi_reference_penalty(\n"
"    uchar reference_base_penalty, intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_inter_shape_penalty(\n"
"    ulong packed_shape_cost, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_inter_shape_penalty(\n"
"    ulong packed_shape_cost, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_inter_shape_penalty(\n"
"    ulong packed_shape_cost, intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_inter_direction_penalty(\n"
"    uchar direction_cost, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_inter_direction_penalty(\n"
"    uchar direction_cost, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_inter_direction_penalty(\n"
"    uchar direction_cost, intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_motion_vector_cost_function(\n"
"    ulong packed_cost_center_delta, uint2 packed_cost_table,\n"
"    uchar cost_precision, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_motion_vector_cost_function(\n"
"    ulong packed_cost_center_delta, uint2 packed_cost_table,\n"
"    uchar cost_precision, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_motion_vector_cost_function(\n"
"    ulong packed_cost_center_delta, uint2 packed_cost_table,\n"
"    uchar cost_precision, intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_source_interlaced_field_polarity(\n"
"    uchar src_field_polarity, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_source_interlaced_field_polarity(\n"
"    uchar src_field_polarity, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_source_interlaced_field_polarity(\n"
"    uchar src_field_polarity, intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_single_reference_interlaced_field_polarity(\n"
"    uchar ref_field_polarity, intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_single_reference_interlaced_field_polarity(\n"
"    uchar ref_field_polarity, intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_single_reference_interlaced_field_polarity(\n"
"    uchar ref_field_polarity, intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_dual_reference_interlaced_field_polarities(\n"
"    uchar fwd_ref_field_polarity, uchar bwd_ref_field_polarity,\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_dual_reference_interlaced_field_polarities(\n"
"    uchar fwd_ref_field_polarity, uchar bwd_ref_field_polarity,\n"
"    intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_dual_reference_interlaced_field_polarities(\n"
"    uchar fwd_ref_field_polarity, uchar bwd_ref_field_polarity,\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_ime_set_ac_only_haar(\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_ref_set_ac_only_haar(\n"
"    intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_sic_set_ac_only_haar(\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"\n"
"ulong __ovld intel_sub_group_avc_ime_get_motion_vectors(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"ulong __ovld intel_sub_group_avc_ref_get_motion_vectors(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"\n"
"ushort __ovld intel_sub_group_avc_ime_get_inter_distortions(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"ushort __ovld intel_sub_group_avc_ref_get_inter_distortions(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"ushort __ovld intel_sub_group_avc_sic_get_inter_distortions(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"\n"
"ushort __ovld intel_sub_group_avc_ime_get_best_inter_distortion(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"ushort __ovld intel_sub_group_avc_ref_get_best_inter_distortion(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"\n"
"uchar __ovld intel_sub_group_avc_ime_get_inter_major_shape(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uchar __ovld intel_sub_group_avc_ref_get_inter_major_shape(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"uchar __ovld intel_sub_group_avc_ime_get_inter_minor_shapes(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uchar __ovld intel_sub_group_avc_ref_get_inter_minor_shapes(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"\n"
"uchar __ovld intel_sub_group_avc_ime_get_inter_directions(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uchar __ovld intel_sub_group_avc_ref_get_inter_directions(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"\n"
"uchar __ovld intel_sub_group_avc_ime_get_inter_motion_vector_count(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uchar __ovld intel_sub_group_avc_ref_get_inter_motion_vector_count(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"\n"
"uint __ovld intel_sub_group_avc_ime_get_inter_reference_ids(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uint __ovld intel_sub_group_avc_ref_get_inter_reference_ids(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"\n"
"uchar __ovld\n"
"intel_sub_group_avc_ime_get_inter_reference_interlaced_field_polarities(\n"
"    uint packed_reference_ids, uint packed_reference_parameter_field_polarities,\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"uchar __ovld\n"
"intel_sub_group_avc_ref_get_inter_reference_interlaced_field_polarities(\n"
"    uint packed_reference_ids, uint packed_reference_parameter_field_polarities,\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"\n"
"// Type conversion functions\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_ime_convert_to_mce_payload(\n"
"    intel_sub_group_avc_ime_payload_t payload);\n"
"intel_sub_group_avc_ime_payload_t __ovld\n"
"intel_sub_group_avc_mce_convert_to_ime_payload(\n"
"    intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_ref_convert_to_mce_payload(\n"
"    intel_sub_group_avc_ref_payload_t payload);\n"
"intel_sub_group_avc_ref_payload_t __ovld\n"
"intel_sub_group_avc_mce_convert_to_ref_payload(\n"
"    intel_sub_group_avc_mce_payload_t payload);\n"
"intel_sub_group_avc_mce_payload_t __ovld\n"
"intel_sub_group_avc_sic_convert_to_mce_payload(\n"
"    intel_sub_group_avc_sic_payload_t payload);\n"
"intel_sub_group_avc_sic_payload_t __ovld\n"
"intel_sub_group_avc_mce_convert_to_sic_payload(\n"
"    intel_sub_group_avc_mce_payload_t payload);\n"
"\n"
"intel_sub_group_avc_mce_result_t __ovld\n"
"intel_sub_group_avc_ime_convert_to_mce_result(\n"
"    intel_sub_group_avc_ime_result_t result);\n"
"intel_sub_group_avc_ime_result_t __ovld\n"
"intel_sub_group_avc_mce_convert_to_ime_result(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"intel_sub_group_avc_mce_result_t __ovld\n"
"intel_sub_group_avc_ref_convert_to_mce_result(\n"
"    intel_sub_group_avc_ref_result_t result);\n"
"intel_sub_group_avc_ref_result_t __ovld\n"
"intel_sub_group_avc_mce_convert_to_ref_result(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"intel_sub_group_avc_mce_result_t __ovld\n"
"intel_sub_group_avc_sic_convert_to_mce_result(\n"
"    intel_sub_group_avc_sic_result_t result);\n"
"intel_sub_group_avc_sic_result_t __ovld\n"
"intel_sub_group_avc_mce_convert_to_sic_result(\n"
"    intel_sub_group_avc_mce_result_t result);\n"
"#pragma OPENCL EXTENSION cl_intel_device_side_avc_motion_estimation : end\n"
"#endif // cl_intel_device_side_avc_motion_estimation\n"
"\n"
"#ifdef cl_amd_media_ops\n"
"uint __ovld amd_bitalign(uint a, uint b, uint c);\n"
"uint2 __ovld amd_bitalign(uint2 a, uint2 b, uint2 c);\n"
"uint3 __ovld amd_bitalign(uint3 a, uint3 b, uint3 c);\n"
"uint4 __ovld amd_bitalign(uint4 a, uint4 b, uint4 c);\n"
"uint8 __ovld amd_bitalign(uint8 a, uint8 b, uint8 c);\n"
"uint16 __ovld amd_bitalign(uint16 a, uint16 b, uint16 c);\n"
"\n"
"uint __ovld amd_bytealign(uint a, uint b, uint c);\n"
"uint2 __ovld amd_bytealign(uint2 a, uint2 b, uint2 c);\n"
"uint3 __ovld amd_bytealign(uint3 a, uint3 b, uint3 c);\n"
"uint4 __ovld amd_bytealign(uint4 a, uint4 b, uint4 c);\n"
"uint8 __ovld amd_bytealign(uint8 a, uint8 b, uint8 c);\n"
"uint16 __ovld amd_bytealign(uint16 a, uint16 b, uint16 c);\n"
"\n"
"uint __ovld amd_lerp(uint a, uint b, uint c);\n"
"uint2 __ovld amd_lerp(uint2 a, uint2 b, uint2 c);\n"
"uint3 __ovld amd_lerp(uint3 a, uint3 b, uint3 c);\n"
"uint4 __ovld amd_lerp(uint4 a, uint4 b, uint4 c);\n"
"uint8 __ovld amd_lerp(uint8 a, uint8 b, uint8 c);\n"
"uint16 __ovld amd_lerp(uint16 a, uint16 b, uint16 c);\n"
"\n"
"uint __ovld amd_pack(float4 v);\n"
"\n"
"uint __ovld amd_sad4(uint4 x, uint4 y, uint z);\n"
"\n"
"uint __ovld amd_sadhi(uint a, uint b, uint c);\n"
"uint2 __ovld amd_sadhi(uint2 a, uint2 b, uint2 c);\n"
"uint3 __ovld amd_sadhi(uint3 a, uint3 b, uint3 c);\n"
"uint4 __ovld amd_sadhi(uint4 a, uint4 b, uint4 c);\n"
"uint8 __ovld amd_sadhi(uint8 a, uint8 b, uint8 c);\n"
"uint16 __ovld amd_sadhi(uint16 a, uint16 b, uint16 c);\n"
"\n"
"uint __ovld amd_sad(uint a, uint b, uint c);\n"
"uint2 __ovld amd_sad(uint2 a, uint2 b, uint2 c);\n"
"uint3 __ovld amd_sad(uint3 a, uint3 b, uint3 c);\n"
"uint4 __ovld amd_sad(uint4 a, uint4 b, uint4 c);\n"
"uint8 __ovld amd_sad(uint8 a, uint8 b, uint8 c);\n"
"uint16 __ovld amd_sad(uint16 a, uint16 b, uint16 c);\n"
"\n"
"float __ovld amd_unpack0(uint a);\n"
"float2 __ovld amd_unpack0(uint2 a);\n"
"float3 __ovld amd_unpack0(uint3 a);\n"
"float4 __ovld amd_unpack0(uint4 a);\n"
"float8 __ovld amd_unpack0(uint8 a);\n"
"float16 __ovld amd_unpack0(uint16 a);\n"
"\n"
"float __ovld amd_unpack1(uint a);\n"
"float2 __ovld amd_unpack1(uint2 a);\n"
"float3 __ovld amd_unpack1(uint3 a);\n"
"float4 __ovld amd_unpack1(uint4 a);\n"
"float8 __ovld amd_unpack1(uint8 a);\n"
"float16 __ovld amd_unpack1(uint16 a);\n"
"\n"
"float __ovld amd_unpack2(uint a);\n"
"float2 __ovld amd_unpack2(uint2 a);\n"
"float3 __ovld amd_unpack2(uint3 a);\n"
"float4 __ovld amd_unpack2(uint4 a);\n"
"float8 __ovld amd_unpack2(uint8 a);\n"
"float16 __ovld amd_unpack2(uint16 a);\n"
"\n"
"float __ovld amd_unpack3(uint a);\n"
"float2 __ovld amd_unpack3(uint2 a);\n"
"float3 __ovld amd_unpack3(uint3 a);\n"
"float4 __ovld amd_unpack3(uint4 a);\n"
"float8 __ovld amd_unpack3(uint8 a);\n"
"float16 __ovld amd_unpack3(uint16 a);\n"
"#endif // cl_amd_media_ops\n"
"\n"
"#ifdef cl_amd_media_ops2\n"
"int __ovld amd_bfe(int src0, uint src1, uint src2);\n"
"int2 __ovld amd_bfe(int2 src0, uint2 src1, uint2 src2);\n"
"int3 __ovld amd_bfe(int3 src0, uint3 src1, uint3 src2);\n"
"int4 __ovld amd_bfe(int4 src0, uint4 src1, uint4 src2);\n"
"int8 __ovld amd_bfe(int8 src0, uint8 src1, uint8 src2);\n"
"int16 __ovld amd_bfe(int16 src0, uint16 src1, uint16 src2);\n"
"\n"
"uint __ovld amd_bfe(uint src0, uint src1, uint src2);\n"
"uint2 __ovld amd_bfe(uint2 src0, uint2 src1, uint2 src2);\n"
"uint3 __ovld amd_bfe(uint3 src0, uint3 src1, uint3 src2);\n"
"uint4 __ovld amd_bfe(uint4 src0, uint4 src1, uint4 src2);\n"
"uint8 __ovld amd_bfe(uint8 src0, uint8 src1, uint8 src2);\n"
"uint16 __ovld amd_bfe(uint16 src0, uint16 src1, uint16 src2);\n"
"\n"
"uint __ovld amd_bfm(uint src0, uint src1);\n"
"uint2 __ovld amd_bfm(uint2 src0, uint2 src1);\n"
"uint3 __ovld amd_bfm(uint3 src0, uint3 src1);\n"
"uint4 __ovld amd_bfm(uint4 src0, uint4 src1);\n"
"uint8 __ovld amd_bfm(uint8 src0, uint8 src1);\n"
"uint16 __ovld amd_bfm(uint16 src0, uint16 src1);\n"
"\n"
"float __ovld amd_max3(float src0, float src1, float src2);\n"
"float2 __ovld amd_max3(float2 src0, float2 src1, float2 src2);\n"
"float3 __ovld amd_max3(float3 src0, float3 src1, float3 src2);\n"
"float4 __ovld amd_max3(float4 src0, float4 src1, float4 src2);\n"
"float8 __ovld amd_max3(float8 src0, float8 src1, float8 src2);\n"
"float16 __ovld amd_max3(float16 src0, float16 src1, float16 src2);\n"
"\n"
"int __ovld amd_max3(int src0, int src1, int src2);\n"
"int2 __ovld amd_max3(int2 src0, int2 src1, int2 src2);\n"
"int3 __ovld amd_max3(int3 src0, int3 src1, int3 src2);\n"
"int4 __ovld amd_max3(int4 src0, int4 src1, int4 src2);\n"
"int8 __ovld amd_max3(int8 src0, int8 src1, int8 src2);\n"
"int16 __ovld amd_max3(int16 src0, int16 src1, int16 src2);\n"
"\n"
"uint __ovld amd_max3(uint src0, uint src1, uint src2);\n"
"uint2 __ovld amd_max3(uint2 src0, uint2 src1, uint2 src2);\n"
"uint3 __ovld amd_max3(uint3 src0, uint3 src1, uint3 src2);\n"
"uint4 __ovld amd_max3(uint4 src0, uint4 src1, uint4 src2);\n"
"uint8 __ovld amd_max3(uint8 src0, uint8 src1, uint8 src2);\n"
"uint16 __ovld amd_max3(uint16 src0, uint16 src1, uint16 src2);\n"
"\n"
"float __ovld amd_median3(float src0, float src1, float src2);\n"
"float2 __ovld amd_median3(float2 src0, float2 src1, float2 src2);\n"
"float3 __ovld amd_median3(float3 src0, float3 src1, float3 src2);\n"
"float4 __ovld amd_median3(float4 src0, float4 src1, float4 src2);\n"
"float8 __ovld amd_median3(float8 src0, float8 src1, float8 src2);\n"
"float16 __ovld amd_median3(float16 src0, float16 src1, float16 src2);\n"
"\n"
"int __ovld amd_median3(int src0, int src1, int src2);\n"
"int2 __ovld amd_median3(int2 src0, int2 src1, int2 src2);\n"
"int3 __ovld amd_median3(int3 src0, int3 src1, int3 src2);\n"
"int4 __ovld amd_median3(int4 src0, int4 src1, int4 src2);\n"
"int8 __ovld amd_median3(int8 src0, int8 src1, int8 src2);\n"
"int16 __ovld amd_median3(int16 src0, int16 src1, int16 src2);\n"
"\n"
"uint __ovld amd_median3(uint src0, uint src1, uint src2);\n"
"uint2 __ovld amd_median3(uint2 src0, uint2 src1, uint2 src2);\n"
"uint3 __ovld amd_median3(uint3 src0, uint3 src1, uint3 src2);\n"
"uint4 __ovld amd_median3(uint4 src0, uint4 src1, uint4 src2);\n"
"uint8 __ovld amd_median3(uint8 src0, uint8 src1, uint8 src2);\n"
"uint16 __ovld amd_median3(uint16 src0, uint16 src1, uint16 src2);\n"
"\n"
"float __ovld amd_min3(float src0, float src1, float src);\n"
"float2 __ovld amd_min3(float2 src0, float2 src1, float2 src);\n"
"float3 __ovld amd_min3(float3 src0, float3 src1, float3 src);\n"
"float4 __ovld amd_min3(float4 src0, float4 src1, float4 src);\n"
"float8 __ovld amd_min3(float8 src0, float8 src1, float8 src);\n"
"float16 __ovld amd_min3(float16 src0, float16 src1, float16 src);\n"
"\n"
"int __ovld amd_min3(int src0, int src1, int src2);\n"
"int2 __ovld amd_min3(int2 src0, int2 src1, int2 src2);\n"
"int3 __ovld amd_min3(int3 src0, int3 src1, int3 src2);\n"
"int4 __ovld amd_min3(int4 src0, int4 src1, int4 src2);\n"
"int8 __ovld amd_min3(int8 src0, int8 src1, int8 src2);\n"
"int16 __ovld amd_min3(int16 src0, int16 src1, int16 src2);\n"
"\n"
"uint __ovld amd_min3(uint src0, uint src1, uint src2);\n"
"uint2 __ovld amd_min3(uint2 src0, uint2 src1, uint2 src2);\n"
"uint3 __ovld amd_min3(uint3 src0, uint3 src1, uint3 src2);\n"
"uint4 __ovld amd_min3(uint4 src0, uint4 src1, uint4 src2);\n"
"uint8 __ovld amd_min3(uint8 src0, uint8 src1, uint8 src2);\n"
"uint16 __ovld amd_min3(uint16 src0, uint16 src1, uint16 src2);\n"
"\n"
"ulong __ovld amd_mqsad(ulong src0, uint src1, ulong src2);\n"
"ulong2 __ovld amd_mqsad(ulong2 src0, uint2 src1, ulong2 src2);\n"
"ulong3 __ovld amd_mqsad(ulong3 src0, uint3 src1, ulong3 src2);\n"
"ulong4 __ovld amd_mqsad(ulong4 src0, uint4 src1, ulong4 src2);\n"
"ulong8 __ovld amd_mqsad(ulong8 src0, uint8 src1, ulong8 src2);\n"
"ulong16 __ovld amd_mqsad(ulong16 src0, uint16 src1, ulong16 src2);\n"
"\n"
"ulong __ovld amd_qsad(ulong src0, uint src1, ulong src2);\n"
"ulong2 __ovld amd_qsad(ulong2 src0, uint2 src1, ulong2 src2);\n"
"ulong3 __ovld amd_qsad(ulong3 src0, uint3 src1, ulong3 src2);\n"
"ulong4 __ovld amd_qsad(ulong4 src0, uint4 src1, ulong4 src2);\n"
"ulong8 __ovld amd_qsad(ulong8 src0, uint8 src1, ulong8 src2);\n"
"ulong16 __ovld amd_qsad(ulong16 src0, uint16 src1, ulong16 src2);\n"
"\n"
"uint __ovld amd_msad(uint src0, uint src1, uint src2);\n"
"uint2 __ovld amd_msad(uint2 src0, uint2 src1, uint2 src2);\n"
"uint3 __ovld amd_msad(uint3 src0, uint3 src1, uint3 src2);\n"
"uint4 __ovld amd_msad(uint4 src0, uint4 src1, uint4 src2);\n"
"uint8 __ovld amd_msad(uint8 src0, uint8 src1, uint8 src2);\n"
"uint16 __ovld amd_msad(uint16 src0, uint16 src1, uint16 src2);\n"
"\n"
"uint __ovld amd_sadd(uint src0, uint src1, uint src2);\n"
"uint2 __ovld amd_sadd(uint2 src0, uint2 src1, uint2 src2);\n"
"uint3 __ovld amd_sadd(uint3 src0, uint3 src1, uint3 src2);\n"
"uint4 __ovld amd_sadd(uint4 src0, uint4 src1, uint4 src2);\n"
"uint8 __ovld amd_sadd(uint8 src0, uint8 src1, uint8 src2);\n"
"uint16 __ovld amd_sadd(uint16 src0, uint16 src1, uint16 src2);\n"
"\n"
"uint __ovld amd_sadw(uint src0, uint src1, uint src2);\n"
"uint2 __ovld amd_sadw(uint2 src0, uint2 src1, uint2 src2);\n"
"uint3 __ovld amd_sadw(uint3 src0, uint3 src1, uint3 src2);\n"
"uint4 __ovld amd_sadw(uint4 src0, uint4 src1, uint4 src2);\n"
"uint8 __ovld amd_sadw(uint8 src0, uint8 src1, uint8 src2);\n"
"uint16 __ovld amd_sadw(uint16 src0, uint16 src1, uint16 src2);\n"
"#endif // cl_amd_media_ops2\n"
"\n"
"// Disable any extensions we may have enabled previously.\n"
"#pragma OPENCL EXTENSION all : disable\n"
"\n"
"#undef __cnfn\n"
"#undef __ovld\n"
"#endif //_OPENCL_H_\n"
"" } , 
 { "/builtins/pconfigintrin.h" , "/*===---- pconfigintrin.h - X86 platform configuration ---------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <pconfigintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __PCONFIGINTRIN_H\n"
"#define __PCONFIGINTRIN_H\n"
"\n"
"#define __PCONFIG_KEY_PROGRAM 0x00000001\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS \\\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"pconfig\")))\n"
"\n"
"static __inline unsigned int __DEFAULT_FN_ATTRS\n"
"_pconfig_u32(unsigned int __leaf, __SIZE_TYPE__ __d[])\n"
"{\n"
"  unsigned int __result;\n"
"  __asm__ (\"pconfig\"\n"
"           : \"=a\" (__result), \"=b\" (__d[0]), \"=c\" (__d[1]), \"=d\" (__d[2])\n"
"           : \"a\" (__leaf), \"b\" (__d[0]), \"c\" (__d[1]), \"d\" (__d[2])\n"
"           : \"cc\");\n"
"  return __result;\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/pkuintrin.h" , "/*===---- pkuintrin.h - PKU intrinsics -------------------------------------===\n"
" *\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <pkuintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __PKUINTRIN_H\n"
"#define __PKUINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"pku\")))\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_rdpkru_u32(void)\n"
"{\n"
"  return __builtin_ia32_rdpkru();\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_wrpkru(unsigned int __val)\n"
"{\n"
"  __builtin_ia32_wrpkru(__val);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/pmmintrin.h" , "/*===---- pmmintrin.h - SSE3 intrinsics ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __PMMINTRIN_H\n"
"#define __PMMINTRIN_H\n"
"\n"
"#include <emmintrin.h>\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS \\\n"
"  __attribute__((__always_inline__, __nodebug__, __target__(\"sse3\"), __min_vector_width__(128)))\n"
"\n"
"/// Loads data from an unaligned memory location to elements in a 128-bit\n"
"///    vector.\n"
"///\n"
"///    If the address of the data is not 16-byte aligned, the instruction may\n"
"///    read two adjacent aligned blocks of memory to retrieve the requested\n"
"///    data.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VLDDQU </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit integer vector containing integer values.\n"
"/// \\returns A 128-bit vector containing the moved values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_lddqu_si128(__m128i const *__p)\n"
"{\n"
"  return (__m128i)__builtin_ia32_lddqu((char const *)__p);\n"
"}\n"
"\n"
"/// Adds the even-indexed values and subtracts the odd-indexed values of\n"
"///    two 128-bit vectors of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDSUBPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the left source operand.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing the right source operand.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the alternating sums and\n"
"///    differences of both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_addsub_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_addsubps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in two\n"
"///    128-bit vectors of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHADDPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The horizontal sums of the values are stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The horizontal sums of the values are stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the horizontal sums of\n"
"///    both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_hadd_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_haddps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in two\n"
"///    128-bit vectors of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHSUBPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The horizontal differences between the values are stored in the lower\n"
"///    bits of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The horizontal differences between the values are stored in the upper\n"
"///    bits of the destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the horizontal\n"
"///    differences of both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_hsub_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_hsubps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Moves and duplicates odd-indexed values from a 128-bit vector\n"
"///    of [4 x float] to float values stored in a 128-bit vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSHDUP </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. \\n\n"
"///    Bits [127:96] of the source are written to bits [127:96] and [95:64] of\n"
"///    the destination. \\n\n"
"///    Bits [63:32] of the source are written to bits [63:32] and [31:0] of the\n"
"///    destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the moved and duplicated\n"
"///    values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_movehdup_ps(__m128 __a)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 1, 1, 3, 3);\n"
"}\n"
"\n"
"/// Duplicates even-indexed values from a 128-bit vector of\n"
"///    [4 x float] to float values stored in a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSLDUP </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] \\n\n"
"///    Bits [95:64] of the source are written to bits [127:96] and [95:64] of\n"
"///    the destination. \\n\n"
"///    Bits [31:0] of the source are written to bits [63:32] and [31:0] of the\n"
"///    destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the moved and duplicated\n"
"///    values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_moveldup_ps(__m128 __a)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 2, 2);\n"
"}\n"
"\n"
"/// Adds the even-indexed values and subtracts the odd-indexed values of\n"
"///    two 128-bit vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDSUBPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing the left source operand.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing the right source operand.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the alternating sums\n"
"///    and differences of both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_addsub_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_addsubpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the pairs of values contained in two 128-bit\n"
"///    vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHADDPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"///    The horizontal sum of the values is stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"///    The horizontal sum of the values is stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the horizontal sums of\n"
"///    both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_hadd_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_haddpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the pairs of values contained in two 128-bit\n"
"///    vectors of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VHSUBPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"///    The horizontal difference of the values is stored in the lower bits of\n"
"///    the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [2 x double] containing one of the source operands.\n"
"///    The horizontal difference of the values is stored in the upper bits of\n"
"///    the destination.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the horizontal\n"
"///    differences of both operands.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_hsub_pd(__m128d __a, __m128d __b)\n"
"{\n"
"  return __builtin_ia32_hsubpd((__v2df)__a, (__v2df)__b);\n"
"}\n"
"\n"
"/// Moves and duplicates one double-precision value to double-precision\n"
"///    values stored in a 128-bit vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_loaddup_pd(double const *dp);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP </c> instruction.\n"
"///\n"
"/// \\param dp\n"
"///    A pointer to a double-precision value to be moved and duplicated.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the moved and\n"
"///    duplicated values.\n"
"#define        _mm_loaddup_pd(dp)        _mm_load1_pd(dp)\n"
"\n"
"/// Moves and duplicates the double-precision value in the lower bits of\n"
"///    a 128-bit vector of [2 x double] to double-precision values stored in a\n"
"///    128-bit vector of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVDDUP </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [2 x double]. Bits [63:0] are written to bits\n"
"///    [127:64] and [63:0] of the destination.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the moved and\n"
"///    duplicated values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_movedup_pd(__m128d __a)\n"
"{\n"
"  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);\n"
"}\n"
"\n"
"/// Establishes a linear address memory range to be monitored and puts\n"
"///    the processor in the monitor event pending state. Data stored in the\n"
"///    monitored address range causes the processor to exit the pending state.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MONITOR </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    The memory range to be monitored. The size of the range is determined by\n"
"///    CPUID function 0000_0005h.\n"
"/// \\param __extensions\n"
"///    Optional extensions for the monitoring state.\n"
"/// \\param __hints\n"
"///    Optional hints for the monitoring state.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_monitor(void const *__p, unsigned __extensions, unsigned __hints)\n"
"{\n"
"  __builtin_ia32_monitor((void *)__p, __extensions, __hints);\n"
"}\n"
"\n"
"/// Used with the MONITOR instruction to wait while the processor is in\n"
"///    the monitor event pending state. Data stored in the monitored address\n"
"///    range causes the processor to exit the pending state.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MWAIT </c> instruction.\n"
"///\n"
"/// \\param __extensions\n"
"///    Optional extensions for the monitoring state, which may vary by\n"
"///    processor.\n"
"/// \\param __hints\n"
"///    Optional hints for the monitoring state, which may vary by processor.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_mwait(unsigned __extensions, unsigned __hints)\n"
"{\n"
"  __builtin_ia32_mwait(__extensions, __hints);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __PMMINTRIN_H */\n"
"" } , 
 { "/builtins/popcntintrin.h" , "/*===---- popcntintrin.h - POPCNT intrinsics -------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __POPCNTINTRIN_H\n"
"#define __POPCNTINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"popcnt\")))\n"
"\n"
"/// Counts the number of bits in the source operand having a value of 1.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> POPCNT </c> instruction.\n"
"///\n"
"/// \\param __A\n"
"///    An unsigned 32-bit integer operand.\n"
"/// \\returns A 32-bit integer containing the number of bits with value 1 in the\n"
"///    source operand.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_popcnt_u32(unsigned int __A)\n"
"{\n"
"  return __builtin_popcount(__A);\n"
"}\n"
"\n"
"/// Counts the number of bits in the source operand having a value of 1.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> POPCNT </c> instruction.\n"
"///\n"
"/// \\param __A\n"
"///    A signed 32-bit integer operand.\n"
"/// \\returns A 32-bit integer containing the number of bits with value 1 in the\n"
"///    source operand.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_popcnt32(int __A)\n"
"{\n"
"  return __builtin_popcount(__A);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Counts the number of bits in the source operand having a value of 1.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> POPCNT </c> instruction.\n"
"///\n"
"/// \\param __A\n"
"///    An unsigned 64-bit integer operand.\n"
"/// \\returns A 64-bit integer containing the number of bits with value 1 in the\n"
"///    source operand.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_mm_popcnt_u64(unsigned long long __A)\n"
"{\n"
"  return __builtin_popcountll(__A);\n"
"}\n"
"\n"
"/// Counts the number of bits in the source operand having a value of 1.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> POPCNT </c> instruction.\n"
"///\n"
"/// \\param __A\n"
"///    A signed 64-bit integer operand.\n"
"/// \\returns A 64-bit integer containing the number of bits with value 1 in the\n"
"///    source operand.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_popcnt64(long long __A)\n"
"{\n"
"  return __builtin_popcountll(__A);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __POPCNTINTRIN_H */\n"
"" } , 
 { "/builtins/prfchwintrin.h" , "/*===---- prfchwintrin.h - PREFETCHW intrinsic -----------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined(__X86INTRIN_H) && !defined(_MM3DNOW_H_INCLUDED)\n"
"#error \"Never use <prfchwintrin.h> directly; include <x86intrin.h> or <mm3dnow.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __PRFCHWINTRIN_H\n"
"#define __PRFCHWINTRIN_H\n"
"\n"
"/// Loads a memory sequence containing the specified memory address into\n"
"///    all data cache levels. The cache-coherency state is set to exclusive.\n"
"///    Data can be read from and written to the cache line without additional\n"
"///    delay.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PREFETCHT0 instruction.\n"
"///\n"
"/// \\param __P\n"
"///    A pointer specifying the memory address to be prefetched.\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__))\n"
"_m_prefetch(void *__P)\n"
"{\n"
"  __builtin_prefetch (__P, 0, 3 /* _MM_HINT_T0 */);\n"
"}\n"
"\n"
"/// Loads a memory sequence containing the specified memory address into\n"
"///    the L1 data cache and sets the cache-coherency to modified. This\n"
"///    provides a hint to the processor that the cache line will be modified.\n"
"///    It is intended for use when the cache line will be written to shortly\n"
"///    after the prefetch is performed.\n"
"///\n"
"///    Note that the effect of this intrinsic is dependent on the processor\n"
"///    implementation.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PREFETCHW instruction.\n"
"///\n"
"/// \\param __P\n"
"///    A pointer specifying the memory address to be prefetched.\n"
"static __inline__ void __attribute__((__always_inline__, __nodebug__))\n"
"_m_prefetchw(void *__P)\n"
"{\n"
"  __builtin_prefetch (__P, 1, 3 /* _MM_HINT_T0 */);\n"
"}\n"
"\n"
"#endif /* __PRFCHWINTRIN_H */\n"
"" } , 
 { "/builtins/ptwriteintrin.h" , "/*===------------ ptwriteintrin.h - PTWRITE intrinsic --------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <ptwriteintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __PTWRITEINTRIN_H\n"
"#define __PTWRITEINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS \\\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"ptwrite\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_ptwrite32(unsigned int __value) {\n"
"  __builtin_ia32_ptwrite32(__value);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_ptwrite64(unsigned long long __value) {\n"
"  __builtin_ia32_ptwrite64(__value);\n"
"}\n"
"\n"
"#endif /* __x86_64__ */\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __PTWRITEINTRIN_H */\n"
"" } , 
 { "/builtins/rdseedintrin.h" , "/*===---- rdseedintrin.h - RDSEED intrinsics -------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <rdseedintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __RDSEEDINTRIN_H\n"
"#define __RDSEEDINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"rdseed\")))\n"
"\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_rdseed16_step(unsigned short *__p)\n"
"{\n"
"  return __builtin_ia32_rdseed16_step(__p);\n"
"}\n"
"\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_rdseed32_step(unsigned int *__p)\n"
"{\n"
"  return __builtin_ia32_rdseed32_step(__p);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_rdseed64_step(unsigned long long *__p)\n"
"{\n"
"  return __builtin_ia32_rdseed64_step(__p);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __RDSEEDINTRIN_H */\n"
"" } , 
 { "/builtins/rtmintrin.h" , "/*===---- rtmintrin.h - RTM intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <rtmintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __RTMINTRIN_H\n"
"#define __RTMINTRIN_H\n"
"\n"
"#define _XBEGIN_STARTED   (~0u)\n"
"#define _XABORT_EXPLICIT  (1 << 0)\n"
"#define _XABORT_RETRY     (1 << 1)\n"
"#define _XABORT_CONFLICT  (1 << 2)\n"
"#define _XABORT_CAPACITY  (1 << 3)\n"
"#define _XABORT_DEBUG     (1 << 4)\n"
"#define _XABORT_NESTED    (1 << 5)\n"
"#define _XABORT_CODE(x)   (((x) >> 24) & 0xFF)\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"rtm\")))\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_xbegin(void)\n"
"{\n"
"  return __builtin_ia32_xbegin();\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xend(void)\n"
"{\n"
"  __builtin_ia32_xend();\n"
"}\n"
"\n"
"#define _xabort(imm) __builtin_ia32_xabort((imm))\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __RTMINTRIN_H */\n"
"" } , 
 { "/builtins/s390intrin.h" , "/*===---- s390intrin.h - SystemZ intrinsics --------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __S390INTRIN_H\n"
"#define __S390INTRIN_H\n"
"\n"
"#ifndef __s390__\n"
"#error \"<s390intrin.h> is for s390 only\"\n"
"#endif\n"
"\n"
"#ifdef __HTM__\n"
"#include <htmintrin.h>\n"
"#endif\n"
"\n"
"#ifdef __VEC__\n"
"#include <vecintrin.h>\n"
"#endif\n"
"\n"
"#endif /* __S390INTRIN_H*/\n"
"" } , 
 { "/builtins/sgxintrin.h" , "/*===---- sgxintrin.h - X86 SGX intrinsics configuration -------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <sgxintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __SGXINTRIN_H\n"
"#define __SGXINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS \\\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"sgx\")))\n"
"\n"
"static __inline unsigned int __DEFAULT_FN_ATTRS\n"
"_enclu_u32(unsigned int __leaf, __SIZE_TYPE__ __d[])\n"
"{\n"
"  unsigned int __result;\n"
"  __asm__ (\"enclu\"\n"
"           : \"=a\" (__result), \"=b\" (__d[0]), \"=c\" (__d[1]), \"=d\" (__d[2])\n"
"           : \"a\" (__leaf), \"b\" (__d[0]), \"c\" (__d[1]), \"d\" (__d[2])\n"
"           : \"cc\");\n"
"  return __result;\n"
"}\n"
"\n"
"static __inline unsigned int __DEFAULT_FN_ATTRS\n"
"_encls_u32(unsigned int __leaf, __SIZE_TYPE__ __d[])\n"
"{\n"
"  unsigned int __result;\n"
"  __asm__ (\"encls\"\n"
"           : \"=a\" (__result), \"=b\" (__d[0]), \"=c\" (__d[1]), \"=d\" (__d[2])\n"
"           : \"a\" (__leaf), \"b\" (__d[0]), \"c\" (__d[1]), \"d\" (__d[2])\n"
"           : \"cc\");\n"
"  return __result;\n"
"}\n"
"\n"
"static __inline unsigned int __DEFAULT_FN_ATTRS\n"
"_enclv_u32(unsigned int __leaf, __SIZE_TYPE__ __d[])\n"
"{\n"
"  unsigned int __result;\n"
"  __asm__ (\"enclv\"\n"
"           : \"=a\" (__result), \"=b\" (__d[0]), \"=c\" (__d[1]), \"=d\" (__d[2])\n"
"           : \"a\" (__leaf), \"b\" (__d[0]), \"c\" (__d[1]), \"d\" (__d[2])\n"
"           : \"cc\");\n"
"  return __result;\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/shaintrin.h" , "/*===---- shaintrin.h - SHA intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <shaintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __SHAINTRIN_H\n"
"#define __SHAINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"sha\"), __min_vector_width__(128)))\n"
"\n"
"#define _mm_sha1rnds4_epu32(V1, V2, M) \\\n"
"  __builtin_ia32_sha1rnds4((__v4si)(__m128i)(V1), (__v4si)(__m128i)(V2), (M))\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha1nexte_epu32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_sha1nexte((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha1msg1_epu32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_sha1msg1((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha1msg2_epu32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_sha1msg2((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha256rnds2_epu32(__m128i __X, __m128i __Y, __m128i __Z)\n"
"{\n"
"  return (__m128i)__builtin_ia32_sha256rnds2((__v4si)__X, (__v4si)__Y, (__v4si)__Z);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha256msg1_epu32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_sha256msg1((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha256msg2_epu32(__m128i __X, __m128i __Y)\n"
"{\n"
"  return (__m128i)__builtin_ia32_sha256msg2((__v4si)__X, (__v4si)__Y);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __SHAINTRIN_H */\n"
"" } , 
 { "/builtins/smmintrin.h" , "/*===---- smmintrin.h - SSE4 intrinsics ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __SMMINTRIN_H\n"
"#define __SMMINTRIN_H\n"
"\n"
"#include <tmmintrin.h>\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"sse4.1\"), __min_vector_width__(128)))\n"
"\n"
"/* SSE4 Rounding macros. */\n"
"#define _MM_FROUND_TO_NEAREST_INT    0x00\n"
"#define _MM_FROUND_TO_NEG_INF        0x01\n"
"#define _MM_FROUND_TO_POS_INF        0x02\n"
"#define _MM_FROUND_TO_ZERO           0x03\n"
"#define _MM_FROUND_CUR_DIRECTION     0x04\n"
"\n"
"#define _MM_FROUND_RAISE_EXC         0x00\n"
"#define _MM_FROUND_NO_EXC            0x08\n"
"\n"
"#define _MM_FROUND_NINT      (_MM_FROUND_RAISE_EXC | _MM_FROUND_TO_NEAREST_INT)\n"
"#define _MM_FROUND_FLOOR     (_MM_FROUND_RAISE_EXC | _MM_FROUND_TO_NEG_INF)\n"
"#define _MM_FROUND_CEIL      (_MM_FROUND_RAISE_EXC | _MM_FROUND_TO_POS_INF)\n"
"#define _MM_FROUND_TRUNC     (_MM_FROUND_RAISE_EXC | _MM_FROUND_TO_ZERO)\n"
"#define _MM_FROUND_RINT      (_MM_FROUND_RAISE_EXC | _MM_FROUND_CUR_DIRECTION)\n"
"#define _MM_FROUND_NEARBYINT (_MM_FROUND_NO_EXC | _MM_FROUND_CUR_DIRECTION)\n"
"\n"
"/// Rounds up each element of the 128-bit vector of [4 x float] to an\n"
"///    integer and returns the rounded values in a 128-bit vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_ceil_ps(__m128 X);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPS / ROUNDPS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float] values to be rounded up.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the rounded values.\n"
"#define _mm_ceil_ps(X)       _mm_round_ps((X), _MM_FROUND_CEIL)\n"
"\n"
"/// Rounds up each element of the 128-bit vector of [2 x double] to an\n"
"///    integer and returns the rounded values in a 128-bit vector of\n"
"///    [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_ceil_pd(__m128d X);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPD / ROUNDPD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [2 x double] values to be rounded up.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the rounded values.\n"
"#define _mm_ceil_pd(X)       _mm_round_pd((X), _MM_FROUND_CEIL)\n"
"\n"
"/// Copies three upper elements of the first 128-bit vector operand to\n"
"///    the corresponding three upper elements of the 128-bit result vector of\n"
"///    [4 x float]. Rounds up the lowest element of the second 128-bit vector\n"
"///    operand to an integer and copies it to the lowest element of the 128-bit\n"
"///    result vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_ceil_ss(__m128 X, __m128 Y);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDSS / ROUNDSS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float]. The values stored in bits [127:32] are\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param Y\n"
"///    A 128-bit vector of [4 x float]. The value stored in bits [31:0] is\n"
"///    rounded up to the nearest integer and copied to the corresponding bits\n"
"///    of the result.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied and rounded\n"
"///    values.\n"
"#define _mm_ceil_ss(X, Y)    _mm_round_ss((X), (Y), _MM_FROUND_CEIL)\n"
"\n"
"/// Copies the upper element of the first 128-bit vector operand to the\n"
"///    corresponding upper element of the 128-bit result vector of [2 x double].\n"
"///    Rounds up the lower element of the second 128-bit vector operand to an\n"
"///    integer and copies it to the lower element of the 128-bit result vector\n"
"///    of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_ceil_sd(__m128d X, __m128d Y);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDSD / ROUNDSD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [2 x double]. The value stored in bits [127:64] is\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param Y\n"
"///    A 128-bit vector of [2 x double]. The value stored in bits [63:0] is\n"
"///    rounded up to the nearest integer and copied to the corresponding bits\n"
"///    of the result.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the copied and rounded\n"
"///    values.\n"
"#define _mm_ceil_sd(X, Y)    _mm_round_sd((X), (Y), _MM_FROUND_CEIL)\n"
"\n"
"/// Rounds down each element of the 128-bit vector of [4 x float] to an\n"
"///    an integer and returns the rounded values in a 128-bit vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_floor_ps(__m128 X);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPS / ROUNDPS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float] values to be rounded down.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the rounded values.\n"
"#define _mm_floor_ps(X)      _mm_round_ps((X), _MM_FROUND_FLOOR)\n"
"\n"
"/// Rounds down each element of the 128-bit vector of [2 x double] to an\n"
"///    integer and returns the rounded values in a 128-bit vector of\n"
"///    [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_floor_pd(__m128d X);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPD / ROUNDPD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\returns A 128-bit vector of [2 x double] containing the rounded values.\n"
"#define _mm_floor_pd(X)      _mm_round_pd((X), _MM_FROUND_FLOOR)\n"
"\n"
"/// Copies three upper elements of the first 128-bit vector operand to\n"
"///    the corresponding three upper elements of the 128-bit result vector of\n"
"///    [4 x float]. Rounds down the lowest element of the second 128-bit vector\n"
"///    operand to an integer and copies it to the lowest element of the 128-bit\n"
"///    result vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_floor_ss(__m128 X, __m128 Y);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDSS / ROUNDSS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float]. The values stored in bits [127:32] are\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param Y\n"
"///    A 128-bit vector of [4 x float]. The value stored in bits [31:0] is\n"
"///    rounded down to the nearest integer and copied to the corresponding bits\n"
"///    of the result.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied and rounded\n"
"///    values.\n"
"#define _mm_floor_ss(X, Y)   _mm_round_ss((X), (Y), _MM_FROUND_FLOOR)\n"
"\n"
"/// Copies the upper element of the first 128-bit vector operand to the\n"
"///    corresponding upper element of the 128-bit result vector of [2 x double].\n"
"///    Rounds down the lower element of the second 128-bit vector operand to an\n"
"///    integer and copies it to the lower element of the 128-bit result vector\n"
"///    of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_floor_sd(__m128d X, __m128d Y);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDSD / ROUNDSD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [2 x double]. The value stored in bits [127:64] is\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param Y\n"
"///    A 128-bit vector of [2 x double]. The value stored in bits [63:0] is\n"
"///    rounded down to the nearest integer and copied to the corresponding bits\n"
"///    of the result.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the copied and rounded\n"
"///    values.\n"
"#define _mm_floor_sd(X, Y)   _mm_round_sd((X), (Y), _MM_FROUND_FLOOR)\n"
"\n"
"/// Rounds each element of the 128-bit vector of [4 x float] to an\n"
"///    integer value according to the rounding control specified by the second\n"
"///    argument and returns the rounded values in a 128-bit vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_round_ps(__m128 X, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPS / ROUNDPS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param M\n"
"///    An integer value that specifies the rounding operation. \\n\n"
"///    Bits [7:4] are reserved. \\n\n"
"///    Bit [3] is a precision exception value: \\n\n"
"///      0: A normal PE exception is used \\n\n"
"///      1: The PE field is not updated \\n\n"
"///    Bit [2] is the rounding control source: \\n\n"
"///      0: Use bits [1:0] of \\a M \\n\n"
"///      1: Use the current MXCSR setting \\n\n"
"///    Bits [1:0] contain the rounding control definition: \\n\n"
"///      00: Nearest \\n\n"
"///      01: Downward (toward negative infinity) \\n\n"
"///      10: Upward (toward positive infinity) \\n\n"
"///      11: Truncated\n"
"/// \\returns A 128-bit vector of [4 x float] containing the rounded values.\n"
"#define _mm_round_ps(X, M) \\\n"
"  (__m128)__builtin_ia32_roundps((__v4sf)(__m128)(X), (M))\n"
"\n"
"/// Copies three upper elements of the first 128-bit vector operand to\n"
"///    the corresponding three upper elements of the 128-bit result vector of\n"
"///    [4 x float]. Rounds the lowest element of the second 128-bit vector\n"
"///    operand to an integer value according to the rounding control specified\n"
"///    by the third argument and copies it to the lowest element of the 128-bit\n"
"///    result vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_round_ss(__m128 X, __m128 Y, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDSS / ROUNDSS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float]. The values stored in bits [127:32] are\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param Y\n"
"///    A 128-bit vector of [4 x float]. The value stored in bits [31:0] is\n"
"///    rounded to the nearest integer using the specified rounding control and\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param M\n"
"///    An integer value that specifies the rounding operation. \\n\n"
"///    Bits [7:4] are reserved. \\n\n"
"///    Bit [3] is a precision exception value: \\n\n"
"///      0: A normal PE exception is used \\n\n"
"///      1: The PE field is not updated \\n\n"
"///    Bit [2] is the rounding control source: \\n\n"
"///      0: Use bits [1:0] of \\a M \\n\n"
"///      1: Use the current MXCSR setting \\n\n"
"///    Bits [1:0] contain the rounding control definition: \\n\n"
"///      00: Nearest \\n\n"
"///      01: Downward (toward negative infinity) \\n\n"
"///      10: Upward (toward positive infinity) \\n\n"
"///      11: Truncated\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied and rounded\n"
"///    values.\n"
"#define _mm_round_ss(X, Y, M) \\\n"
"  (__m128)__builtin_ia32_roundss((__v4sf)(__m128)(X), \\\n"
"                                 (__v4sf)(__m128)(Y), (M))\n"
"\n"
"/// Rounds each element of the 128-bit vector of [2 x double] to an\n"
"///    integer value according to the rounding control specified by the second\n"
"///    argument and returns the rounded values in a 128-bit vector of\n"
"///    [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_round_pd(__m128d X, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDPD / ROUNDPD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param M\n"
"///    An integer value that specifies the rounding operation. \\n\n"
"///    Bits [7:4] are reserved. \\n\n"
"///    Bit [3] is a precision exception value: \\n\n"
"///      0: A normal PE exception is used \\n\n"
"///      1: The PE field is not updated \\n\n"
"///    Bit [2] is the rounding control source: \\n\n"
"///      0: Use bits [1:0] of \\a M \\n\n"
"///      1: Use the current MXCSR setting \\n\n"
"///    Bits [1:0] contain the rounding control definition: \\n\n"
"///      00: Nearest \\n\n"
"///      01: Downward (toward negative infinity) \\n\n"
"///      10: Upward (toward positive infinity) \\n\n"
"///      11: Truncated\n"
"/// \\returns A 128-bit vector of [2 x double] containing the rounded values.\n"
"#define _mm_round_pd(X, M) \\\n"
"  (__m128d)__builtin_ia32_roundpd((__v2df)(__m128d)(X), (M))\n"
"\n"
"/// Copies the upper element of the first 128-bit vector operand to the\n"
"///    corresponding upper element of the 128-bit result vector of [2 x double].\n"
"///    Rounds the lower element of the second 128-bit vector operand to an\n"
"///    integer value according to the rounding control specified by the third\n"
"///    argument and copies it to the lower element of the 128-bit result vector\n"
"///    of [2 x double].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_round_sd(__m128d X, __m128d Y, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VROUNDSD / ROUNDSD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [2 x double]. The value stored in bits [127:64] is\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param Y\n"
"///    A 128-bit vector of [2 x double]. The value stored in bits [63:0] is\n"
"///    rounded to the nearest integer using the specified rounding control and\n"
"///    copied to the corresponding bits of the result.\n"
"/// \\param M\n"
"///    An integer value that specifies the rounding operation. \\n\n"
"///    Bits [7:4] are reserved. \\n\n"
"///    Bit [3] is a precision exception value: \\n\n"
"///      0: A normal PE exception is used \\n\n"
"///      1: The PE field is not updated \\n\n"
"///    Bit [2] is the rounding control source: \\n\n"
"///      0: Use bits [1:0] of \\a M \\n\n"
"///      1: Use the current MXCSR setting \\n\n"
"///    Bits [1:0] contain the rounding control definition: \\n\n"
"///      00: Nearest \\n\n"
"///      01: Downward (toward negative infinity) \\n\n"
"///      10: Upward (toward positive infinity) \\n\n"
"///      11: Truncated\n"
"/// \\returns A 128-bit vector of [2 x double] containing the copied and rounded\n"
"///    values.\n"
"#define _mm_round_sd(X, Y, M) \\\n"
"  (__m128d)__builtin_ia32_roundsd((__v2df)(__m128d)(X), \\\n"
"                                  (__v2df)(__m128d)(Y), (M))\n"
"\n"
"/* SSE4 Packed Blending Intrinsics.  */\n"
"/// Returns a 128-bit vector of [2 x double] where the values are\n"
"///    selected from either the first or second operand as specified by the\n"
"///    third operand, the control mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_blend_pd(__m128d V1, __m128d V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDPD / BLENDPD </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param V2\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param M\n"
"///    An immediate integer operand, with mask bits [1:0] specifying how the\n"
"///    values are to be copied. The position of the mask bit corresponds to the\n"
"///    index of a copied value. When a mask bit is 0, the corresponding 64-bit\n"
"///    element in operand \\a V1 is copied to the same position in the result.\n"
"///    When a mask bit is 1, the corresponding 64-bit element in operand \\a V2\n"
"///    is copied to the same position in the result.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the copied values.\n"
"#define _mm_blend_pd(V1, V2, M) \\\n"
"  (__m128d) __builtin_ia32_blendpd ((__v2df)(__m128d)(V1), \\\n"
"                                    (__v2df)(__m128d)(V2), (int)(M))\n"
"\n"
"/// Returns a 128-bit vector of [4 x float] where the values are selected\n"
"///    from either the first or second operand as specified by the third\n"
"///    operand, the control mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_blend_ps(__m128 V1, __m128 V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDPS / BLENDPS </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param V2\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param M\n"
"///    An immediate integer operand, with mask bits [3:0] specifying how the\n"
"///    values are to be copied. The position of the mask bit corresponds to the\n"
"///    index of a copied value. When a mask bit is 0, the corresponding 32-bit\n"
"///    element in operand \\a V1 is copied to the same position in the result.\n"
"///    When a mask bit is 1, the corresponding 32-bit element in operand \\a V2\n"
"///    is copied to the same position in the result.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied values.\n"
"#define _mm_blend_ps(V1, V2, M) \\\n"
"  (__m128) __builtin_ia32_blendps ((__v4sf)(__m128)(V1), \\\n"
"                                   (__v4sf)(__m128)(V2), (int)(M))\n"
"\n"
"/// Returns a 128-bit vector of [2 x double] where the values are\n"
"///    selected from either the first or second operand as specified by the\n"
"///    third operand, the control mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDVPD / BLENDVPD </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param __M\n"
"///    A 128-bit vector operand, with mask bits 127 and 63 specifying how the\n"
"///    values are to be copied. The position of the mask bit corresponds to the\n"
"///    most significant bit of a copied value. When a mask bit is 0, the\n"
"///    corresponding 64-bit element in operand \\a __V1 is copied to the same\n"
"///    position in the result. When a mask bit is 1, the corresponding 64-bit\n"
"///    element in operand \\a __V2 is copied to the same position in the result.\n"
"/// \\returns A 128-bit vector of [2 x double] containing the copied values.\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_blendv_pd (__m128d __V1, __m128d __V2, __m128d __M)\n"
"{\n"
"  return (__m128d) __builtin_ia32_blendvpd ((__v2df)__V1, (__v2df)__V2,\n"
"                                            (__v2df)__M);\n"
"}\n"
"\n"
"/// Returns a 128-bit vector of [4 x float] where the values are\n"
"///    selected from either the first or second operand as specified by the\n"
"///    third operand, the control mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDVPS / BLENDVPS </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __M\n"
"///    A 128-bit vector operand, with mask bits 127, 95, 63, and 31 specifying\n"
"///    how the values are to be copied. The position of the mask bit corresponds\n"
"///    to the most significant bit of a copied value. When a mask bit is 0, the\n"
"///    corresponding 32-bit element in operand \\a __V1 is copied to the same\n"
"///    position in the result. When a mask bit is 1, the corresponding 32-bit\n"
"///    element in operand \\a __V2 is copied to the same position in the result.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_blendv_ps (__m128 __V1, __m128 __V2, __m128 __M)\n"
"{\n"
"  return (__m128) __builtin_ia32_blendvps ((__v4sf)__V1, (__v4sf)__V2,\n"
"                                           (__v4sf)__M);\n"
"}\n"
"\n"
"/// Returns a 128-bit vector of [16 x i8] where the values are selected\n"
"///    from either of the first or second operand as specified by the third\n"
"///    operand, the control mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPBLENDVB / PBLENDVB </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\param __M\n"
"///    A 128-bit vector operand, with mask bits 127, 119, 111...7 specifying\n"
"///    how the values are to be copied. The position of the mask bit corresponds\n"
"///    to the most significant bit of a copied value. When a mask bit is 0, the\n"
"///    corresponding 8-bit element in operand \\a __V1 is copied to the same\n"
"///    position in the result. When a mask bit is 1, the corresponding 8-bit\n"
"///    element in operand \\a __V2 is copied to the same position in the result.\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the copied values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_blendv_epi8 (__m128i __V1, __m128i __V2, __m128i __M)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pblendvb128 ((__v16qi)__V1, (__v16qi)__V2,\n"
"                                               (__v16qi)__M);\n"
"}\n"
"\n"
"/// Returns a 128-bit vector of [8 x i16] where the values are selected\n"
"///    from either of the first or second operand as specified by the third\n"
"///    operand, the control mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_blend_epi16(__m128i V1, __m128i V2, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPBLENDW / PBLENDW </c> instruction.\n"
"///\n"
"/// \\param V1\n"
"///    A 128-bit vector of [8 x i16].\n"
"/// \\param V2\n"
"///    A 128-bit vector of [8 x i16].\n"
"/// \\param M\n"
"///    An immediate integer operand, with mask bits [7:0] specifying how the\n"
"///    values are to be copied. The position of the mask bit corresponds to the\n"
"///    index of a copied value. When a mask bit is 0, the corresponding 16-bit\n"
"///    element in operand \\a V1 is copied to the same position in the result.\n"
"///    When a mask bit is 1, the corresponding 16-bit element in operand \\a V2\n"
"///    is copied to the same position in the result.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the copied values.\n"
"#define _mm_blend_epi16(V1, V2, M) \\\n"
"  (__m128i) __builtin_ia32_pblendw128 ((__v8hi)(__m128i)(V1), \\\n"
"                                       (__v8hi)(__m128i)(V2), (int)(M))\n"
"\n"
"/* SSE4 Dword Multiply Instructions.  */\n"
"/// Multiples corresponding elements of two 128-bit vectors of [4 x i32]\n"
"///    and returns the lower 32 bits of the each product in a 128-bit vector of\n"
"///    [4 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMULLD / PMULLD </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit integer vector.\n"
"/// \\param __V2\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the products of both operands.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_mullo_epi32 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) ((__v4su)__V1 * (__v4su)__V2);\n"
"}\n"
"\n"
"/// Multiplies corresponding even-indexed elements of two 128-bit\n"
"///    vectors of [4 x i32] and returns a 128-bit vector of [2 x i64]\n"
"///    containing the products.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMULDQ / PMULDQ </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the products of both\n"
"///    operands.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_mul_epi32 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pmuldq128 ((__v4si)__V1, (__v4si)__V2);\n"
"}\n"
"\n"
"/* SSE4 Floating Point Dot Product Instructions.  */\n"
"/// Computes the dot product of the two 128-bit vectors of [4 x float]\n"
"///    and returns it in the elements of the 128-bit result vector of\n"
"///    [4 x float].\n"
"///\n"
"///    The immediate integer operand controls which input elements\n"
"///    will contribute to the dot product, and where the final results are\n"
"///    returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_dp_ps(__m128 X, __m128 Y, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDPPS / DPPS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param Y\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param M\n"
"///    An immediate integer operand. Mask bits [7:4] determine which elements\n"
"///    of the input vectors are used, with bit [4] corresponding to the lowest\n"
"///    element and bit [7] corresponding to the highest element of each [4 x\n"
"///    float] vector. If a bit is set, the corresponding elements from the two\n"
"///    input vectors are used as an input for dot product; otherwise that input\n"
"///    is treated as zero. Bits [3:0] determine which elements of the result\n"
"///    will receive a copy of the final dot product, with bit [0] corresponding\n"
"///    to the lowest element and bit [3] corresponding to the highest element of\n"
"///    each [4 x float] subvector. If a bit is set, the dot product is returned\n"
"///    in the corresponding element; otherwise that element is set to zero.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the dot product.\n"
"#define _mm_dp_ps(X, Y, M) \\\n"
"  (__m128) __builtin_ia32_dpps((__v4sf)(__m128)(X), \\\n"
"                               (__v4sf)(__m128)(Y), (M))\n"
"\n"
"/// Computes the dot product of the two 128-bit vectors of [2 x double]\n"
"///    and returns it in the elements of the 128-bit result vector of\n"
"///    [2 x double].\n"
"///\n"
"///    The immediate integer operand controls which input\n"
"///    elements will contribute to the dot product, and where the final results\n"
"///    are returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128d _mm_dp_pd(__m128d X, __m128d Y, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDPPD / DPPD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param Y\n"
"///    A 128-bit vector of [2 x double].\n"
"/// \\param M\n"
"///    An immediate integer operand. Mask bits [5:4] determine which elements\n"
"///    of the input vectors are used, with bit [4] corresponding to the lowest\n"
"///    element and bit [5] corresponding to the highest element of each of [2 x\n"
"///    double] vector. If a bit is set, the corresponding elements from the two\n"
"///    input vectors are used as an input for dot product; otherwise that input\n"
"///    is treated as zero. Bits [1:0] determine which elements of the result\n"
"///    will receive a copy of the final dot product, with bit [0] corresponding\n"
"///    to the lowest element and bit [1] corresponding to the highest element of\n"
"///    each [2 x double] vector. If a bit is set, the dot product is returned in\n"
"///    the corresponding element; otherwise that element is set to zero.\n"
"#define _mm_dp_pd(X, Y, M) \\\n"
"  (__m128d) __builtin_ia32_dppd((__v2df)(__m128d)(X), \\\n"
"                                (__v2df)(__m128d)(Y), (M))\n"
"\n"
"/* SSE4 Streaming Load Hint Instruction.  */\n"
"/// Loads integer values from a 128-bit aligned memory location to a\n"
"///    128-bit integer vector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVNTDQA / MOVNTDQA </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A pointer to a 128-bit aligned memory location that contains the integer\n"
"///    values.\n"
"/// \\returns A 128-bit integer vector containing the data stored at the\n"
"///    specified memory location.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_stream_load_si128 (__m128i const *__V)\n"
"{\n"
"  return (__m128i) __builtin_nontemporal_load ((const __v2di *) __V);\n"
"}\n"
"\n"
"/* SSE4 Packed Integer Min/Max Instructions.  */\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [16 x i8] and returns a 128-bit vector of [16 x i8] containing the lesser\n"
"///    of the two values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMINSB / PMINSB </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [16 x i8]\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the lesser values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_min_epi8 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pminsb128 ((__v16qi) __V1, (__v16qi) __V2);\n"
"}\n"
"\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [16 x i8] and returns a 128-bit vector of [16 x i8] containing the\n"
"///    greater value of the two.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMAXSB / PMAXSB </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\returns A 128-bit vector of [16 x i8] containing the greater values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_max_epi8 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pmaxsb128 ((__v16qi) __V1, (__v16qi) __V2);\n"
"}\n"
"\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [8 x u16] and returns a 128-bit vector of [8 x u16] containing the lesser\n"
"///    value of the two.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMINUW / PMINUW </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [8 x u16].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [8 x u16].\n"
"/// \\returns A 128-bit vector of [8 x u16] containing the lesser values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_min_epu16 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pminuw128 ((__v8hi) __V1, (__v8hi) __V2);\n"
"}\n"
"\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [8 x u16] and returns a 128-bit vector of [8 x u16] containing the\n"
"///    greater value of the two.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMAXUW / PMAXUW </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [8 x u16].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [8 x u16].\n"
"/// \\returns A 128-bit vector of [8 x u16] containing the greater values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_max_epu16 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pmaxuw128 ((__v8hi) __V1, (__v8hi) __V2);\n"
"}\n"
"\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [4 x i32] and returns a 128-bit vector of [4 x i32] containing the lesser\n"
"///    value of the two.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMINSD / PMINSD </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the lesser values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_min_epi32 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pminsd128 ((__v4si) __V1, (__v4si) __V2);\n"
"}\n"
"\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [4 x i32] and returns a 128-bit vector of [4 x i32] containing the\n"
"///    greater value of the two.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMAXSD / PMAXSD </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the greater values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_max_epi32 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pmaxsd128 ((__v4si) __V1, (__v4si) __V2);\n"
"}\n"
"\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [4 x u32] and returns a 128-bit vector of [4 x u32] containing the lesser\n"
"///    value of the two.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMINUD / PMINUD </c>  instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [4 x u32].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [4 x u32].\n"
"/// \\returns A 128-bit vector of [4 x u32] containing the lesser values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_min_epu32 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pminud128((__v4si) __V1, (__v4si) __V2);\n"
"}\n"
"\n"
"/// Compares the corresponding elements of two 128-bit vectors of\n"
"///    [4 x u32] and returns a 128-bit vector of [4 x u32] containing the\n"
"///    greater value of the two.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMAXUD / PMAXUD </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [4 x u32].\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [4 x u32].\n"
"/// \\returns A 128-bit vector of [4 x u32] containing the greater values.\n"
"static __inline__  __m128i __DEFAULT_FN_ATTRS\n"
"_mm_max_epu32 (__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_pmaxud128((__v4si) __V1, (__v4si) __V2);\n"
"}\n"
"\n"
"/* SSE4 Insertion and Extraction from XMM Register Instructions.  */\n"
"/// Takes the first argument \\a X and inserts an element from the second\n"
"///    argument \\a Y as selected by the third argument \\a N. That result then\n"
"///    has elements zeroed out also as selected by the third argument \\a N. The\n"
"///    resulting 128-bit vector of [4 x float] is then returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_insert_ps(__m128 X, __m128 Y, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VINSERTPS </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector source operand of [4 x float]. With the exception of\n"
"///    those bits in the result copied from parameter \\a Y and zeroed by bits\n"
"///    [3:0] of \\a N, all bits from this parameter are copied to the result.\n"
"/// \\param Y\n"
"///    A 128-bit vector source operand of [4 x float]. One single-precision\n"
"///    floating-point element from this source, as determined by the immediate\n"
"///    parameter, is copied to the result.\n"
"/// \\param N\n"
"///    Specifies which bits from operand \\a Y will be copied, which bits in the\n"
"///    result they will be be copied to, and which bits in the result will be\n"
"///    cleared. The following assignments are made: \\n\n"
"///    Bits [7:6] specify the bits to copy from operand \\a Y: \\n\n"
"///      00: Selects bits [31:0] from operand \\a Y. \\n\n"
"///      01: Selects bits [63:32] from operand \\a Y. \\n\n"
"///      10: Selects bits [95:64] from operand \\a Y. \\n\n"
"///      11: Selects bits [127:96] from operand \\a Y. \\n\n"
"///    Bits [5:4] specify the bits in the result to which the selected bits\n"
"///    from operand \\a Y are copied: \\n\n"
"///      00: Copies the selected bits from \\a Y to result bits [31:0]. \\n\n"
"///      01: Copies the selected bits from \\a Y to result bits [63:32]. \\n\n"
"///      10: Copies the selected bits from \\a Y to result bits [95:64]. \\n\n"
"///      11: Copies the selected bits from \\a Y to result bits [127:96]. \\n\n"
"///    Bits[3:0]: If any of these bits are set, the corresponding result\n"
"///    element is cleared.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied\n"
"///    single-precision floating point elements from the operands.\n"
"#define _mm_insert_ps(X, Y, N) __builtin_ia32_insertps128((X), (Y), (N))\n"
"\n"
"/// Extracts a 32-bit integer from a 128-bit vector of [4 x float] and\n"
"///    returns it, using the immediate value parameter \\a N as a selector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_extract_ps(__m128 X, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VEXTRACTPS / EXTRACTPS </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param N\n"
"///    An immediate value. Bits [1:0] determines which bits from the argument\n"
"///    \\a X are extracted and returned: \\n\n"
"///    00: Bits [31:0] of parameter \\a X are returned. \\n\n"
"///    01: Bits [63:32] of parameter \\a X are returned. \\n\n"
"///    10: Bits [95:64] of parameter \\a X are returned. \\n\n"
"///    11: Bits [127:96] of parameter \\a X are returned.\n"
"/// \\returns A 32-bit integer containing the extracted 32 bits of float data.\n"
"#define _mm_extract_ps(X, N) (__extension__                      \\\n"
"  ({ union { int __i; float __f; } __t;  \\\n"
"     __t.__f = __builtin_ia32_vec_ext_v4sf((__v4sf)(__m128)(X), (int)(N)); \\\n"
"     __t.__i;}))\n"
"\n"
"/* Miscellaneous insert and extract macros.  */\n"
"/* Extract a single-precision float from X at index N into D.  */\n"
"#define _MM_EXTRACT_FLOAT(D, X, N) \\\n"
"  { (D) = __builtin_ia32_vec_ext_v4sf((__v4sf)(__m128)(X), (int)(N)); }\n"
"\n"
"/* Or together 2 sets of indexes (X and Y) with the zeroing bits (Z) to create\n"
"   an index suitable for _mm_insert_ps.  */\n"
"#define _MM_MK_INSERTPS_NDX(X, Y, Z) (((X) << 6) | ((Y) << 4) | (Z))\n"
"\n"
"/* Extract a float from X at index N into the first index of the return.  */\n"
"#define _MM_PICK_OUT_PS(X, N) _mm_insert_ps (_mm_setzero_ps(), (X),   \\\n"
"                                             _MM_MK_INSERTPS_NDX((N), 0, 0x0e))\n"
"\n"
"/* Insert int into packed integer array at index.  */\n"
"/// Constructs a 128-bit vector of [16 x i8] by first making a copy of\n"
"///    the 128-bit integer vector parameter, and then inserting the lower 8 bits\n"
"///    of an integer parameter \\a I into an offset specified by the immediate\n"
"///    value parameter \\a N.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_insert_epi8(__m128i X, int I, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPINSRB / PINSRB </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit integer vector of [16 x i8]. This vector is copied to the\n"
"///    result and then one of the sixteen elements in the result vector is\n"
"///    replaced by the lower 8 bits of \\a I.\n"
"/// \\param I\n"
"///    An integer. The lower 8 bits of this operand are written to the result\n"
"///    beginning at the offset specified by \\a N.\n"
"/// \\param N\n"
"///    An immediate value. Bits [3:0] specify the bit offset in the result at\n"
"///    which the lower 8 bits of \\a I are written. \\n\n"
"///    0000: Bits [7:0] of the result are used for insertion. \\n\n"
"///    0001: Bits [15:8] of the result are used for insertion. \\n\n"
"///    0010: Bits [23:16] of the result are used for insertion. \\n\n"
"///    0011: Bits [31:24] of the result are used for insertion. \\n\n"
"///    0100: Bits [39:32] of the result are used for insertion. \\n\n"
"///    0101: Bits [47:40] of the result are used for insertion. \\n\n"
"///    0110: Bits [55:48] of the result are used for insertion. \\n\n"
"///    0111: Bits [63:56] of the result are used for insertion. \\n\n"
"///    1000: Bits [71:64] of the result are used for insertion. \\n\n"
"///    1001: Bits [79:72] of the result are used for insertion. \\n\n"
"///    1010: Bits [87:80] of the result are used for insertion. \\n\n"
"///    1011: Bits [95:88] of the result are used for insertion. \\n\n"
"///    1100: Bits [103:96] of the result are used for insertion. \\n\n"
"///    1101: Bits [111:104] of the result are used for insertion. \\n\n"
"///    1110: Bits [119:112] of the result are used for insertion. \\n\n"
"///    1111: Bits [127:120] of the result are used for insertion.\n"
"/// \\returns A 128-bit integer vector containing the constructed values.\n"
"#define _mm_insert_epi8(X, I, N) \\\n"
"  (__m128i)__builtin_ia32_vec_set_v16qi((__v16qi)(__m128i)(X), \\\n"
"                                        (int)(I), (int)(N))\n"
"\n"
"/// Constructs a 128-bit vector of [4 x i32] by first making a copy of\n"
"///    the 128-bit integer vector parameter, and then inserting the 32-bit\n"
"///    integer parameter \\a I at the offset specified by the immediate value\n"
"///    parameter \\a N.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_insert_epi32(__m128i X, int I, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPINSRD / PINSRD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit integer vector of [4 x i32]. This vector is copied to the\n"
"///    result and then one of the four elements in the result vector is\n"
"///    replaced by \\a I.\n"
"/// \\param I\n"
"///    A 32-bit integer that is written to the result beginning at the offset\n"
"///    specified by \\a N.\n"
"/// \\param N\n"
"///    An immediate value. Bits [1:0] specify the bit offset in the result at\n"
"///    which the integer \\a I is written. \\n\n"
"///    00: Bits [31:0] of the result are used for insertion. \\n\n"
"///    01: Bits [63:32] of the result are used for insertion. \\n\n"
"///    10: Bits [95:64] of the result are used for insertion. \\n\n"
"///    11: Bits [127:96] of the result are used for insertion.\n"
"/// \\returns A 128-bit integer vector containing the constructed values.\n"
"#define _mm_insert_epi32(X, I, N) \\\n"
"  (__m128i)__builtin_ia32_vec_set_v4si((__v4si)(__m128i)(X), \\\n"
"                                       (int)(I), (int)(N))\n"
"\n"
"#ifdef __x86_64__\n"
"/// Constructs a 128-bit vector of [2 x i64] by first making a copy of\n"
"///    the 128-bit integer vector parameter, and then inserting the 64-bit\n"
"///    integer parameter \\a I, using the immediate value parameter \\a N as an\n"
"///    insertion location selector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_insert_epi64(__m128i X, long long I, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPINSRQ / PINSRQ </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit integer vector of [2 x i64]. This vector is copied to the\n"
"///    result and then one of the two elements in the result vector is replaced\n"
"///    by \\a I.\n"
"/// \\param I\n"
"///    A 64-bit integer that is written to the result beginning at the offset\n"
"///    specified by \\a N.\n"
"/// \\param N\n"
"///    An immediate value. Bit [0] specifies the bit offset in the result at\n"
"///    which the integer \\a I is written. \\n\n"
"///    0: Bits [63:0] of the result are used for insertion. \\n\n"
"///    1: Bits [127:64] of the result are used for insertion. \\n\n"
"/// \\returns A 128-bit integer vector containing the constructed values.\n"
"#define _mm_insert_epi64(X, I, N) \\\n"
"  (__m128i)__builtin_ia32_vec_set_v2di((__v2di)(__m128i)(X), \\\n"
"                                       (long long)(I), (int)(N))\n"
"#endif /* __x86_64__ */\n"
"\n"
"/* Extract int from packed integer array at index.  This returns the element\n"
" * as a zero extended value, so it is unsigned.\n"
" */\n"
"/// Extracts an 8-bit element from the 128-bit integer vector of\n"
"///    [16 x i8], using the immediate value parameter \\a N as a selector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_extract_epi8(__m128i X, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPEXTRB / PEXTRB </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit integer vector.\n"
"/// \\param N\n"
"///    An immediate value. Bits [3:0] specify which 8-bit vector element from\n"
"///    the argument \\a X to extract and copy to the result. \\n\n"
"///    0000: Bits [7:0] of parameter \\a X are extracted. \\n\n"
"///    0001: Bits [15:8] of the parameter \\a X are extracted. \\n\n"
"///    0010: Bits [23:16] of the parameter \\a X are extracted. \\n\n"
"///    0011: Bits [31:24] of the parameter \\a X are extracted. \\n\n"
"///    0100: Bits [39:32] of the parameter \\a X are extracted. \\n\n"
"///    0101: Bits [47:40] of the parameter \\a X are extracted. \\n\n"
"///    0110: Bits [55:48] of the parameter \\a X are extracted. \\n\n"
"///    0111: Bits [63:56] of the parameter \\a X are extracted. \\n\n"
"///    1000: Bits [71:64] of the parameter \\a X are extracted. \\n\n"
"///    1001: Bits [79:72] of the parameter \\a X are extracted. \\n\n"
"///    1010: Bits [87:80] of the parameter \\a X are extracted. \\n\n"
"///    1011: Bits [95:88] of the parameter \\a X are extracted. \\n\n"
"///    1100: Bits [103:96] of the parameter \\a X are extracted. \\n\n"
"///    1101: Bits [111:104] of the parameter \\a X are extracted. \\n\n"
"///    1110: Bits [119:112] of the parameter \\a X are extracted. \\n\n"
"///    1111: Bits [127:120] of the parameter \\a X are extracted.\n"
"/// \\returns  An unsigned integer, whose lower 8 bits are selected from the\n"
"///    128-bit integer vector parameter and the remaining bits are assigned\n"
"///    zeros.\n"
"#define _mm_extract_epi8(X, N) \\\n"
"  (int)(unsigned char)__builtin_ia32_vec_ext_v16qi((__v16qi)(__m128i)(X), \\\n"
"                                                   (int)(N))\n"
"\n"
"/// Extracts a 32-bit element from the 128-bit integer vector of\n"
"///    [4 x i32], using the immediate value parameter \\a N as a selector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_extract_epi32(__m128i X, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPEXTRD / PEXTRD </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit integer vector.\n"
"/// \\param N\n"
"///    An immediate value. Bits [1:0] specify which 32-bit vector element from\n"
"///    the argument \\a X to extract and copy to the result. \\n\n"
"///    00: Bits [31:0] of the parameter \\a X are extracted. \\n\n"
"///    01: Bits [63:32] of the parameter \\a X are extracted. \\n\n"
"///    10: Bits [95:64] of the parameter \\a X are extracted. \\n\n"
"///    11: Bits [127:96] of the parameter \\a X are exracted.\n"
"/// \\returns  An integer, whose lower 32 bits are selected from the 128-bit\n"
"///    integer vector parameter and the remaining bits are assigned zeros.\n"
"#define _mm_extract_epi32(X, N) \\\n"
"  (int)__builtin_ia32_vec_ext_v4si((__v4si)(__m128i)(X), (int)(N))\n"
"\n"
"#ifdef __x86_64__\n"
"/// Extracts a 64-bit element from the 128-bit integer vector of\n"
"///    [2 x i64], using the immediate value parameter \\a N as a selector.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// long long _mm_extract_epi64(__m128i X, const int N);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPEXTRQ / PEXTRQ </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit integer vector.\n"
"/// \\param N\n"
"///    An immediate value. Bit [0] specifies which 64-bit vector element from\n"
"///    the argument \\a X to return. \\n\n"
"///    0: Bits [63:0] are returned. \\n\n"
"///    1: Bits [127:64] are returned. \\n\n"
"/// \\returns  A 64-bit integer.\n"
"#define _mm_extract_epi64(X, N) \\\n"
"  (long long)__builtin_ia32_vec_ext_v2di((__v2di)(__m128i)(X), (int)(N))\n"
"#endif /* __x86_64 */\n"
"\n"
"/* SSE4 128-bit Packed Integer Comparisons.  */\n"
"/// Tests whether the specified bits in a 128-bit integer vector are all\n"
"///    zeros.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.\n"
"///\n"
"/// \\param __M\n"
"///    A 128-bit integer vector containing the bits to be tested.\n"
"/// \\param __V\n"
"///    A 128-bit integer vector selecting which bits to test in operand \\a __M.\n"
"/// \\returns TRUE if the specified bits are all zeros; FALSE otherwise.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_testz_si128(__m128i __M, __m128i __V)\n"
"{\n"
"  return __builtin_ia32_ptestz128((__v2di)__M, (__v2di)__V);\n"
"}\n"
"\n"
"/// Tests whether the specified bits in a 128-bit integer vector are all\n"
"///    ones.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.\n"
"///\n"
"/// \\param __M\n"
"///    A 128-bit integer vector containing the bits to be tested.\n"
"/// \\param __V\n"
"///    A 128-bit integer vector selecting which bits to test in operand \\a __M.\n"
"/// \\returns TRUE if the specified bits are all ones; FALSE otherwise.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_testc_si128(__m128i __M, __m128i __V)\n"
"{\n"
"  return __builtin_ia32_ptestc128((__v2di)__M, (__v2di)__V);\n"
"}\n"
"\n"
"/// Tests whether the specified bits in a 128-bit integer vector are\n"
"///    neither all zeros nor all ones.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.\n"
"///\n"
"/// \\param __M\n"
"///    A 128-bit integer vector containing the bits to be tested.\n"
"/// \\param __V\n"
"///    A 128-bit integer vector selecting which bits to test in operand \\a __M.\n"
"/// \\returns TRUE if the specified bits are neither all zeros nor all ones;\n"
"///    FALSE otherwise.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_testnzc_si128(__m128i __M, __m128i __V)\n"
"{\n"
"  return __builtin_ia32_ptestnzc128((__v2di)__M, (__v2di)__V);\n"
"}\n"
"\n"
"/// Tests whether the specified bits in a 128-bit integer vector are all\n"
"///    ones.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_test_all_ones(__m128i V);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.\n"
"///\n"
"/// \\param V\n"
"///    A 128-bit integer vector containing the bits to be tested.\n"
"/// \\returns TRUE if the bits specified in the operand are all set to 1; FALSE\n"
"///    otherwise.\n"
"#define _mm_test_all_ones(V) _mm_testc_si128((V), _mm_cmpeq_epi32((V), (V)))\n"
"\n"
"/// Tests whether the specified bits in a 128-bit integer vector are\n"
"///    neither all zeros nor all ones.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_test_mix_ones_zeros(__m128i M, __m128i V);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.\n"
"///\n"
"/// \\param M\n"
"///    A 128-bit integer vector containing the bits to be tested.\n"
"/// \\param V\n"
"///    A 128-bit integer vector selecting which bits to test in operand \\a M.\n"
"/// \\returns TRUE if the specified bits are neither all zeros nor all ones;\n"
"///    FALSE otherwise.\n"
"#define _mm_test_mix_ones_zeros(M, V) _mm_testnzc_si128((M), (V))\n"
"\n"
"/// Tests whether the specified bits in a 128-bit integer vector are all\n"
"///    zeros.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_test_all_zeros(__m128i M, __m128i V);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.\n"
"///\n"
"/// \\param M\n"
"///    A 128-bit integer vector containing the bits to be tested.\n"
"/// \\param V\n"
"///    A 128-bit integer vector selecting which bits to test in operand \\a M.\n"
"/// \\returns TRUE if the specified bits are all zeros; FALSE otherwise.\n"
"#define _mm_test_all_zeros(M, V) _mm_testz_si128 ((M), (V))\n"
"\n"
"/* SSE4 64-bit Packed Integer Comparisons.  */\n"
"/// Compares each of the corresponding 64-bit values of the 128-bit\n"
"///    integer vectors for equality.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPEQQ / PCMPEQQ </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit integer vector.\n"
"/// \\param __V2\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_epi64(__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i)((__v2di)__V1 == (__v2di)__V2);\n"
"}\n"
"\n"
"/* SSE4 Packed Integer Sign-Extension.  */\n"
"/// Sign-extends each of the lower eight 8-bit integer elements of a\n"
"///    128-bit vector of [16 x i8] to 16-bit values and returns them in a\n"
"///    128-bit vector of [8 x i16]. The upper eight elements of the input vector\n"
"///    are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVSXBW / PMOVSXBW </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [16 x i8]. The lower eight 8-bit elements are sign-\n"
"///    extended to 16-bit values.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the sign-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi8_epi16(__m128i __V)\n"
"{\n"
"  /* This function always performs a signed extension, but __v16qi is a char\n"
"     which may be signed or unsigned, so use __v16qs. */\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8hi);\n"
"}\n"
"\n"
"/// Sign-extends each of the lower four 8-bit integer elements of a\n"
"///    128-bit vector of [16 x i8] to 32-bit values and returns them in a\n"
"///    128-bit vector of [4 x i32]. The upper twelve elements of the input\n"
"///    vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVSXBD / PMOVSXBD </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [16 x i8]. The lower four 8-bit elements are\n"
"///    sign-extended to 32-bit values.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the sign-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi8_epi32(__m128i __V)\n"
"{\n"
"  /* This function always performs a signed extension, but __v16qi is a char\n"
"     which may be signed or unsigned, so use __v16qs. */\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4si);\n"
"}\n"
"\n"
"/// Sign-extends each of the lower two 8-bit integer elements of a\n"
"///    128-bit integer vector of [16 x i8] to 64-bit values and returns them in\n"
"///    a 128-bit vector of [2 x i64]. The upper fourteen elements of the input\n"
"///    vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVSXBQ / PMOVSXBQ </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [16 x i8]. The lower two 8-bit elements are\n"
"///    sign-extended to 64-bit values.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the sign-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi8_epi64(__m128i __V)\n"
"{\n"
"  /* This function always performs a signed extension, but __v16qi is a char\n"
"     which may be signed or unsigned, so use __v16qs. */\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1), __v2di);\n"
"}\n"
"\n"
"/// Sign-extends each of the lower four 16-bit integer elements of a\n"
"///    128-bit integer vector of [8 x i16] to 32-bit values and returns them in\n"
"///    a 128-bit vector of [4 x i32]. The upper four elements of the input\n"
"///    vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVSXWD / PMOVSXWD </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [8 x i16]. The lower four 16-bit elements are\n"
"///    sign-extended to 32-bit values.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the sign-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi16_epi32(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4si);\n"
"}\n"
"\n"
"/// Sign-extends each of the lower two 16-bit integer elements of a\n"
"///    128-bit integer vector of [8 x i16] to 64-bit values and returns them in\n"
"///    a 128-bit vector of [2 x i64]. The upper six elements of the input\n"
"///    vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVSXWQ / PMOVSXWQ </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [8 x i16]. The lower two 16-bit elements are\n"
"///     sign-extended to 64-bit values.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the sign-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi16_epi64(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1), __v2di);\n"
"}\n"
"\n"
"/// Sign-extends each of the lower two 32-bit integer elements of a\n"
"///    128-bit integer vector of [4 x i32] to 64-bit values and returns them in\n"
"///    a 128-bit vector of [2 x i64]. The upper two elements of the input vector\n"
"///    are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVSXDQ / PMOVSXDQ </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [4 x i32]. The lower two 32-bit elements are\n"
"///    sign-extended to 64-bit values.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the sign-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepi32_epi64(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v4si)__V, (__v4si)__V, 0, 1), __v2di);\n"
"}\n"
"\n"
"/* SSE4 Packed Integer Zero-Extension.  */\n"
"/// Zero-extends each of the lower eight 8-bit integer elements of a\n"
"///    128-bit vector of [16 x i8] to 16-bit values and returns them in a\n"
"///    128-bit vector of [8 x i16]. The upper eight elements of the input vector\n"
"///    are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVZXBW / PMOVZXBW </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [16 x i8]. The lower eight 8-bit elements are\n"
"///    zero-extended to 16-bit values.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the zero-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepu8_epi16(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8hi);\n"
"}\n"
"\n"
"/// Zero-extends each of the lower four 8-bit integer elements of a\n"
"///    128-bit vector of [16 x i8] to 32-bit values and returns them in a\n"
"///    128-bit vector of [4 x i32]. The upper twelve elements of the input\n"
"///    vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVZXBD / PMOVZXBD </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [16 x i8]. The lower four 8-bit elements are\n"
"///    zero-extended to 32-bit values.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the zero-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepu8_epi32(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4si);\n"
"}\n"
"\n"
"/// Zero-extends each of the lower two 8-bit integer elements of a\n"
"///    128-bit integer vector of [16 x i8] to 64-bit values and returns them in\n"
"///    a 128-bit vector of [2 x i64]. The upper fourteen elements of the input\n"
"///    vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVZXBQ / PMOVZXBQ </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [16 x i8]. The lower two 8-bit elements are\n"
"///    zero-extended to 64-bit values.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the zero-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepu8_epi64(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1), __v2di);\n"
"}\n"
"\n"
"/// Zero-extends each of the lower four 16-bit integer elements of a\n"
"///    128-bit integer vector of [8 x i16] to 32-bit values and returns them in\n"
"///    a 128-bit vector of [4 x i32]. The upper four elements of the input\n"
"///    vector are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVZXWD / PMOVZXWD </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [8 x i16]. The lower four 16-bit elements are\n"
"///    zero-extended to 32-bit values.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the zero-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepu16_epi32(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4si);\n"
"}\n"
"\n"
"/// Zero-extends each of the lower two 16-bit integer elements of a\n"
"///    128-bit integer vector of [8 x i16] to 64-bit values and returns them in\n"
"///    a 128-bit vector of [2 x i64]. The upper six elements of the input vector\n"
"///    are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVZXWQ / PMOVZXWQ </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [8 x i16]. The lower two 16-bit elements are\n"
"///    zero-extended to 64-bit values.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the zero-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepu16_epi64(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1), __v2di);\n"
"}\n"
"\n"
"/// Zero-extends each of the lower two 32-bit integer elements of a\n"
"///    128-bit integer vector of [4 x i32] to 64-bit values and returns them in\n"
"///    a 128-bit vector of [2 x i64]. The upper two elements of the input vector\n"
"///    are unused.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPMOVZXDQ / PMOVZXDQ </c> instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [4 x i32]. The lower two 32-bit elements are\n"
"///    zero-extended to 64-bit values.\n"
"/// \\returns A 128-bit vector of [2 x i64] containing the zero-extended values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cvtepu32_epi64(__m128i __V)\n"
"{\n"
"  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v4su)__V, (__v4su)__V, 0, 1), __v2di);\n"
"}\n"
"\n"
"/* SSE4 Pack with Unsigned Saturation.  */\n"
"/// Converts 32-bit signed integers from both 128-bit integer vector\n"
"///    operands into 16-bit unsigned integers, and returns the packed result.\n"
"///    Values greater than 0xFFFF are saturated to 0xFFFF. Values less than\n"
"///    0x0000 are saturated to 0x0000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPACKUSDW / PACKUSDW </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit vector of [4 x i32]. Each 32-bit element is treated as a\n"
"///    signed integer and is converted to a 16-bit unsigned integer with\n"
"///    saturation. Values greater than 0xFFFF are saturated to 0xFFFF. Values\n"
"///    less than 0x0000 are saturated to 0x0000. The converted [4 x i16] values\n"
"///    are written to the lower 64 bits of the result.\n"
"/// \\param __V2\n"
"///    A 128-bit vector of [4 x i32]. Each 32-bit element is treated as a\n"
"///    signed integer and is converted to a 16-bit unsigned integer with\n"
"///    saturation. Values greater than 0xFFFF are saturated to 0xFFFF. Values\n"
"///    less than 0x0000 are saturated to 0x0000. The converted [4 x i16] values\n"
"///    are written to the higher 64 bits of the result.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the converted values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_packus_epi32(__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i) __builtin_ia32_packusdw128((__v4si)__V1, (__v4si)__V2);\n"
"}\n"
"\n"
"/* SSE4 Multiple Packed Sums of Absolute Difference.  */\n"
"/// Subtracts 8-bit unsigned integer values and computes the absolute\n"
"///    values of the differences to the corresponding bits in the destination.\n"
"///    Then sums of the absolute differences are returned according to the bit\n"
"///    fields in the immediate operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_mpsadbw_epu8(__m128i X, __m128i Y, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMPSADBW / MPSADBW </c> instruction.\n"
"///\n"
"/// \\param X\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\param Y\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying how the absolute differences are to\n"
"///    be calculated, according to the following algorithm:\n"
"///    \\code\n"
"///    // M2 represents bit 2 of the immediate operand\n"
"///    // M10 represents bits [1:0] of the immediate operand\n"
"///    i = M2 * 4;\n"
"///    j = M10 * 4;\n"
"///    for (k = 0; k < 8; k = k + 1) {\n"
"///      d0 = abs(X[i + k + 0] - Y[j + 0]);\n"
"///      d1 = abs(X[i + k + 1] - Y[j + 1]);\n"
"///      d2 = abs(X[i + k + 2] - Y[j + 2]);\n"
"///      d3 = abs(X[i + k + 3] - Y[j + 3]);\n"
"///      r[k] = d0 + d1 + d2 + d3;\n"
"///    }\n"
"///    \\endcode\n"
"/// \\returns A 128-bit integer vector containing the sums of the sets of\n"
"///    absolute differences between both operands.\n"
"#define _mm_mpsadbw_epu8(X, Y, M) \\\n"
"  (__m128i) __builtin_ia32_mpsadbw128((__v16qi)(__m128i)(X), \\\n"
"                                      (__v16qi)(__m128i)(Y), (M))\n"
"\n"
"/// Finds the minimum unsigned 16-bit element in the input 128-bit\n"
"///    vector of [8 x u16] and returns it and along with its index.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPHMINPOSUW / PHMINPOSUW </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param __V\n"
"///    A 128-bit vector of [8 x u16].\n"
"/// \\returns A 128-bit value where bits [15:0] contain the minimum value found\n"
"///    in parameter \\a __V, bits [18:16] contain the index of the minimum value\n"
"///    and the remaining bits are set to 0.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_minpos_epu16(__m128i __V)\n"
"{\n"
"  return (__m128i) __builtin_ia32_phminposuw128((__v8hi)__V);\n"
"}\n"
"\n"
"/* Handle the sse4.2 definitions here. */\n"
"\n"
"/* These definitions are normally in nmmintrin.h, but gcc puts them in here\n"
"   so we'll do the same.  */\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"sse4.2\")))\n"
"\n"
"/* These specify the type of data that we're comparing.  */\n"
"#define _SIDD_UBYTE_OPS                 0x00\n"
"#define _SIDD_UWORD_OPS                 0x01\n"
"#define _SIDD_SBYTE_OPS                 0x02\n"
"#define _SIDD_SWORD_OPS                 0x03\n"
"\n"
"/* These specify the type of comparison operation.  */\n"
"#define _SIDD_CMP_EQUAL_ANY             0x00\n"
"#define _SIDD_CMP_RANGES                0x04\n"
"#define _SIDD_CMP_EQUAL_EACH            0x08\n"
"#define _SIDD_CMP_EQUAL_ORDERED         0x0c\n"
"\n"
"/* These macros specify the polarity of the operation.  */\n"
"#define _SIDD_POSITIVE_POLARITY         0x00\n"
"#define _SIDD_NEGATIVE_POLARITY         0x10\n"
"#define _SIDD_MASKED_POSITIVE_POLARITY  0x20\n"
"#define _SIDD_MASKED_NEGATIVE_POLARITY  0x30\n"
"\n"
"/* These macros are used in _mm_cmpXstri() to specify the return.  */\n"
"#define _SIDD_LEAST_SIGNIFICANT         0x00\n"
"#define _SIDD_MOST_SIGNIFICANT          0x40\n"
"\n"
"/* These macros are used in _mm_cmpXstri() to specify the return.  */\n"
"#define _SIDD_BIT_MASK                  0x00\n"
"#define _SIDD_UNIT_MASK                 0x40\n"
"\n"
"/* SSE4.2 Packed Comparison Intrinsics.  */\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with implicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns a 128-bit integer vector representing the result\n"
"///    mask of the comparison.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_cmpistrm(__m128i A, __m128i B, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPISTRM / PCMPISTRM </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words, the type of comparison to perform, and the format of the return\n"
"///    value. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"///    Bit [6]: Determines whether the result is zero-extended or expanded to 16\n"
"///             bytes. \\n\n"
"///      0: The result is zero-extended to 16 bytes. \\n\n"
"///      1: The result is expanded to 16 bytes (this expansion is performed by\n"
"///         repeating each bit 8 or 16 times).\n"
"/// \\returns Returns a 128-bit integer vector representing the result mask of\n"
"///    the comparison.\n"
"#define _mm_cmpistrm(A, B, M) \\\n"
"  (__m128i)__builtin_ia32_pcmpistrm128((__v16qi)(__m128i)(A), \\\n"
"                                       (__v16qi)(__m128i)(B), (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with implicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns an integer representing the result index of the\n"
"///    comparison.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpistri(__m128i A, __m128i B, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words, the type of comparison to perform, and the format of the return\n"
"///    value. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"///    Bit [6]: Determines whether the index of the lowest set bit or the\n"
"///             highest set bit is returned. \\n\n"
"///      0: The index of the least significant set bit. \\n\n"
"///      1: The index of the most significant set bit. \\n\n"
"/// \\returns Returns an integer representing the result index of the comparison.\n"
"#define _mm_cmpistri(A, B, M) \\\n"
"  (int)__builtin_ia32_pcmpistri128((__v16qi)(__m128i)(A), \\\n"
"                                   (__v16qi)(__m128i)(B), (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with explicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns a 128-bit integer vector representing the result\n"
"///    mask of the comparison.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_cmpestrm(__m128i A, int LA, __m128i B, int LB, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPESTRM / PCMPESTRM </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LA\n"
"///    An integer that specifies the length of the string in \\a A.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LB\n"
"///    An integer that specifies the length of the string in \\a B.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words, the type of comparison to perform, and the format of the return\n"
"///    value. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"///    Bit [6]: Determines whether the result is zero-extended or expanded to 16\n"
"///             bytes. \\n\n"
"///      0: The result is zero-extended to 16 bytes. \\n\n"
"///      1: The result is expanded to 16 bytes (this expansion is performed by\n"
"///         repeating each bit 8 or 16 times). \\n\n"
"/// \\returns Returns a 128-bit integer vector representing the result mask of\n"
"///    the comparison.\n"
"#define _mm_cmpestrm(A, LA, B, LB, M) \\\n"
"  (__m128i)__builtin_ia32_pcmpestrm128((__v16qi)(__m128i)(A), (int)(LA), \\\n"
"                                       (__v16qi)(__m128i)(B), (int)(LB), \\\n"
"                                       (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with explicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns an integer representing the result index of the\n"
"///    comparison.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpestri(__m128i A, int LA, __m128i B, int LB, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LA\n"
"///    An integer that specifies the length of the string in \\a A.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LB\n"
"///    An integer that specifies the length of the string in \\a B.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words, the type of comparison to perform, and the format of the return\n"
"///    value. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"///    Bit [6]: Determines whether the index of the lowest set bit or the\n"
"///             highest set bit is returned. \\n\n"
"///      0: The index of the least significant set bit. \\n\n"
"///      1: The index of the most significant set bit. \\n\n"
"/// \\returns Returns an integer representing the result index of the comparison.\n"
"#define _mm_cmpestri(A, LA, B, LB, M) \\\n"
"  (int)__builtin_ia32_pcmpestri128((__v16qi)(__m128i)(A), (int)(LA), \\\n"
"                                   (__v16qi)(__m128i)(B), (int)(LB), \\\n"
"                                   (int)(M))\n"
"\n"
"/* SSE4.2 Packed Comparison Intrinsics and EFlag Reading.  */\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with implicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the bit mask is zero and the length of the\n"
"///    string in \\a B is the maximum, otherwise, returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpistra(__m128i A, __m128i B, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"/// \\returns Returns 1 if the bit mask is zero and the length of the string in\n"
"///    \\a B is the maximum; otherwise, returns 0.\n"
"#define _mm_cmpistra(A, B, M) \\\n"
"  (int)__builtin_ia32_pcmpistria128((__v16qi)(__m128i)(A), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with implicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the bit mask is non-zero, otherwise, returns\n"
"///    0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpistrc(__m128i A, __m128i B, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B.\n"
"/// \\returns Returns 1 if the bit mask is non-zero, otherwise, returns 0.\n"
"#define _mm_cmpistrc(A, B, M) \\\n"
"  (int)__builtin_ia32_pcmpistric128((__v16qi)(__m128i)(A), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with implicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns bit 0 of the resulting bit mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpistro(__m128i A, __m128i B, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"/// \\returns Returns bit 0 of the resulting bit mask.\n"
"#define _mm_cmpistro(A, B, M) \\\n"
"  (int)__builtin_ia32_pcmpistrio128((__v16qi)(__m128i)(A), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with implicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the length of the string in \\a A is less than\n"
"///    the maximum, otherwise, returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpistrs(__m128i A, __m128i B, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"/// \\returns Returns 1 if the length of the string in \\a A is less than the\n"
"///    maximum, otherwise, returns 0.\n"
"#define _mm_cmpistrs(A, B, M) \\\n"
"  (int)__builtin_ia32_pcmpistris128((__v16qi)(__m128i)(A), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with implicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the length of the string in \\a B is less than\n"
"///    the maximum, otherwise, returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpistrz(__m128i A, __m128i B, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B.\n"
"/// \\returns Returns 1 if the length of the string in \\a B is less than the\n"
"///    maximum, otherwise, returns 0.\n"
"#define _mm_cmpistrz(A, B, M) \\\n"
"  (int)__builtin_ia32_pcmpistriz128((__v16qi)(__m128i)(A), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with explicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the bit mask is zero and the length of the\n"
"///    string in \\a B is the maximum, otherwise, returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpestra(__m128i A, int LA, __m128i B, int LB, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LA\n"
"///    An integer that specifies the length of the string in \\a A.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LB\n"
"///    An integer that specifies the length of the string in \\a B.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B.\n"
"/// \\returns Returns 1 if the bit mask is zero and the length of the string in\n"
"///    \\a B is the maximum, otherwise, returns 0.\n"
"#define _mm_cmpestra(A, LA, B, LB, M) \\\n"
"  (int)__builtin_ia32_pcmpestria128((__v16qi)(__m128i)(A), (int)(LA), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(LB), \\\n"
"                                    (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with explicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the resulting mask is non-zero, otherwise,\n"
"///    returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpestrc(__m128i A, int LA, __m128i B, int LB, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LA\n"
"///    An integer that specifies the length of the string in \\a A.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LB\n"
"///    An integer that specifies the length of the string in \\a B.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"/// \\returns Returns 1 if the resulting mask is non-zero, otherwise, returns 0.\n"
"#define _mm_cmpestrc(A, LA, B, LB, M) \\\n"
"  (int)__builtin_ia32_pcmpestric128((__v16qi)(__m128i)(A), (int)(LA), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(LB), \\\n"
"                                    (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with explicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns bit 0 of the resulting bit mask.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpestro(__m128i A, int LA, __m128i B, int LB, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LA\n"
"///    An integer that specifies the length of the string in \\a A.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LB\n"
"///    An integer that specifies the length of the string in \\a B.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B.\n"
"/// \\returns Returns bit 0 of the resulting bit mask.\n"
"#define _mm_cmpestro(A, LA, B, LB, M) \\\n"
"  (int)__builtin_ia32_pcmpestrio128((__v16qi)(__m128i)(A), (int)(LA), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(LB), \\\n"
"                                    (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with explicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the length of the string in \\a A is less than\n"
"///    the maximum, otherwise, returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpestrs(__m128i A, int LA, __m128i B, int LB, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>\n"
"/// instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LA\n"
"///    An integer that specifies the length of the string in \\a A.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LB\n"
"///    An integer that specifies the length of the string in \\a B.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement in the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B. \\n\n"
"/// \\returns Returns 1 if the length of the string in \\a A is less than the\n"
"///    maximum, otherwise, returns 0.\n"
"#define _mm_cmpestrs(A, LA, B, LB, M) \\\n"
"  (int)__builtin_ia32_pcmpestris128((__v16qi)(__m128i)(A), (int)(LA), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(LB), \\\n"
"                                    (int)(M))\n"
"\n"
"/// Uses the immediate operand \\a M to perform a comparison of string\n"
"///    data with explicitly defined lengths that is contained in source operands\n"
"///    \\a A and \\a B. Returns 1 if the length of the string in \\a B is less than\n"
"///    the maximum, otherwise, returns 0.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_cmpestrz(__m128i A, int LA, __m128i B, int LB, const int M);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPESTRI </c> instruction.\n"
"///\n"
"/// \\param A\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LA\n"
"///    An integer that specifies the length of the string in \\a A.\n"
"/// \\param B\n"
"///    A 128-bit integer vector containing one of the source operands to be\n"
"///    compared.\n"
"/// \\param LB\n"
"///    An integer that specifies the length of the string in \\a B.\n"
"/// \\param M\n"
"///    An 8-bit immediate operand specifying whether the characters are bytes or\n"
"///    words and the type of comparison to perform. \\n\n"
"///    Bits [1:0]: Determine source data format. \\n\n"
"///      00: 16 unsigned bytes  \\n\n"
"///      01: 8 unsigned words \\n\n"
"///      10: 16 signed bytes \\n\n"
"///      11: 8 signed words \\n\n"
"///    Bits [3:2]: Determine comparison type and aggregation method. \\n\n"
"///      00: Subset: Each character in \\a B is compared for equality with all\n"
"///          the characters in \\a A. \\n\n"
"///      01: Ranges: Each character in \\a B is compared to \\a A. The comparison\n"
"///          basis is greater than or equal for even-indexed elements in \\a A,\n"
"///          and less than or equal for odd-indexed elements in \\a A. \\n\n"
"///      10: Match: Compare each pair of corresponding characters in \\a A and\n"
"///          \\a B for equality. \\n\n"
"///      11: Substring: Search \\a B for substring matches of \\a A. \\n\n"
"///    Bits [5:4]: Determine whether to perform a one's complement on the bit\n"
"///                mask of the comparison results. \\n\n"
"///      00: No effect. \\n\n"
"///      01: Negate the bit mask. \\n\n"
"///      10: No effect. \\n\n"
"///      11: Negate the bit mask only for bits with an index less than or equal\n"
"///          to the size of \\a A or \\a B.\n"
"/// \\returns Returns 1 if the length of the string in \\a B is less than the\n"
"///    maximum, otherwise, returns 0.\n"
"#define _mm_cmpestrz(A, LA, B, LB, M) \\\n"
"  (int)__builtin_ia32_pcmpestriz128((__v16qi)(__m128i)(A), (int)(LA), \\\n"
"                                    (__v16qi)(__m128i)(B), (int)(LB), \\\n"
"                                    (int)(M))\n"
"\n"
"/* SSE4.2 Compare Packed Data -- Greater Than.  */\n"
"/// Compares each of the corresponding 64-bit values of the 128-bit\n"
"///    integer vectors to determine if the values in the first operand are\n"
"///    greater than those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPCMPGTQ / PCMPGTQ </c> instruction.\n"
"///\n"
"/// \\param __V1\n"
"///    A 128-bit integer vector.\n"
"/// \\param __V2\n"
"///    A 128-bit integer vector.\n"
"/// \\returns A 128-bit integer vector containing the comparison results.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_epi64(__m128i __V1, __m128i __V2)\n"
"{\n"
"  return (__m128i)((__v2di)__V1 > (__v2di)__V2);\n"
"}\n"
"\n"
"/* SSE4.2 Accumulate CRC32.  */\n"
"/// Adds the unsigned integer operand to the CRC-32C checksum of the\n"
"///    unsigned char operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CRC32B </c> instruction.\n"
"///\n"
"/// \\param __C\n"
"///    An unsigned integer operand to add to the CRC-32C checksum of operand\n"
"///    \\a  __D.\n"
"/// \\param __D\n"
"///    An unsigned 8-bit integer operand used to compute the CRC-32C checksum.\n"
"/// \\returns The result of adding operand \\a __C to the CRC-32C checksum of\n"
"///    operand \\a __D.\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_mm_crc32_u8(unsigned int __C, unsigned char __D)\n"
"{\n"
"  return __builtin_ia32_crc32qi(__C, __D);\n"
"}\n"
"\n"
"/// Adds the unsigned integer operand to the CRC-32C checksum of the\n"
"///    unsigned short operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CRC32W </c> instruction.\n"
"///\n"
"/// \\param __C\n"
"///    An unsigned integer operand to add to the CRC-32C checksum of operand\n"
"///    \\a __D.\n"
"/// \\param __D\n"
"///    An unsigned 16-bit integer operand used to compute the CRC-32C checksum.\n"
"/// \\returns The result of adding operand \\a __C to the CRC-32C checksum of\n"
"///    operand \\a __D.\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_mm_crc32_u16(unsigned int __C, unsigned short __D)\n"
"{\n"
"  return __builtin_ia32_crc32hi(__C, __D);\n"
"}\n"
"\n"
"/// Adds the first unsigned integer operand to the CRC-32C checksum of\n"
"///    the second unsigned integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CRC32L </c> instruction.\n"
"///\n"
"/// \\param __C\n"
"///    An unsigned integer operand to add to the CRC-32C checksum of operand\n"
"///    \\a __D.\n"
"/// \\param __D\n"
"///    An unsigned 32-bit integer operand used to compute the CRC-32C checksum.\n"
"/// \\returns The result of adding operand \\a __C to the CRC-32C checksum of\n"
"///    operand \\a __D.\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"_mm_crc32_u32(unsigned int __C, unsigned int __D)\n"
"{\n"
"  return __builtin_ia32_crc32si(__C, __D);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Adds the unsigned integer operand to the CRC-32C checksum of the\n"
"///    unsigned 64-bit integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CRC32Q </c> instruction.\n"
"///\n"
"/// \\param __C\n"
"///    An unsigned integer operand to add to the CRC-32C checksum of operand\n"
"///    \\a __D.\n"
"/// \\param __D\n"
"///    An unsigned 64-bit integer operand used to compute the CRC-32C checksum.\n"
"/// \\returns The result of adding operand \\a __C to the CRC-32C checksum of\n"
"///    operand \\a __D.\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"_mm_crc32_u64(unsigned long long __C, unsigned long long __D)\n"
"{\n"
"  return __builtin_ia32_crc32di(__C, __D);\n"
"}\n"
"#endif /* __x86_64__ */\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#include <popcntintrin.h>\n"
"\n"
"#endif /* __SMMINTRIN_H */\n"
"" } , 
 { "/builtins/stdalign.h" , "/*===---- stdalign.h - Standard header for alignment ------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __STDALIGN_H\n"
"#define __STDALIGN_H\n"
"\n"
"#ifndef __cplusplus\n"
"#define alignas _Alignas\n"
"#define alignof _Alignof\n"
"#endif\n"
"\n"
"#define __alignas_is_defined 1\n"
"#define __alignof_is_defined 1\n"
"\n"
"#endif /* __STDALIGN_H */\n"
"" } , 
 { "/builtins/stdarg.h" , "/*===---- stdarg.h - Variable argument handling ----------------------------===\n"
" *\n"
" * Copyright (c) 2008 Eli Friedman\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __STDARG_H\n"
"#define __STDARG_H\n"
"\n"
"#ifndef _VA_LIST\n"
"typedef __builtin_va_list va_list;\n"
"#define _VA_LIST\n"
"#endif\n"
"#define va_start(ap, param) __builtin_va_start(ap, param)\n"
"#define va_end(ap)          __builtin_va_end(ap)\n"
"#define va_arg(ap, type)    __builtin_va_arg(ap, type)\n"
"\n"
"/* GCC always defines __va_copy, but does not define va_copy unless in c99 mode\n"
" * or -ansi is not specified, since it was not part of C90.\n"
" */\n"
"#define __va_copy(d,s) __builtin_va_copy(d,s)\n"
"\n"
"#if __STDC_VERSION__ >= 199901L || __cplusplus >= 201103L || !defined(__STRICT_ANSI__)\n"
"#define va_copy(dest, src)  __builtin_va_copy(dest, src)\n"
"#endif\n"
"\n"
"#ifndef __GNUC_VA_LIST\n"
"#define __GNUC_VA_LIST 1\n"
"typedef __builtin_va_list __gnuc_va_list;\n"
"#endif\n"
"\n"
"#endif /* __STDARG_H */\n"
"" } , 
 { "/builtins/stdatomic.h" , "/*===---- stdatomic.h - Standard header for atomic types and operations -----===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __CLANG_STDATOMIC_H\n"
"#define __CLANG_STDATOMIC_H\n"
"\n"
"/* If we're hosted, fall back to the system's stdatomic.h. FreeBSD, for\n"
" * example, already has a Clang-compatible stdatomic.h header.\n"
" */\n"
"#if __STDC_HOSTED__ && __has_include_next(<stdatomic.h>)\n"
"# include_next <stdatomic.h>\n"
"#else\n"
"\n"
"#include <stddef.h>\n"
"#include <stdint.h>\n"
"\n"
"#ifdef __cplusplus\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"/* 7.17.1 Introduction */\n"
"\n"
"#define ATOMIC_BOOL_LOCK_FREE       __CLANG_ATOMIC_BOOL_LOCK_FREE\n"
"#define ATOMIC_CHAR_LOCK_FREE       __CLANG_ATOMIC_CHAR_LOCK_FREE\n"
"#define ATOMIC_CHAR16_T_LOCK_FREE   __CLANG_ATOMIC_CHAR16_T_LOCK_FREE\n"
"#define ATOMIC_CHAR32_T_LOCK_FREE   __CLANG_ATOMIC_CHAR32_T_LOCK_FREE\n"
"#define ATOMIC_WCHAR_T_LOCK_FREE    __CLANG_ATOMIC_WCHAR_T_LOCK_FREE\n"
"#define ATOMIC_SHORT_LOCK_FREE      __CLANG_ATOMIC_SHORT_LOCK_FREE\n"
"#define ATOMIC_INT_LOCK_FREE        __CLANG_ATOMIC_INT_LOCK_FREE\n"
"#define ATOMIC_LONG_LOCK_FREE       __CLANG_ATOMIC_LONG_LOCK_FREE\n"
"#define ATOMIC_LLONG_LOCK_FREE      __CLANG_ATOMIC_LLONG_LOCK_FREE\n"
"#define ATOMIC_POINTER_LOCK_FREE    __CLANG_ATOMIC_POINTER_LOCK_FREE\n"
"\n"
"/* 7.17.2 Initialization */\n"
"\n"
"#define ATOMIC_VAR_INIT(value) (value)\n"
"#define atomic_init __c11_atomic_init\n"
"\n"
"/* 7.17.3 Order and consistency */\n"
"\n"
"typedef enum memory_order {\n"
"  memory_order_relaxed = __ATOMIC_RELAXED,\n"
"  memory_order_consume = __ATOMIC_CONSUME,\n"
"  memory_order_acquire = __ATOMIC_ACQUIRE,\n"
"  memory_order_release = __ATOMIC_RELEASE,\n"
"  memory_order_acq_rel = __ATOMIC_ACQ_REL,\n"
"  memory_order_seq_cst = __ATOMIC_SEQ_CST\n"
"} memory_order;\n"
"\n"
"#define kill_dependency(y) (y)\n"
"\n"
"/* 7.17.4 Fences */\n"
"\n"
"/* These should be provided by the libc implementation. */\n"
"void atomic_thread_fence(memory_order);\n"
"void atomic_signal_fence(memory_order);\n"
"\n"
"#define atomic_thread_fence(order) __c11_atomic_thread_fence(order)\n"
"#define atomic_signal_fence(order) __c11_atomic_signal_fence(order)\n"
"\n"
"/* 7.17.5 Lock-free property */\n"
"\n"
"#define atomic_is_lock_free(obj) __c11_atomic_is_lock_free(sizeof(*(obj)))\n"
"\n"
"/* 7.17.6 Atomic integer types */\n"
"\n"
"#ifdef __cplusplus\n"
"typedef _Atomic(bool)               atomic_bool;\n"
"#else\n"
"typedef _Atomic(_Bool)              atomic_bool;\n"
"#endif\n"
"typedef _Atomic(char)               atomic_char;\n"
"typedef _Atomic(signed char)        atomic_schar;\n"
"typedef _Atomic(unsigned char)      atomic_uchar;\n"
"typedef _Atomic(short)              atomic_short;\n"
"typedef _Atomic(unsigned short)     atomic_ushort;\n"
"typedef _Atomic(int)                atomic_int;\n"
"typedef _Atomic(unsigned int)       atomic_uint;\n"
"typedef _Atomic(long)               atomic_long;\n"
"typedef _Atomic(unsigned long)      atomic_ulong;\n"
"typedef _Atomic(long long)          atomic_llong;\n"
"typedef _Atomic(unsigned long long) atomic_ullong;\n"
"typedef _Atomic(uint_least16_t)     atomic_char16_t;\n"
"typedef _Atomic(uint_least32_t)     atomic_char32_t;\n"
"typedef _Atomic(wchar_t)            atomic_wchar_t;\n"
"typedef _Atomic(int_least8_t)       atomic_int_least8_t;\n"
"typedef _Atomic(uint_least8_t)      atomic_uint_least8_t;\n"
"typedef _Atomic(int_least16_t)      atomic_int_least16_t;\n"
"typedef _Atomic(uint_least16_t)     atomic_uint_least16_t;\n"
"typedef _Atomic(int_least32_t)      atomic_int_least32_t;\n"
"typedef _Atomic(uint_least32_t)     atomic_uint_least32_t;\n"
"typedef _Atomic(int_least64_t)      atomic_int_least64_t;\n"
"typedef _Atomic(uint_least64_t)     atomic_uint_least64_t;\n"
"typedef _Atomic(int_fast8_t)        atomic_int_fast8_t;\n"
"typedef _Atomic(uint_fast8_t)       atomic_uint_fast8_t;\n"
"typedef _Atomic(int_fast16_t)       atomic_int_fast16_t;\n"
"typedef _Atomic(uint_fast16_t)      atomic_uint_fast16_t;\n"
"typedef _Atomic(int_fast32_t)       atomic_int_fast32_t;\n"
"typedef _Atomic(uint_fast32_t)      atomic_uint_fast32_t;\n"
"typedef _Atomic(int_fast64_t)       atomic_int_fast64_t;\n"
"typedef _Atomic(uint_fast64_t)      atomic_uint_fast64_t;\n"
"typedef _Atomic(intptr_t)           atomic_intptr_t;\n"
"typedef _Atomic(uintptr_t)          atomic_uintptr_t;\n"
"typedef _Atomic(size_t)             atomic_size_t;\n"
"typedef _Atomic(ptrdiff_t)          atomic_ptrdiff_t;\n"
"typedef _Atomic(intmax_t)           atomic_intmax_t;\n"
"typedef _Atomic(uintmax_t)          atomic_uintmax_t;\n"
"\n"
"/* 7.17.7 Operations on atomic types */\n"
"\n"
"#define atomic_store(object, desired) __c11_atomic_store(object, desired, __ATOMIC_SEQ_CST)\n"
"#define atomic_store_explicit __c11_atomic_store\n"
"\n"
"#define atomic_load(object) __c11_atomic_load(object, __ATOMIC_SEQ_CST)\n"
"#define atomic_load_explicit __c11_atomic_load\n"
"\n"
"#define atomic_exchange(object, desired) __c11_atomic_exchange(object, desired, __ATOMIC_SEQ_CST)\n"
"#define atomic_exchange_explicit __c11_atomic_exchange\n"
"\n"
"#define atomic_compare_exchange_strong(object, expected, desired) __c11_atomic_compare_exchange_strong(object, expected, desired, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST)\n"
"#define atomic_compare_exchange_strong_explicit __c11_atomic_compare_exchange_strong\n"
"\n"
"#define atomic_compare_exchange_weak(object, expected, desired) __c11_atomic_compare_exchange_weak(object, expected, desired, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST)\n"
"#define atomic_compare_exchange_weak_explicit __c11_atomic_compare_exchange_weak\n"
"\n"
"#define atomic_fetch_add(object, operand) __c11_atomic_fetch_add(object, operand, __ATOMIC_SEQ_CST)\n"
"#define atomic_fetch_add_explicit __c11_atomic_fetch_add\n"
"\n"
"#define atomic_fetch_sub(object, operand) __c11_atomic_fetch_sub(object, operand, __ATOMIC_SEQ_CST)\n"
"#define atomic_fetch_sub_explicit __c11_atomic_fetch_sub\n"
"\n"
"#define atomic_fetch_or(object, operand) __c11_atomic_fetch_or(object, operand, __ATOMIC_SEQ_CST)\n"
"#define atomic_fetch_or_explicit __c11_atomic_fetch_or\n"
"\n"
"#define atomic_fetch_xor(object, operand) __c11_atomic_fetch_xor(object, operand, __ATOMIC_SEQ_CST)\n"
"#define atomic_fetch_xor_explicit __c11_atomic_fetch_xor\n"
"\n"
"#define atomic_fetch_and(object, operand) __c11_atomic_fetch_and(object, operand, __ATOMIC_SEQ_CST)\n"
"#define atomic_fetch_and_explicit __c11_atomic_fetch_and\n"
"\n"
"/* 7.17.8 Atomic flag type and operations */\n"
"\n"
"typedef struct atomic_flag { atomic_bool _Value; } atomic_flag;\n"
"\n"
"#define ATOMIC_FLAG_INIT { 0 }\n"
"\n"
"/* These should be provided by the libc implementation. */\n"
"#ifdef __cplusplus\n"
"bool atomic_flag_test_and_set(volatile atomic_flag *);\n"
"bool atomic_flag_test_and_set_explicit(volatile atomic_flag *, memory_order);\n"
"#else\n"
"_Bool atomic_flag_test_and_set(volatile atomic_flag *);\n"
"_Bool atomic_flag_test_and_set_explicit(volatile atomic_flag *, memory_order);\n"
"#endif\n"
"void atomic_flag_clear(volatile atomic_flag *);\n"
"void atomic_flag_clear_explicit(volatile atomic_flag *, memory_order);\n"
"\n"
"#define atomic_flag_test_and_set(object) __c11_atomic_exchange(&(object)->_Value, 1, __ATOMIC_SEQ_CST)\n"
"#define atomic_flag_test_and_set_explicit(object, order) __c11_atomic_exchange(&(object)->_Value, 1, order)\n"
"\n"
"#define atomic_flag_clear(object) __c11_atomic_store(&(object)->_Value, 0, __ATOMIC_SEQ_CST)\n"
"#define atomic_flag_clear_explicit(object, order) __c11_atomic_store(&(object)->_Value, 0, order)\n"
"\n"
"#ifdef __cplusplus\n"
"}\n"
"#endif\n"
"\n"
"#endif /* __STDC_HOSTED__ */\n"
"#endif /* __CLANG_STDATOMIC_H */\n"
"\n"
"" } , 
 { "/builtins/stdbool.h" , "/*===---- stdbool.h - Standard header for booleans -------------------------===\n"
" *\n"
" * Copyright (c) 2008 Eli Friedman\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __STDBOOL_H\n"
"#define __STDBOOL_H\n"
"\n"
"/* Don't define bool, true, and false in C++, except as a GNU extension. */\n"
"#ifndef __cplusplus\n"
"#define bool _Bool\n"
"#define true 1\n"
"#define false 0\n"
"#elif defined(__GNUC__) && !defined(__STRICT_ANSI__)\n"
"/* Define _Bool as a GNU extension. */\n"
"#define _Bool bool\n"
"#if __cplusplus < 201103L\n"
"/* For C++98, define bool, false, true as a GNU extension. */\n"
"#define bool  bool\n"
"#define false false\n"
"#define true  true\n"
"#endif\n"
"#endif\n"
"\n"
"#define __bool_true_false_are_defined 1\n"
"\n"
"#endif /* __STDBOOL_H */\n"
"" } , 
 { "/builtins/stddef.h" , "/*===---- stddef.h - Basic type definitions --------------------------------===\n"
" *\n"
" * Copyright (c) 2008 Eli Friedman\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined(__STDDEF_H) || defined(__need_ptrdiff_t) ||                       \\\n"
"    defined(__need_size_t) || defined(__need_wchar_t) ||                       \\\n"
"    defined(__need_NULL) || defined(__need_wint_t)\n"
"\n"
"#if !defined(__need_ptrdiff_t) && !defined(__need_size_t) &&                   \\\n"
"    !defined(__need_wchar_t) && !defined(__need_NULL) &&                       \\\n"
"    !defined(__need_wint_t)\n"
"/* Always define miscellaneous pieces when modules are available. */\n"
"#if !__has_feature(modules)\n"
"#define __STDDEF_H\n"
"#endif\n"
"#define __need_ptrdiff_t\n"
"#define __need_size_t\n"
"#define __need_wchar_t\n"
"#define __need_NULL\n"
"#define __need_STDDEF_H_misc\n"
"/* __need_wint_t is intentionally not defined here. */\n"
"#endif\n"
"\n"
"#if defined(__need_ptrdiff_t)\n"
"#if !defined(_PTRDIFF_T) || __has_feature(modules)\n"
"/* Always define ptrdiff_t when modules are available. */\n"
"#if !__has_feature(modules)\n"
"#define _PTRDIFF_T\n"
"#endif\n"
"typedef __PTRDIFF_TYPE__ ptrdiff_t;\n"
"#endif\n"
"#undef __need_ptrdiff_t\n"
"#endif /* defined(__need_ptrdiff_t) */\n"
"\n"
"#if defined(__need_size_t)\n"
"#if !defined(_SIZE_T) || __has_feature(modules)\n"
"/* Always define size_t when modules are available. */\n"
"#if !__has_feature(modules)\n"
"#define _SIZE_T\n"
"#endif\n"
"typedef __SIZE_TYPE__ size_t;\n"
"#endif\n"
"#undef __need_size_t\n"
"#endif /*defined(__need_size_t) */\n"
"\n"
"#if defined(__need_STDDEF_H_misc)\n"
"/* ISO9899:2011 7.20 (C11 Annex K): Define rsize_t if __STDC_WANT_LIB_EXT1__ is\n"
" * enabled. */\n"
"#if (defined(__STDC_WANT_LIB_EXT1__) && __STDC_WANT_LIB_EXT1__ >= 1 && \\\n"
"     !defined(_RSIZE_T)) || __has_feature(modules)\n"
"/* Always define rsize_t when modules are available. */\n"
"#if !__has_feature(modules)\n"
"#define _RSIZE_T\n"
"#endif\n"
"typedef __SIZE_TYPE__ rsize_t;\n"
"#endif\n"
"#endif /* defined(__need_STDDEF_H_misc) */\n"
"\n"
"#if defined(__need_wchar_t)\n"
"#ifndef __cplusplus\n"
"/* Always define wchar_t when modules are available. */\n"
"#if !defined(_WCHAR_T) || __has_feature(modules)\n"
"#if !__has_feature(modules)\n"
"#define _WCHAR_T\n"
"#if defined(_MSC_EXTENSIONS)\n"
"#define _WCHAR_T_DEFINED\n"
"#endif\n"
"#endif\n"
"typedef __WCHAR_TYPE__ wchar_t;\n"
"#endif\n"
"#endif\n"
"#undef __need_wchar_t\n"
"#endif /* defined(__need_wchar_t) */\n"
"\n"
"#if defined(__need_NULL)\n"
"#undef NULL\n"
"#ifdef __cplusplus\n"
"#  if !defined(__MINGW32__) && !defined(_MSC_VER)\n"
"#    define NULL __null\n"
"#  else\n"
"#    define NULL 0\n"
"#  endif\n"
"#else\n"
"#  define NULL ((void*)0)\n"
"#endif\n"
"#ifdef __cplusplus\n"
"#if defined(_MSC_EXTENSIONS) && defined(_NATIVE_NULLPTR_SUPPORTED)\n"
"namespace std { typedef decltype(nullptr) nullptr_t; }\n"
"using ::std::nullptr_t;\n"
"#endif\n"
"#endif\n"
"#undef __need_NULL\n"
"#endif /* defined(__need_NULL) */\n"
"\n"
"#if defined(__need_STDDEF_H_misc)\n"
"#if __STDC_VERSION__ >= 201112L || __cplusplus >= 201103L\n"
"#include \"__stddef_max_align_t.h\"\n"
"#endif\n"
"#define offsetof(t, d) __builtin_offsetof(t, d)\n"
"#undef __need_STDDEF_H_misc\n"
"#endif  /* defined(__need_STDDEF_H_misc) */\n"
"\n"
"/* Some C libraries expect to see a wint_t here. Others (notably MinGW) will use\n"
"__WINT_TYPE__ directly; accommodate both by requiring __need_wint_t */\n"
"#if defined(__need_wint_t)\n"
"/* Always define wint_t when modules are available. */\n"
"#if !defined(_WINT_T) || __has_feature(modules)\n"
"#if !__has_feature(modules)\n"
"#define _WINT_T\n"
"#endif\n"
"typedef __WINT_TYPE__ wint_t;\n"
"#endif\n"
"#undef __need_wint_t\n"
"#endif /* __need_wint_t */\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/stdint.h" , "/*===---- stdint.h - Standard header for sized integer types --------------===*\\\n"
" *\n"
" * Copyright (c) 2009 Chris Lattner\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
"\\*===----------------------------------------------------------------------===*/\n"
"\n"
"\n"
"/* If we're hosted, fall back to the system's stdint.h, which might have\n"
" * additional definitions.\n"
" */\n"
"#if __STDC_HOSTED__ && __has_include_next(<stdint.h>)\n"
"\n"
"// C99 7.18.3 Limits of other integer types\n"
"//\n"
"//  Footnote 219, 220: C++ implementations should define these macros only when\n"
"//  __STDC_LIMIT_MACROS is defined before <stdint.h> is included.\n"
"//\n"
"//  Footnote 222: C++ implementations should define these macros only when\n"
"//  __STDC_CONSTANT_MACROS is defined before <stdint.h> is included.\n"
"//\n"
"// C++11 [cstdint.syn]p2:\n"
"//\n"
"//  The macros defined by <cstdint> are provided unconditionally. In particular,\n"
"//  the symbols __STDC_LIMIT_MACROS and __STDC_CONSTANT_MACROS (mentioned in\n"
"//  footnotes 219, 220, and 222 in the C standard) play no role in C++.\n"
"//\n"
"// C11 removed the problematic footnotes.\n"
"//\n"
"// Work around this inconsistency by always defining those macros in C++ mode,\n"
"// so that a C library implementation which follows the C99 standard can be\n"
"// used in C++.\n"
"# ifdef __cplusplus\n"
"#  if !defined(__STDC_LIMIT_MACROS)\n"
"#   define __STDC_LIMIT_MACROS\n"
"#   define __STDC_LIMIT_MACROS_DEFINED_BY_CLANG\n"
"#  endif\n"
"#  if !defined(__STDC_CONSTANT_MACROS)\n"
"#   define __STDC_CONSTANT_MACROS\n"
"#   define __STDC_CONSTANT_MACROS_DEFINED_BY_CLANG\n"
"#  endif\n"
"# endif\n"
"\n"
"# include_next <stdint.h>\n"
"\n"
"# ifdef __STDC_LIMIT_MACROS_DEFINED_BY_CLANG\n"
"#  undef __STDC_LIMIT_MACROS\n"
"#  undef __STDC_LIMIT_MACROS_DEFINED_BY_CLANG\n"
"# endif\n"
"# ifdef __STDC_CONSTANT_MACROS_DEFINED_BY_CLANG\n"
"#  undef __STDC_CONSTANT_MACROS\n"
"#  undef __STDC_CONSTANT_MACROS_DEFINED_BY_CLANG\n"
"# endif\n"
"\n"
"#else\n"
"#ifndef __CLANG_STDINT_H2\n"
"#define __CLANG_STDINT_H2\n"
"\n"
"/* C99 7.18.1.1 Exact-width integer types.\n"
" * C99 7.18.1.2 Minimum-width integer types.\n"
" * C99 7.18.1.3 Fastest minimum-width integer types.\n"
" *\n"
" * The standard requires that exact-width type be defined for 8-, 16-, 32-, and\n"
" * 64-bit types if they are implemented. Other exact width types are optional.\n"
" * This implementation defines an exact-width types for every integer width\n"
" * that is represented in the standard integer types.\n"
" *\n"
" * The standard also requires minimum-width types be defined for 8-, 16-, 32-,\n"
" * and 64-bit widths regardless of whether there are corresponding exact-width\n"
" * types.\n"
" *\n"
" * To accommodate targets that are missing types that are exactly 8, 16, 32, or\n"
" * 64 bits wide, this implementation takes an approach of cascading\n"
" * redefinitions, redefining __int_leastN_t to successively smaller exact-width\n"
" * types. It is therefore important that the types are defined in order of\n"
" * descending widths.\n"
" *\n"
" * We currently assume that the minimum-width types and the fastest\n"
" * minimum-width types are the same. This is allowed by the standard, but is\n"
" * suboptimal.\n"
" *\n"
" * In violation of the standard, some targets do not implement a type that is\n"
" * wide enough to represent all of the required widths (8-, 16-, 32-, 64-bit).\n"
" * To accommodate these targets, a required minimum-width type is only\n"
" * defined if there exists an exact-width type of equal or greater width.\n"
" */\n"
"\n"
"#ifdef __INT64_TYPE__\n"
"# ifndef __int8_t_defined /* glibc sys/types.h also defines int64_t*/\n"
"typedef __INT64_TYPE__ int64_t;\n"
"# endif /* __int8_t_defined */\n"
"typedef __UINT64_TYPE__ uint64_t;\n"
"# define __int_least64_t int64_t\n"
"# define __uint_least64_t uint64_t\n"
"# define __int_least32_t int64_t\n"
"# define __uint_least32_t uint64_t\n"
"# define __int_least16_t int64_t\n"
"# define __uint_least16_t uint64_t\n"
"# define __int_least8_t int64_t\n"
"# define __uint_least8_t uint64_t\n"
"#endif /* __INT64_TYPE__ */\n"
"\n"
"#ifdef __int_least64_t\n"
"typedef __int_least64_t int_least64_t;\n"
"typedef __uint_least64_t uint_least64_t;\n"
"typedef __int_least64_t int_fast64_t;\n"
"typedef __uint_least64_t uint_fast64_t;\n"
"#endif /* __int_least64_t */\n"
"\n"
"#ifdef __INT56_TYPE__\n"
"typedef __INT56_TYPE__ int56_t;\n"
"typedef __UINT56_TYPE__ uint56_t;\n"
"typedef int56_t int_least56_t;\n"
"typedef uint56_t uint_least56_t;\n"
"typedef int56_t int_fast56_t;\n"
"typedef uint56_t uint_fast56_t;\n"
"# define __int_least32_t int56_t\n"
"# define __uint_least32_t uint56_t\n"
"# define __int_least16_t int56_t\n"
"# define __uint_least16_t uint56_t\n"
"# define __int_least8_t int56_t\n"
"# define __uint_least8_t uint56_t\n"
"#endif /* __INT56_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT48_TYPE__\n"
"typedef __INT48_TYPE__ int48_t;\n"
"typedef __UINT48_TYPE__ uint48_t;\n"
"typedef int48_t int_least48_t;\n"
"typedef uint48_t uint_least48_t;\n"
"typedef int48_t int_fast48_t;\n"
"typedef uint48_t uint_fast48_t;\n"
"# define __int_least32_t int48_t\n"
"# define __uint_least32_t uint48_t\n"
"# define __int_least16_t int48_t\n"
"# define __uint_least16_t uint48_t\n"
"# define __int_least8_t int48_t\n"
"# define __uint_least8_t uint48_t\n"
"#endif /* __INT48_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT40_TYPE__\n"
"typedef __INT40_TYPE__ int40_t;\n"
"typedef __UINT40_TYPE__ uint40_t;\n"
"typedef int40_t int_least40_t;\n"
"typedef uint40_t uint_least40_t;\n"
"typedef int40_t int_fast40_t;\n"
"typedef uint40_t uint_fast40_t;\n"
"# define __int_least32_t int40_t\n"
"# define __uint_least32_t uint40_t\n"
"# define __int_least16_t int40_t\n"
"# define __uint_least16_t uint40_t\n"
"# define __int_least8_t int40_t\n"
"# define __uint_least8_t uint40_t\n"
"#endif /* __INT40_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT32_TYPE__\n"
"\n"
"# ifndef __int8_t_defined /* glibc sys/types.h also defines int32_t*/\n"
"typedef __INT32_TYPE__ int32_t;\n"
"# endif /* __int8_t_defined */\n"
"\n"
"# ifndef __uint32_t_defined  /* more glibc compatibility */\n"
"# define __uint32_t_defined\n"
"typedef __UINT32_TYPE__ uint32_t;\n"
"# endif /* __uint32_t_defined */\n"
"\n"
"# define __int_least32_t int32_t\n"
"# define __uint_least32_t uint32_t\n"
"# define __int_least16_t int32_t\n"
"# define __uint_least16_t uint32_t\n"
"# define __int_least8_t int32_t\n"
"# define __uint_least8_t uint32_t\n"
"#endif /* __INT32_TYPE__ */\n"
"\n"
"#ifdef __int_least32_t\n"
"typedef __int_least32_t int_least32_t;\n"
"typedef __uint_least32_t uint_least32_t;\n"
"typedef __int_least32_t int_fast32_t;\n"
"typedef __uint_least32_t uint_fast32_t;\n"
"#endif /* __int_least32_t */\n"
"\n"
"#ifdef __INT24_TYPE__\n"
"typedef __INT24_TYPE__ int24_t;\n"
"typedef __UINT24_TYPE__ uint24_t;\n"
"typedef int24_t int_least24_t;\n"
"typedef uint24_t uint_least24_t;\n"
"typedef int24_t int_fast24_t;\n"
"typedef uint24_t uint_fast24_t;\n"
"# define __int_least16_t int24_t\n"
"# define __uint_least16_t uint24_t\n"
"# define __int_least8_t int24_t\n"
"# define __uint_least8_t uint24_t\n"
"#endif /* __INT24_TYPE__ */\n"
"\n"
"#ifdef __INT16_TYPE__\n"
"#ifndef __int8_t_defined /* glibc sys/types.h also defines int16_t*/\n"
"typedef __INT16_TYPE__ int16_t;\n"
"#endif /* __int8_t_defined */\n"
"typedef __UINT16_TYPE__ uint16_t;\n"
"# define __int_least16_t int16_t\n"
"# define __uint_least16_t uint16_t\n"
"# define __int_least8_t int16_t\n"
"# define __uint_least8_t uint16_t\n"
"#endif /* __INT16_TYPE__ */\n"
"\n"
"#ifdef __int_least16_t\n"
"typedef __int_least16_t int_least16_t;\n"
"typedef __uint_least16_t uint_least16_t;\n"
"typedef __int_least16_t int_fast16_t;\n"
"typedef __uint_least16_t uint_fast16_t;\n"
"#endif /* __int_least16_t */\n"
"\n"
"\n"
"#ifdef __INT8_TYPE__\n"
"#ifndef __int8_t_defined  /* glibc sys/types.h also defines int8_t*/\n"
"typedef __INT8_TYPE__ int8_t;\n"
"#endif /* __int8_t_defined */\n"
"typedef __UINT8_TYPE__ uint8_t;\n"
"# define __int_least8_t int8_t\n"
"# define __uint_least8_t uint8_t\n"
"#endif /* __INT8_TYPE__ */\n"
"\n"
"#ifdef __int_least8_t\n"
"typedef __int_least8_t int_least8_t;\n"
"typedef __uint_least8_t uint_least8_t;\n"
"typedef __int_least8_t int_fast8_t;\n"
"typedef __uint_least8_t uint_fast8_t;\n"
"#endif /* __int_least8_t */\n"
"\n"
"/* prevent glibc sys/types.h from defining conflicting types */\n"
"#ifndef __int8_t_defined\n"
"# define __int8_t_defined\n"
"#endif /* __int8_t_defined */\n"
"\n"
"/* C99 7.18.1.4 Integer types capable of holding object pointers.\n"
" */\n"
"#define __stdint_join3(a,b,c) a ## b ## c\n"
"\n"
"#ifndef _INTPTR_T\n"
"#ifndef __intptr_t_defined\n"
"typedef __INTPTR_TYPE__ intptr_t;\n"
"#define __intptr_t_defined\n"
"#define _INTPTR_T\n"
"#endif\n"
"#endif\n"
"\n"
"#ifndef _UINTPTR_T\n"
"typedef __UINTPTR_TYPE__ uintptr_t;\n"
"#define _UINTPTR_T\n"
"#endif\n"
"\n"
"/* C99 7.18.1.5 Greatest-width integer types.\n"
" */\n"
"typedef __INTMAX_TYPE__  intmax_t;\n"
"typedef __UINTMAX_TYPE__ uintmax_t;\n"
"\n"
"/* C99 7.18.4 Macros for minimum-width integer constants.\n"
" *\n"
" * The standard requires that integer constant macros be defined for all the\n"
" * minimum-width types defined above. As 8-, 16-, 32-, and 64-bit minimum-width\n"
" * types are required, the corresponding integer constant macros are defined\n"
" * here. This implementation also defines minimum-width types for every other\n"
" * integer width that the target implements, so corresponding macros are\n"
" * defined below, too.\n"
" *\n"
" * These macros are defined using the same successive-shrinking approach as\n"
" * the type definitions above. It is likewise important that macros are defined\n"
" * in order of decending width.\n"
" *\n"
" * Note that C++ should not check __STDC_CONSTANT_MACROS here, contrary to the\n"
" * claims of the C standard (see C++ 18.3.1p2, [cstdint.syn]).\n"
" */\n"
"\n"
"#define __int_c_join(a, b) a ## b\n"
"#define __int_c(v, suffix) __int_c_join(v, suffix)\n"
"#define __uint_c(v, suffix) __int_c_join(v##U, suffix)\n"
"\n"
"\n"
"#ifdef __INT64_TYPE__\n"
"# ifdef __INT64_C_SUFFIX__\n"
"#  define __int64_c_suffix __INT64_C_SUFFIX__\n"
"#  define __int32_c_suffix __INT64_C_SUFFIX__\n"
"#  define __int16_c_suffix __INT64_C_SUFFIX__\n"
"#  define  __int8_c_suffix __INT64_C_SUFFIX__\n"
"# else\n"
"#  undef __int64_c_suffix\n"
"#  undef __int32_c_suffix\n"
"#  undef __int16_c_suffix\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT64_C_SUFFIX__ */\n"
"#endif /* __INT64_TYPE__ */\n"
"\n"
"#ifdef __int_least64_t\n"
"# ifdef __int64_c_suffix\n"
"#  define INT64_C(v) __int_c(v, __int64_c_suffix)\n"
"#  define UINT64_C(v) __uint_c(v, __int64_c_suffix)\n"
"# else\n"
"#  define INT64_C(v) v\n"
"#  define UINT64_C(v) v ## U\n"
"# endif /* __int64_c_suffix */\n"
"#endif /* __int_least64_t */\n"
"\n"
"\n"
"#ifdef __INT56_TYPE__\n"
"# ifdef __INT56_C_SUFFIX__\n"
"#  define INT56_C(v) __int_c(v, __INT56_C_SUFFIX__)\n"
"#  define UINT56_C(v) __uint_c(v, __INT56_C_SUFFIX__)\n"
"#  define __int32_c_suffix __INT56_C_SUFFIX__\n"
"#  define __int16_c_suffix __INT56_C_SUFFIX__\n"
"#  define __int8_c_suffix  __INT56_C_SUFFIX__\n"
"# else\n"
"#  define INT56_C(v) v\n"
"#  define UINT56_C(v) v ## U\n"
"#  undef __int32_c_suffix\n"
"#  undef __int16_c_suffix\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT56_C_SUFFIX__ */\n"
"#endif /* __INT56_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT48_TYPE__\n"
"# ifdef __INT48_C_SUFFIX__\n"
"#  define INT48_C(v) __int_c(v, __INT48_C_SUFFIX__)\n"
"#  define UINT48_C(v) __uint_c(v, __INT48_C_SUFFIX__)\n"
"#  define __int32_c_suffix __INT48_C_SUFFIX__\n"
"#  define __int16_c_suffix __INT48_C_SUFFIX__\n"
"#  define __int8_c_suffix  __INT48_C_SUFFIX__\n"
"# else\n"
"#  define INT48_C(v) v\n"
"#  define UINT48_C(v) v ## U\n"
"#  undef __int32_c_suffix\n"
"#  undef __int16_c_suffix\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT48_C_SUFFIX__ */\n"
"#endif /* __INT48_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT40_TYPE__\n"
"# ifdef __INT40_C_SUFFIX__\n"
"#  define INT40_C(v) __int_c(v, __INT40_C_SUFFIX__)\n"
"#  define UINT40_C(v) __uint_c(v, __INT40_C_SUFFIX__)\n"
"#  define __int32_c_suffix __INT40_C_SUFFIX__\n"
"#  define __int16_c_suffix __INT40_C_SUFFIX__\n"
"#  define __int8_c_suffix  __INT40_C_SUFFIX__\n"
"# else\n"
"#  define INT40_C(v) v\n"
"#  define UINT40_C(v) v ## U\n"
"#  undef __int32_c_suffix\n"
"#  undef __int16_c_suffix\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT40_C_SUFFIX__ */\n"
"#endif /* __INT40_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT32_TYPE__\n"
"# ifdef __INT32_C_SUFFIX__\n"
"#  define __int32_c_suffix __INT32_C_SUFFIX__\n"
"#  define __int16_c_suffix __INT32_C_SUFFIX__\n"
"#  define __int8_c_suffix  __INT32_C_SUFFIX__\n"
"#else\n"
"#  undef __int32_c_suffix\n"
"#  undef __int16_c_suffix\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT32_C_SUFFIX__ */\n"
"#endif /* __INT32_TYPE__ */\n"
"\n"
"#ifdef __int_least32_t\n"
"# ifdef __int32_c_suffix\n"
"#  define INT32_C(v) __int_c(v, __int32_c_suffix)\n"
"#  define UINT32_C(v) __uint_c(v, __int32_c_suffix)\n"
"# else\n"
"#  define INT32_C(v) v\n"
"#  define UINT32_C(v) v ## U\n"
"# endif /* __int32_c_suffix */\n"
"#endif /* __int_least32_t */\n"
"\n"
"\n"
"#ifdef __INT24_TYPE__\n"
"# ifdef __INT24_C_SUFFIX__\n"
"#  define INT24_C(v) __int_c(v, __INT24_C_SUFFIX__)\n"
"#  define UINT24_C(v) __uint_c(v, __INT24_C_SUFFIX__)\n"
"#  define __int16_c_suffix __INT24_C_SUFFIX__\n"
"#  define __int8_c_suffix  __INT24_C_SUFFIX__\n"
"# else\n"
"#  define INT24_C(v) v\n"
"#  define UINT24_C(v) v ## U\n"
"#  undef __int16_c_suffix\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT24_C_SUFFIX__ */\n"
"#endif /* __INT24_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT16_TYPE__\n"
"# ifdef __INT16_C_SUFFIX__\n"
"#  define __int16_c_suffix __INT16_C_SUFFIX__\n"
"#  define __int8_c_suffix  __INT16_C_SUFFIX__\n"
"#else\n"
"#  undef __int16_c_suffix\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT16_C_SUFFIX__ */\n"
"#endif /* __INT16_TYPE__ */\n"
"\n"
"#ifdef __int_least16_t\n"
"# ifdef __int16_c_suffix\n"
"#  define INT16_C(v) __int_c(v, __int16_c_suffix)\n"
"#  define UINT16_C(v) __uint_c(v, __int16_c_suffix)\n"
"# else\n"
"#  define INT16_C(v) v\n"
"#  define UINT16_C(v) v ## U\n"
"# endif /* __int16_c_suffix */\n"
"#endif /* __int_least16_t */\n"
"\n"
"\n"
"#ifdef __INT8_TYPE__\n"
"# ifdef __INT8_C_SUFFIX__\n"
"#  define __int8_c_suffix __INT8_C_SUFFIX__\n"
"#else\n"
"#  undef  __int8_c_suffix\n"
"# endif /* __INT8_C_SUFFIX__ */\n"
"#endif /* __INT8_TYPE__ */\n"
"\n"
"#ifdef __int_least8_t\n"
"# ifdef __int8_c_suffix\n"
"#  define INT8_C(v) __int_c(v, __int8_c_suffix)\n"
"#  define UINT8_C(v) __uint_c(v, __int8_c_suffix)\n"
"# else\n"
"#  define INT8_C(v) v\n"
"#  define UINT8_C(v) v ## U\n"
"# endif /* __int8_c_suffix */\n"
"#endif /* __int_least8_t */\n"
"\n"
"\n"
"/* C99 7.18.2.1 Limits of exact-width integer types.\n"
" * C99 7.18.2.2 Limits of minimum-width integer types.\n"
" * C99 7.18.2.3 Limits of fastest minimum-width integer types.\n"
" *\n"
" * The presence of limit macros are completely optional in C99.  This\n"
" * implementation defines limits for all of the types (exact- and\n"
" * minimum-width) that it defines above, using the limits of the minimum-width\n"
" * type for any types that do not have exact-width representations.\n"
" *\n"
" * As in the type definitions, this section takes an approach of\n"
" * successive-shrinking to determine which limits to use for the standard (8,\n"
" * 16, 32, 64) bit widths when they don't have exact representations. It is\n"
" * therefore important that the definitions be kept in order of decending\n"
" * widths.\n"
" *\n"
" * Note that C++ should not check __STDC_LIMIT_MACROS here, contrary to the\n"
" * claims of the C standard (see C++ 18.3.1p2, [cstdint.syn]).\n"
" */\n"
"\n"
"#ifdef __INT64_TYPE__\n"
"# define INT64_MAX           INT64_C( 9223372036854775807)\n"
"# define INT64_MIN         (-INT64_C( 9223372036854775807)-1)\n"
"# define UINT64_MAX         UINT64_C(18446744073709551615)\n"
"# define __INT_LEAST64_MIN   INT64_MIN\n"
"# define __INT_LEAST64_MAX   INT64_MAX\n"
"# define __UINT_LEAST64_MAX UINT64_MAX\n"
"# define __INT_LEAST32_MIN   INT64_MIN\n"
"# define __INT_LEAST32_MAX   INT64_MAX\n"
"# define __UINT_LEAST32_MAX UINT64_MAX\n"
"# define __INT_LEAST16_MIN   INT64_MIN\n"
"# define __INT_LEAST16_MAX   INT64_MAX\n"
"# define __UINT_LEAST16_MAX UINT64_MAX\n"
"# define __INT_LEAST8_MIN    INT64_MIN\n"
"# define __INT_LEAST8_MAX    INT64_MAX\n"
"# define __UINT_LEAST8_MAX  UINT64_MAX\n"
"#endif /* __INT64_TYPE__ */\n"
"\n"
"#ifdef __INT_LEAST64_MIN\n"
"# define INT_LEAST64_MIN   __INT_LEAST64_MIN\n"
"# define INT_LEAST64_MAX   __INT_LEAST64_MAX\n"
"# define UINT_LEAST64_MAX __UINT_LEAST64_MAX\n"
"# define INT_FAST64_MIN    __INT_LEAST64_MIN\n"
"# define INT_FAST64_MAX    __INT_LEAST64_MAX\n"
"# define UINT_FAST64_MAX  __UINT_LEAST64_MAX\n"
"#endif /* __INT_LEAST64_MIN */\n"
"\n"
"\n"
"#ifdef __INT56_TYPE__\n"
"# define INT56_MAX           INT56_C(36028797018963967)\n"
"# define INT56_MIN         (-INT56_C(36028797018963967)-1)\n"
"# define UINT56_MAX         UINT56_C(72057594037927935)\n"
"# define INT_LEAST56_MIN     INT56_MIN\n"
"# define INT_LEAST56_MAX     INT56_MAX\n"
"# define UINT_LEAST56_MAX   UINT56_MAX\n"
"# define INT_FAST56_MIN      INT56_MIN\n"
"# define INT_FAST56_MAX      INT56_MAX\n"
"# define UINT_FAST56_MAX    UINT56_MAX\n"
"# define __INT_LEAST32_MIN   INT56_MIN\n"
"# define __INT_LEAST32_MAX   INT56_MAX\n"
"# define __UINT_LEAST32_MAX UINT56_MAX\n"
"# define __INT_LEAST16_MIN   INT56_MIN\n"
"# define __INT_LEAST16_MAX   INT56_MAX\n"
"# define __UINT_LEAST16_MAX UINT56_MAX\n"
"# define __INT_LEAST8_MIN    INT56_MIN\n"
"# define __INT_LEAST8_MAX    INT56_MAX\n"
"# define __UINT_LEAST8_MAX  UINT56_MAX\n"
"#endif /* __INT56_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT48_TYPE__\n"
"# define INT48_MAX           INT48_C(140737488355327)\n"
"# define INT48_MIN         (-INT48_C(140737488355327)-1)\n"
"# define UINT48_MAX         UINT48_C(281474976710655)\n"
"# define INT_LEAST48_MIN     INT48_MIN\n"
"# define INT_LEAST48_MAX     INT48_MAX\n"
"# define UINT_LEAST48_MAX   UINT48_MAX\n"
"# define INT_FAST48_MIN      INT48_MIN\n"
"# define INT_FAST48_MAX      INT48_MAX\n"
"# define UINT_FAST48_MAX    UINT48_MAX\n"
"# define __INT_LEAST32_MIN   INT48_MIN\n"
"# define __INT_LEAST32_MAX   INT48_MAX\n"
"# define __UINT_LEAST32_MAX UINT48_MAX\n"
"# define __INT_LEAST16_MIN   INT48_MIN\n"
"# define __INT_LEAST16_MAX   INT48_MAX\n"
"# define __UINT_LEAST16_MAX UINT48_MAX\n"
"# define __INT_LEAST8_MIN    INT48_MIN\n"
"# define __INT_LEAST8_MAX    INT48_MAX\n"
"# define __UINT_LEAST8_MAX  UINT48_MAX\n"
"#endif /* __INT48_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT40_TYPE__\n"
"# define INT40_MAX           INT40_C(549755813887)\n"
"# define INT40_MIN         (-INT40_C(549755813887)-1)\n"
"# define UINT40_MAX         UINT40_C(1099511627775)\n"
"# define INT_LEAST40_MIN     INT40_MIN\n"
"# define INT_LEAST40_MAX     INT40_MAX\n"
"# define UINT_LEAST40_MAX   UINT40_MAX\n"
"# define INT_FAST40_MIN      INT40_MIN\n"
"# define INT_FAST40_MAX      INT40_MAX\n"
"# define UINT_FAST40_MAX    UINT40_MAX\n"
"# define __INT_LEAST32_MIN   INT40_MIN\n"
"# define __INT_LEAST32_MAX   INT40_MAX\n"
"# define __UINT_LEAST32_MAX UINT40_MAX\n"
"# define __INT_LEAST16_MIN   INT40_MIN\n"
"# define __INT_LEAST16_MAX   INT40_MAX\n"
"# define __UINT_LEAST16_MAX UINT40_MAX\n"
"# define __INT_LEAST8_MIN    INT40_MIN\n"
"# define __INT_LEAST8_MAX    INT40_MAX\n"
"# define __UINT_LEAST8_MAX  UINT40_MAX\n"
"#endif /* __INT40_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT32_TYPE__\n"
"# define INT32_MAX           INT32_C(2147483647)\n"
"# define INT32_MIN         (-INT32_C(2147483647)-1)\n"
"# define UINT32_MAX         UINT32_C(4294967295)\n"
"# define __INT_LEAST32_MIN   INT32_MIN\n"
"# define __INT_LEAST32_MAX   INT32_MAX\n"
"# define __UINT_LEAST32_MAX UINT32_MAX\n"
"# define __INT_LEAST16_MIN   INT32_MIN\n"
"# define __INT_LEAST16_MAX   INT32_MAX\n"
"# define __UINT_LEAST16_MAX UINT32_MAX\n"
"# define __INT_LEAST8_MIN    INT32_MIN\n"
"# define __INT_LEAST8_MAX    INT32_MAX\n"
"# define __UINT_LEAST8_MAX  UINT32_MAX\n"
"#endif /* __INT32_TYPE__ */\n"
"\n"
"#ifdef __INT_LEAST32_MIN\n"
"# define INT_LEAST32_MIN   __INT_LEAST32_MIN\n"
"# define INT_LEAST32_MAX   __INT_LEAST32_MAX\n"
"# define UINT_LEAST32_MAX __UINT_LEAST32_MAX\n"
"# define INT_FAST32_MIN    __INT_LEAST32_MIN\n"
"# define INT_FAST32_MAX    __INT_LEAST32_MAX\n"
"# define UINT_FAST32_MAX  __UINT_LEAST32_MAX\n"
"#endif /* __INT_LEAST32_MIN */\n"
"\n"
"\n"
"#ifdef __INT24_TYPE__\n"
"# define INT24_MAX           INT24_C(8388607)\n"
"# define INT24_MIN         (-INT24_C(8388607)-1)\n"
"# define UINT24_MAX         UINT24_C(16777215)\n"
"# define INT_LEAST24_MIN     INT24_MIN\n"
"# define INT_LEAST24_MAX     INT24_MAX\n"
"# define UINT_LEAST24_MAX   UINT24_MAX\n"
"# define INT_FAST24_MIN      INT24_MIN\n"
"# define INT_FAST24_MAX      INT24_MAX\n"
"# define UINT_FAST24_MAX    UINT24_MAX\n"
"# define __INT_LEAST16_MIN   INT24_MIN\n"
"# define __INT_LEAST16_MAX   INT24_MAX\n"
"# define __UINT_LEAST16_MAX UINT24_MAX\n"
"# define __INT_LEAST8_MIN    INT24_MIN\n"
"# define __INT_LEAST8_MAX    INT24_MAX\n"
"# define __UINT_LEAST8_MAX  UINT24_MAX\n"
"#endif /* __INT24_TYPE__ */\n"
"\n"
"\n"
"#ifdef __INT16_TYPE__\n"
"#define INT16_MAX            INT16_C(32767)\n"
"#define INT16_MIN          (-INT16_C(32767)-1)\n"
"#define UINT16_MAX          UINT16_C(65535)\n"
"# define __INT_LEAST16_MIN   INT16_MIN\n"
"# define __INT_LEAST16_MAX   INT16_MAX\n"
"# define __UINT_LEAST16_MAX UINT16_MAX\n"
"# define __INT_LEAST8_MIN    INT16_MIN\n"
"# define __INT_LEAST8_MAX    INT16_MAX\n"
"# define __UINT_LEAST8_MAX  UINT16_MAX\n"
"#endif /* __INT16_TYPE__ */\n"
"\n"
"#ifdef __INT_LEAST16_MIN\n"
"# define INT_LEAST16_MIN   __INT_LEAST16_MIN\n"
"# define INT_LEAST16_MAX   __INT_LEAST16_MAX\n"
"# define UINT_LEAST16_MAX __UINT_LEAST16_MAX\n"
"# define INT_FAST16_MIN    __INT_LEAST16_MIN\n"
"# define INT_FAST16_MAX    __INT_LEAST16_MAX\n"
"# define UINT_FAST16_MAX  __UINT_LEAST16_MAX\n"
"#endif /* __INT_LEAST16_MIN */\n"
"\n"
"\n"
"#ifdef __INT8_TYPE__\n"
"# define INT8_MAX            INT8_C(127)\n"
"# define INT8_MIN          (-INT8_C(127)-1)\n"
"# define UINT8_MAX          UINT8_C(255)\n"
"# define __INT_LEAST8_MIN    INT8_MIN\n"
"# define __INT_LEAST8_MAX    INT8_MAX\n"
"# define __UINT_LEAST8_MAX  UINT8_MAX\n"
"#endif /* __INT8_TYPE__ */\n"
"\n"
"#ifdef __INT_LEAST8_MIN\n"
"# define INT_LEAST8_MIN   __INT_LEAST8_MIN\n"
"# define INT_LEAST8_MAX   __INT_LEAST8_MAX\n"
"# define UINT_LEAST8_MAX __UINT_LEAST8_MAX\n"
"# define INT_FAST8_MIN    __INT_LEAST8_MIN\n"
"# define INT_FAST8_MAX    __INT_LEAST8_MAX\n"
"# define UINT_FAST8_MAX  __UINT_LEAST8_MAX\n"
"#endif /* __INT_LEAST8_MIN */\n"
"\n"
"/* Some utility macros */\n"
"#define  __INTN_MIN(n)  __stdint_join3( INT, n, _MIN)\n"
"#define  __INTN_MAX(n)  __stdint_join3( INT, n, _MAX)\n"
"#define __UINTN_MAX(n)  __stdint_join3(UINT, n, _MAX)\n"
"#define  __INTN_C(n, v) __stdint_join3( INT, n, _C(v))\n"
"#define __UINTN_C(n, v) __stdint_join3(UINT, n, _C(v))\n"
"\n"
"/* C99 7.18.2.4 Limits of integer types capable of holding object pointers. */\n"
"/* C99 7.18.3 Limits of other integer types. */\n"
"\n"
"#define  INTPTR_MIN  (-__INTPTR_MAX__-1)\n"
"#define  INTPTR_MAX    __INTPTR_MAX__\n"
"#define UINTPTR_MAX   __UINTPTR_MAX__\n"
"#define PTRDIFF_MIN (-__PTRDIFF_MAX__-1)\n"
"#define PTRDIFF_MAX   __PTRDIFF_MAX__\n"
"#define    SIZE_MAX      __SIZE_MAX__\n"
"\n"
"/* ISO9899:2011 7.20 (C11 Annex K): Define RSIZE_MAX if __STDC_WANT_LIB_EXT1__\n"
" * is enabled. */\n"
"#if defined(__STDC_WANT_LIB_EXT1__) && __STDC_WANT_LIB_EXT1__ >= 1\n"
"#define   RSIZE_MAX            (SIZE_MAX >> 1)\n"
"#endif\n"
"\n"
"/* C99 7.18.2.5 Limits of greatest-width integer types. */\n"
"#define  INTMAX_MIN (-__INTMAX_MAX__-1)\n"
"#define  INTMAX_MAX   __INTMAX_MAX__\n"
"#define UINTMAX_MAX  __UINTMAX_MAX__\n"
"\n"
"/* C99 7.18.3 Limits of other integer types. */\n"
"#define SIG_ATOMIC_MIN __INTN_MIN(__SIG_ATOMIC_WIDTH__)\n"
"#define SIG_ATOMIC_MAX __INTN_MAX(__SIG_ATOMIC_WIDTH__)\n"
"#ifdef __WINT_UNSIGNED__\n"
"# define WINT_MIN       __UINTN_C(__WINT_WIDTH__, 0)\n"
"# define WINT_MAX       __UINTN_MAX(__WINT_WIDTH__)\n"
"#else\n"
"# define WINT_MIN       __INTN_MIN(__WINT_WIDTH__)\n"
"# define WINT_MAX       __INTN_MAX(__WINT_WIDTH__)\n"
"#endif\n"
"\n"
"#ifndef WCHAR_MAX\n"
"# define WCHAR_MAX __WCHAR_MAX__\n"
"#endif\n"
"#ifndef WCHAR_MIN\n"
"# if __WCHAR_MAX__ == __INTN_MAX(__WCHAR_WIDTH__)\n"
"#  define WCHAR_MIN __INTN_MIN(__WCHAR_WIDTH__)\n"
"# else\n"
"#  define WCHAR_MIN __UINTN_C(__WCHAR_WIDTH__, 0)\n"
"# endif\n"
"#endif\n"
"\n"
"/* 7.18.4.2 Macros for greatest-width integer constants. */\n"
"#define  INTMAX_C(v) __int_c(v,  __INTMAX_C_SUFFIX__)\n"
"#define UINTMAX_C(v) __int_c(v, __UINTMAX_C_SUFFIX__)\n"
"\n"
"#endif /* __CLANG_STDINT_H2 */\n"
"#endif /* __STDC_HOSTED__ */\n"
"" } , 
 { "/builtins/stdnoreturn.h" , "/*===---- stdnoreturn.h - Standard header for noreturn macro ---------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __STDNORETURN_H\n"
"#define __STDNORETURN_H\n"
"\n"
"#define noreturn _Noreturn\n"
"#define __noreturn_is_defined 1\n"
"\n"
"#endif /* __STDNORETURN_H */\n"
"" } , 
 { "/builtins/tbmintrin.h" , "/*===---- tbmintrin.h - TBM intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __X86INTRIN_H\n"
"#error \"Never use <tbmintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __TBMINTRIN_H\n"
"#define __TBMINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"tbm\")))\n"
"\n"
"#define __bextri_u32(a, b) \\\n"
"  ((unsigned int)__builtin_ia32_bextri_u32((unsigned int)(a), \\\n"
"                                           (unsigned int)(b)))\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blcfill_u32(unsigned int __a)\n"
"{\n"
"  return __a & (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blci_u32(unsigned int __a)\n"
"{\n"
"  return __a | ~(__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blcic_u32(unsigned int __a)\n"
"{\n"
"  return ~__a & (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blcmsk_u32(unsigned int __a)\n"
"{\n"
"  return __a ^ (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blcs_u32(unsigned int __a)\n"
"{\n"
"  return __a | (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blsfill_u32(unsigned int __a)\n"
"{\n"
"  return __a | (__a - 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__blsic_u32(unsigned int __a)\n"
"{\n"
"  return ~__a | (__a - 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__t1mskc_u32(unsigned int __a)\n"
"{\n"
"  return ~__a | (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned int __DEFAULT_FN_ATTRS\n"
"__tzmsk_u32(unsigned int __a)\n"
"{\n"
"  return ~__a & (__a - 1);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"#define __bextri_u64(a, b) \\\n"
"  ((unsigned long long)__builtin_ia32_bextri_u64((unsigned long long)(a), \\\n"
"                                                 (unsigned long long)(b)))\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blcfill_u64(unsigned long long __a)\n"
"{\n"
"  return __a & (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blci_u64(unsigned long long __a)\n"
"{\n"
"  return __a | ~(__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blcic_u64(unsigned long long __a)\n"
"{\n"
"  return ~__a & (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blcmsk_u64(unsigned long long __a)\n"
"{\n"
"  return __a ^ (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blcs_u64(unsigned long long __a)\n"
"{\n"
"  return __a | (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blsfill_u64(unsigned long long __a)\n"
"{\n"
"  return __a | (__a - 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__blsic_u64(unsigned long long __a)\n"
"{\n"
"  return ~__a | (__a - 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__t1mskc_u64(unsigned long long __a)\n"
"{\n"
"  return ~__a | (__a + 1);\n"
"}\n"
"\n"
"static __inline__ unsigned long long __DEFAULT_FN_ATTRS\n"
"__tzmsk_u64(unsigned long long __a)\n"
"{\n"
"  return ~__a & (__a - 1);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __TBMINTRIN_H */\n"
"" } , 
 { "/builtins/tgmath.h" , "/*===---- tgmath.h - Standard header for type generic math ----------------===*\\\n"
" *\n"
" * Copyright (c) 2009 Howard Hinnant\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
"\\*===----------------------------------------------------------------------===*/\n"
"\n"
"#ifndef __CLANG_TGMATH_H\n"
"#define __CLANG_TGMATH_H\n"
"\n"
"/* C99 7.22 Type-generic math <tgmath.h>. */\n"
"#include <math.h>\n"
"\n"
"/*\n"
" * Allow additional definitions and implementation-defined values on Apple\n"
" * platforms. This is done after #include <math.h> to avoid depcycle conflicts\n"
" * between libcxx and darwin in C++ modules builds.\n"
" */\n"
"#if defined(__APPLE__) && __STDC_HOSTED__ && __has_include_next(<tgmath.h>)\n"
"#  include_next <tgmath.h>\n"
"#else\n"
"\n"
"/* C++ handles type genericity with overloading in math.h. */\n"
"#ifndef __cplusplus\n"
"#include <complex.h>\n"
"\n"
"#define _TG_ATTRSp __attribute__((__overloadable__))\n"
"#define _TG_ATTRS __attribute__((__overloadable__, __always_inline__))\n"
"\n"
"// promotion\n"
"\n"
"typedef void _Argument_type_is_not_arithmetic;\n"
"static _Argument_type_is_not_arithmetic __tg_promote(...)\n"
"  __attribute__((__unavailable__,__overloadable__));\n"
"static double               _TG_ATTRSp __tg_promote(int);\n"
"static double               _TG_ATTRSp __tg_promote(unsigned int);\n"
"static double               _TG_ATTRSp __tg_promote(long);\n"
"static double               _TG_ATTRSp __tg_promote(unsigned long);\n"
"static double               _TG_ATTRSp __tg_promote(long long);\n"
"static double               _TG_ATTRSp __tg_promote(unsigned long long);\n"
"static float                _TG_ATTRSp __tg_promote(float);\n"
"static double               _TG_ATTRSp __tg_promote(double);\n"
"static long double          _TG_ATTRSp __tg_promote(long double);\n"
"static float _Complex       _TG_ATTRSp __tg_promote(float _Complex);\n"
"static double _Complex      _TG_ATTRSp __tg_promote(double _Complex);\n"
"static long double _Complex _TG_ATTRSp __tg_promote(long double _Complex);\n"
"\n"
"#define __tg_promote1(__x)           (__typeof__(__tg_promote(__x)))\n"
"#define __tg_promote2(__x, __y)      (__typeof__(__tg_promote(__x) + \\\n"
"                                                 __tg_promote(__y)))\n"
"#define __tg_promote3(__x, __y, __z) (__typeof__(__tg_promote(__x) + \\\n"
"                                                 __tg_promote(__y) + \\\n"
"                                                 __tg_promote(__z)))\n"
"\n"
"// acos\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_acos(float __x) {return acosf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_acos(double __x) {return acos(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_acos(long double __x) {return acosl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_acos(float _Complex __x) {return cacosf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_acos(double _Complex __x) {return cacos(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_acos(long double _Complex __x) {return cacosl(__x);}\n"
"\n"
"#undef acos\n"
"#define acos(__x) __tg_acos(__tg_promote1((__x))(__x))\n"
"\n"
"// asin\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_asin(float __x) {return asinf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_asin(double __x) {return asin(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_asin(long double __x) {return asinl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_asin(float _Complex __x) {return casinf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_asin(double _Complex __x) {return casin(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_asin(long double _Complex __x) {return casinl(__x);}\n"
"\n"
"#undef asin\n"
"#define asin(__x) __tg_asin(__tg_promote1((__x))(__x))\n"
"\n"
"// atan\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_atan(float __x) {return atanf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_atan(double __x) {return atan(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_atan(long double __x) {return atanl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_atan(float _Complex __x) {return catanf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_atan(double _Complex __x) {return catan(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_atan(long double _Complex __x) {return catanl(__x);}\n"
"\n"
"#undef atan\n"
"#define atan(__x) __tg_atan(__tg_promote1((__x))(__x))\n"
"\n"
"// acosh\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_acosh(float __x) {return acoshf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_acosh(double __x) {return acosh(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_acosh(long double __x) {return acoshl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_acosh(float _Complex __x) {return cacoshf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_acosh(double _Complex __x) {return cacosh(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_acosh(long double _Complex __x) {return cacoshl(__x);}\n"
"\n"
"#undef acosh\n"
"#define acosh(__x) __tg_acosh(__tg_promote1((__x))(__x))\n"
"\n"
"// asinh\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_asinh(float __x) {return asinhf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_asinh(double __x) {return asinh(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_asinh(long double __x) {return asinhl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_asinh(float _Complex __x) {return casinhf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_asinh(double _Complex __x) {return casinh(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_asinh(long double _Complex __x) {return casinhl(__x);}\n"
"\n"
"#undef asinh\n"
"#define asinh(__x) __tg_asinh(__tg_promote1((__x))(__x))\n"
"\n"
"// atanh\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_atanh(float __x) {return atanhf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_atanh(double __x) {return atanh(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_atanh(long double __x) {return atanhl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_atanh(float _Complex __x) {return catanhf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_atanh(double _Complex __x) {return catanh(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_atanh(long double _Complex __x) {return catanhl(__x);}\n"
"\n"
"#undef atanh\n"
"#define atanh(__x) __tg_atanh(__tg_promote1((__x))(__x))\n"
"\n"
"// cos\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_cos(float __x) {return cosf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_cos(double __x) {return cos(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_cos(long double __x) {return cosl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cos(float _Complex __x) {return ccosf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cos(double _Complex __x) {return ccos(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cos(long double _Complex __x) {return ccosl(__x);}\n"
"\n"
"#undef cos\n"
"#define cos(__x) __tg_cos(__tg_promote1((__x))(__x))\n"
"\n"
"// sin\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_sin(float __x) {return sinf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_sin(double __x) {return sin(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_sin(long double __x) {return sinl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sin(float _Complex __x) {return csinf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sin(double _Complex __x) {return csin(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sin(long double _Complex __x) {return csinl(__x);}\n"
"\n"
"#undef sin\n"
"#define sin(__x) __tg_sin(__tg_promote1((__x))(__x))\n"
"\n"
"// tan\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_tan(float __x) {return tanf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_tan(double __x) {return tan(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_tan(long double __x) {return tanl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_tan(float _Complex __x) {return ctanf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_tan(double _Complex __x) {return ctan(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_tan(long double _Complex __x) {return ctanl(__x);}\n"
"\n"
"#undef tan\n"
"#define tan(__x) __tg_tan(__tg_promote1((__x))(__x))\n"
"\n"
"// cosh\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_cosh(float __x) {return coshf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_cosh(double __x) {return cosh(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_cosh(long double __x) {return coshl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cosh(float _Complex __x) {return ccoshf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cosh(double _Complex __x) {return ccosh(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cosh(long double _Complex __x) {return ccoshl(__x);}\n"
"\n"
"#undef cosh\n"
"#define cosh(__x) __tg_cosh(__tg_promote1((__x))(__x))\n"
"\n"
"// sinh\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_sinh(float __x) {return sinhf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_sinh(double __x) {return sinh(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_sinh(long double __x) {return sinhl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sinh(float _Complex __x) {return csinhf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sinh(double _Complex __x) {return csinh(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sinh(long double _Complex __x) {return csinhl(__x);}\n"
"\n"
"#undef sinh\n"
"#define sinh(__x) __tg_sinh(__tg_promote1((__x))(__x))\n"
"\n"
"// tanh\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_tanh(float __x) {return tanhf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_tanh(double __x) {return tanh(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_tanh(long double __x) {return tanhl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_tanh(float _Complex __x) {return ctanhf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_tanh(double _Complex __x) {return ctanh(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_tanh(long double _Complex __x) {return ctanhl(__x);}\n"
"\n"
"#undef tanh\n"
"#define tanh(__x) __tg_tanh(__tg_promote1((__x))(__x))\n"
"\n"
"// exp\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_exp(float __x) {return expf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_exp(double __x) {return exp(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_exp(long double __x) {return expl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_exp(float _Complex __x) {return cexpf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_exp(double _Complex __x) {return cexp(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_exp(long double _Complex __x) {return cexpl(__x);}\n"
"\n"
"#undef exp\n"
"#define exp(__x) __tg_exp(__tg_promote1((__x))(__x))\n"
"\n"
"// log\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_log(float __x) {return logf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_log(double __x) {return log(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_log(long double __x) {return logl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_log(float _Complex __x) {return clogf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_log(double _Complex __x) {return clog(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_log(long double _Complex __x) {return clogl(__x);}\n"
"\n"
"#undef log\n"
"#define log(__x) __tg_log(__tg_promote1((__x))(__x))\n"
"\n"
"// pow\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_pow(float __x, float __y) {return powf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_pow(double __x, double __y) {return pow(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_pow(long double __x, long double __y) {return powl(__x, __y);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_pow(float _Complex __x, float _Complex __y) {return cpowf(__x, __y);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_pow(double _Complex __x, double _Complex __y) {return cpow(__x, __y);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_pow(long double _Complex __x, long double _Complex __y)\n"
"    {return cpowl(__x, __y);}\n"
"\n"
"#undef pow\n"
"#define pow(__x, __y) __tg_pow(__tg_promote2((__x), (__y))(__x), \\\n"
"                               __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// sqrt\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_sqrt(float __x) {return sqrtf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_sqrt(double __x) {return sqrt(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_sqrt(long double __x) {return sqrtl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sqrt(float _Complex __x) {return csqrtf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sqrt(double _Complex __x) {return csqrt(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_sqrt(long double _Complex __x) {return csqrtl(__x);}\n"
"\n"
"#undef sqrt\n"
"#define sqrt(__x) __tg_sqrt(__tg_promote1((__x))(__x))\n"
"\n"
"// fabs\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_fabs(float __x) {return fabsf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_fabs(double __x) {return fabs(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_fabs(long double __x) {return fabsl(__x);}\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_fabs(float _Complex __x) {return cabsf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_fabs(double _Complex __x) {return cabs(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_fabs(long double _Complex __x) {return cabsl(__x);}\n"
"\n"
"#undef fabs\n"
"#define fabs(__x) __tg_fabs(__tg_promote1((__x))(__x))\n"
"\n"
"// atan2\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_atan2(float __x, float __y) {return atan2f(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_atan2(double __x, double __y) {return atan2(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_atan2(long double __x, long double __y) {return atan2l(__x, __y);}\n"
"\n"
"#undef atan2\n"
"#define atan2(__x, __y) __tg_atan2(__tg_promote2((__x), (__y))(__x), \\\n"
"                                   __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// cbrt\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_cbrt(float __x) {return cbrtf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_cbrt(double __x) {return cbrt(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_cbrt(long double __x) {return cbrtl(__x);}\n"
"\n"
"#undef cbrt\n"
"#define cbrt(__x) __tg_cbrt(__tg_promote1((__x))(__x))\n"
"\n"
"// ceil\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_ceil(float __x) {return ceilf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_ceil(double __x) {return ceil(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_ceil(long double __x) {return ceill(__x);}\n"
"\n"
"#undef ceil\n"
"#define ceil(__x) __tg_ceil(__tg_promote1((__x))(__x))\n"
"\n"
"// copysign\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_copysign(float __x, float __y) {return copysignf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_copysign(double __x, double __y) {return copysign(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_copysign(long double __x, long double __y) {return copysignl(__x, __y);}\n"
"\n"
"#undef copysign\n"
"#define copysign(__x, __y) __tg_copysign(__tg_promote2((__x), (__y))(__x), \\\n"
"                                         __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// erf\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_erf(float __x) {return erff(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_erf(double __x) {return erf(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_erf(long double __x) {return erfl(__x);}\n"
"\n"
"#undef erf\n"
"#define erf(__x) __tg_erf(__tg_promote1((__x))(__x))\n"
"\n"
"// erfc\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_erfc(float __x) {return erfcf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_erfc(double __x) {return erfc(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_erfc(long double __x) {return erfcl(__x);}\n"
"\n"
"#undef erfc\n"
"#define erfc(__x) __tg_erfc(__tg_promote1((__x))(__x))\n"
"\n"
"// exp2\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_exp2(float __x) {return exp2f(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_exp2(double __x) {return exp2(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_exp2(long double __x) {return exp2l(__x);}\n"
"\n"
"#undef exp2\n"
"#define exp2(__x) __tg_exp2(__tg_promote1((__x))(__x))\n"
"\n"
"// expm1\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_expm1(float __x) {return expm1f(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_expm1(double __x) {return expm1(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_expm1(long double __x) {return expm1l(__x);}\n"
"\n"
"#undef expm1\n"
"#define expm1(__x) __tg_expm1(__tg_promote1((__x))(__x))\n"
"\n"
"// fdim\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_fdim(float __x, float __y) {return fdimf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_fdim(double __x, double __y) {return fdim(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_fdim(long double __x, long double __y) {return fdiml(__x, __y);}\n"
"\n"
"#undef fdim\n"
"#define fdim(__x, __y) __tg_fdim(__tg_promote2((__x), (__y))(__x), \\\n"
"                                 __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// floor\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_floor(float __x) {return floorf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_floor(double __x) {return floor(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_floor(long double __x) {return floorl(__x);}\n"
"\n"
"#undef floor\n"
"#define floor(__x) __tg_floor(__tg_promote1((__x))(__x))\n"
"\n"
"// fma\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_fma(float __x, float __y, float __z)\n"
"    {return fmaf(__x, __y, __z);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_fma(double __x, double __y, double __z)\n"
"    {return fma(__x, __y, __z);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_fma(long double __x,long double __y, long double __z)\n"
"    {return fmal(__x, __y, __z);}\n"
"\n"
"#undef fma\n"
"#define fma(__x, __y, __z)                                \\\n"
"        __tg_fma(__tg_promote3((__x), (__y), (__z))(__x), \\\n"
"                 __tg_promote3((__x), (__y), (__z))(__y), \\\n"
"                 __tg_promote3((__x), (__y), (__z))(__z))\n"
"\n"
"// fmax\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_fmax(float __x, float __y) {return fmaxf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_fmax(double __x, double __y) {return fmax(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_fmax(long double __x, long double __y) {return fmaxl(__x, __y);}\n"
"\n"
"#undef fmax\n"
"#define fmax(__x, __y) __tg_fmax(__tg_promote2((__x), (__y))(__x), \\\n"
"                                 __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// fmin\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_fmin(float __x, float __y) {return fminf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_fmin(double __x, double __y) {return fmin(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_fmin(long double __x, long double __y) {return fminl(__x, __y);}\n"
"\n"
"#undef fmin\n"
"#define fmin(__x, __y) __tg_fmin(__tg_promote2((__x), (__y))(__x), \\\n"
"                                 __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// fmod\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_fmod(float __x, float __y) {return fmodf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_fmod(double __x, double __y) {return fmod(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_fmod(long double __x, long double __y) {return fmodl(__x, __y);}\n"
"\n"
"#undef fmod\n"
"#define fmod(__x, __y) __tg_fmod(__tg_promote2((__x), (__y))(__x), \\\n"
"                                 __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// frexp\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_frexp(float __x, int* __y) {return frexpf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_frexp(double __x, int* __y) {return frexp(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_frexp(long double __x, int* __y) {return frexpl(__x, __y);}\n"
"\n"
"#undef frexp\n"
"#define frexp(__x, __y) __tg_frexp(__tg_promote1((__x))(__x), __y)\n"
"\n"
"// hypot\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_hypot(float __x, float __y) {return hypotf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_hypot(double __x, double __y) {return hypot(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_hypot(long double __x, long double __y) {return hypotl(__x, __y);}\n"
"\n"
"#undef hypot\n"
"#define hypot(__x, __y) __tg_hypot(__tg_promote2((__x), (__y))(__x), \\\n"
"                                   __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// ilogb\n"
"\n"
"static int\n"
"    _TG_ATTRS\n"
"    __tg_ilogb(float __x) {return ilogbf(__x);}\n"
"\n"
"static int\n"
"    _TG_ATTRS\n"
"    __tg_ilogb(double __x) {return ilogb(__x);}\n"
"\n"
"static int\n"
"    _TG_ATTRS\n"
"    __tg_ilogb(long double __x) {return ilogbl(__x);}\n"
"\n"
"#undef ilogb\n"
"#define ilogb(__x) __tg_ilogb(__tg_promote1((__x))(__x))\n"
"\n"
"// ldexp\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_ldexp(float __x, int __y) {return ldexpf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_ldexp(double __x, int __y) {return ldexp(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_ldexp(long double __x, int __y) {return ldexpl(__x, __y);}\n"
"\n"
"#undef ldexp\n"
"#define ldexp(__x, __y) __tg_ldexp(__tg_promote1((__x))(__x), __y)\n"
"\n"
"// lgamma\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_lgamma(float __x) {return lgammaf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_lgamma(double __x) {return lgamma(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_lgamma(long double __x) {return lgammal(__x);}\n"
"\n"
"#undef lgamma\n"
"#define lgamma(__x) __tg_lgamma(__tg_promote1((__x))(__x))\n"
"\n"
"// llrint\n"
"\n"
"static long long\n"
"    _TG_ATTRS\n"
"    __tg_llrint(float __x) {return llrintf(__x);}\n"
"\n"
"static long long\n"
"    _TG_ATTRS\n"
"    __tg_llrint(double __x) {return llrint(__x);}\n"
"\n"
"static long long\n"
"    _TG_ATTRS\n"
"    __tg_llrint(long double __x) {return llrintl(__x);}\n"
"\n"
"#undef llrint\n"
"#define llrint(__x) __tg_llrint(__tg_promote1((__x))(__x))\n"
"\n"
"// llround\n"
"\n"
"static long long\n"
"    _TG_ATTRS\n"
"    __tg_llround(float __x) {return llroundf(__x);}\n"
"\n"
"static long long\n"
"    _TG_ATTRS\n"
"    __tg_llround(double __x) {return llround(__x);}\n"
"\n"
"static long long\n"
"    _TG_ATTRS\n"
"    __tg_llround(long double __x) {return llroundl(__x);}\n"
"\n"
"#undef llround\n"
"#define llround(__x) __tg_llround(__tg_promote1((__x))(__x))\n"
"\n"
"// log10\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_log10(float __x) {return log10f(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_log10(double __x) {return log10(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_log10(long double __x) {return log10l(__x);}\n"
"\n"
"#undef log10\n"
"#define log10(__x) __tg_log10(__tg_promote1((__x))(__x))\n"
"\n"
"// log1p\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_log1p(float __x) {return log1pf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_log1p(double __x) {return log1p(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_log1p(long double __x) {return log1pl(__x);}\n"
"\n"
"#undef log1p\n"
"#define log1p(__x) __tg_log1p(__tg_promote1((__x))(__x))\n"
"\n"
"// log2\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_log2(float __x) {return log2f(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_log2(double __x) {return log2(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_log2(long double __x) {return log2l(__x);}\n"
"\n"
"#undef log2\n"
"#define log2(__x) __tg_log2(__tg_promote1((__x))(__x))\n"
"\n"
"// logb\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_logb(float __x) {return logbf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_logb(double __x) {return logb(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_logb(long double __x) {return logbl(__x);}\n"
"\n"
"#undef logb\n"
"#define logb(__x) __tg_logb(__tg_promote1((__x))(__x))\n"
"\n"
"// lrint\n"
"\n"
"static long\n"
"    _TG_ATTRS\n"
"    __tg_lrint(float __x) {return lrintf(__x);}\n"
"\n"
"static long\n"
"    _TG_ATTRS\n"
"    __tg_lrint(double __x) {return lrint(__x);}\n"
"\n"
"static long\n"
"    _TG_ATTRS\n"
"    __tg_lrint(long double __x) {return lrintl(__x);}\n"
"\n"
"#undef lrint\n"
"#define lrint(__x) __tg_lrint(__tg_promote1((__x))(__x))\n"
"\n"
"// lround\n"
"\n"
"static long\n"
"    _TG_ATTRS\n"
"    __tg_lround(float __x) {return lroundf(__x);}\n"
"\n"
"static long\n"
"    _TG_ATTRS\n"
"    __tg_lround(double __x) {return lround(__x);}\n"
"\n"
"static long\n"
"    _TG_ATTRS\n"
"    __tg_lround(long double __x) {return lroundl(__x);}\n"
"\n"
"#undef lround\n"
"#define lround(__x) __tg_lround(__tg_promote1((__x))(__x))\n"
"\n"
"// nearbyint\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_nearbyint(float __x) {return nearbyintf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_nearbyint(double __x) {return nearbyint(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_nearbyint(long double __x) {return nearbyintl(__x);}\n"
"\n"
"#undef nearbyint\n"
"#define nearbyint(__x) __tg_nearbyint(__tg_promote1((__x))(__x))\n"
"\n"
"// nextafter\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_nextafter(float __x, float __y) {return nextafterf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_nextafter(double __x, double __y) {return nextafter(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_nextafter(long double __x, long double __y) {return nextafterl(__x, __y);}\n"
"\n"
"#undef nextafter\n"
"#define nextafter(__x, __y) __tg_nextafter(__tg_promote2((__x), (__y))(__x), \\\n"
"                                           __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// nexttoward\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_nexttoward(float __x, long double __y) {return nexttowardf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_nexttoward(double __x, long double __y) {return nexttoward(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_nexttoward(long double __x, long double __y) {return nexttowardl(__x, __y);}\n"
"\n"
"#undef nexttoward\n"
"#define nexttoward(__x, __y) __tg_nexttoward(__tg_promote1((__x))(__x), (__y))\n"
"\n"
"// remainder\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_remainder(float __x, float __y) {return remainderf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_remainder(double __x, double __y) {return remainder(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_remainder(long double __x, long double __y) {return remainderl(__x, __y);}\n"
"\n"
"#undef remainder\n"
"#define remainder(__x, __y) __tg_remainder(__tg_promote2((__x), (__y))(__x), \\\n"
"                                           __tg_promote2((__x), (__y))(__y))\n"
"\n"
"// remquo\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_remquo(float __x, float __y, int* __z)\n"
"    {return remquof(__x, __y, __z);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_remquo(double __x, double __y, int* __z)\n"
"    {return remquo(__x, __y, __z);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_remquo(long double __x,long double __y, int* __z)\n"
"    {return remquol(__x, __y, __z);}\n"
"\n"
"#undef remquo\n"
"#define remquo(__x, __y, __z)                         \\\n"
"        __tg_remquo(__tg_promote2((__x), (__y))(__x), \\\n"
"                    __tg_promote2((__x), (__y))(__y), \\\n"
"                    (__z))\n"
"\n"
"// rint\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_rint(float __x) {return rintf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_rint(double __x) {return rint(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_rint(long double __x) {return rintl(__x);}\n"
"\n"
"#undef rint\n"
"#define rint(__x) __tg_rint(__tg_promote1((__x))(__x))\n"
"\n"
"// round\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_round(float __x) {return roundf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_round(double __x) {return round(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_round(long double __x) {return roundl(__x);}\n"
"\n"
"#undef round\n"
"#define round(__x) __tg_round(__tg_promote1((__x))(__x))\n"
"\n"
"// scalbn\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_scalbn(float __x, int __y) {return scalbnf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_scalbn(double __x, int __y) {return scalbn(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_scalbn(long double __x, int __y) {return scalbnl(__x, __y);}\n"
"\n"
"#undef scalbn\n"
"#define scalbn(__x, __y) __tg_scalbn(__tg_promote1((__x))(__x), __y)\n"
"\n"
"// scalbln\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_scalbln(float __x, long __y) {return scalblnf(__x, __y);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_scalbln(double __x, long __y) {return scalbln(__x, __y);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_scalbln(long double __x, long __y) {return scalblnl(__x, __y);}\n"
"\n"
"#undef scalbln\n"
"#define scalbln(__x, __y) __tg_scalbln(__tg_promote1((__x))(__x), __y)\n"
"\n"
"// tgamma\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_tgamma(float __x) {return tgammaf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_tgamma(double __x) {return tgamma(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_tgamma(long double __x) {return tgammal(__x);}\n"
"\n"
"#undef tgamma\n"
"#define tgamma(__x) __tg_tgamma(__tg_promote1((__x))(__x))\n"
"\n"
"// trunc\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_trunc(float __x) {return truncf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_trunc(double __x) {return trunc(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_trunc(long double __x) {return truncl(__x);}\n"
"\n"
"#undef trunc\n"
"#define trunc(__x) __tg_trunc(__tg_promote1((__x))(__x))\n"
"\n"
"// carg\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_carg(float __x) {return atan2f(0.F, __x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_carg(double __x) {return atan2(0., __x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_carg(long double __x) {return atan2l(0.L, __x);}\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_carg(float _Complex __x) {return cargf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_carg(double _Complex __x) {return carg(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_carg(long double _Complex __x) {return cargl(__x);}\n"
"\n"
"#undef carg\n"
"#define carg(__x) __tg_carg(__tg_promote1((__x))(__x))\n"
"\n"
"// cimag\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_cimag(float __x) {return 0;}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_cimag(double __x) {return 0;}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_cimag(long double __x) {return 0;}\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_cimag(float _Complex __x) {return cimagf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_cimag(double _Complex __x) {return cimag(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_cimag(long double _Complex __x) {return cimagl(__x);}\n"
"\n"
"#undef cimag\n"
"#define cimag(__x) __tg_cimag(__tg_promote1((__x))(__x))\n"
"\n"
"// conj\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_conj(float __x) {return __x;}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_conj(double __x) {return __x;}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_conj(long double __x) {return __x;}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_conj(float _Complex __x) {return conjf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_conj(double _Complex __x) {return conj(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_conj(long double _Complex __x) {return conjl(__x);}\n"
"\n"
"#undef conj\n"
"#define conj(__x) __tg_conj(__tg_promote1((__x))(__x))\n"
"\n"
"// cproj\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cproj(float __x) {return cprojf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cproj(double __x) {return cproj(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cproj(long double __x) {return cprojl(__x);}\n"
"\n"
"static float _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cproj(float _Complex __x) {return cprojf(__x);}\n"
"\n"
"static double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cproj(double _Complex __x) {return cproj(__x);}\n"
"\n"
"static long double _Complex\n"
"    _TG_ATTRS\n"
"    __tg_cproj(long double _Complex __x) {return cprojl(__x);}\n"
"\n"
"#undef cproj\n"
"#define cproj(__x) __tg_cproj(__tg_promote1((__x))(__x))\n"
"\n"
"// creal\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_creal(float __x) {return __x;}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_creal(double __x) {return __x;}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_creal(long double __x) {return __x;}\n"
"\n"
"static float\n"
"    _TG_ATTRS\n"
"    __tg_creal(float _Complex __x) {return crealf(__x);}\n"
"\n"
"static double\n"
"    _TG_ATTRS\n"
"    __tg_creal(double _Complex __x) {return creal(__x);}\n"
"\n"
"static long double\n"
"    _TG_ATTRS\n"
"    __tg_creal(long double _Complex __x) {return creall(__x);}\n"
"\n"
"#undef creal\n"
"#define creal(__x) __tg_creal(__tg_promote1((__x))(__x))\n"
"\n"
"#undef _TG_ATTRSp\n"
"#undef _TG_ATTRS\n"
"\n"
"#endif /* __cplusplus */\n"
"#endif /* __has_include_next */\n"
"#endif /* __CLANG_TGMATH_H */\n"
"" } , 
 { "/builtins/tmmintrin.h" , "/*===---- tmmintrin.h - SSSE3 intrinsics -----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __TMMINTRIN_H\n"
"#define __TMMINTRIN_H\n"
"\n"
"#include <pmmintrin.h>\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"ssse3\"), __min_vector_width__(64)))\n"
"#define __DEFAULT_FN_ATTRS_MMX __attribute__((__always_inline__, __nodebug__, __target__(\"mmx,ssse3\"), __min_vector_width__(64)))\n"
"\n"
"/// Computes the absolute value of each of the packed 8-bit signed\n"
"///    integers in the source operand and stores the 8-bit unsigned integer\n"
"///    results in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PABSB instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [8 x i8].\n"
"/// \\returns A 64-bit integer vector containing the absolute values of the\n"
"///    elements in the operand.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_abs_pi8(__m64 __a)\n"
"{\n"
"    return (__m64)__builtin_ia32_pabsb((__v8qi)__a);\n"
"}\n"
"\n"
"/// Computes the absolute value of each of the packed 8-bit signed\n"
"///    integers in the source operand and stores the 8-bit unsigned integer\n"
"///    results in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPABSB instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [16 x i8].\n"
"/// \\returns A 128-bit integer vector containing the absolute values of the\n"
"///    elements in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_abs_epi8(__m128i __a)\n"
"{\n"
"    return (__m128i)__builtin_ia32_pabsb128((__v16qi)__a);\n"
"}\n"
"\n"
"/// Computes the absolute value of each of the packed 16-bit signed\n"
"///    integers in the source operand and stores the 16-bit unsigned integer\n"
"///    results in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PABSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [4 x i16].\n"
"/// \\returns A 64-bit integer vector containing the absolute values of the\n"
"///    elements in the operand.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_abs_pi16(__m64 __a)\n"
"{\n"
"    return (__m64)__builtin_ia32_pabsw((__v4hi)__a);\n"
"}\n"
"\n"
"/// Computes the absolute value of each of the packed 16-bit signed\n"
"///    integers in the source operand and stores the 16-bit unsigned integer\n"
"///    results in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPABSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16].\n"
"/// \\returns A 128-bit integer vector containing the absolute values of the\n"
"///    elements in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_abs_epi16(__m128i __a)\n"
"{\n"
"    return (__m128i)__builtin_ia32_pabsw128((__v8hi)__a);\n"
"}\n"
"\n"
"/// Computes the absolute value of each of the packed 32-bit signed\n"
"///    integers in the source operand and stores the 32-bit unsigned integer\n"
"///    results in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PABSD instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [2 x i32].\n"
"/// \\returns A 64-bit integer vector containing the absolute values of the\n"
"///    elements in the operand.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_abs_pi32(__m64 __a)\n"
"{\n"
"    return (__m64)__builtin_ia32_pabsd((__v2si)__a);\n"
"}\n"
"\n"
"/// Computes the absolute value of each of the packed 32-bit signed\n"
"///    integers in the source operand and stores the 32-bit unsigned integer\n"
"///    results in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPABSD instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x i32].\n"
"/// \\returns A 128-bit integer vector containing the absolute values of the\n"
"///    elements in the operand.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_abs_epi32(__m128i __a)\n"
"{\n"
"    return (__m128i)__builtin_ia32_pabsd128((__v4si)__a);\n"
"}\n"
"\n"
"/// Concatenates the two 128-bit integer vector operands, and\n"
"///    right-shifts the result by the number of bytes specified in the immediate\n"
"///    operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128i _mm_alignr_epi8(__m128i a, __m128i b, const int n);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the \\c PALIGNR instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector of [16 x i8] containing one of the source operands.\n"
"/// \\param b\n"
"///    A 128-bit vector of [16 x i8] containing one of the source operands.\n"
"/// \\param n\n"
"///    An immediate operand specifying how many bytes to right-shift the result.\n"
"/// \\returns A 128-bit integer vector containing the concatenated right-shifted\n"
"///    value.\n"
"#define _mm_alignr_epi8(a, b, n) \\\n"
"  (__m128i)__builtin_ia32_palignr128((__v16qi)(__m128i)(a), \\\n"
"                                     (__v16qi)(__m128i)(b), (n))\n"
"\n"
"/// Concatenates the two 64-bit integer vector operands, and right-shifts\n"
"///    the result by the number of bytes specified in the immediate operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m64 _mm_alignr_pi8(__m64 a, __m64 b, const int n);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the \\c PALIGNR instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 64-bit vector of [8 x i8] containing one of the source operands.\n"
"/// \\param b\n"
"///    A 64-bit vector of [8 x i8] containing one of the source operands.\n"
"/// \\param n\n"
"///    An immediate operand specifying how many bytes to right-shift the result.\n"
"/// \\returns A 64-bit integer vector containing the concatenated right-shifted\n"
"///    value.\n"
"#define _mm_alignr_pi8(a, b, n) \\\n"
"  (__m64)__builtin_ia32_palignr((__v8qi)(__m64)(a), (__v8qi)(__m64)(b), (n))\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in 2 packed\n"
"///    128-bit vectors of [8 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPHADDW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the horizontal sums of\n"
"///    both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hadd_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_phaddw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in 2 packed\n"
"///    128-bit vectors of [4 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPHADDD instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x i32] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x i32] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the horizontal sums of\n"
"///    both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hadd_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_phaddd128((__v4si)__a, (__v4si)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in 2 packed\n"
"///    64-bit vectors of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PHADDW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 64-bit vector of [4 x i16] containing the horizontal sums of both\n"
"///    operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_hadd_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_phaddw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in 2 packed\n"
"///    64-bit vectors of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PHADDD instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [2 x i32] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [2 x i32] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 64-bit vector of [2 x i32] containing the horizontal sums of both\n"
"///    operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_hadd_pi32(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_phaddd((__v2si)__a, (__v2si)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in 2 packed\n"
"///    128-bit vectors of [8 x i16]. Positive sums greater than 0x7FFF are\n"
"///    saturated to 0x7FFF. Negative sums less than 0x8000 are saturated to\n"
"///    0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPHADDSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the horizontal saturated\n"
"///    sums of both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hadds_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_phaddsw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Horizontally adds the adjacent pairs of values contained in 2 packed\n"
"///    64-bit vectors of [4 x i16]. Positive sums greater than 0x7FFF are\n"
"///    saturated to 0x7FFF. Negative sums less than 0x8000 are saturated to\n"
"///    0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PHADDSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the lower bits of the\n"
"///    destination.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal sums of the values are stored in the upper bits of the\n"
"///    destination.\n"
"/// \\returns A 64-bit vector of [4 x i16] containing the horizontal saturated\n"
"///    sums of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_hadds_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_phaddsw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in 2\n"
"///    packed 128-bit vectors of [8 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPHSUBW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the lower bits of\n"
"///    the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the upper bits of\n"
"///    the destination.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the horizontal differences\n"
"///    of both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hsub_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_phsubw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in 2\n"
"///    packed 128-bit vectors of [4 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPHSUBD instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x i32] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the lower bits of\n"
"///    the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x i32] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the upper bits of\n"
"///    the destination.\n"
"/// \\returns A 128-bit vector of [4 x i32] containing the horizontal differences\n"
"///    of both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hsub_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_phsubd128((__v4si)__a, (__v4si)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in 2\n"
"///    packed 64-bit vectors of [4 x i16].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PHSUBW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the lower bits of\n"
"///    the destination.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the upper bits of\n"
"///    the destination.\n"
"/// \\returns A 64-bit vector of [4 x i16] containing the horizontal differences\n"
"///    of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_hsub_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_phsubw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in 2\n"
"///    packed 64-bit vectors of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PHSUBD instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [2 x i32] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the lower bits of\n"
"///    the destination.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [2 x i32] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the upper bits of\n"
"///    the destination.\n"
"/// \\returns A 64-bit vector of [2 x i32] containing the horizontal differences\n"
"///    of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_hsub_pi32(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_phsubd((__v2si)__a, (__v2si)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in 2\n"
"///    packed 128-bit vectors of [8 x i16]. Positive differences greater than\n"
"///    0x7FFF are saturated to 0x7FFF. Negative differences less than 0x8000 are\n"
"///    saturated to 0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPHSUBSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the lower bits of\n"
"///    the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the upper bits of\n"
"///    the destination.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the horizontal saturated\n"
"///    differences of both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hsubs_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_phsubsw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Horizontally subtracts the adjacent pairs of values contained in 2\n"
"///    packed 64-bit vectors of [4 x i16]. Positive differences greater than\n"
"///    0x7FFF are saturated to 0x7FFF. Negative differences less than 0x8000 are\n"
"///    saturated to 0x8000.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PHSUBSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the lower bits of\n"
"///    the destination.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands. The\n"
"///    horizontal differences between the values are stored in the upper bits of\n"
"///    the destination.\n"
"/// \\returns A 64-bit vector of [4 x i16] containing the horizontal saturated\n"
"///    differences of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_hsubs_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_phsubsw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Multiplies corresponding pairs of packed 8-bit unsigned integer\n"
"///    values contained in the first source operand and packed 8-bit signed\n"
"///    integer values contained in the second source operand, adds pairs of\n"
"///    contiguous products with signed saturation, and writes the 16-bit sums to\n"
"///    the corresponding bits in the destination.\n"
"///\n"
"///    For example, bits [7:0] of both operands are multiplied, bits [15:8] of\n"
"///    both operands are multiplied, and the sum of both results is written to\n"
"///    bits [15:0] of the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPMADDUBSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the first source operand.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing the second source operand.\n"
"/// \\returns A 128-bit integer vector containing the sums of products of both\n"
"///    operands: \\n\n"
"///    \\a R0 := (\\a __a0 * \\a __b0) + (\\a __a1 * \\a __b1) \\n\n"
"///    \\a R1 := (\\a __a2 * \\a __b2) + (\\a __a3 * \\a __b3) \\n\n"
"///    \\a R2 := (\\a __a4 * \\a __b4) + (\\a __a5 * \\a __b5) \\n\n"
"///    \\a R3 := (\\a __a6 * \\a __b6) + (\\a __a7 * \\a __b7) \\n\n"
"///    \\a R4 := (\\a __a8 * \\a __b8) + (\\a __a9 * \\a __b9) \\n\n"
"///    \\a R5 := (\\a __a10 * \\a __b10) + (\\a __a11 * \\a __b11) \\n\n"
"///    \\a R6 := (\\a __a12 * \\a __b12) + (\\a __a13 * \\a __b13) \\n\n"
"///    \\a R7 := (\\a __a14 * \\a __b14) + (\\a __a15 * \\a __b15)\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maddubs_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_pmaddubsw128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Multiplies corresponding pairs of packed 8-bit unsigned integer\n"
"///    values contained in the first source operand and packed 8-bit signed\n"
"///    integer values contained in the second source operand, adds pairs of\n"
"///    contiguous products with signed saturation, and writes the 16-bit sums to\n"
"///    the corresponding bits in the destination.\n"
"///\n"
"///    For example, bits [7:0] of both operands are multiplied, bits [15:8] of\n"
"///    both operands are multiplied, and the sum of both results is written to\n"
"///    bits [15:0] of the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PMADDUBSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing the first source operand.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing the second source operand.\n"
"/// \\returns A 64-bit integer vector containing the sums of products of both\n"
"///    operands: \\n\n"
"///    \\a R0 := (\\a __a0 * \\a __b0) + (\\a __a1 * \\a __b1) \\n\n"
"///    \\a R1 := (\\a __a2 * \\a __b2) + (\\a __a3 * \\a __b3) \\n\n"
"///    \\a R2 := (\\a __a4 * \\a __b4) + (\\a __a5 * \\a __b5) \\n\n"
"///    \\a R3 := (\\a __a6 * \\a __b6) + (\\a __a7 * \\a __b7)\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_maddubs_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_pmaddubsw((__v8qi)__a, (__v8qi)__b);\n"
"}\n"
"\n"
"/// Multiplies packed 16-bit signed integer values, truncates the 32-bit\n"
"///    products to the 18 most significant bits by right-shifting, rounds the\n"
"///    truncated value by adding 1, and writes bits [16:1] to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPMULHRSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [8 x i16] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [8 x i16] containing the rounded and scaled\n"
"///    products of both operands.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_mulhrs_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_pmulhrsw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// Multiplies packed 16-bit signed integer values, truncates the 32-bit\n"
"///    products to the 18 most significant bits by right-shifting, rounds the\n"
"///    truncated value by adding 1, and writes bits [16:1] to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PMULHRSW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [4 x i16] containing one of the source operands.\n"
"/// \\returns A 64-bit vector of [4 x i16] containing the rounded and scaled\n"
"///    products of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_mulhrs_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_pmulhrsw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Copies the 8-bit integers from a 128-bit integer vector to the\n"
"///    destination or clears 8-bit values in the destination, as specified by\n"
"///    the second source operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPSHUFB instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing control bytes corresponding to\n"
"///    positions in the destination:\n"
"///    Bit 7: \\n\n"
"///    1: Clear the corresponding byte in the destination. \\n\n"
"///    0: Copy the selected source byte to the corresponding byte in the\n"
"///    destination. \\n\n"
"///    Bits [6:4] Reserved.  \\n\n"
"///    Bits [3:0] select the source byte to be copied.\n"
"/// \\returns A 128-bit integer vector containing the copied or cleared values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_shuffle_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_pshufb128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// Copies the 8-bit integers from a 64-bit integer vector to the\n"
"///    destination or clears 8-bit values in the destination, as specified by\n"
"///    the second source operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PSHUFB instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing control bytes corresponding to\n"
"///    positions in the destination:\n"
"///    Bit 7: \\n\n"
"///    1: Clear the corresponding byte in the destination. \\n\n"
"///    0: Copy the selected source byte to the corresponding byte in the\n"
"///    destination. \\n\n"
"///    Bits [3:0] select the source byte to be copied.\n"
"/// \\returns A 64-bit integer vector containing the copied or cleared values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_shuffle_pi8(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_pshufb((__v8qi)__a, (__v8qi)__b);\n"
"}\n"
"\n"
"/// For each 8-bit integer in the first source operand, perform one of\n"
"///    the following actions as specified by the second source operand.\n"
"///\n"
"///    If the byte in the second source is negative, calculate the two's\n"
"///    complement of the corresponding byte in the first source, and write that\n"
"///    value to the destination. If the byte in the second source is positive,\n"
"///    copy the corresponding byte from the first source to the destination. If\n"
"///    the byte in the second source is zero, clear the corresponding byte in\n"
"///    the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPSIGNB instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing control bytes corresponding to\n"
"///    positions in the destination.\n"
"/// \\returns A 128-bit integer vector containing the resultant values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sign_epi8(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_psignb128((__v16qi)__a, (__v16qi)__b);\n"
"}\n"
"\n"
"/// For each 16-bit integer in the first source operand, perform one of\n"
"///    the following actions as specified by the second source operand.\n"
"///\n"
"///    If the word in the second source is negative, calculate the two's\n"
"///    complement of the corresponding word in the first source, and write that\n"
"///    value to the destination. If the word in the second source is positive,\n"
"///    copy the corresponding word from the first source to the destination. If\n"
"///    the word in the second source is zero, clear the corresponding word in\n"
"///    the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPSIGNW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing control words corresponding to\n"
"///    positions in the destination.\n"
"/// \\returns A 128-bit integer vector containing the resultant values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sign_epi16(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_psignw128((__v8hi)__a, (__v8hi)__b);\n"
"}\n"
"\n"
"/// For each 32-bit integer in the first source operand, perform one of\n"
"///    the following actions as specified by the second source operand.\n"
"///\n"
"///    If the doubleword in the second source is negative, calculate the two's\n"
"///    complement of the corresponding word in the first source, and write that\n"
"///    value to the destination. If the doubleword in the second source is\n"
"///    positive, copy the corresponding word from the first source to the\n"
"///    destination. If the doubleword in the second source is zero, clear the\n"
"///    corresponding word in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c VPSIGND instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 128-bit integer vector containing control doublewords corresponding to\n"
"///    positions in the destination.\n"
"/// \\returns A 128-bit integer vector containing the resultant values.\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sign_epi32(__m128i __a, __m128i __b)\n"
"{\n"
"    return (__m128i)__builtin_ia32_psignd128((__v4si)__a, (__v4si)__b);\n"
"}\n"
"\n"
"/// For each 8-bit integer in the first source operand, perform one of\n"
"///    the following actions as specified by the second source operand.\n"
"///\n"
"///    If the byte in the second source is negative, calculate the two's\n"
"///    complement of the corresponding byte in the first source, and write that\n"
"///    value to the destination. If the byte in the second source is positive,\n"
"///    copy the corresponding byte from the first source to the destination. If\n"
"///    the byte in the second source is zero, clear the corresponding byte in\n"
"///    the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PSIGNB instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing control bytes corresponding to\n"
"///    positions in the destination.\n"
"/// \\returns A 64-bit integer vector containing the resultant values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_sign_pi8(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_psignb((__v8qi)__a, (__v8qi)__b);\n"
"}\n"
"\n"
"/// For each 16-bit integer in the first source operand, perform one of\n"
"///    the following actions as specified by the second source operand.\n"
"///\n"
"///    If the word in the second source is negative, calculate the two's\n"
"///    complement of the corresponding word in the first source, and write that\n"
"///    value to the destination. If the word in the second source is positive,\n"
"///    copy the corresponding word from the first source to the destination. If\n"
"///    the word in the second source is zero, clear the corresponding word in\n"
"///    the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PSIGNW instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing control words corresponding to\n"
"///    positions in the destination.\n"
"/// \\returns A 64-bit integer vector containing the resultant values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_sign_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_psignw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// For each 32-bit integer in the first source operand, perform one of\n"
"///    the following actions as specified by the second source operand.\n"
"///\n"
"///    If the doubleword in the second source is negative, calculate the two's\n"
"///    complement of the corresponding doubleword in the first source, and\n"
"///    write that value to the destination. If the doubleword in the second\n"
"///    source is positive, copy the corresponding doubleword from the first\n"
"///    source to the destination. If the doubleword in the second source is\n"
"///    zero, clear the corresponding doubleword in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the \\c PSIGND instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing the values to be copied.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing two control doublewords corresponding\n"
"///    to positions in the destination.\n"
"/// \\returns A 64-bit integer vector containing the resultant values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_sign_pi32(__m64 __a, __m64 __b)\n"
"{\n"
"    return (__m64)__builtin_ia32_psignd((__v2si)__a, (__v2si)__b);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __DEFAULT_FN_ATTRS_MMX\n"
"\n"
"#endif /* __TMMINTRIN_H */\n"
"" } , 
 { "/builtins/unwind.h" , "/*===---- unwind.h - Stack unwinding ----------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"/* See \"Data Definitions for libgcc_s\" in the Linux Standard Base.*/\n"
"\n"
"#if defined(__APPLE__) && __has_include_next(<unwind.h>)\n"
"/* Darwin (from 11.x on) provide an unwind.h. If that's available,\n"
" * use it. libunwind wraps some of its definitions in #ifdef _GNU_SOURCE,\n"
" * so define that around the include.*/\n"
"# ifndef _GNU_SOURCE\n"
"#  define _SHOULD_UNDEFINE_GNU_SOURCE\n"
"#  define _GNU_SOURCE\n"
"# endif\n"
"// libunwind's unwind.h reflects the current visibility.  However, Mozilla\n"
"// builds with -fvisibility=hidden and relies on gcc's unwind.h to reset the\n"
"// visibility to default and export its contents.  gcc also allows users to\n"
"// override its override by #defining HIDE_EXPORTS (but note, this only obeys\n"
"// the user's -fvisibility setting; it doesn't hide any exports on its own).  We\n"
"// imitate gcc's header here:\n"
"# ifdef HIDE_EXPORTS\n"
"#  include_next <unwind.h>\n"
"# else\n"
"#  pragma GCC visibility push(default)\n"
"#  include_next <unwind.h>\n"
"#  pragma GCC visibility pop\n"
"# endif\n"
"# ifdef _SHOULD_UNDEFINE_GNU_SOURCE\n"
"#  undef _GNU_SOURCE\n"
"#  undef _SHOULD_UNDEFINE_GNU_SOURCE\n"
"# endif\n"
"#else\n"
"\n"
"#ifndef __CLANG_UNWIND_H\n"
"#define __CLANG_UNWIND_H\n"
"\n"
"#include <stdint.h>\n"
"\n"
"#ifdef __cplusplus\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"/* It is a bit strange for a header to play with the visibility of the\n"
"   symbols it declares, but this matches gcc's behavior and some programs\n"
"   depend on it */\n"
"#ifndef HIDE_EXPORTS\n"
"#pragma GCC visibility push(default)\n"
"#endif\n"
"\n"
"typedef uintptr_t _Unwind_Word;\n"
"typedef intptr_t _Unwind_Sword;\n"
"typedef uintptr_t _Unwind_Ptr;\n"
"typedef uintptr_t _Unwind_Internal_Ptr;\n"
"typedef uint64_t _Unwind_Exception_Class;\n"
"\n"
"typedef intptr_t _sleb128_t;\n"
"typedef uintptr_t _uleb128_t;\n"
"\n"
"struct _Unwind_Context;\n"
"#if defined(__arm__) && !(defined(__USING_SJLJ_EXCEPTIONS__) || defined(__ARM_DWARF_EH__))\n"
"struct _Unwind_Control_Block;\n"
"typedef struct _Unwind_Control_Block _Unwind_Exception; /* Alias */\n"
"#else\n"
"struct _Unwind_Exception;\n"
"typedef struct _Unwind_Exception _Unwind_Exception;\n"
"#endif\n"
"typedef enum {\n"
"  _URC_NO_REASON = 0,\n"
"#if defined(__arm__) && !defined(__USING_SJLJ_EXCEPTIONS__) && \\\n"
"    !defined(__ARM_DWARF_EH__)\n"
"  _URC_OK = 0, /* used by ARM EHABI */\n"
"#endif\n"
"  _URC_FOREIGN_EXCEPTION_CAUGHT = 1,\n"
"\n"
"  _URC_FATAL_PHASE2_ERROR = 2,\n"
"  _URC_FATAL_PHASE1_ERROR = 3,\n"
"  _URC_NORMAL_STOP = 4,\n"
"\n"
"  _URC_END_OF_STACK = 5,\n"
"  _URC_HANDLER_FOUND = 6,\n"
"  _URC_INSTALL_CONTEXT = 7,\n"
"  _URC_CONTINUE_UNWIND = 8,\n"
"#if defined(__arm__) && !defined(__USING_SJLJ_EXCEPTIONS__) && \\\n"
"    !defined(__ARM_DWARF_EH__)\n"
"  _URC_FAILURE = 9 /* used by ARM EHABI */\n"
"#endif\n"
"} _Unwind_Reason_Code;\n"
"\n"
"typedef enum {\n"
"  _UA_SEARCH_PHASE = 1,\n"
"  _UA_CLEANUP_PHASE = 2,\n"
"\n"
"  _UA_HANDLER_FRAME = 4,\n"
"  _UA_FORCE_UNWIND = 8,\n"
"  _UA_END_OF_STACK = 16 /* gcc extension to C++ ABI */\n"
"} _Unwind_Action;\n"
"\n"
"typedef void (*_Unwind_Exception_Cleanup_Fn)(_Unwind_Reason_Code,\n"
"                                             _Unwind_Exception *);\n"
"\n"
"#if defined(__arm__) && !(defined(__USING_SJLJ_EXCEPTIONS__) || defined(__ARM_DWARF_EH__))\n"
"typedef struct _Unwind_Control_Block _Unwind_Control_Block;\n"
"typedef uint32_t _Unwind_EHT_Header;\n"
"\n"
"struct _Unwind_Control_Block {\n"
"  uint64_t exception_class;\n"
"  void (*exception_cleanup)(_Unwind_Reason_Code, _Unwind_Control_Block *);\n"
"  /* unwinder cache (private fields for the unwinder's use) */\n"
"  struct {\n"
"    uint32_t reserved1; /* forced unwind stop function, 0 if not forced */\n"
"    uint32_t reserved2; /* personality routine */\n"
"    uint32_t reserved3; /* callsite */\n"
"    uint32_t reserved4; /* forced unwind stop argument */\n"
"    uint32_t reserved5;\n"
"  } unwinder_cache;\n"
"  /* propagation barrier cache (valid after phase 1) */\n"
"  struct {\n"
"    uint32_t sp;\n"
"    uint32_t bitpattern[5];\n"
"  } barrier_cache;\n"
"  /* cleanup cache (preserved over cleanup) */\n"
"  struct {\n"
"    uint32_t bitpattern[4];\n"
"  } cleanup_cache;\n"
"  /* personality cache (for personality's benefit) */\n"
"  struct {\n"
"    uint32_t fnstart;         /* function start address */\n"
"    _Unwind_EHT_Header *ehtp; /* pointer to EHT entry header word */\n"
"    uint32_t additional;      /* additional data */\n"
"    uint32_t reserved1;\n"
"  } pr_cache;\n"
"  long long int : 0; /* force alignment of next item to 8-byte boundary */\n"
"} __attribute__((__aligned__(8)));\n"
"#else\n"
"struct _Unwind_Exception {\n"
"  _Unwind_Exception_Class exception_class;\n"
"  _Unwind_Exception_Cleanup_Fn exception_cleanup;\n"
"#if !defined (__USING_SJLJ_EXCEPTIONS__) && defined (__SEH__)\n"
"  _Unwind_Word private_[6];\n"
"#else\n"
"  _Unwind_Word private_1;\n"
"  _Unwind_Word private_2;\n"
"#endif\n"
"  /* The Itanium ABI requires that _Unwind_Exception objects are \"double-word\n"
"   * aligned\".  GCC has interpreted this to mean \"use the maximum useful\n"
"   * alignment for the target\"; so do we. */\n"
"} __attribute__((__aligned__));\n"
"#endif\n"
"\n"
"typedef _Unwind_Reason_Code (*_Unwind_Stop_Fn)(int, _Unwind_Action,\n"
"                                               _Unwind_Exception_Class,\n"
"                                               _Unwind_Exception *,\n"
"                                               struct _Unwind_Context *,\n"
"                                               void *);\n"
"\n"
"typedef _Unwind_Reason_Code (*_Unwind_Personality_Fn)(int, _Unwind_Action,\n"
"                                                      _Unwind_Exception_Class,\n"
"                                                      _Unwind_Exception *,\n"
"                                                      struct _Unwind_Context *);\n"
"typedef _Unwind_Personality_Fn __personality_routine;\n"
"\n"
"typedef _Unwind_Reason_Code (*_Unwind_Trace_Fn)(struct _Unwind_Context *,\n"
"                                                void *);\n"
"\n"
"#if defined(__arm__) && !(defined(__USING_SJLJ_EXCEPTIONS__) || defined(__ARM_DWARF_EH__))\n"
"typedef enum {\n"
"  _UVRSC_CORE = 0,        /* integer register */\n"
"  _UVRSC_VFP = 1,         /* vfp */\n"
"  _UVRSC_WMMXD = 3,       /* Intel WMMX data register */\n"
"  _UVRSC_WMMXC = 4        /* Intel WMMX control register */\n"
"} _Unwind_VRS_RegClass;\n"
"\n"
"typedef enum {\n"
"  _UVRSD_UINT32 = 0,\n"
"  _UVRSD_VFPX = 1,\n"
"  _UVRSD_UINT64 = 3,\n"
"  _UVRSD_FLOAT = 4,\n"
"  _UVRSD_DOUBLE = 5\n"
"} _Unwind_VRS_DataRepresentation;\n"
"\n"
"typedef enum {\n"
"  _UVRSR_OK = 0,\n"
"  _UVRSR_NOT_IMPLEMENTED = 1,\n"
"  _UVRSR_FAILED = 2\n"
"} _Unwind_VRS_Result;\n"
"\n"
"typedef uint32_t _Unwind_State;\n"
"#define _US_VIRTUAL_UNWIND_FRAME  ((_Unwind_State)0)\n"
"#define _US_UNWIND_FRAME_STARTING ((_Unwind_State)1)\n"
"#define _US_UNWIND_FRAME_RESUME   ((_Unwind_State)2)\n"
"#define _US_ACTION_MASK           ((_Unwind_State)3)\n"
"#define _US_FORCE_UNWIND          ((_Unwind_State)8)\n"
"\n"
"_Unwind_VRS_Result _Unwind_VRS_Get(struct _Unwind_Context *__context,\n"
"  _Unwind_VRS_RegClass __regclass,\n"
"  uint32_t __regno,\n"
"  _Unwind_VRS_DataRepresentation __representation,\n"
"  void *__valuep);\n"
"\n"
"_Unwind_VRS_Result _Unwind_VRS_Set(struct _Unwind_Context *__context,\n"
"  _Unwind_VRS_RegClass __regclass,\n"
"  uint32_t __regno,\n"
"  _Unwind_VRS_DataRepresentation __representation,\n"
"  void *__valuep);\n"
"\n"
"static __inline__\n"
"_Unwind_Word _Unwind_GetGR(struct _Unwind_Context *__context, int __index) {\n"
"  _Unwind_Word __value;\n"
"  _Unwind_VRS_Get(__context, _UVRSC_CORE, __index, _UVRSD_UINT32, &__value);\n"
"  return __value;\n"
"}\n"
"\n"
"static __inline__\n"
"void _Unwind_SetGR(struct _Unwind_Context *__context, int __index,\n"
"                   _Unwind_Word __value) {\n"
"  _Unwind_VRS_Set(__context, _UVRSC_CORE, __index, _UVRSD_UINT32, &__value);\n"
"}\n"
"\n"
"static __inline__\n"
"_Unwind_Word _Unwind_GetIP(struct _Unwind_Context *__context) {\n"
"  _Unwind_Word __ip = _Unwind_GetGR(__context, 15);\n"
"  return __ip & ~(_Unwind_Word)(0x1); /* Remove thumb mode bit. */\n"
"}\n"
"\n"
"static __inline__\n"
"void _Unwind_SetIP(struct _Unwind_Context *__context, _Unwind_Word __value) {\n"
"  _Unwind_Word __thumb_mode_bit = _Unwind_GetGR(__context, 15) & 0x1;\n"
"  _Unwind_SetGR(__context, 15, __value | __thumb_mode_bit);\n"
"}\n"
"#else\n"
"_Unwind_Word _Unwind_GetGR(struct _Unwind_Context *, int);\n"
"void _Unwind_SetGR(struct _Unwind_Context *, int, _Unwind_Word);\n"
"\n"
"_Unwind_Word _Unwind_GetIP(struct _Unwind_Context *);\n"
"void _Unwind_SetIP(struct _Unwind_Context *, _Unwind_Word);\n"
"#endif\n"
"\n"
"\n"
"_Unwind_Word _Unwind_GetIPInfo(struct _Unwind_Context *, int *);\n"
"\n"
"_Unwind_Word _Unwind_GetCFA(struct _Unwind_Context *);\n"
"\n"
"_Unwind_Word _Unwind_GetBSP(struct _Unwind_Context *);\n"
"\n"
"void *_Unwind_GetLanguageSpecificData(struct _Unwind_Context *);\n"
"\n"
"_Unwind_Ptr _Unwind_GetRegionStart(struct _Unwind_Context *);\n"
"\n"
"/* DWARF EH functions; currently not available on Darwin/ARM */\n"
"#if !defined(__APPLE__) || !defined(__arm__)\n"
"_Unwind_Reason_Code _Unwind_RaiseException(_Unwind_Exception *);\n"
"_Unwind_Reason_Code _Unwind_ForcedUnwind(_Unwind_Exception *, _Unwind_Stop_Fn,\n"
"                                         void *);\n"
"void _Unwind_DeleteException(_Unwind_Exception *);\n"
"void _Unwind_Resume(_Unwind_Exception *);\n"
"_Unwind_Reason_Code _Unwind_Resume_or_Rethrow(_Unwind_Exception *);\n"
"\n"
"#endif\n"
"\n"
"_Unwind_Reason_Code _Unwind_Backtrace(_Unwind_Trace_Fn, void *);\n"
"\n"
"/* setjmp(3)/longjmp(3) stuff */\n"
"typedef struct SjLj_Function_Context *_Unwind_FunctionContext_t;\n"
"\n"
"void _Unwind_SjLj_Register(_Unwind_FunctionContext_t);\n"
"void _Unwind_SjLj_Unregister(_Unwind_FunctionContext_t);\n"
"_Unwind_Reason_Code _Unwind_SjLj_RaiseException(_Unwind_Exception *);\n"
"_Unwind_Reason_Code _Unwind_SjLj_ForcedUnwind(_Unwind_Exception *,\n"
"                                              _Unwind_Stop_Fn, void *);\n"
"void _Unwind_SjLj_Resume(_Unwind_Exception *);\n"
"_Unwind_Reason_Code _Unwind_SjLj_Resume_or_Rethrow(_Unwind_Exception *);\n"
"\n"
"void *_Unwind_FindEnclosingFunction(void *);\n"
"\n"
"#ifdef __APPLE__\n"
"\n"
"_Unwind_Ptr _Unwind_GetDataRelBase(struct _Unwind_Context *)\n"
"    __attribute__((__unavailable__));\n"
"_Unwind_Ptr _Unwind_GetTextRelBase(struct _Unwind_Context *)\n"
"    __attribute__((__unavailable__));\n"
"\n"
"/* Darwin-specific functions */\n"
"void __register_frame(const void *);\n"
"void __deregister_frame(const void *);\n"
"\n"
"struct dwarf_eh_bases {\n"
"  uintptr_t tbase;\n"
"  uintptr_t dbase;\n"
"  uintptr_t func;\n"
"};\n"
"void *_Unwind_Find_FDE(const void *, struct dwarf_eh_bases *);\n"
"\n"
"void __register_frame_info_bases(const void *, void *, void *, void *)\n"
"  __attribute__((__unavailable__));\n"
"void __register_frame_info(const void *, void *) __attribute__((__unavailable__));\n"
"void __register_frame_info_table_bases(const void *, void*, void *, void *)\n"
"  __attribute__((__unavailable__));\n"
"void __register_frame_info_table(const void *, void *)\n"
"  __attribute__((__unavailable__));\n"
"void __register_frame_table(const void *) __attribute__((__unavailable__));\n"
"void __deregister_frame_info(const void *) __attribute__((__unavailable__));\n"
"void __deregister_frame_info_bases(const void *)__attribute__((__unavailable__));\n"
"\n"
"#else\n"
"\n"
"_Unwind_Ptr _Unwind_GetDataRelBase(struct _Unwind_Context *);\n"
"_Unwind_Ptr _Unwind_GetTextRelBase(struct _Unwind_Context *);\n"
"\n"
"#endif\n"
"\n"
"\n"
"#ifndef HIDE_EXPORTS\n"
"#pragma GCC visibility pop\n"
"#endif\n"
"\n"
"#ifdef __cplusplus\n"
"}\n"
"#endif\n"
"\n"
"#endif /* __CLANG_UNWIND_H */\n"
"\n"
"#endif\n"
"\n"
"" } , 
 { "/builtins/vadefs.h" , "/* ===-------- vadefs.h ---------------------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"/* Only include this if we are aiming for MSVC compatibility. */\n"
"#ifndef _MSC_VER\n"
"#include_next <vadefs.h>\n"
"#else\n"
"\n"
"#ifndef __clang_vadefs_h\n"
"#define __clang_vadefs_h\n"
"\n"
"#include_next <vadefs.h>\n"
"\n"
"/* Override macros from vadefs.h with definitions that work with Clang. */\n"
"#ifdef _crt_va_start\n"
"#undef _crt_va_start\n"
"#define _crt_va_start(ap, param) __builtin_va_start(ap, param)\n"
"#endif\n"
"#ifdef _crt_va_end\n"
"#undef _crt_va_end\n"
"#define _crt_va_end(ap)          __builtin_va_end(ap)\n"
"#endif\n"
"#ifdef _crt_va_arg\n"
"#undef _crt_va_arg\n"
"#define _crt_va_arg(ap, type)    __builtin_va_arg(ap, type)\n"
"#endif\n"
"\n"
"/* VS 2015 switched to double underscore names, which is an improvement, but now\n"
" * we have to intercept those names too.\n"
" */\n"
"#ifdef __crt_va_start\n"
"#undef __crt_va_start\n"
"#define __crt_va_start(ap, param) __builtin_va_start(ap, param)\n"
"#endif\n"
"#ifdef __crt_va_end\n"
"#undef __crt_va_end\n"
"#define __crt_va_end(ap)          __builtin_va_end(ap)\n"
"#endif\n"
"#ifdef __crt_va_arg\n"
"#undef __crt_va_arg\n"
"#define __crt_va_arg(ap, type)    __builtin_va_arg(ap, type)\n"
"#endif\n"
"\n"
"#endif\n"
"#endif\n"
"" } , 
 { "/builtins/vaesintrin.h" , "/*===------------------ vaesintrin.h - VAES intrinsics ---------------------===\n"
" *\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <vaesintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __VAESINTRIN_H\n"
"#define __VAESINTRIN_H\n"
"\n"
"/* Default attributes for YMM forms. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"vaes\"), __min_vector_width__(256)))\n"
"\n"
"/* Default attributes for ZMM forms. */\n"
"#define __DEFAULT_FN_ATTRS_F __attribute__((__always_inline__, __nodebug__, __target__(\"avx512f,vaes\"), __min_vector_width__(512)))\n"
"\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS\n"
" _mm256_aesenc_epi128(__m256i __A, __m256i __B)\n"
"{\n"
"  return (__m256i) __builtin_ia32_aesenc256((__v4di) __A,\n"
"              (__v4di) __B);\n"
"}\n"
"\n"
"static __inline__ __m512i __DEFAULT_FN_ATTRS_F\n"
" _mm512_aesenc_epi128(__m512i __A, __m512i __B)\n"
"{\n"
"  return (__m512i) __builtin_ia32_aesenc512((__v8di) __A,\n"
"              (__v8di) __B);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS\n"
" _mm256_aesdec_epi128(__m256i __A, __m256i __B)\n"
"{\n"
"  return (__m256i) __builtin_ia32_aesdec256((__v4di) __A,\n"
"              (__v4di) __B);\n"
"}\n"
"\n"
"static __inline__ __m512i __DEFAULT_FN_ATTRS_F\n"
" _mm512_aesdec_epi128(__m512i __A, __m512i __B)\n"
"{\n"
"  return (__m512i) __builtin_ia32_aesdec512((__v8di) __A,\n"
"              (__v8di) __B);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS\n"
" _mm256_aesenclast_epi128(__m256i __A, __m256i __B)\n"
"{\n"
"  return (__m256i) __builtin_ia32_aesenclast256((__v4di) __A,\n"
"              (__v4di) __B);\n"
"}\n"
"\n"
"static __inline__ __m512i __DEFAULT_FN_ATTRS_F\n"
" _mm512_aesenclast_epi128(__m512i __A, __m512i __B)\n"
"{\n"
"  return (__m512i) __builtin_ia32_aesenclast512((__v8di) __A,\n"
"              (__v8di) __B);\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS\n"
" _mm256_aesdeclast_epi128(__m256i __A, __m256i __B)\n"
"{\n"
"  return (__m256i) __builtin_ia32_aesdeclast256((__v4di) __A,\n"
"              (__v4di) __B);\n"
"}\n"
"\n"
"static __inline__ __m512i __DEFAULT_FN_ATTRS_F\n"
" _mm512_aesdeclast_epi128(__m512i __A, __m512i __B)\n"
"{\n"
"  return (__m512i) __builtin_ia32_aesdeclast512((__v8di) __A,\n"
"              (__v8di) __B);\n"
"}\n"
"\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __DEFAULT_FN_ATTRS_F\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/varargs.h" , "/*===---- varargs.h - Variable argument handling -------------------------------------===\n"
"*\n"
"* Permission is hereby granted, free of charge, to any person obtaining a copy\n"
"* of this software and associated documentation files (the \"Software\"), to deal\n"
"* in the Software without restriction, including without limitation the rights\n"
"* to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
"* copies of the Software, and to permit persons to whom the Software is\n"
"* furnished to do so, subject to the following conditions:\n"
"*\n"
"* The above copyright notice and this permission notice shall be included in\n"
"* all copies or substantial portions of the Software.\n"
"*\n"
"* THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
"* IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
"* FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
"* AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
"* LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
"* OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
"* THE SOFTWARE.\n"
"*\n"
"*===-----------------------------------------------------------------------===\n"
"*/\n"
"#ifndef __VARARGS_H\n"
"#define __VARARGS_H\n"
"  #error \"Please use <stdarg.h> instead of <varargs.h>\"\n"
"#endif\n"
"" } , 
 { "/builtins/vpclmulqdqintrin.h" , "/*===------------ vpclmulqdqintrin.h - VPCLMULQDQ intrinsics ---------------===\n"
" *\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <vpclmulqdqintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __VPCLMULQDQINTRIN_H\n"
"#define __VPCLMULQDQINTRIN_H\n"
"\n"
"#define _mm256_clmulepi64_epi128(A, B, I) \\\n"
"  (__m256i)__builtin_ia32_pclmulqdq256((__v4di)(__m256i)(A),  \\\n"
"                                       (__v4di)(__m256i)(B),  \\\n"
"                                       (char)(I))\n"
"\n"
"#define _mm512_clmulepi64_epi128(A, B, I) \\\n"
"  (__m512i)__builtin_ia32_pclmulqdq512((__v8di)(__m512i)(A),  \\\n"
"                                       (__v8di)(__m512i)(B),  \\\n"
"                                       (char)(I))\n"
"\n"
"#endif /* __VPCLMULQDQINTRIN_H */\n"
"\n"
"" } , 
 { "/builtins/waitpkgintrin.h" , "/*===----------------------- waitpkgintrin.h - WAITPKG --------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <waitpkgintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __WAITPKGINTRIN_H\n"
"#define __WAITPKGINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS \\\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"waitpkg\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_umonitor (void * __address)\n"
"{\n"
"  __builtin_ia32_umonitor (__address);\n"
"}\n"
"\n"
"static __inline__ unsigned char __DEFAULT_FN_ATTRS\n"
"_umwait (unsigned int __control, unsigned long long __counter)\n"
"{\n"
"  return __builtin_ia32_umwait (__control,\n"
"    (unsigned int)(__counter >> 32), (unsigned int)__counter);\n"
"}\n"
"\n"
"static __inline__ unsigned char __DEFAULT_FN_ATTRS\n"
"_tpause (unsigned int __control, unsigned long long __counter)\n"
"{\n"
"  return __builtin_ia32_tpause (__control,\n"
"    (unsigned int)(__counter >> 32), (unsigned int)__counter);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif /* __WAITPKGINTRIN_H */\n"
"" } , 
 { "/builtins/wbnoinvdintrin.h" , "/*===-------------- wbnoinvdintrin.h - wbnoinvd intrinsic-------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#if !defined __X86INTRIN_H && !defined __IMMINTRIN_H\n"
"#error \"Never use <wbnoinvdintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __WBNOINVDINTRIN_H\n"
"#define __WBNOINVDINTRIN_H\n"
"\n"
"static __inline__ void\n"
"  __attribute__((__always_inline__, __nodebug__,  __target__(\"wbnoinvd\")))\n"
"_wbnoinvd (void)\n"
"{\n"
"  __builtin_ia32_wbnoinvd ();\n"
"}\n"
"\n"
"#endif /* __WBNOINVDINTRIN_H */\n"
"" } , 
 { "/builtins/wmmintrin.h" , "/*===---- wmmintrin.h - AES intrinsics ------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __WMMINTRIN_H\n"
"#define __WMMINTRIN_H\n"
"\n"
"#include <emmintrin.h>\n"
"\n"
"#include <__wmmintrin_aes.h>\n"
"\n"
"#include <__wmmintrin_pclmul.h>\n"
"\n"
"#endif /* __WMMINTRIN_H */\n"
"" } , 
 { "/builtins/x86intrin.h" , "/*===---- x86intrin.h - X86 intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __X86INTRIN_H\n"
"#define __X86INTRIN_H\n"
"\n"
"#include <ia32intrin.h>\n"
"\n"
"#include <immintrin.h>\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__3dNOW__)\n"
"#include <mm3dnow.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__PRFCHW__)\n"
"#include <prfchwintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__SSE4A__)\n"
"#include <ammintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__FMA4__)\n"
"#include <fma4intrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__XOP__)\n"
"#include <xopintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__TBM__)\n"
"#include <tbmintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__LWP__)\n"
"#include <lwpintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__MWAITX__)\n"
"#include <mwaitxintrin.h>\n"
"#endif\n"
"\n"
"#if !defined(_MSC_VER) || __has_feature(modules) || defined(__CLZERO__)\n"
"#include <clzerointrin.h>\n"
"#endif\n"
"\n"
"\n"
"#endif /* __X86INTRIN_H */\n"
"" } , 
 { "/builtins/xmmintrin.h" , "/*===---- xmmintrin.h - SSE intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __XMMINTRIN_H\n"
"#define __XMMINTRIN_H\n"
"\n"
"#include <mmintrin.h>\n"
"\n"
"typedef int __v4si __attribute__((__vector_size__(16)));\n"
"typedef float __v4sf __attribute__((__vector_size__(16)));\n"
"typedef float __m128 __attribute__((__vector_size__(16)));\n"
"\n"
"/* Unsigned types */\n"
"typedef unsigned int __v4su __attribute__((__vector_size__(16)));\n"
"\n"
"/* This header should only be included in a hosted environment as it depends on\n"
" * a standard library to provide allocation routines. */\n"
"#if __STDC_HOSTED__\n"
"#include <mm_malloc.h>\n"
"#endif\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"sse\"), __min_vector_width__(128)))\n"
"#define __DEFAULT_FN_ATTRS_MMX __attribute__((__always_inline__, __nodebug__, __target__(\"mmx,sse\"), __min_vector_width__(64)))\n"
"\n"
"/// Adds the 32-bit float values in the low-order bits of the operands.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDSS / ADDSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The lower 32 bits of this operand are used in the calculation.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The lower 32 bits of this operand are used in the calculation.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 32 bits contain the sum\n"
"///    of the lower 32 bits of both operands. The upper 96 bits are copied from\n"
"///    the upper 96 bits of the first source operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_add_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  __a[0] += __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Adds two 128-bit vectors of [4 x float], and returns the results of\n"
"///    the addition.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VADDPS / ADDPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the sums of both\n"
"///    operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_add_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)((__v4sf)__a + (__v4sf)__b);\n"
"}\n"
"\n"
"/// Subtracts the 32-bit float value in the low-order bits of the second\n"
"///    operand from the corresponding value in the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSUBSS / SUBSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the minuend. The lower 32 bits\n"
"///    of this operand are used in the calculation.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing the subtrahend. The lower 32\n"
"///    bits of this operand are used in the calculation.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 32 bits contain the\n"
"///    difference of the lower 32 bits of both operands. The upper 96 bits are\n"
"///    copied from the upper 96 bits of the first source operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_sub_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  __a[0] -= __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Subtracts each of the values of the second operand from the first\n"
"///    operand, both of which are 128-bit vectors of [4 x float] and returns\n"
"///    the results of the subtraction.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSUBPS / SUBPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the minuend.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing the subtrahend.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the differences between\n"
"///    both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_sub_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)((__v4sf)__a - (__v4sf)__b);\n"
"}\n"
"\n"
"/// Multiplies two 32-bit float values in the low-order bits of the\n"
"///    operands.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMULSS / MULSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The lower 32 bits of this operand are used in the calculation.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"///    The lower 32 bits of this operand are used in the calculation.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the product of the lower\n"
"///    32 bits of both operands. The upper 96 bits are copied from the upper 96\n"
"///    bits of the first source operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_mul_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  __a[0] *= __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Multiplies two 128-bit vectors of [4 x float] and returns the\n"
"///    results of the multiplication.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMULPS / MULPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the products of both\n"
"///    operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_mul_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)((__v4sf)__a * (__v4sf)__b);\n"
"}\n"
"\n"
"/// Divides the value in the low-order 32 bits of the first operand by\n"
"///    the corresponding value in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDIVSS / DIVSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the dividend. The lower 32\n"
"///    bits of this operand are used in the calculation.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing the divisor. The lower 32 bits\n"
"///    of this operand are used in the calculation.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the quotients of the\n"
"///    lower 32 bits of both operands. The upper 96 bits are copied from the\n"
"///    upper 96 bits of the first source operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_div_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  __a[0] /= __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Divides two 128-bit vectors of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VDIVPS / DIVPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the dividend.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing the divisor.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the quotients of both\n"
"///    operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_div_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)((__v4sf)__a / (__v4sf)__b);\n"
"}\n"
"\n"
"/// Calculates the square root of the value stored in the low-order bits\n"
"///    of a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSQRTSS / SQRTSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the calculation.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the square root of the\n"
"///    value in the low-order bits of the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_sqrt_ss(__m128 __a)\n"
"{\n"
"  return (__m128)__builtin_ia32_sqrtss((__v4sf)__a);\n"
"}\n"
"\n"
"/// Calculates the square roots of the values stored in a 128-bit vector\n"
"///    of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSQRTPS / SQRTPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the square roots of the\n"
"///    values in the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_sqrt_ps(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_sqrtps((__v4sf)__a);\n"
"}\n"
"\n"
"/// Calculates the approximate reciprocal of the value stored in the\n"
"///    low-order bits of a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VRCPSS / RCPSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the calculation.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the approximate\n"
"///    reciprocal of the value in the low-order bits of the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_rcp_ss(__m128 __a)\n"
"{\n"
"  return (__m128)__builtin_ia32_rcpss((__v4sf)__a);\n"
"}\n"
"\n"
"/// Calculates the approximate reciprocals of the values stored in a\n"
"///    128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VRCPPS / RCPPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the approximate\n"
"///    reciprocals of the values in the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_rcp_ps(__m128 __a)\n"
"{\n"
"  return (__m128)__builtin_ia32_rcpps((__v4sf)__a);\n"
"}\n"
"\n"
"/// Calculates the approximate reciprocal of the square root of the value\n"
"///    stored in the low-order bits of a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VRSQRTSS / RSQRTSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the calculation.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the approximate\n"
"///    reciprocal of the square root of the value in the low-order bits of the\n"
"///    operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_rsqrt_ss(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_rsqrtss((__v4sf)__a);\n"
"}\n"
"\n"
"/// Calculates the approximate reciprocals of the square roots of the\n"
"///    values stored in a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VRSQRTPS / RSQRTPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the approximate\n"
"///    reciprocals of the square roots of the values in the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_rsqrt_ps(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_rsqrtps((__v4sf)__a);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands and returns the lesser value in the low-order bits of the\n"
"///    vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMINSS / MINSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 32 bits contain the\n"
"///    minimum value between both operands. The upper 96 bits are copied from\n"
"///    the upper 96 bits of the first source operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_min_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_minss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 128-bit vectors of [4 x float] and returns the lesser\n"
"///    of each pair of values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMINPS / MINPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the minimum values\n"
"///    between both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_min_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_minps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands and returns the greater value in the low-order bits of a 128-bit\n"
"///    vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMAXSS / MAXSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 32 bits contain the\n"
"///    maximum value between both operands. The upper 96 bits are copied from\n"
"///    the upper 96 bits of the first source operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_max_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_maxss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 128-bit vectors of [4 x float] and returns the greater\n"
"///    of each pair of values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMAXPS / MAXPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the maximum values\n"
"///    between both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_max_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_maxps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 128-bit vectors of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VANDPS / ANDPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the bitwise AND of the\n"
"///    values between both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_and_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)((__v4su)__a & (__v4su)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise AND of two 128-bit vectors of [4 x float], using\n"
"///    the one's complement of the values contained in the first source\n"
"///    operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VANDNPS / ANDNPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the first source operand. The\n"
"///    one's complement of this value is used in the bitwise AND.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing the second source operand.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the bitwise AND of the\n"
"///    one's complement of the first operand and the values in the second\n"
"///    operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_andnot_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)(~(__v4su)__a & (__v4su)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise OR of two 128-bit vectors of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VORPS / ORPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the bitwise OR of the\n"
"///    values between both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_or_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)((__v4su)__a | (__v4su)__b);\n"
"}\n"
"\n"
"/// Performs a bitwise exclusive OR of two 128-bit vectors of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the source operands.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the bitwise exclusive OR\n"
"///    of the values between both operands.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_xor_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)((__v4su)__a ^ (__v4su)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands for equality and returns the result of the comparison in the\n"
"///    low-order bits of a vector [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPEQSS / CMPEQSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpeqss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] for equality.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPEQPS / CMPEQPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpeq_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpeqps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is less than the\n"
"///    corresponding value in the second operand and returns the result of the\n"
"///    comparison in the low-order bits of a vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTSS / CMPLTSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmplt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpltss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are less than those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTPS / CMPLTPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmplt_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpltps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is less than or\n"
"///    equal to the corresponding value in the second operand and returns the\n"
"///    result of the comparison in the low-order bits of a vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLESS / CMPLESS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmple_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpless((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are less than or equal to those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLEPS / CMPLEPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmple_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpleps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is greater than\n"
"///    the corresponding value in the second operand and returns the result of\n"
"///    the comparison in the low-order bits of a vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTSS / CMPLTSS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_shufflevector((__v4sf)__a,\n"
"                                         (__v4sf)__builtin_ia32_cmpltss((__v4sf)__b, (__v4sf)__a),\n"
"                                         4, 1, 2, 3);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are greater than those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLTPS / CMPLTPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpgt_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpltps((__v4sf)__b, (__v4sf)__a);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is greater than\n"
"///    or equal to the corresponding value in the second operand and returns\n"
"///    the result of the comparison in the low-order bits of a vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLESS / CMPLESS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpge_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_shufflevector((__v4sf)__a,\n"
"                                         (__v4sf)__builtin_ia32_cmpless((__v4sf)__b, (__v4sf)__a),\n"
"                                         4, 1, 2, 3);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are greater than or equal to those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPLEPS / CMPLEPS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpge_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpleps((__v4sf)__b, (__v4sf)__a);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands for inequality and returns the result of the comparison in the\n"
"///    low-order bits of a vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNEQSS / CMPNEQSS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpneq_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpneqss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] for inequality.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNEQPS / CMPNEQPS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpneq_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpneqps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is not less than\n"
"///    the corresponding value in the second operand and returns the result of\n"
"///    the comparison in the low-order bits of a vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTSS / CMPNLTSS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpnlt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpnltss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are not less than those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTPS / CMPNLTPS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpnlt_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is not less than\n"
"///    or equal to the corresponding value in the second operand and returns\n"
"///    the result of the comparison in the low-order bits of a vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLESS / CMPNLESS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpnle_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpnless((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are not less than or equal to those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLEPS / CMPNLEPS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpnle_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is not greater\n"
"///    than the corresponding value in the second operand and returns the\n"
"///    result of the comparison in the low-order bits of a vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTSS / CMPNLTSS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpngt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_shufflevector((__v4sf)__a,\n"
"                                         (__v4sf)__builtin_ia32_cmpnltss((__v4sf)__b, (__v4sf)__a),\n"
"                                         4, 1, 2, 3);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are not greater than those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLTPS / CMPNLTPS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpngt_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__b, (__v4sf)__a);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is not greater\n"
"///    than or equal to the corresponding value in the second operand and\n"
"///    returns the result of the comparison in the low-order bits of a vector\n"
"///    of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLESS / CMPNLESS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpnge_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_shufflevector((__v4sf)__a,\n"
"                                         (__v4sf)__builtin_ia32_cmpnless((__v4sf)__b, (__v4sf)__a),\n"
"                                         4, 1, 2, 3);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are not greater than or equal to those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPNLEPS / CMPNLEPS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpnge_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__b, (__v4sf)__a);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is ordered with\n"
"///    respect to the corresponding value in the second operand and returns the\n"
"///    result of the comparison in the low-order bits of a vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPORDSS / CMPORDSS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpord_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpordss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are ordered with respect to those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPORDPS / CMPORDPS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpord_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpordps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the value in the first operand is unordered\n"
"///    with respect to the corresponding value in the second operand and\n"
"///    returns the result of the comparison in the low-order bits of a vector\n"
"///    of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPUNORDSS / CMPUNORDSS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float] containing one of the operands. The lower\n"
"///    32 bits of this operand are used in the comparison.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results\n"
"///    in the low-order bits.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpunord_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpunordss((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding 32-bit float values of the\n"
"///    128-bit vectors of [4 x float] to determine if the values in the first\n"
"///    operand are unordered with respect to those in the second operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCMPUNORDPS / CMPUNORDPS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 128-bit vector of [4 x float] containing the comparison results.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cmpunord_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return (__m128)__builtin_ia32_cmpunordps((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands for equality and returns the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the\n"
"///    two lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comieq_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_comieq((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the first operand is less than the second\n"
"///    operand and returns the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comilt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_comilt((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the first operand is less than or equal to the\n"
"///    second operand and returns the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comile_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_comile((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the first operand is greater than the second\n"
"///    operand and returns the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the\n"
"///     two lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comigt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_comigt((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the first operand is greater than or equal to\n"
"///    the second operand and returns the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comige_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_comige((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Compares two 32-bit float values in the low-order bits of both\n"
"///    operands to determine if the first operand is not equal to the second\n"
"///    operand and returns the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 1 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the\n"
"///     two lower 32-bit values is NaN, 1 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_comineq_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_comineq((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Performs an unordered comparison of two 32-bit float values using\n"
"///    the low-order bits of both operands to determine equality and returns\n"
"///    the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomieq_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_ucomieq((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Performs an unordered comparison of two 32-bit float values using\n"
"///    the low-order bits of both operands to determine if the first operand is\n"
"///    less than the second operand and returns the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomilt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_ucomilt((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Performs an unordered comparison of two 32-bit float values using\n"
"///    the low-order bits of both operands to determine if the first operand is\n"
"///    less than or equal to the second operand and returns the result of the\n"
"///    comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomile_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_ucomile((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Performs an unordered comparison of two 32-bit float values using\n"
"///    the low-order bits of both operands to determine if the first operand is\n"
"///    greater than the second operand and returns the result of the\n"
"///    comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomigt_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_ucomigt((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Performs an unordered comparison of two 32-bit float values using\n"
"///    the low-order bits of both operands to determine if the first operand is\n"
"///    greater than or equal to the second operand and returns the result of\n"
"///    the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 0 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///     lower 32-bit values is NaN, 0 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomige_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_ucomige((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Performs an unordered comparison of two 32-bit float values using\n"
"///    the low-order bits of both operands to determine inequality and returns\n"
"///    the result of the comparison.\n"
"///\n"
"///    If either of the two lower 32-bit values is NaN, 1 is returned.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the comparison.\n"
"/// \\returns An integer containing the comparison results. If either of the two\n"
"///    lower 32-bit values is NaN, 1 is returned.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_ucomineq_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_ia32_ucomineq((__v4sf)__a, (__v4sf)__b);\n"
"}\n"
"\n"
"/// Converts a float value contained in the lower 32 bits of a vector of\n"
"///    [4 x float] into a 32-bit integer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSS2SI / CVTSS2SI </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the conversion.\n"
"/// \\returns A 32-bit integer containing the converted value.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvtss_si32(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_cvtss2si((__v4sf)__a);\n"
"}\n"
"\n"
"/// Converts a float value contained in the lower 32 bits of a vector of\n"
"///    [4 x float] into a 32-bit integer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSS2SI / CVTSS2SI </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the conversion.\n"
"/// \\returns A 32-bit integer containing the converted value.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvt_ss2si(__m128 __a)\n"
"{\n"
"  return _mm_cvtss_si32(__a);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"\n"
"/// Converts a float value contained in the lower 32 bits of a vector of\n"
"///    [4 x float] into a 64-bit integer.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSS2SI / CVTSS2SI </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the conversion.\n"
"/// \\returns A 64-bit integer containing the converted value.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_mm_cvtss_si64(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_cvtss2si64((__v4sf)__a);\n"
"}\n"
"\n"
"#endif\n"
"\n"
"/// Converts two low-order float values in a 128-bit vector of\n"
"///    [4 x float] into a 64-bit vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPS2PI </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 64-bit integer vector containing the converted values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtps_pi32(__m128 __a)\n"
"{\n"
"  return (__m64)__builtin_ia32_cvtps2pi((__v4sf)__a);\n"
"}\n"
"\n"
"/// Converts two low-order float values in a 128-bit vector of\n"
"///    [4 x float] into a 64-bit vector of [2 x i32].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPS2PI </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 64-bit integer vector containing the converted values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvt_ps2pi(__m128 __a)\n"
"{\n"
"  return _mm_cvtps_pi32(__a);\n"
"}\n"
"\n"
"/// Converts a float value contained in the lower 32 bits of a vector of\n"
"///    [4 x float] into a 32-bit integer, truncating the result when it is\n"
"///    inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTSS2SI / CVTTSS2SI </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the conversion.\n"
"/// \\returns A 32-bit integer containing the converted value.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvttss_si32(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_cvttss2si((__v4sf)__a);\n"
"}\n"
"\n"
"/// Converts a float value contained in the lower 32 bits of a vector of\n"
"///    [4 x float] into a 32-bit integer, truncating the result when it is\n"
"///    inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTSS2SI / CVTTSS2SI </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the conversion.\n"
"/// \\returns A 32-bit integer containing the converted value.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_cvtt_ss2si(__m128 __a)\n"
"{\n"
"  return _mm_cvttss_si32(__a);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"/// Converts a float value contained in the lower 32 bits of a vector of\n"
"///    [4 x float] into a 64-bit integer, truncating the result when it is\n"
"///    inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTTSS2SI / CVTTSS2SI </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the conversion.\n"
"/// \\returns A 64-bit integer containing the converted value.\n"
"static __inline__ long long __DEFAULT_FN_ATTRS\n"
"_mm_cvttss_si64(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_cvttss2si64((__v4sf)__a);\n"
"}\n"
"#endif\n"
"\n"
"/// Converts two low-order float values in a 128-bit vector of\n"
"///    [4 x float] into a 64-bit vector of [2 x i32], truncating the result\n"
"///    when it is inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTTPS2PI / VTTPS2PI </c>\n"
"///   instructions.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 64-bit integer vector containing the converted values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvttps_pi32(__m128 __a)\n"
"{\n"
"  return (__m64)__builtin_ia32_cvttps2pi((__v4sf)__a);\n"
"}\n"
"\n"
"/// Converts two low-order float values in a 128-bit vector of [4 x\n"
"///    float] into a 64-bit vector of [2 x i32], truncating the result when it\n"
"///    is inexact.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTTPS2PI </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\returns A 64-bit integer vector containing the converted values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtt_ps2pi(__m128 __a)\n"
"{\n"
"  return _mm_cvttps_pi32(__a);\n"
"}\n"
"\n"
"/// Converts a 32-bit signed integer value into a floating point value\n"
"///    and writes it to the lower 32 bits of the destination. The remaining\n"
"///    higher order elements of the destination vector are copied from the\n"
"///    corresponding elements in the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSI2SS / CVTSI2SS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 32-bit signed integer operand containing the value to be converted.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 32 bits contain the\n"
"///    converted value of the second operand. The upper 96 bits are copied from\n"
"///    the upper 96 bits of the first operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi32_ss(__m128 __a, int __b)\n"
"{\n"
"  __a[0] = __b;\n"
"  return __a;\n"
"}\n"
"\n"
"/// Converts a 32-bit signed integer value into a floating point value\n"
"///    and writes it to the lower 32 bits of the destination. The remaining\n"
"///    higher order elements of the destination are copied from the\n"
"///    corresponding elements in the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSI2SS / CVTSI2SS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 32-bit signed integer operand containing the value to be converted.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 32 bits contain the\n"
"///    converted value of the second operand. The upper 96 bits are copied from\n"
"///    the upper 96 bits of the first operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cvt_si2ss(__m128 __a, int __b)\n"
"{\n"
"  return _mm_cvtsi32_ss(__a, __b);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"\n"
"/// Converts a 64-bit signed integer value into a floating point value\n"
"///    and writes it to the lower 32 bits of the destination. The remaining\n"
"///    higher order elements of the destination are copied from the\n"
"///    corresponding elements in the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VCVTSI2SS / CVTSI2SS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 64-bit signed integer operand containing the value to be converted.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 32 bits contain the\n"
"///    converted value of the second operand. The upper 96 bits are copied from\n"
"///    the upper 96 bits of the first operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_cvtsi64_ss(__m128 __a, long long __b)\n"
"{\n"
"  __a[0] = __b;\n"
"  return __a;\n"
"}\n"
"\n"
"#endif\n"
"\n"
"/// Converts two elements of a 64-bit vector of [2 x i32] into two\n"
"///    floating point values and writes them to the lower 64-bits of the\n"
"///    destination. The remaining higher order elements of the destination are\n"
"///    copied from the corresponding elements in the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 64-bit vector of [2 x i32]. The elements in this vector are converted\n"
"///    and written to the corresponding low-order elements in the destination.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 64 bits contain the\n"
"///    converted value of the second operand. The upper 64 bits are copied from\n"
"///    the upper 64 bits of the first operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpi32_ps(__m128 __a, __m64 __b)\n"
"{\n"
"  return __builtin_ia32_cvtpi2ps((__v4sf)__a, (__v2si)__b);\n"
"}\n"
"\n"
"/// Converts two elements of a 64-bit vector of [2 x i32] into two\n"
"///    floating point values and writes them to the lower 64-bits of the\n"
"///    destination. The remaining higher order elements of the destination are\n"
"///    copied from the corresponding elements in the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param __b\n"
"///    A 64-bit vector of [2 x i32]. The elements in this vector are converted\n"
"///    and written to the corresponding low-order elements in the destination.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 64 bits contain the\n"
"///    converted value from the second operand. The upper 64 bits are copied\n"
"///    from the upper 64 bits of the first operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvt_pi2ps(__m128 __a, __m64 __b)\n"
"{\n"
"  return _mm_cvtpi32_ps(__a, __b);\n"
"}\n"
"\n"
"/// Extracts a float value contained in the lower 32 bits of a vector of\n"
"///    [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are\n"
"///    used in the extraction.\n"
"/// \\returns A 32-bit float containing the extracted value.\n"
"static __inline__ float __DEFAULT_FN_ATTRS\n"
"_mm_cvtss_f32(__m128 __a)\n"
"{\n"
"  return __a[0];\n"
"}\n"
"\n"
"/// Loads two packed float values from the address \\a __p into the\n"
"///     high-order bits of a 128-bit vector of [4 x float]. The low-order bits\n"
"///     are copied from the low-order bits of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVHPD / MOVHPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. Bits [63:0] are written to bits [63:0]\n"
"///    of the destination.\n"
"/// \\param __p\n"
"///    A pointer to two packed float values. Bits [63:0] are written to bits\n"
"///    [127:64] of the destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the moved values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_loadh_pi(__m128 __a, const __m64 *__p)\n"
"{\n"
"  typedef float __mm_loadh_pi_v2f32 __attribute__((__vector_size__(8)));\n"
"  struct __mm_loadh_pi_struct {\n"
"    __mm_loadh_pi_v2f32 __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  __mm_loadh_pi_v2f32 __b = ((struct __mm_loadh_pi_struct*)__p)->__u;\n"
"  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);\n"
"  return __builtin_shufflevector(__a, __bb, 0, 1, 4, 5);\n"
"}\n"
"\n"
"/// Loads two packed float values from the address \\a __p into the\n"
"///    low-order bits of a 128-bit vector of [4 x float]. The high-order bits\n"
"///    are copied from the high-order bits of the first operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVLPD / MOVLPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. Bits [127:64] are written to bits\n"
"///    [127:64] of the destination.\n"
"/// \\param __p\n"
"///    A pointer to two packed float values. Bits [63:0] are written to bits\n"
"///    [63:0] of the destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the moved values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_loadl_pi(__m128 __a, const __m64 *__p)\n"
"{\n"
"  typedef float __mm_loadl_pi_v2f32 __attribute__((__vector_size__(8)));\n"
"  struct __mm_loadl_pi_struct {\n"
"    __mm_loadl_pi_v2f32 __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  __mm_loadl_pi_v2f32 __b = ((struct __mm_loadl_pi_struct*)__p)->__u;\n"
"  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);\n"
"  return __builtin_shufflevector(__a, __bb, 4, 5, 2, 3);\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float]. The lower\n"
"///    32 bits of the vector are initialized with the single-precision\n"
"///    floating-point value loaded from a specified memory location. The upper\n"
"///    96 bits are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 32-bit memory location containing a single-precision\n"
"///    floating-point value.\n"
"/// \\returns An initialized 128-bit floating-point vector of [4 x float]. The\n"
"///    lower 32 bits contain the value loaded from the memory location. The\n"
"///    upper 96 bits are set to zero.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_load_ss(const float *__p)\n"
"{\n"
"  struct __mm_load_ss_struct {\n"
"    float __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  float __u = ((struct __mm_load_ss_struct*)__p)->__u;\n"
"  return __extension__ (__m128){ __u, 0, 0, 0 };\n"
"}\n"
"\n"
"/// Loads a 32-bit float value and duplicates it to all four vector\n"
"///    elements of a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBROADCASTSS / MOVSS + shuffling </c>\n"
"///    instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a float value to be loaded and duplicated.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the loaded and\n"
"///    duplicated values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_load1_ps(const float *__p)\n"
"{\n"
"  struct __mm_load1_ps_struct {\n"
"    float __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  float __u = ((struct __mm_load1_ps_struct*)__p)->__u;\n"
"  return __extension__ (__m128){ __u, __u, __u, __u };\n"
"}\n"
"\n"
"#define        _mm_load_ps1(p) _mm_load1_ps(p)\n"
"\n"
"/// Loads a 128-bit floating-point vector of [4 x float] from an aligned\n"
"///    memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location has to be 128-bit aligned.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the loaded values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_load_ps(const float *__p)\n"
"{\n"
"  return *(__m128*)__p;\n"
"}\n"
"\n"
"/// Loads a 128-bit floating-point vector of [4 x float] from an\n"
"///    unaligned memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPS / MOVUPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the loaded values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_loadu_ps(const float *__p)\n"
"{\n"
"  struct __loadu_ps {\n"
"    __m128 __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  return ((struct __loadu_ps*)__p)->__v;\n"
"}\n"
"\n"
"/// Loads four packed float values, in reverse order, from an aligned\n"
"///    memory location to 32-bit elements in a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS + shuffling </c>\n"
"///    instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location has to be 128-bit aligned.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the moved values, loaded\n"
"///    in reverse order.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_loadr_ps(const float *__p)\n"
"{\n"
"  __m128 __a = _mm_load_ps(__p);\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);\n"
"}\n"
"\n"
"/// Create a 128-bit vector of [4 x float] with undefined values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic has no corresponding instruction.\n"
"///\n"
"/// \\returns A 128-bit vector of [4 x float] containing undefined values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_undefined_ps(void)\n"
"{\n"
"  return (__m128)__builtin_ia32_undef128();\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float]. The lower\n"
"///    32 bits of the vector are initialized with the specified single-precision\n"
"///    floating-point value. The upper 96 bits are set to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A single-precision floating-point value used to initialize the lower 32\n"
"///    bits of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [4 x float]. The\n"
"///    lower 32 bits contain the value provided in the source operand. The\n"
"///    upper 96 bits are set to zero.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_set_ss(float __w)\n"
"{\n"
"  return __extension__ (__m128){ __w, 0, 0, 0 };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float], with each\n"
"///    of the four single-precision floating-point vector elements set to the\n"
"///    specified single-precision floating-point value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS / PERMILPS </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A single-precision floating-point value used to initialize each vector\n"
"///    element of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [4 x float].\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_set1_ps(float __w)\n"
"{\n"
"  return __extension__ (__m128){ __w, __w, __w, __w };\n"
"}\n"
"\n"
"/* Microsoft specific. */\n"
"/// Constructs a 128-bit floating-point vector of [4 x float], with each\n"
"///    of the four single-precision floating-point vector elements set to the\n"
"///    specified single-precision floating-point value.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPERMILPS / PERMILPS </c> instruction.\n"
"///\n"
"/// \\param __w\n"
"///    A single-precision floating-point value used to initialize each vector\n"
"///    element of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [4 x float].\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_set_ps1(float __w)\n"
"{\n"
"    return _mm_set1_ps(__w);\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float]\n"
"///    initialized with the specified single-precision floating-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __z\n"
"///    A single-precision floating-point value used to initialize bits [127:96]\n"
"///    of the result.\n"
"/// \\param __y\n"
"///    A single-precision floating-point value used to initialize bits [95:64]\n"
"///    of the result.\n"
"/// \\param __x\n"
"///    A single-precision floating-point value used to initialize bits [63:32]\n"
"///    of the result.\n"
"/// \\param __w\n"
"///    A single-precision floating-point value used to initialize bits [31:0]\n"
"///    of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [4 x float].\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_set_ps(float __z, float __y, float __x, float __w)\n"
"{\n"
"  return __extension__ (__m128){ __w, __x, __y, __z };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float],\n"
"///    initialized in reverse order with the specified 32-bit single-precision\n"
"///    float-point values.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic is a utility function and does not correspond to a specific\n"
"///    instruction.\n"
"///\n"
"/// \\param __z\n"
"///    A single-precision floating-point value used to initialize bits [31:0]\n"
"///    of the result.\n"
"/// \\param __y\n"
"///    A single-precision floating-point value used to initialize bits [63:32]\n"
"///    of the result.\n"
"/// \\param __x\n"
"///    A single-precision floating-point value used to initialize bits [95:64]\n"
"///    of the result.\n"
"/// \\param __w\n"
"///    A single-precision floating-point value used to initialize bits [127:96]\n"
"///    of the result.\n"
"/// \\returns An initialized 128-bit floating-point vector of [4 x float].\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_setr_ps(float __z, float __y, float __x, float __w)\n"
"{\n"
"  return __extension__ (__m128){ __z, __y, __x, __w };\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float] initialized\n"
"///    to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instruction.\n"
"///\n"
"/// \\returns An initialized 128-bit floating-point vector of [4 x float] with\n"
"///    all elements set to zero.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_setzero_ps(void)\n"
"{\n"
"  return __extension__ (__m128){ 0, 0, 0, 0 };\n"
"}\n"
"\n"
"/// Stores the upper 64 bits of a 128-bit vector of [4 x float] to a\n"
"///    memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPEXTRQ / PEXTRQ </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 64-bit memory location.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeh_pi(__m64 *__p, __m128 __a)\n"
"{\n"
"  __builtin_ia32_storehps((__v2si *)__p, (__v4sf)__a);\n"
"}\n"
"\n"
"/// Stores the lower 64 bits of a 128-bit vector of [4 x float] to a\n"
"///     memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVLPS / MOVLPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a memory location that will receive the float values.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storel_pi(__m64 *__p, __m128 __a)\n"
"{\n"
"  __builtin_ia32_storelps((__v2si *)__p, (__v4sf)__a);\n"
"}\n"
"\n"
"/// Stores the lower 32 bits of a 128-bit vector of [4 x float] to a\n"
"///     memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 32-bit memory location.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store_ss(float *__p, __m128 __a)\n"
"{\n"
"  struct __mm_store_ss_struct {\n"
"    float __u;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __mm_store_ss_struct*)__p)->__u = __a[0];\n"
"}\n"
"\n"
"/// Stores a 128-bit vector of [4 x float] to an unaligned memory\n"
"///    location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVUPS / MOVUPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location does not have to be aligned.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storeu_ps(float *__p, __m128 __a)\n"
"{\n"
"  struct __storeu_ps {\n"
"    __m128 __v;\n"
"  } __attribute__((__packed__, __may_alias__));\n"
"  ((struct __storeu_ps*)__p)->__v = __a;\n"
"}\n"
"\n"
"/// Stores a 128-bit vector of [4 x float] into an aligned memory\n"
"///    location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location has to be 16-byte aligned.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store_ps(float *__p, __m128 __a)\n"
"{\n"
"  *(__m128*)__p = __a;\n"
"}\n"
"\n"
"/// Stores the lower 32 bits of a 128-bit vector of [4 x float] into\n"
"///    four contiguous elements in an aligned memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to <c> VMOVAPS / MOVAPS + shuffling </c>\n"
"///    instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] whose lower 32 bits are stored to each\n"
"///    of the four contiguous elements pointed by \\a __p.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store1_ps(float *__p, __m128 __a)\n"
"{\n"
"  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 0, 0);\n"
"  _mm_store_ps(__p, __a);\n"
"}\n"
"\n"
"/// Stores the lower 32 bits of a 128-bit vector of [4 x float] into\n"
"///    four contiguous elements in an aligned memory location.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to <c> VMOVAPS / MOVAPS + shuffling </c>\n"
"///    instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] whose lower 32 bits are stored to each\n"
"///    of the four contiguous elements pointed by \\a __p.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_store_ps1(float *__p, __m128 __a)\n"
"{\n"
"  _mm_store1_ps(__p, __a);\n"
"}\n"
"\n"
"/// Stores float values from a 128-bit vector of [4 x float] to an\n"
"///    aligned memory location in reverse order.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS + shuffling </c>\n"
"///    instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit memory location. The address of the memory\n"
"///    location has to be 128-bit aligned.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the values to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_storer_ps(float *__p, __m128 __a)\n"
"{\n"
"  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);\n"
"  _mm_store_ps(__p, __a);\n"
"}\n"
"\n"
"#define _MM_HINT_ET0 7\n"
"#define _MM_HINT_ET1 6\n"
"#define _MM_HINT_T0  3\n"
"#define _MM_HINT_T1  2\n"
"#define _MM_HINT_T2  1\n"
"#define _MM_HINT_NTA 0\n"
"\n"
"#ifndef _MSC_VER\n"
"/* FIXME: We have to #define this because \"sel\" must be a constant integer, and\n"
"   Sema doesn't do any form of constant propagation yet. */\n"
"\n"
"/// Loads one cache line of data from the specified address to a location\n"
"///    closer to the processor.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// void _mm_prefetch(const void * a, const int sel);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> PREFETCHNTA </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A pointer to a memory location containing a cache line of data.\n"
"/// \\param sel\n"
"///    A predefined integer constant specifying the type of prefetch\n"
"///    operation: \\n\n"
"///    _MM_HINT_NTA: Move data using the non-temporal access (NTA) hint. The\n"
"///    PREFETCHNTA instruction will be generated. \\n\n"
"///    _MM_HINT_T0: Move data using the T0 hint. The PREFETCHT0 instruction will\n"
"///    be generated. \\n\n"
"///    _MM_HINT_T1: Move data using the T1 hint. The PREFETCHT1 instruction will\n"
"///    be generated. \\n\n"
"///    _MM_HINT_T2: Move data using the T2 hint. The PREFETCHT2 instruction will\n"
"///    be generated.\n"
"#define _mm_prefetch(a, sel) (__builtin_prefetch((void *)(a), \\\n"
"                                                 ((sel) >> 2) & 1, (sel) & 0x3))\n"
"#endif\n"
"\n"
"/// Stores a 64-bit integer in the specified aligned memory location. To\n"
"///    minimize caching, the data is flagged as non-temporal (unlikely to be\n"
"///    used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MOVNTQ </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to an aligned memory location used to store the register value.\n"
"/// \\param __a\n"
"///    A 64-bit integer containing the value to be stored.\n"
"static __inline__ void __DEFAULT_FN_ATTRS_MMX\n"
"_mm_stream_pi(__m64 *__p, __m64 __a)\n"
"{\n"
"  __builtin_ia32_movntq(__p, __a);\n"
"}\n"
"\n"
"/// Moves packed float values from a 128-bit vector of [4 x float] to a\n"
"///    128-bit aligned memory location. To minimize caching, the data is flagged\n"
"///    as non-temporal (unlikely to be used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVNTPS / MOVNTPS </c> instruction.\n"
"///\n"
"/// \\param __p\n"
"///    A pointer to a 128-bit aligned memory location that will receive the\n"
"///    single-precision floating-point values.\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float] containing the values to be moved.\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_mm_stream_ps(float *__p, __m128 __a)\n"
"{\n"
"  __builtin_nontemporal_store((__v4sf)__a, (__v4sf*)__p);\n"
"}\n"
"\n"
"#if defined(__cplusplus)\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"/// Forces strong memory ordering (serialization) between store\n"
"///    instructions preceding this instruction and store instructions following\n"
"///    this instruction, ensuring the system completes all previous stores\n"
"///    before executing subsequent stores.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> SFENCE </c> instruction.\n"
"///\n"
"void _mm_sfence(void);\n"
"\n"
"#if defined(__cplusplus)\n"
"} // extern \"C\"\n"
"#endif\n"
"\n"
"/// Extracts 16-bit element from a 64-bit vector of [4 x i16] and\n"
"///    returns it, as specified by the immediate integer operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// int _mm_extract_pi16(__m64 a, int n);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VPEXTRW / PEXTRW </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 64-bit vector of [4 x i16].\n"
"/// \\param n\n"
"///    An immediate integer operand that determines which bits are extracted: \\n\n"
"///    0: Bits [15:0] are copied to the destination. \\n\n"
"///    1: Bits [31:16] are copied to the destination. \\n\n"
"///    2: Bits [47:32] are copied to the destination. \\n\n"
"///    3: Bits [63:48] are copied to the destination.\n"
"/// \\returns A 16-bit integer containing the extracted 16 bits of packed data.\n"
"#define _mm_extract_pi16(a, n) \\\n"
"  (int)__builtin_ia32_vec_ext_v4hi((__m64)a, (int)n)\n"
"\n"
"/// Copies data from the 64-bit vector of [4 x i16] to the destination,\n"
"///    and inserts the lower 16-bits of an integer operand at the 16-bit offset\n"
"///    specified by the immediate operand \\a n.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m64 _mm_insert_pi16(__m64 a, int d, int n);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> PINSRW </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 64-bit vector of [4 x i16].\n"
"/// \\param d\n"
"///    An integer. The lower 16-bit value from this operand is written to the\n"
"///    destination at the offset specified by operand \\a n.\n"
"/// \\param n\n"
"///    An immediate integer operant that determines which the bits to be used\n"
"///    in the destination. \\n\n"
"///    0: Bits [15:0] are copied to the destination. \\n\n"
"///    1: Bits [31:16] are copied to the destination. \\n\n"
"///    2: Bits [47:32] are copied to the destination. \\n\n"
"///    3: Bits [63:48] are copied to the destination.  \\n\n"
"///    The remaining bits in the destination are copied from the corresponding\n"
"///    bits in operand \\a a.\n"
"/// \\returns A 64-bit integer vector containing the copied packed data from the\n"
"///    operands.\n"
"#define _mm_insert_pi16(a, d, n) \\\n"
"  (__m64)__builtin_ia32_vec_set_v4hi((__m64)a, (int)d, (int)n)\n"
"\n"
"/// Compares each of the corresponding packed 16-bit integer values of\n"
"///    the 64-bit integer vectors, and writes the greater value to the\n"
"///    corresponding bits in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMAXSW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the comparison results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_max_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_pmaxsw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding packed 8-bit unsigned integer\n"
"///    values of the 64-bit integer vectors, and writes the greater value to the\n"
"///    corresponding bits in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMAXUB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the comparison results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_max_pu8(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_pmaxub((__v8qi)__a, (__v8qi)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding packed 16-bit integer values of\n"
"///    the 64-bit integer vectors, and writes the lesser value to the\n"
"///    corresponding bits in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMINSW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the comparison results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_min_pi16(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_pminsw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Compares each of the corresponding packed 8-bit unsigned integer\n"
"///    values of the 64-bit integer vectors, and writes the lesser value to the\n"
"///    corresponding bits in the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMINUB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the comparison results.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_min_pu8(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_pminub((__v8qi)__a, (__v8qi)__b);\n"
"}\n"
"\n"
"/// Takes the most significant bit from each 8-bit element in a 64-bit\n"
"///    integer vector to create an 8-bit mask value. Zero-extends the value to\n"
"///    32-bit integer and writes it to the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMOVMSKB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing the values with bits to be extracted.\n"
"/// \\returns The most significant bit from each 8-bit element in \\a __a,\n"
"///    written to bits [7:0].\n"
"static __inline__ int __DEFAULT_FN_ATTRS_MMX\n"
"_mm_movemask_pi8(__m64 __a)\n"
"{\n"
"  return __builtin_ia32_pmovmskb((__v8qi)__a);\n"
"}\n"
"\n"
"/// Multiplies packed 16-bit unsigned integer values and writes the\n"
"///    high-order 16 bits of each 32-bit product to the corresponding bits in\n"
"///    the destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PMULHUW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the products of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_mulhi_pu16(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_pmulhuw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Shuffles the 4 16-bit integers from a 64-bit integer vector to the\n"
"///    destination, as specified by the immediate value operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m64 _mm_shuffle_pi16(__m64 a, const int n);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSHUFW </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 64-bit integer vector containing the values to be shuffled.\n"
"/// \\param n\n"
"///    An immediate value containing an 8-bit value specifying which elements to\n"
"///    copy from \\a a. The destinations within the 64-bit destination are\n"
"///    assigned values as follows: \\n\n"
"///    Bits [1:0] are used to assign values to bits [15:0] in the\n"
"///    destination. \\n\n"
"///    Bits [3:2] are used to assign values to bits [31:16] in the\n"
"///    destination. \\n\n"
"///    Bits [5:4] are used to assign values to bits [47:32] in the\n"
"///    destination. \\n\n"
"///    Bits [7:6] are used to assign values to bits [63:48] in the\n"
"///    destination. \\n\n"
"///    Bit value assignments: \\n\n"
"///    00: assigned from bits [15:0] of \\a a. \\n\n"
"///    01: assigned from bits [31:16] of \\a a. \\n\n"
"///    10: assigned from bits [47:32] of \\a a. \\n\n"
"///    11: assigned from bits [63:48] of \\a a.\n"
"/// \\returns A 64-bit integer vector containing the shuffled values.\n"
"#define _mm_shuffle_pi16(a, n) \\\n"
"  (__m64)__builtin_ia32_pshufw((__v4hi)(__m64)(a), (n))\n"
"\n"
"/// Conditionally copies the values from each 8-bit element in the first\n"
"///    64-bit integer vector operand to the specified memory location, as\n"
"///    specified by the most significant bit in the corresponding element in the\n"
"///    second 64-bit integer vector operand.\n"
"///\n"
"///    To minimize caching, the data is flagged as non-temporal\n"
"///    (unlikely to be used again soon).\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> MASKMOVQ </c> instruction.\n"
"///\n"
"/// \\param __d\n"
"///    A 64-bit integer vector containing the values with elements to be copied.\n"
"/// \\param __n\n"
"///    A 64-bit integer vector operand. The most significant bit from each 8-bit\n"
"///    element determines whether the corresponding element in operand \\a __d\n"
"///    is copied. If the most significant bit of a given element is 1, the\n"
"///    corresponding element in operand \\a __d is copied.\n"
"/// \\param __p\n"
"///    A pointer to a 64-bit memory location that will receive the conditionally\n"
"///    copied integer values. The address of the memory location does not have\n"
"///    to be aligned.\n"
"static __inline__ void __DEFAULT_FN_ATTRS_MMX\n"
"_mm_maskmove_si64(__m64 __d, __m64 __n, char *__p)\n"
"{\n"
"  __builtin_ia32_maskmovq((__v8qi)__d, (__v8qi)__n, __p);\n"
"}\n"
"\n"
"/// Computes the rounded averages of the packed unsigned 8-bit integer\n"
"///    values and writes the averages to the corresponding bits in the\n"
"///    destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PAVGB </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the averages of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_avg_pu8(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_pavgb((__v8qi)__a, (__v8qi)__b);\n"
"}\n"
"\n"
"/// Computes the rounded averages of the packed unsigned 16-bit integer\n"
"///    values and writes the averages to the corresponding bits in the\n"
"///    destination.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PAVGW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector containing the averages of both operands.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_avg_pu16(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_pavgw((__v4hi)__a, (__v4hi)__b);\n"
"}\n"
"\n"
"/// Subtracts the corresponding 8-bit unsigned integer values of the two\n"
"///    64-bit vector operands and computes the absolute value for each of the\n"
"///    difference. Then sum of the 8 absolute differences is written to the\n"
"///    bits [15:0] of the destination; the remaining bits [63:16] are cleared.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> PSADBW </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\param __b\n"
"///    A 64-bit integer vector containing one of the source operands.\n"
"/// \\returns A 64-bit integer vector whose lower 16 bits contain the sums of the\n"
"///    sets of absolute differences between both operands. The upper bits are\n"
"///    cleared.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_sad_pu8(__m64 __a, __m64 __b)\n"
"{\n"
"  return (__m64)__builtin_ia32_psadbw((__v8qi)__a, (__v8qi)__b);\n"
"}\n"
"\n"
"#if defined(__cplusplus)\n"
"extern \"C\" {\n"
"#endif\n"
"\n"
"/// Returns the contents of the MXCSR register as a 32-bit unsigned\n"
"///    integer value.\n"
"///\n"
"///    There are several groups of macros associated with this\n"
"///    intrinsic, including:\n"
"///    <ul>\n"
"///    <li>\n"
"///      For checking exception states: _MM_EXCEPT_INVALID, _MM_EXCEPT_DIV_ZERO,\n"
"///      _MM_EXCEPT_DENORM, _MM_EXCEPT_OVERFLOW, _MM_EXCEPT_UNDERFLOW,\n"
"///      _MM_EXCEPT_INEXACT. There is a convenience wrapper\n"
"///      _MM_GET_EXCEPTION_STATE().\n"
"///    </li>\n"
"///    <li>\n"
"///      For checking exception masks: _MM_MASK_UNDERFLOW, _MM_MASK_OVERFLOW,\n"
"///      _MM_MASK_INVALID, _MM_MASK_DENORM, _MM_MASK_DIV_ZERO, _MM_MASK_INEXACT.\n"
"///      There is a convenience wrapper _MM_GET_EXCEPTION_MASK().\n"
"///    </li>\n"
"///    <li>\n"
"///      For checking rounding modes: _MM_ROUND_NEAREST, _MM_ROUND_DOWN,\n"
"///      _MM_ROUND_UP, _MM_ROUND_TOWARD_ZERO. There is a convenience wrapper\n"
"///      _MM_GET_ROUNDING_MODE().\n"
"///    </li>\n"
"///    <li>\n"
"///      For checking flush-to-zero mode: _MM_FLUSH_ZERO_ON, _MM_FLUSH_ZERO_OFF.\n"
"///      There is a convenience wrapper _MM_GET_FLUSH_ZERO_MODE().\n"
"///    </li>\n"
"///    <li>\n"
"///      For checking denormals-are-zero mode: _MM_DENORMALS_ZERO_ON,\n"
"///      _MM_DENORMALS_ZERO_OFF. There is a convenience wrapper\n"
"///      _MM_GET_DENORMALS_ZERO_MODE().\n"
"///    </li>\n"
"///    </ul>\n"
"///\n"
"///    For example, the following expression checks if an overflow exception has\n"
"///    occurred:\n"
"///    \\code\n"
"///      ( _mm_getcsr() & _MM_EXCEPT_OVERFLOW )\n"
"///    \\endcode\n"
"///\n"
"///    The following expression gets the current rounding mode:\n"
"///    \\code\n"
"///      _MM_GET_ROUNDING_MODE()\n"
"///    \\endcode\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSTMXCSR / STMXCSR </c> instruction.\n"
"///\n"
"/// \\returns A 32-bit unsigned integer containing the contents of the MXCSR\n"
"///    register.\n"
"unsigned int _mm_getcsr(void);\n"
"\n"
"/// Sets the MXCSR register with the 32-bit unsigned integer value.\n"
"///\n"
"///    There are several groups of macros associated with this intrinsic,\n"
"///    including:\n"
"///    <ul>\n"
"///    <li>\n"
"///      For setting exception states: _MM_EXCEPT_INVALID, _MM_EXCEPT_DIV_ZERO,\n"
"///      _MM_EXCEPT_DENORM, _MM_EXCEPT_OVERFLOW, _MM_EXCEPT_UNDERFLOW,\n"
"///      _MM_EXCEPT_INEXACT. There is a convenience wrapper\n"
"///      _MM_SET_EXCEPTION_STATE(x) where x is one of these macros.\n"
"///    </li>\n"
"///    <li>\n"
"///      For setting exception masks: _MM_MASK_UNDERFLOW, _MM_MASK_OVERFLOW,\n"
"///      _MM_MASK_INVALID, _MM_MASK_DENORM, _MM_MASK_DIV_ZERO, _MM_MASK_INEXACT.\n"
"///      There is a convenience wrapper _MM_SET_EXCEPTION_MASK(x) where x is one\n"
"///      of these macros.\n"
"///    </li>\n"
"///    <li>\n"
"///      For setting rounding modes: _MM_ROUND_NEAREST, _MM_ROUND_DOWN,\n"
"///      _MM_ROUND_UP, _MM_ROUND_TOWARD_ZERO. There is a convenience wrapper\n"
"///      _MM_SET_ROUNDING_MODE(x) where x is one of these macros.\n"
"///    </li>\n"
"///    <li>\n"
"///      For setting flush-to-zero mode: _MM_FLUSH_ZERO_ON, _MM_FLUSH_ZERO_OFF.\n"
"///      There is a convenience wrapper _MM_SET_FLUSH_ZERO_MODE(x) where x is\n"
"///      one of these macros.\n"
"///    </li>\n"
"///    <li>\n"
"///      For setting denormals-are-zero mode: _MM_DENORMALS_ZERO_ON,\n"
"///      _MM_DENORMALS_ZERO_OFF. There is a convenience wrapper\n"
"///      _MM_SET_DENORMALS_ZERO_MODE(x) where x is one of these macros.\n"
"///    </li>\n"
"///    </ul>\n"
"///\n"
"///    For example, the following expression causes subsequent floating-point\n"
"///    operations to round up:\n"
"///      _mm_setcsr(_mm_getcsr() | _MM_ROUND_UP)\n"
"///\n"
"///    The following example sets the DAZ and FTZ flags:\n"
"///    \\code\n"
"///    void setFlags() {\n"
"///      _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);\n"
"///      _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON);\n"
"///    }\n"
"///    \\endcode\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VLDMXCSR / LDMXCSR </c> instruction.\n"
"///\n"
"/// \\param __i\n"
"///    A 32-bit unsigned integer value to be written to the MXCSR register.\n"
"void _mm_setcsr(unsigned int __i);\n"
"\n"
"#if defined(__cplusplus)\n"
"} // extern \"C\"\n"
"#endif\n"
"\n"
"/// Selects 4 float values from the 128-bit operands of [4 x float], as\n"
"///    specified by the immediate value operand.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// \\code\n"
"/// __m128 _mm_shuffle_ps(__m128 a, __m128 b, const int mask);\n"
"/// \\endcode\n"
"///\n"
"/// This intrinsic corresponds to the <c> VSHUFPS / SHUFPS </c> instruction.\n"
"///\n"
"/// \\param a\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param b\n"
"///    A 128-bit vector of [4 x float].\n"
"/// \\param mask\n"
"///    An immediate value containing an 8-bit value specifying which elements to\n"
"///    copy from \\a a and \\a b. \\n\n"
"///    Bits [3:0] specify the values copied from operand \\a a. \\n\n"
"///    Bits [7:4] specify the values copied from operand \\a b. \\n\n"
"///    The destinations within the 128-bit destination are assigned values as\n"
"///    follows: \\n\n"
"///    Bits [1:0] are used to assign values to bits [31:0] in the\n"
"///    destination. \\n\n"
"///    Bits [3:2] are used to assign values to bits [63:32] in the\n"
"///    destination. \\n\n"
"///    Bits [5:4] are used to assign values to bits [95:64] in the\n"
"///    destination. \\n\n"
"///    Bits [7:6] are used to assign values to bits [127:96] in the\n"
"///    destination. \\n\n"
"///    Bit value assignments: \\n\n"
"///    00: Bits [31:0] copied from the specified operand. \\n\n"
"///    01: Bits [63:32] copied from the specified operand. \\n\n"
"///    10: Bits [95:64] copied from the specified operand. \\n\n"
"///    11: Bits [127:96] copied from the specified operand.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the shuffled values.\n"
"#define _mm_shuffle_ps(a, b, mask) \\\n"
"  (__m128)__builtin_ia32_shufps((__v4sf)(__m128)(a), (__v4sf)(__m128)(b), \\\n"
"                                (int)(mask))\n"
"\n"
"/// Unpacks the high-order (index 2,3) values from two 128-bit vectors of\n"
"///    [4 x float] and interleaves them into a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKHPS / UNPCKHPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. \\n\n"
"///    Bits [95:64] are written to bits [31:0] of the destination. \\n\n"
"///    Bits [127:96] are written to bits [95:64] of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float].\n"
"///    Bits [95:64] are written to bits [63:32] of the destination. \\n\n"
"///    Bits [127:96] are written to bits [127:96] of the destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the interleaved values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_unpackhi_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 2, 6, 3, 7);\n"
"}\n"
"\n"
"/// Unpacks the low-order (index 0,1) values from two 128-bit vectors of\n"
"///    [4 x float] and interleaves them into a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPS / UNPCKLPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit vector of [4 x float]. \\n\n"
"///    Bits [31:0] are written to bits [31:0] of the destination.  \\n\n"
"///    Bits [63:32] are written to bits [95:64] of the destination.\n"
"/// \\param __b\n"
"///    A 128-bit vector of [4 x float]. \\n\n"
"///    Bits [31:0] are written to bits [63:32] of the destination. \\n\n"
"///    Bits [63:32] are written to bits [127:96] of the destination.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the interleaved values.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_unpacklo_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 4, 1, 5);\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float]. The lower\n"
"///    32 bits are set to the lower 32 bits of the second parameter. The upper\n"
"///    96 bits are set to the upper 96 bits of the first parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VBLENDPS / BLENDPS / MOVSS </c>\n"
"///    instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [4 x float]. The upper 96 bits are\n"
"///    written to the upper 96 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit floating-point vector of [4 x float]. The lower 32 bits are\n"
"///    written to the lower 32 bits of the result.\n"
"/// \\returns A 128-bit floating-point vector of [4 x float].\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_move_ss(__m128 __a, __m128 __b)\n"
"{\n"
"  __a[0] = __b[0];\n"
"  return __a;\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float]. The lower\n"
"///    64 bits are set to the upper 64 bits of the second parameter. The upper\n"
"///    64 bits are set to the upper 64 bits of the first parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKHPD / UNPCKHPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [4 x float]. The upper 64 bits are\n"
"///    written to the upper 64 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit floating-point vector of [4 x float]. The upper 64 bits are\n"
"///    written to the lower 64 bits of the result.\n"
"/// \\returns A 128-bit floating-point vector of [4 x float].\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_movehl_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 6, 7, 2, 3);\n"
"}\n"
"\n"
"/// Constructs a 128-bit floating-point vector of [4 x float]. The lower\n"
"///    64 bits are set to the lower 64 bits of the first parameter. The upper\n"
"///    64 bits are set to the lower 64 bits of the second parameter.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [4 x float]. The lower 64 bits are\n"
"///    written to the lower 64 bits of the result.\n"
"/// \\param __b\n"
"///    A 128-bit floating-point vector of [4 x float]. The lower 64 bits are\n"
"///    written to the upper 64 bits of the result.\n"
"/// \\returns A 128-bit floating-point vector of [4 x float].\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_movelh_ps(__m128 __a, __m128 __b)\n"
"{\n"
"  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 1, 4, 5);\n"
"}\n"
"\n"
"/// Converts a 64-bit vector of [4 x i16] into a 128-bit vector of [4 x\n"
"///    float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [4 x i16]. The elements of the destination are copied\n"
"///    from the corresponding elements in this operand.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied and converted\n"
"///    values from the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpi16_ps(__m64 __a)\n"
"{\n"
"  __m64 __b, __c;\n"
"  __m128 __r;\n"
"\n"
"  __b = _mm_setzero_si64();\n"
"  __b = _mm_cmpgt_pi16(__b, __a);\n"
"  __c = _mm_unpackhi_pi16(__a, __b);\n"
"  __r = _mm_setzero_ps();\n"
"  __r = _mm_cvtpi32_ps(__r, __c);\n"
"  __r = _mm_movelh_ps(__r, __r);\n"
"  __c = _mm_unpacklo_pi16(__a, __b);\n"
"  __r = _mm_cvtpi32_ps(__r, __c);\n"
"\n"
"  return __r;\n"
"}\n"
"\n"
"/// Converts a 64-bit vector of 16-bit unsigned integer values into a\n"
"///    128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of 16-bit unsigned integer values. The elements of the\n"
"///    destination are copied from the corresponding elements in this operand.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied and converted\n"
"///    values from the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpu16_ps(__m64 __a)\n"
"{\n"
"  __m64 __b, __c;\n"
"  __m128 __r;\n"
"\n"
"  __b = _mm_setzero_si64();\n"
"  __c = _mm_unpackhi_pi16(__a, __b);\n"
"  __r = _mm_setzero_ps();\n"
"  __r = _mm_cvtpi32_ps(__r, __c);\n"
"  __r = _mm_movelh_ps(__r, __r);\n"
"  __c = _mm_unpacklo_pi16(__a, __b);\n"
"  __r = _mm_cvtpi32_ps(__r, __c);\n"
"\n"
"  return __r;\n"
"}\n"
"\n"
"/// Converts the lower four 8-bit values from a 64-bit vector of [8 x i8]\n"
"///    into a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [8 x i8]. The elements of the destination are copied\n"
"///    from the corresponding lower 4 elements in this operand.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied and converted\n"
"///    values from the operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpi8_ps(__m64 __a)\n"
"{\n"
"  __m64 __b;\n"
"\n"
"  __b = _mm_setzero_si64();\n"
"  __b = _mm_cmpgt_pi8(__b, __a);\n"
"  __b = _mm_unpacklo_pi8(__a, __b);\n"
"\n"
"  return _mm_cvtpi16_ps(__b);\n"
"}\n"
"\n"
"/// Converts the lower four unsigned 8-bit integer values from a 64-bit\n"
"///    vector of [8 x u8] into a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of unsigned 8-bit integer values. The elements of the\n"
"///    destination are copied from the corresponding lower 4 elements in this\n"
"///    operand.\n"
"/// \\returns A 128-bit vector of [4 x float] containing the copied and converted\n"
"///    values from the source operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpu8_ps(__m64 __a)\n"
"{\n"
"  __m64 __b;\n"
"\n"
"  __b = _mm_setzero_si64();\n"
"  __b = _mm_unpacklo_pi8(__a, __b);\n"
"\n"
"  return _mm_cvtpi16_ps(__b);\n"
"}\n"
"\n"
"/// Converts the two 32-bit signed integer values from each 64-bit vector\n"
"///    operand of [2 x i32] into a 128-bit vector of [4 x float].\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 64-bit vector of [2 x i32]. The lower elements of the destination are\n"
"///    copied from the elements in this operand.\n"
"/// \\param __b\n"
"///    A 64-bit vector of [2 x i32]. The upper elements of the destination are\n"
"///    copied from the elements in this operand.\n"
"/// \\returns A 128-bit vector of [4 x float] whose lower 64 bits contain the\n"
"///    copied and converted values from the first operand. The upper 64 bits\n"
"///    contain the copied and converted values from the second operand.\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtpi32x2_ps(__m64 __a, __m64 __b)\n"
"{\n"
"  __m128 __c;\n"
"\n"
"  __c = _mm_setzero_ps();\n"
"  __c = _mm_cvtpi32_ps(__c, __b);\n"
"  __c = _mm_movelh_ps(__c, __c);\n"
"\n"
"  return _mm_cvtpi32_ps(__c, __a);\n"
"}\n"
"\n"
"/// Converts each single-precision floating-point element of a 128-bit\n"
"///    floating-point vector of [4 x float] into a 16-bit signed integer, and\n"
"///    packs the results into a 64-bit integer vector of [4 x i16].\n"
"///\n"
"///    If the floating-point element is NaN or infinity, or if the\n"
"///    floating-point element is greater than 0x7FFFFFFF or less than -0x8000,\n"
"///    it is converted to 0x8000. Otherwise if the floating-point element is\n"
"///    greater than 0x7FFF, it is converted to 0x7FFF.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPS2PI + COMPOSITE </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [4 x float].\n"
"/// \\returns A 64-bit integer vector of [4 x i16] containing the converted\n"
"///    values.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtps_pi16(__m128 __a)\n"
"{\n"
"  __m64 __b, __c;\n"
"\n"
"  __b = _mm_cvtps_pi32(__a);\n"
"  __a = _mm_movehl_ps(__a, __a);\n"
"  __c = _mm_cvtps_pi32(__a);\n"
"\n"
"  return _mm_packs_pi32(__b, __c);\n"
"}\n"
"\n"
"/// Converts each single-precision floating-point element of a 128-bit\n"
"///    floating-point vector of [4 x float] into an 8-bit signed integer, and\n"
"///    packs the results into the lower 32 bits of a 64-bit integer vector of\n"
"///    [8 x i8]. The upper 32 bits of the vector are set to 0.\n"
"///\n"
"///    If the floating-point element is NaN or infinity, or if the\n"
"///    floating-point element is greater than 0x7FFFFFFF or less than -0x80, it\n"
"///    is converted to 0x80. Otherwise if the floating-point element is greater\n"
"///    than 0x7F, it is converted to 0x7F.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> CVTPS2PI + COMPOSITE </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    128-bit floating-point vector of [4 x float].\n"
"/// \\returns A 64-bit integer vector of [8 x i8]. The lower 32 bits contain the\n"
"///    converted values and the uppper 32 bits are set to zero.\n"
"static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX\n"
"_mm_cvtps_pi8(__m128 __a)\n"
"{\n"
"  __m64 __b, __c;\n"
"\n"
"  __b = _mm_cvtps_pi16(__a);\n"
"  __c = _mm_setzero_si64();\n"
"\n"
"  return _mm_packs_pi16(__b, __c);\n"
"}\n"
"\n"
"/// Extracts the sign bits from each single-precision floating-point\n"
"///    element of a 128-bit floating-point vector of [4 x float] and returns the\n"
"///    sign bits in bits [0:3] of the result. Bits [31:4] of the result are set\n"
"///    to zero.\n"
"///\n"
"/// \\headerfile <x86intrin.h>\n"
"///\n"
"/// This intrinsic corresponds to the <c> VMOVMSKPS / MOVMSKPS </c> instruction.\n"
"///\n"
"/// \\param __a\n"
"///    A 128-bit floating-point vector of [4 x float].\n"
"/// \\returns A 32-bit integer value. Bits [3:0] contain the sign bits from each\n"
"///    single-precision floating-point element of the parameter. Bits [31:4] are\n"
"///    set to zero.\n"
"static __inline__ int __DEFAULT_FN_ATTRS\n"
"_mm_movemask_ps(__m128 __a)\n"
"{\n"
"  return __builtin_ia32_movmskps((__v4sf)__a);\n"
"}\n"
"\n"
"\n"
"#define _MM_ALIGN16 __attribute__((aligned(16)))\n"
"\n"
"#define _MM_SHUFFLE(z, y, x, w) (((z) << 6) | ((y) << 4) | ((x) << 2) | (w))\n"
"\n"
"#define _MM_EXCEPT_INVALID    (0x0001)\n"
"#define _MM_EXCEPT_DENORM     (0x0002)\n"
"#define _MM_EXCEPT_DIV_ZERO   (0x0004)\n"
"#define _MM_EXCEPT_OVERFLOW   (0x0008)\n"
"#define _MM_EXCEPT_UNDERFLOW  (0x0010)\n"
"#define _MM_EXCEPT_INEXACT    (0x0020)\n"
"#define _MM_EXCEPT_MASK       (0x003f)\n"
"\n"
"#define _MM_MASK_INVALID      (0x0080)\n"
"#define _MM_MASK_DENORM       (0x0100)\n"
"#define _MM_MASK_DIV_ZERO     (0x0200)\n"
"#define _MM_MASK_OVERFLOW     (0x0400)\n"
"#define _MM_MASK_UNDERFLOW    (0x0800)\n"
"#define _MM_MASK_INEXACT      (0x1000)\n"
"#define _MM_MASK_MASK         (0x1f80)\n"
"\n"
"#define _MM_ROUND_NEAREST     (0x0000)\n"
"#define _MM_ROUND_DOWN        (0x2000)\n"
"#define _MM_ROUND_UP          (0x4000)\n"
"#define _MM_ROUND_TOWARD_ZERO (0x6000)\n"
"#define _MM_ROUND_MASK        (0x6000)\n"
"\n"
"#define _MM_FLUSH_ZERO_MASK   (0x8000)\n"
"#define _MM_FLUSH_ZERO_ON     (0x8000)\n"
"#define _MM_FLUSH_ZERO_OFF    (0x0000)\n"
"\n"
"#define _MM_GET_EXCEPTION_MASK() (_mm_getcsr() & _MM_MASK_MASK)\n"
"#define _MM_GET_EXCEPTION_STATE() (_mm_getcsr() & _MM_EXCEPT_MASK)\n"
"#define _MM_GET_FLUSH_ZERO_MODE() (_mm_getcsr() & _MM_FLUSH_ZERO_MASK)\n"
"#define _MM_GET_ROUNDING_MODE() (_mm_getcsr() & _MM_ROUND_MASK)\n"
"\n"
"#define _MM_SET_EXCEPTION_MASK(x) (_mm_setcsr((_mm_getcsr() & ~_MM_MASK_MASK) | (x)))\n"
"#define _MM_SET_EXCEPTION_STATE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_EXCEPT_MASK) | (x)))\n"
"#define _MM_SET_FLUSH_ZERO_MODE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_FLUSH_ZERO_MASK) | (x)))\n"
"#define _MM_SET_ROUNDING_MODE(x) (_mm_setcsr((_mm_getcsr() & ~_MM_ROUND_MASK) | (x)))\n"
"\n"
"#define _MM_TRANSPOSE4_PS(row0, row1, row2, row3) \\\n"
"do { \\\n"
"  __m128 tmp3, tmp2, tmp1, tmp0; \\\n"
"  tmp0 = _mm_unpacklo_ps((row0), (row1)); \\\n"
"  tmp2 = _mm_unpacklo_ps((row2), (row3)); \\\n"
"  tmp1 = _mm_unpackhi_ps((row0), (row1)); \\\n"
"  tmp3 = _mm_unpackhi_ps((row2), (row3)); \\\n"
"  (row0) = _mm_movelh_ps(tmp0, tmp2); \\\n"
"  (row1) = _mm_movehl_ps(tmp2, tmp0); \\\n"
"  (row2) = _mm_movelh_ps(tmp1, tmp3); \\\n"
"  (row3) = _mm_movehl_ps(tmp3, tmp1); \\\n"
"} while (0)\n"
"\n"
"/* Aliases for compatibility. */\n"
"#define _m_pextrw _mm_extract_pi16\n"
"#define _m_pinsrw _mm_insert_pi16\n"
"#define _m_pmaxsw _mm_max_pi16\n"
"#define _m_pmaxub _mm_max_pu8\n"
"#define _m_pminsw _mm_min_pi16\n"
"#define _m_pminub _mm_min_pu8\n"
"#define _m_pmovmskb _mm_movemask_pi8\n"
"#define _m_pmulhuw _mm_mulhi_pu16\n"
"#define _m_pshufw _mm_shuffle_pi16\n"
"#define _m_maskmovq _mm_maskmove_si64\n"
"#define _m_pavgb _mm_avg_pu8\n"
"#define _m_pavgw _mm_avg_pu16\n"
"#define _m_psadbw _mm_sad_pu8\n"
"#define _m_ _mm_\n"
"#define _m_ _mm_\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __DEFAULT_FN_ATTRS_MMX\n"
"\n"
"/* Ugly hack for backwards-compatibility (compatible with gcc) */\n"
"#if defined(__SSE2__) && !__building_module(_Builtin_intrinsics)\n"
"#include <emmintrin.h>\n"
"#endif\n"
"\n"
"#endif /* __XMMINTRIN_H */\n"
"" } , 
 { "/builtins/xopintrin.h" , "/*===---- xopintrin.h - XOP intrinsics -------------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __X86INTRIN_H\n"
"#error \"Never use <xopintrin.h> directly; include <x86intrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __XOPINTRIN_H\n"
"#define __XOPINTRIN_H\n"
"\n"
"#include <fma4intrin.h>\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__, __target__(\"xop\"), __min_vector_width__(128)))\n"
"#define __DEFAULT_FN_ATTRS256 __attribute__((__always_inline__, __nodebug__, __target__(\"xop\"), __min_vector_width__(256)))\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maccs_epi16(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacssww((__v8hi)__A, (__v8hi)__B, (__v8hi)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_macc_epi16(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacsww((__v8hi)__A, (__v8hi)__B, (__v8hi)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maccsd_epi16(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacsswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maccd_epi16(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maccs_epi32(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacssdd((__v4si)__A, (__v4si)__B, (__v4si)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_macc_epi32(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacsdd((__v4si)__A, (__v4si)__B, (__v4si)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maccslo_epi32(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacssdql((__v4si)__A, (__v4si)__B, (__v2di)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_macclo_epi32(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacsdql((__v4si)__A, (__v4si)__B, (__v2di)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maccshi_epi32(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacssdqh((__v4si)__A, (__v4si)__B, (__v2di)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_macchi_epi32(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmacsdqh((__v4si)__A, (__v4si)__B, (__v2di)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maddsd_epi16(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmadcsswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_maddd_epi16(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpmadcswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddw_epi8(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddbw((__v16qi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddd_epi8(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddbd((__v16qi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddq_epi8(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddbq((__v16qi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddd_epi16(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddwd((__v8hi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddq_epi16(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddwq((__v8hi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddq_epi32(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphadddq((__v4si)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddw_epu8(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddubw((__v16qi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddd_epu8(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddubd((__v16qi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddq_epu8(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddubq((__v16qi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddd_epu16(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphadduwd((__v8hi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddq_epu16(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphadduwq((__v8hi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_haddq_epu32(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphaddudq((__v4si)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hsubw_epi8(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphsubbw((__v16qi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hsubd_epi16(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphsubwd((__v8hi)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_hsubq_epi32(__m128i __A)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vphsubdq((__v4si)__A);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_cmov_si128(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)(((__v2du)__A & (__v2du)__C) | ((__v2du)__B & ~(__v2du)__C));\n"
"}\n"
"\n"
"static __inline__ __m256i __DEFAULT_FN_ATTRS256\n"
"_mm256_cmov_si256(__m256i __A, __m256i __B, __m256i __C)\n"
"{\n"
"  return (__m256i)(((__v4du)__A & (__v4du)__C) | ((__v4du)__B & ~(__v4du)__C));\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_perm_epi8(__m128i __A, __m128i __B, __m128i __C)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpperm((__v16qi)__A, (__v16qi)__B, (__v16qi)__C);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_rot_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vprotb((__v16qi)__A, (__v16qi)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_rot_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vprotw((__v8hi)__A, (__v8hi)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_rot_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vprotd((__v4si)__A, (__v4si)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_rot_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vprotq((__v2di)__A, (__v2di)__B);\n"
"}\n"
"\n"
"#define _mm_roti_epi8(A, N) \\\n"
"  (__m128i)__builtin_ia32_vprotbi((__v16qi)(__m128i)(A), (N))\n"
"\n"
"#define _mm_roti_epi16(A, N) \\\n"
"  (__m128i)__builtin_ia32_vprotwi((__v8hi)(__m128i)(A), (N))\n"
"\n"
"#define _mm_roti_epi32(A, N) \\\n"
"  (__m128i)__builtin_ia32_vprotdi((__v4si)(__m128i)(A), (N))\n"
"\n"
"#define _mm_roti_epi64(A, N) \\\n"
"  (__m128i)__builtin_ia32_vprotqi((__v2di)(__m128i)(A), (N))\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_shl_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshlb((__v16qi)__A, (__v16qi)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_shl_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshlw((__v8hi)__A, (__v8hi)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_shl_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshld((__v4si)__A, (__v4si)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_shl_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshlq((__v2di)__A, (__v2di)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshab((__v16qi)__A, (__v16qi)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshaw((__v8hi)__A, (__v8hi)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshad((__v4si)__A, (__v4si)__B);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_sha_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return (__m128i)__builtin_ia32_vpshaq((__v2di)__A, (__v2di)__B);\n"
"}\n"
"\n"
"#define _mm_com_epu8(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(A), \\\n"
"                                  (__v16qi)(__m128i)(B), (N))\n"
"\n"
"#define _mm_com_epu16(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(A), \\\n"
"                                  (__v8hi)(__m128i)(B), (N))\n"
"\n"
"#define _mm_com_epu32(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(A), \\\n"
"                                  (__v4si)(__m128i)(B), (N))\n"
"\n"
"#define _mm_com_epu64(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(A), \\\n"
"                                  (__v2di)(__m128i)(B), (N))\n"
"\n"
"#define _mm_com_epi8(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(A), \\\n"
"                                 (__v16qi)(__m128i)(B), (N))\n"
"\n"
"#define _mm_com_epi16(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(A), \\\n"
"                                 (__v8hi)(__m128i)(B), (N))\n"
"\n"
"#define _mm_com_epi32(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(A), \\\n"
"                                 (__v4si)(__m128i)(B), (N))\n"
"\n"
"#define _mm_com_epi64(A, B, N) \\\n"
"  (__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(A), \\\n"
"                                 (__v2di)(__m128i)(B), (N))\n"
"\n"
"#define _MM_PCOMCTRL_LT    0\n"
"#define _MM_PCOMCTRL_LE    1\n"
"#define _MM_PCOMCTRL_GT    2\n"
"#define _MM_PCOMCTRL_GE    3\n"
"#define _MM_PCOMCTRL_EQ    4\n"
"#define _MM_PCOMCTRL_NEQ   5\n"
"#define _MM_PCOMCTRL_FALSE 6\n"
"#define _MM_PCOMCTRL_TRUE  7\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epu8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu8(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epu16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu16(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epu32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu32(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epu64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epu64(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epi8(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi8(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epi16(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi16(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epi32(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi32(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comlt_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_LT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comle_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_LE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comgt_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_GT);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comge_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_GE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comeq_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_EQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comneq_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_NEQ);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comfalse_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_FALSE);\n"
"}\n"
"\n"
"static __inline__ __m128i __DEFAULT_FN_ATTRS\n"
"_mm_comtrue_epi64(__m128i __A, __m128i __B)\n"
"{\n"
"  return _mm_com_epi64(__A, __B, _MM_PCOMCTRL_TRUE);\n"
"}\n"
"\n"
"#define _mm_permute2_pd(X, Y, C, I) \\\n"
"  (__m128d)__builtin_ia32_vpermil2pd((__v2df)(__m128d)(X), \\\n"
"                                     (__v2df)(__m128d)(Y), \\\n"
"                                     (__v2di)(__m128i)(C), (I))\n"
"\n"
"#define _mm256_permute2_pd(X, Y, C, I) \\\n"
"  (__m256d)__builtin_ia32_vpermil2pd256((__v4df)(__m256d)(X), \\\n"
"                                        (__v4df)(__m256d)(Y), \\\n"
"                                        (__v4di)(__m256i)(C), (I))\n"
"\n"
"#define _mm_permute2_ps(X, Y, C, I) \\\n"
"  (__m128)__builtin_ia32_vpermil2ps((__v4sf)(__m128)(X), (__v4sf)(__m128)(Y), \\\n"
"                                    (__v4si)(__m128i)(C), (I))\n"
"\n"
"#define _mm256_permute2_ps(X, Y, C, I) \\\n"
"  (__m256)__builtin_ia32_vpermil2ps256((__v8sf)(__m256)(X), \\\n"
"                                       (__v8sf)(__m256)(Y), \\\n"
"                                       (__v8si)(__m256i)(C), (I))\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_frcz_ss(__m128 __A)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfrczss((__v4sf)__A);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_frcz_sd(__m128d __A)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfrczsd((__v2df)__A);\n"
"}\n"
"\n"
"static __inline__ __m128 __DEFAULT_FN_ATTRS\n"
"_mm_frcz_ps(__m128 __A)\n"
"{\n"
"  return (__m128)__builtin_ia32_vfrczps((__v4sf)__A);\n"
"}\n"
"\n"
"static __inline__ __m128d __DEFAULT_FN_ATTRS\n"
"_mm_frcz_pd(__m128d __A)\n"
"{\n"
"  return (__m128d)__builtin_ia32_vfrczpd((__v2df)__A);\n"
"}\n"
"\n"
"static __inline__ __m256 __DEFAULT_FN_ATTRS256\n"
"_mm256_frcz_ps(__m256 __A)\n"
"{\n"
"  return (__m256)__builtin_ia32_vfrczps256((__v8sf)__A);\n"
"}\n"
"\n"
"static __inline__ __m256d __DEFAULT_FN_ATTRS256\n"
"_mm256_frcz_pd(__m256d __A)\n"
"{\n"
"  return (__m256d)__builtin_ia32_vfrczpd256((__v4df)__A);\n"
"}\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"#undef __DEFAULT_FN_ATTRS256\n"
"\n"
"#endif /* __XOPINTRIN_H */\n"
"" } , 
 { "/builtins/xsavecintrin.h" , "/*===---- xsavecintrin.h - XSAVEC intrinsic --------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <xsavecintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __XSAVECINTRIN_H\n"
"#define __XSAVECINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"xsavec\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsavec(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsavec(__p, __m);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsavec64(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsavec64(__p, __m);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/xsaveintrin.h" , "/*===---- xsaveintrin.h - XSAVE intrinsic ----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <xsaveintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __XSAVEINTRIN_H\n"
"#define __XSAVEINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"xsave\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsave(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsave(__p, __m);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xrstor(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xrstor(__p, __m);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsave64(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsave64(__p, __m);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xrstor64(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xrstor64(__p, __m);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/xsaveoptintrin.h" , "/*===---- xsaveoptintrin.h - XSAVEOPT intrinsic ----------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <xsaveoptintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __XSAVEOPTINTRIN_H\n"
"#define __XSAVEOPTINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"xsaveopt\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsaveopt(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsaveopt(__p, __m);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsaveopt64(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsaveopt64(__p, __m);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/xsavesintrin.h" , "/*===---- xsavesintrin.h - XSAVES intrinsic --------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <xsavesintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __XSAVESINTRIN_H\n"
"#define __XSAVESINTRIN_H\n"
"\n"
"/* Define the default attributes for the functions in this file. */\n"
"#define __DEFAULT_FN_ATTRS __attribute__((__always_inline__, __nodebug__,  __target__(\"xsaves\")))\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsaves(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsaves(__p, __m);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xrstors(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xrstors(__p, __m);\n"
"}\n"
"\n"
"#ifdef __x86_64__\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xrstors64(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xrstors64(__p, __m);\n"
"}\n"
"\n"
"static __inline__ void __DEFAULT_FN_ATTRS\n"
"_xsaves64(void *__p, unsigned long long __m) {\n"
"  __builtin_ia32_xsaves64(__p, __m);\n"
"}\n"
"#endif\n"
"\n"
"#undef __DEFAULT_FN_ATTRS\n"
"\n"
"#endif\n"
"" } , 
 { "/builtins/xtestintrin.h" , "/*===---- xtestintrin.h - XTEST intrinsic ----------------------------------===\n"
" *\n"
" * Permission is hereby granted, free of charge, to any person obtaining a copy\n"
" * of this software and associated documentation files (the \"Software\"), to deal\n"
" * in the Software without restriction, including without limitation the rights\n"
" * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n"
" * copies of the Software, and to permit persons to whom the Software is\n"
" * furnished to do so, subject to the following conditions:\n"
" *\n"
" * The above copyright notice and this permission notice shall be included in\n"
" * all copies or substantial portions of the Software.\n"
" *\n"
" * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n"
" * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n"
" * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n"
" * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n"
" * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n"
" * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n"
" * THE SOFTWARE.\n"
" *\n"
" *===-----------------------------------------------------------------------===\n"
" */\n"
"\n"
"#ifndef __IMMINTRIN_H\n"
"#error \"Never use <xtestintrin.h> directly; include <immintrin.h> instead.\"\n"
"#endif\n"
"\n"
"#ifndef __XTESTINTRIN_H\n"
"#define __XTESTINTRIN_H\n"
"\n"
"/* xtest returns non-zero if the instruction is executed within an RTM or active\n"
" * HLE region. */\n"
"/* FIXME: This can be an either or for RTM/HLE. Deal with this when HLE is\n"
" * supported. */\n"
"static __inline__ int\n"
"    __attribute__((__always_inline__, __nodebug__, __target__(\"rtm\")))\n"
"    _xtest(void) {\n"
"  return __builtin_ia32_xtest();\n"
"}\n"
"\n"
"#endif\n"
"" } , 

    {}
};


